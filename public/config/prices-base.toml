# Generated by scripts/convert-litellm-to-toml.ts

[metadata]
version = "2025.12.25"
checksum = "3f82ad0600074c8830b810a1156a8a42d7f5096e206fba783cb7fba5909b064a"

[models."claude-3-5-haiku-20241022"]
cache_creation_input_token_cost = 0.000001
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 8e-8
deprecation_date = "2025-10-01"
input_cost_per_token = 8e-7
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000004
supports_assistant_prefill = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tool_use_system_prompt_tokens = 264

[models."claude-3-5-haiku-20241022".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-5-haiku-latest"]
cache_creation_input_token_cost = 0.00000125
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 1e-7
deprecation_date = "2025-10-01"
input_cost_per_token = 0.000001
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000005
supports_assistant_prefill = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tool_use_system_prompt_tokens = 264

[models."claude-3-5-haiku-latest".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-5-sonnet-20240620"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
deprecation_date = "2025-06-01"
input_cost_per_token = 0.000003
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-3-5-sonnet-20241022"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
deprecation_date = "2025-10-01"
input_cost_per_token = 0.000003
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tool_use_system_prompt_tokens = 159

[models."claude-3-5-sonnet-20241022".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-5-sonnet-latest"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
deprecation_date = "2025-06-01"
input_cost_per_token = 0.000003
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tool_use_system_prompt_tokens = 159

[models."claude-3-5-sonnet-latest".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-7-sonnet-20250219"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
deprecation_date = "2026-02-19"
input_cost_per_token = 0.000003
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tool_use_system_prompt_tokens = 159

[models."claude-3-7-sonnet-20250219".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-7-sonnet-latest"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
deprecation_date = "2025-06-01"
input_cost_per_token = 0.000003
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-3-7-sonnet-latest".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-haiku-20240307"]
cache_creation_input_token_cost = 3e-7
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00000125
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 264

[models."claude-3-opus-20240229"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 0.0000015
deprecation_date = "2026-05-01"
input_cost_per_token = 0.000015
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 395

[models."claude-3-opus-latest"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 0.0000015
deprecation_date = "2025-03-01"
input_cost_per_token = 0.000015
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 395

[models."claude-4-opus-20250514"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-4-opus-20250514".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-4-sonnet-20250514"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "anthropic"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 1000000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-4-sonnet-20250514".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-haiku-4-5"]
cache_creation_input_token_cost = 0.00000125
cache_creation_input_token_cost_above_1hr = 0.000002
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000005
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."claude-haiku-4-5-20251001"]
cache_creation_input_token_cost = 0.00000125
cache_creation_input_token_cost_above_1hr = 0.000002
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000005
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."claude-opus-4-1"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.00003
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4-1".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-opus-4-1-20250805"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.00003
cache_read_input_token_cost = 0.0000015
deprecation_date = "2026-08-05"
input_cost_per_token = 0.000015
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4-1-20250805".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-opus-4-20250514"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.00003
cache_read_input_token_cost = 0.0000015
deprecation_date = "2026-05-14"
input_cost_per_token = 0.000015
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4-20250514".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-opus-4-5"]
cache_creation_input_token_cost = 0.00000625
cache_creation_input_token_cost_above_1hr = 0.00001
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000025
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4-5".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-opus-4-5-20251101"]
cache_creation_input_token_cost = 0.00000625
cache_creation_input_token_cost_above_1hr = 0.00001
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000025
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4-5-20251101".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-sonnet-4-20250514"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
deprecation_date = "2026-05-14"
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "anthropic"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-sonnet-4-20250514".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-sonnet-4-5"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 346

[models."claude-sonnet-4-5".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-sonnet-4-5-20250929"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tool_use_system_prompt_tokens = 346

[models."claude-sonnet-4-5-20250929".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-sonnet-4-5-20250929-v1:0"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."gemini-1.0-pro"]
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 32760
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#google_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.0-pro-001"]
deprecation_date = "2025-04-09"
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 32760
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.0-pro-002"]
deprecation_date = "2025-04-09"
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 32760
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.0-pro-vision"]
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
litellm_provider = "vertex_ai-vision-models"
max_images_per_prompt = 16
max_input_tokens = 16384
max_output_tokens = 2048
max_tokens = 2048
max_video_length = 2
max_videos_per_prompt = 1
mode = "chat"
output_cost_per_token = 0.0000015
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.0-pro-vision-001"]
deprecation_date = "2025-04-09"
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
litellm_provider = "vertex_ai-vision-models"
max_images_per_prompt = 16
max_input_tokens = 16384
max_output_tokens = 2048
max_tokens = 2048
max_video_length = 2
max_videos_per_prompt = 1
mode = "chat"
output_cost_per_token = 0.0000015
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.0-ultra"]
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 8192
max_output_tokens = 2048
max_tokens = 8192
mode = "chat"
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015
source = "As of Jun, 2024. There is no available doc on vertex ai pricing gemini-1.0-ultra-001. Using gemini-1.0-pro pricing. Got max_tokens info here: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.0-ultra-001"]
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 8192
max_output_tokens = 2048
max_tokens = 8192
mode = "chat"
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015
source = "As of Jun, 2024. There is no available doc on vertex ai pricing gemini-1.0-ultra-001. Using gemini-1.0-pro pricing. Got max_tokens info here: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.5-flash"]
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1000000
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 7.5e-8
output_cost_per_character_above_128k_tokens = 1.5e-7
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.5-flash-001"]
deprecation_date = "2025-05-24"
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1000000
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 7.5e-8
output_cost_per_character_above_128k_tokens = 1.5e-7
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.5-flash-002"]
deprecation_date = "2025-09-24"
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 7.5e-8
output_cost_per_character_above_128k_tokens = 1.5e-7
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-1.5-flash"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.5-flash-exp-0827"]
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token = 4.688e-9
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1000000
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 1.875e-8
output_cost_per_character_above_128k_tokens = 3.75e-8
output_cost_per_token = 4.6875e-9
output_cost_per_token_above_128k_tokens = 9.375e-9
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.5-flash-preview-0514"]
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1000000
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 1.875e-8
output_cost_per_character_above_128k_tokens = 3.75e-8
output_cost_per_token = 4.6875e-9
output_cost_per_token_above_128k_tokens = 9.375e-9
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.5-pro"]
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 0.00000125
input_cost_per_token_above_128k_tokens = 0.0000025
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 0.000005
output_cost_per_token_above_128k_tokens = 0.00001
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.5-pro-001"]
deprecation_date = "2025-05-24"
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 0.00000125
input_cost_per_token_above_128k_tokens = 0.0000025
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 0.000005
output_cost_per_token_above_128k_tokens = 0.00001
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.5-pro-002"]
deprecation_date = "2025-09-24"
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 0.00000125
input_cost_per_token_above_128k_tokens = 0.0000025
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 0.000005
output_cost_per_token_above_128k_tokens = 0.00001
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-1.5-pro"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.5-pro-preview-0215"]
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 7.8125e-8
input_cost_per_token_above_128k_tokens = 1.5625e-7
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 3.125e-7
output_cost_per_token_above_128k_tokens = 6.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gemini-1.5-pro-preview-0409"]
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 7.8125e-8
input_cost_per_token_above_128k_tokens = 1.5625e-7
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 3.125e-7
output_cost_per_token_above_128k_tokens = 6.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."gemini-1.5-pro-preview-0514"]
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 7.8125e-8
input_cost_per_token_above_128k_tokens = 1.5625e-7
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 3.125e-7
output_cost_per_token_above_128k_tokens = 6.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gemini-2.0-flash"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 4e-7
source = "https://ai.google.dev/pricing#2_0flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-2.0-flash-001"]
cache_read_input_token_cost = 3.75e-8
deprecation_date = "2026-02-05"
input_cost_per_audio_token = 0.000001
input_cost_per_token = 1.5e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 6e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.0-flash-exp"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 1.5e-7
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 6e-7
output_cost_per_token_above_128k_tokens = 0
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.0-flash-lite"]
cache_read_input_token_cost = 1.875e-8
input_cost_per_audio_token = 7.5e-8
input_cost_per_token = 7.5e-8
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 50
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 3e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.0-flash-lite-001"]
cache_read_input_token_cost = 1.875e-8
deprecation_date = "2026-02-25"
input_cost_per_audio_token = 7.5e-8
input_cost_per_token = 7.5e-8
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 50
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 3e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.0-flash-live-preview-04-09"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000003
input_cost_per_image = 0.000003
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.000003
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_audio_token = 0.000012
output_cost_per_token = 0.000002
rpm = 10
source = "https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini#gemini-2-0-flash-live-preview-04-09"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
supports_audio_output = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini-2.0-flash-preview-image-generation"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 4e-7
source = "https://ai.google.dev/pricing#2_0flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.0-flash-thinking-exp"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.0-flash-thinking-exp-01-21"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65536
max_pdf_size_mb = 30
max_tokens = 65536
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = false
supports_function_calling = false
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = false
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.0-pro-exp-02-05"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 2097152
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-flash"]
cache_read_input_token_cost = 3e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-flash-lite"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 5e-7
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-flash-lite-preview-06-17"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 5e-7
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-flash-lite-preview-09-2025"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 3e-7
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
source = "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-flash-preview-04-17"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 1.5e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000035
output_cost_per_token = 6e-7
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-flash-preview-05-20"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-flash-preview-09-2025"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
source = "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-pro"]
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-pro-exp-03-25"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-pro-preview-03-25"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_audio_token = 0.00000125
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-pro-preview-05-06"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_audio_token = 0.00000125
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supported_regions = ["global"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-pro-preview-06-05"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_audio_token = 0.00000125
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-pro-preview-tts"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
source = "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-pro-preview"
supported_modalities = ["text"]
supported_output_modalities = ["audio"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-3-flash-preview"]
cache_read_input_token_cost = 5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 5e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.000003
output_cost_per_token = 0.000003
source = "https://ai.google.dev/pricing/gemini-3"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-3-pro-preview"]
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost = 2e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
input_cost_per_token = 0.000002
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_batches = 0.000001
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.000012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_batches = 0.000006
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true

[models."gemini-flash-experimental"]
input_cost_per_character = 0
input_cost_per_token = 0
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0
output_cost_per_token = 0
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/gemini-experimental"
supports_function_calling = false
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-live-2.5-flash-preview-native-audio-09-2025"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000003
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_audio_token = 0.000012
output_cost_per_token = 0.000002
source = "https://ai.google.dev/gemini-api/docs/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-pro"]
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 32760
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-pro-experimental"]
input_cost_per_character = 0
input_cost_per_token = 0
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0
output_cost_per_token = 0
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/gemini-experimental"
supports_function_calling = false
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-pro-vision"]
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
litellm_provider = "vertex_ai-vision-models"
max_images_per_prompt = 16
max_input_tokens = 16384
max_output_tokens = 2048
max_tokens = 2048
max_video_length = 2
max_videos_per_prompt = 1
mode = "chat"
output_cost_per_token = 0.0000015
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."gemini/gemini-1.5-flash"]
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 1.5e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
rpm = 2000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-flash-001"]
cache_creation_input_token_cost = 0.000001
cache_read_input_token_cost = 1.875e-8
deprecation_date = "2025-05-24"
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 1.5e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
rpm = 2000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-flash-002"]
cache_creation_input_token_cost = 0.000001
cache_read_input_token_cost = 1.875e-8
deprecation_date = "2025-09-24"
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 1.5e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
rpm = 2000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-flash-8b"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 4000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-flash-8b-exp-0827"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1000000
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 4000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-flash-8b-exp-0924"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 4000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-flash-exp-0827"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 2000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-flash-latest"]
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 1.5e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
rpm = 2000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-pro"]
input_cost_per_token = 0.0000035
input_cost_per_token_above_128k_tokens = 0.000007
litellm_provider = "gemini"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000105
output_cost_per_token_above_128k_tokens = 0.000021
rpm = 1000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-pro-001"]
deprecation_date = "2025-05-24"
input_cost_per_token = 0.0000035
input_cost_per_token_above_128k_tokens = 0.000007
litellm_provider = "gemini"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000105
output_cost_per_token_above_128k_tokens = 0.000021
rpm = 1000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-pro-002"]
deprecation_date = "2025-09-24"
input_cost_per_token = 0.0000035
input_cost_per_token_above_128k_tokens = 0.000007
litellm_provider = "gemini"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000105
output_cost_per_token_above_128k_tokens = 0.000021
rpm = 1000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-pro-exp-0801"]
input_cost_per_token = 0.0000035
input_cost_per_token_above_128k_tokens = 0.000007
litellm_provider = "gemini"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000105
output_cost_per_token_above_128k_tokens = 0.000021
rpm = 1000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-pro-exp-0827"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
litellm_provider = "gemini"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 1000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-pro-latest"]
input_cost_per_token = 0.0000035
input_cost_per_token_above_128k_tokens = 0.000007
litellm_provider = "gemini"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000105
output_cost_per_token_above_128k_tokens = 0.000021
rpm = 1000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-2.0-flash"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 4e-7
rpm = 10000
source = "https://ai.google.dev/pricing#2_0flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 10000000

[models."gemini/gemini-2.0-flash-001"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 4e-7
rpm = 10000
source = "https://ai.google.dev/pricing#2_0flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = false
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 10000000

[models."gemini/gemini-2.0-flash-exp"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 10
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = true
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 4000000

[models."gemini/gemini-2.0-flash-lite"]
cache_read_input_token_cost = 1.875e-8
input_cost_per_audio_token = 7.5e-8
input_cost_per_token = 7.5e-8
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 50
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 3e-7
rpm = 4000
source = "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.0-flash-lite"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = true
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 4000000

[models."gemini/gemini-2.0-flash-lite-preview-02-05"]
cache_read_input_token_cost = 1.875e-8
input_cost_per_audio_token = 7.5e-8
input_cost_per_token = 7.5e-8
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 3e-7
rpm = 60000
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash-lite"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 10000000

[models."gemini/gemini-2.0-flash-live-001"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.0000021
input_cost_per_image = 0.0000021
input_cost_per_token = 3.5e-7
input_cost_per_video_per_second = 0.0000021
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_audio_token = 0.0000085
output_cost_per_token = 0.0000015
rpm = 10
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2-0-flash-live-001"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
supports_audio_output = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.0-flash-preview-image-generation"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 4e-7
rpm = 10000
source = "https://ai.google.dev/pricing#2_0flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 10000000

[models."gemini/gemini-2.0-flash-thinking-exp"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65536
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 10
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = true
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 4000000

[models."gemini/gemini-2.0-flash-thinking-exp-01-21"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65536
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 10
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 4000000

[models."gemini/gemini-2.0-pro-exp-02-05"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 2097152
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 2
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supports_audio_input = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true
tpm = 1000000

[models."gemini/gemini-2.5-computer-use-preview-10-2025"]
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "gemini"
max_images_per_prompt = 3000
max_input_tokens = 128000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
rpm = 2000
source = "https://ai.google.dev/gemini-api/docs/computer-use"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_computer_use = true
supports_function_calling = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 800000

[models."gemini/gemini-2.5-flash"]
cache_read_input_token_cost = 3e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
rpm = 100000
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 8000000

[models."gemini/gemini-2.5-flash-lite"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 5e-7
input_cost_per_token = 1e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
rpm = 15
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-lite"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.5-flash-lite-preview-06-17"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 5e-7
input_cost_per_token = 1e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
rpm = 15
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-lite"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.5-flash-lite-preview-09-2025"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 3e-7
input_cost_per_token = 1e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
rpm = 15
source = "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.5-flash-preview-04-17"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 1.5e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000035
output_cost_per_token = 6e-7
rpm = 10
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.5-flash-preview-05-20"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
rpm = 10
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.5-flash-preview-09-2025"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
rpm = 15
source = "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.5-flash-preview-tts"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 1.5e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000035
output_cost_per_token = 6e-7
rpm = 10
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text"]
supported_output_modalities = ["audio"]
supports_audio_output = false
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.5-pro"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
rpm = 2000
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true
tpm = 800000

[models."gemini/gemini-2.5-pro-exp-03-25"]
cache_read_input_token_cost = 0
input_cost_per_token = 0
input_cost_per_token_above_200k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0
output_cost_per_token_above_200k_tokens = 0
rpm = 5
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.5-pro-preview-03-25"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
rpm = 10000
source = "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-pro-preview"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 10000000

[models."gemini/gemini-2.5-pro-preview-05-06"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
rpm = 10000
source = "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-pro-preview"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 10000000

[models."gemini/gemini-2.5-pro-preview-06-05"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
rpm = 10000
source = "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-pro-preview"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 10000000

[models."gemini/gemini-2.5-pro-preview-tts"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
rpm = 10000
source = "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-pro-preview"
supported_modalities = ["text"]
supported_output_modalities = ["audio"]
supports_audio_output = false
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 10000000

[models."gemini/gemini-3-flash-preview"]
cache_read_input_token_cost = 5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 5e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.000003
output_cost_per_token = 0.000003
rpm = 2000
source = "https://ai.google.dev/pricing/gemini-3"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 800000

[models."gemini/gemini-3-pro-preview"]
cache_read_input_token_cost = 2e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
input_cost_per_token = 0.000002
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_batches = 0.000001
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.000012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_batches = 0.000006
rpm = 2000
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true
tpm = 800000

[models."gemini/gemini-exp-1114"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 1000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-exp-1114".metadata]
notes = "Rate limits not documented for gemini-exp-1114. Assuming same as gemini-1.5-pro."
supports_tool_choice = true

[models."gemini/gemini-exp-1206"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 2097152
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 1000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-exp-1206".metadata]
notes = "Rate limits not documented for gemini-exp-1206. Assuming same as gemini-1.5-pro."
supports_tool_choice = true

[models."gemini/gemini-flash-latest"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
rpm = 15
source = "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-flash-lite-latest"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 3e-7
input_cost_per_token = 1e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
rpm = 15
source = "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-gemma-2-27b-it"]
input_cost_per_token = 3.5e-7
litellm_provider = "gemini"
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000105
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."gemini/gemini-gemma-2-9b-it"]
input_cost_per_token = 3.5e-7
litellm_provider = "gemini"
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000105
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."gemini/gemini-live-2.5-flash-preview-native-audio-09-2025"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000003
input_cost_per_token = 3e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_audio_token = 0.000012
output_cost_per_token = 0.000002
rpm = 100000
source = "https://ai.google.dev/gemini-api/docs/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 8000000

[models."gemini/gemini-pro"]
input_cost_per_token = 3.5e-7
input_cost_per_token_above_128k_tokens = 7e-7
litellm_provider = "gemini"
max_input_tokens = 32760
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000105
output_cost_per_token_above_128k_tokens = 0.0000021
rpd = 30000
rpm = 360
source = "https://ai.google.dev/gemini-api/docs/models/gemini"
supports_function_calling = true
supports_tool_choice = true
tpm = 120000

[models."gemini/gemini-pro-vision"]
input_cost_per_token = 3.5e-7
input_cost_per_token_above_128k_tokens = 7e-7
litellm_provider = "gemini"
max_input_tokens = 30720
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 0.00000105
output_cost_per_token_above_128k_tokens = 0.0000021
rpd = 30000
rpm = 360
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true
tpm = 120000

[models."gemini/gemma-3-27b-it"]
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "gemini"
max_input_tokens = 131072
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
source = "https://aistudio.google.com"
supports_audio_output = false
supports_function_calling = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."gemini/learnlm-1.5-pro-experimental"]
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "gemini"
max_input_tokens = 32767
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
source = "https://aistudio.google.com"
supports_audio_output = false
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-3.5-turbo"]
input_cost_per_token = 5e-7
litellm_provider = "openai"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 4097
mode = "chat"
output_cost_per_token = 0.0000015
supports_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-0125"]
input_cost_per_token = 5e-7
litellm_provider = "openai"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 16385
mode = "chat"
output_cost_per_token = 0.0000015
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-0301"]
input_cost_per_token = 0.0000015
litellm_provider = "openai"
max_input_tokens = 4097
max_output_tokens = 4096
max_tokens = 4097
mode = "chat"
output_cost_per_token = 0.000002
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-0613"]
input_cost_per_token = 0.0000015
litellm_provider = "openai"
max_input_tokens = 4097
max_output_tokens = 4096
max_tokens = 4097
mode = "chat"
output_cost_per_token = 0.000002
supports_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-1106"]
deprecation_date = "2026-09-28"
input_cost_per_token = 0.000001
litellm_provider = "openai"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 16385
mode = "chat"
output_cost_per_token = 0.000002
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-16k"]
input_cost_per_token = 0.000003
litellm_provider = "openai"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 16385
mode = "chat"
output_cost_per_token = 0.000004
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-16k-0613"]
input_cost_per_token = 0.000003
litellm_provider = "openai"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 16385
mode = "chat"
output_cost_per_token = 0.000004
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4"]
input_cost_per_token = 0.00003
litellm_provider = "openai"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-0125-preview"]
deprecation_date = "2026-03-26"
input_cost_per_token = 0.00001
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-0314"]
input_cost_per_token = 0.00003
litellm_provider = "openai"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00006
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-0613"]
deprecation_date = "2025-06-06"
input_cost_per_token = 0.00003
litellm_provider = "openai"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-1106-preview"]
deprecation_date = "2026-03-26"
input_cost_per_token = 0.00001
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-1106-vision-preview"]
deprecation_date = "2024-12-06"
input_cost_per_token = 0.00001
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_pdf_input = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4-32k"]
input_cost_per_token = 0.00006
litellm_provider = "openai"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00012
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-32k-0314"]
input_cost_per_token = 0.00006
litellm_provider = "openai"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00012
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-32k-0613"]
input_cost_per_token = 0.00006
litellm_provider = "openai"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00012
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-turbo"]
input_cost_per_token = 0.00001
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4-turbo-2024-04-09"]
input_cost_per_token = 0.00001
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4-turbo-preview"]
input_cost_per_token = 0.00001
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-vision-preview"]
deprecation_date = "2024-12-06"
input_cost_per_token = 0.00001
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_pdf_input = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4.1"]
cache_read_input_token_cost = 5e-7
cache_read_input_token_cost_priority = 8.75e-7
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
input_cost_per_token_priority = 0.0000035
litellm_provider = "openai"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000008
output_cost_per_token_batches = 0.000004
output_cost_per_token_priority = 0.000014
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4.1-2025-04-14"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
litellm_provider = "openai"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000008
output_cost_per_token_batches = 0.000004
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4.1-mini"]
cache_read_input_token_cost = 1e-7
cache_read_input_token_cost_priority = 1.75e-7
input_cost_per_token = 4e-7
input_cost_per_token_batches = 2e-7
input_cost_per_token_priority = 7e-7
litellm_provider = "openai"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000016
output_cost_per_token_batches = 8e-7
output_cost_per_token_priority = 0.0000028
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4.1-mini-2025-04-14"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 4e-7
input_cost_per_token_batches = 2e-7
litellm_provider = "openai"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000016
output_cost_per_token_batches = 8e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4.1-nano"]
cache_read_input_token_cost = 2.5e-8
cache_read_input_token_cost_priority = 5e-8
input_cost_per_token = 1e-7
input_cost_per_token_batches = 5e-8
input_cost_per_token_priority = 2e-7
litellm_provider = "openai"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 4e-7
output_cost_per_token_batches = 2e-7
output_cost_per_token_priority = 8e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4.1-nano-2025-04-14"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 1e-7
input_cost_per_token_batches = 5e-8
litellm_provider = "openai"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 4e-7
output_cost_per_token_batches = 2e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4.5-preview"]
cache_read_input_token_cost = 0.0000375
input_cost_per_token = 0.000075
input_cost_per_token_batches = 0.0000375
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00015
output_cost_per_token_batches = 0.000075
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4.5-preview-2025-02-27"]
cache_read_input_token_cost = 0.0000375
deprecation_date = "2025-07-14"
input_cost_per_token = 0.000075
input_cost_per_token_batches = 0.0000375
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00015
output_cost_per_token_batches = 0.000075
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o"]
cache_read_input_token_cost = 0.00000125
cache_read_input_token_cost_priority = 0.000002125
input_cost_per_token = 0.0000025
input_cost_per_token_batches = 0.00000125
input_cost_per_token_priority = 0.00000425
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_batches = 0.000005
output_cost_per_token_priority = 0.000017
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o-2024-05-13"]
input_cost_per_token = 0.000005
input_cost_per_token_batches = 0.0000025
input_cost_per_token_priority = 0.00000875
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_batches = 0.0000075
output_cost_per_token_priority = 0.00002625
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o-2024-08-06"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
input_cost_per_token_batches = 0.00000125
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_batches = 0.000005
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o-2024-11-20"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
input_cost_per_token_batches = 0.00000125
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_batches = 0.000005
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o-audio-preview"]
input_cost_per_audio_token = 0.0001
input_cost_per_token = 0.0000025
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_audio_token = 0.0002
output_cost_per_token = 0.00001
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-audio-preview-2024-10-01"]
input_cost_per_audio_token = 0.0001
input_cost_per_token = 0.0000025
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_audio_token = 0.0002
output_cost_per_token = 0.00001
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-audio-preview-2024-12-17"]
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.0000025
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00001
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-audio-preview-2025-06-03"]
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.0000025
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00001
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini"]
cache_read_input_token_cost = 7.5e-8
cache_read_input_token_cost_priority = 1.25e-7
input_cost_per_token = 1.5e-7
input_cost_per_token_batches = 7.5e-8
input_cost_per_token_priority = 2.5e-7
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 6e-7
output_cost_per_token_batches = 3e-7
output_cost_per_token_priority = 0.000001
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o-mini-2024-07-18"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.5e-7
input_cost_per_token_batches = 7.5e-8
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 6e-7
output_cost_per_token_batches = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o-mini-2024-07-18".search_context_cost_per_query]
search_context_size_high = 0.03
search_context_size_low = 0.025
search_context_size_medium = 0.0275

[models."gpt-4o-mini-audio-preview"]
input_cost_per_audio_token = 0.00001
input_cost_per_token = 1.5e-7
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_audio_token = 0.00002
output_cost_per_token = 6e-7
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini-audio-preview-2024-12-17"]
input_cost_per_audio_token = 0.00001
input_cost_per_token = 1.5e-7
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_audio_token = 0.00002
output_cost_per_token = 6e-7
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini-realtime-preview"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_token_cost = 3e-7
input_cost_per_audio_token = 0.00001
input_cost_per_token = 6e-7
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini-realtime-preview-2024-12-17"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_token_cost = 3e-7
input_cost_per_audio_token = 0.00001
input_cost_per_token = 6e-7
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini-search-preview"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.5e-7
input_cost_per_token_batches = 7.5e-8
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 6e-7
output_cost_per_token_batches = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gpt-4o-mini-search-preview".search_context_cost_per_query]
search_context_size_high = 0.03
search_context_size_low = 0.025
search_context_size_medium = 0.0275

[models."gpt-4o-mini-search-preview-2025-03-11"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.5e-7
input_cost_per_token_batches = 7.5e-8
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 6e-7
output_cost_per_token_batches = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o-realtime-preview"]
cache_read_input_token_cost = 0.0000025
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.000005
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00002
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-realtime-preview-2024-10-01"]
cache_creation_input_audio_token_cost = 0.00002
cache_read_input_token_cost = 0.0000025
input_cost_per_audio_token = 0.0001
input_cost_per_token = 0.000005
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.0002
output_cost_per_token = 0.00002
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-realtime-preview-2024-12-17"]
cache_read_input_token_cost = 0.0000025
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.000005
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00002
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-realtime-preview-2025-06-03"]
cache_read_input_token_cost = 0.0000025
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.000005
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00002
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-search-preview"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
input_cost_per_token_batches = 0.00000125
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_batches = 0.000005
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gpt-4o-search-preview".search_context_cost_per_query]
search_context_size_high = 0.05
search_context_size_low = 0.03
search_context_size_medium = 0.035

[models."gpt-4o-search-preview-2025-03-11"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
input_cost_per_token_batches = 0.00000125
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_batches = 0.000005
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_flex = 6.25e-8
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_flex = 6.25e-7
input_cost_per_token_priority = 0.0000025
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_flex = 0.000005
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5-2025-08-07"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_flex = 6.25e-8
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_flex = 6.25e-7
input_cost_per_token_priority = 0.0000025
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_flex = 0.000005
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5-chat"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = false
supports_native_streaming = true
supports_parallel_function_calling = false
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = false
supports_vision = true

[models."gpt-5-chat-latest"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = false
supports_native_streaming = true
supports_parallel_function_calling = false
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = false
supports_vision = true

[models."gpt-5-mini"]
cache_read_input_token_cost = 2.5e-8
cache_read_input_token_cost_flex = 1.25e-8
cache_read_input_token_cost_priority = 4.5e-8
input_cost_per_token = 2.5e-7
input_cost_per_token_flex = 1.25e-7
input_cost_per_token_priority = 4.5e-7
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000002
output_cost_per_token_flex = 0.000001
output_cost_per_token_priority = 0.0000036
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5-mini-2025-08-07"]
cache_read_input_token_cost = 2.5e-8
cache_read_input_token_cost_flex = 1.25e-8
cache_read_input_token_cost_priority = 4.5e-8
input_cost_per_token = 2.5e-7
input_cost_per_token_flex = 1.25e-7
input_cost_per_token_priority = 4.5e-7
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000002
output_cost_per_token_flex = 0.000001
output_cost_per_token_priority = 0.0000036
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5-nano"]
cache_read_input_token_cost = 5e-9
cache_read_input_token_cost_flex = 2.5e-9
input_cost_per_token = 5e-8
input_cost_per_token_flex = 2.5e-8
input_cost_per_token_priority = 0.0000025
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 4e-7
output_cost_per_token_flex = 2e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5-nano-2025-08-07"]
cache_read_input_token_cost = 5e-9
cache_read_input_token_cost_flex = 2.5e-9
input_cost_per_token = 5e-8
input_cost_per_token_flex = 2.5e-8
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 4e-7
output_cost_per_token_flex = 2e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5.1"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5.1-2025-11-13"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5.1-chat-latest"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = false
supports_native_streaming = true
supports_parallel_function_calling = false
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = false
supports_vision = true

[models."gpt-5.2"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
litellm_provider = "openai"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5.2-2025-12-11"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
litellm_provider = "openai"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5.2-chat-latest"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-realtime"]
cache_creation_input_audio_token_cost = 4e-7
cache_read_input_token_cost = 4e-7
input_cost_per_audio_token = 0.000032
input_cost_per_image = 0.000005
input_cost_per_token = 0.000004
litellm_provider = "openai"
max_input_tokens = 32000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.000064
output_cost_per_token = 0.000016
supported_endpoints = ["/v1/realtime"]
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text", "audio"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-realtime-2025-08-28"]
cache_creation_input_audio_token_cost = 4e-7
cache_read_input_token_cost = 4e-7
input_cost_per_audio_token = 0.000032
input_cost_per_image = 0.000005
input_cost_per_token = 0.000004
litellm_provider = "openai"
max_input_tokens = 32000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.000064
output_cost_per_token = 0.000016
supported_endpoints = ["/v1/realtime"]
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text", "audio"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-realtime-mini"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_audio_token_cost = 3e-7
input_cost_per_audio_token = 0.00001
input_cost_per_token = 6e-7
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024
supported_endpoints = ["/v1/realtime"]
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text", "audio"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."o1"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."o1-2024-12-17"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."o1-mini"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.0000044
supports_pdf_input = true
supports_prompt_caching = true
supports_vision = true

[models."o1-mini-2024-09-12"]
cache_read_input_token_cost = 0.0000015
deprecation_date = "2025-10-27"
input_cost_per_token = 0.000003
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.000012
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_vision = true

[models."o1-preview"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.00006
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_vision = true

[models."o1-preview-2024-09-12"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.00006
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_vision = true

[models."o3"]
cache_read_input_token_cost = 5e-7
cache_read_input_token_cost_flex = 2.5e-7
cache_read_input_token_cost_priority = 8.75e-7
input_cost_per_token = 0.000002
input_cost_per_token_flex = 0.000001
input_cost_per_token_priority = 0.0000035
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.000008
output_cost_per_token_flex = 0.000004
output_cost_per_token_priority = 0.000014
supported_endpoints = ["/v1/responses", "/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = false
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_tool_choice = true
supports_vision = true

[models."o3-2025-04-16"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.000008
supported_endpoints = ["/v1/responses", "/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = false
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_tool_choice = true
supports_vision = true

[models."o3-mini"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.0000044
supports_function_calling = true
supports_parallel_function_calling = false
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = false

[models."o3-mini-2025-01-31"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.0000044
supports_function_calling = true
supports_parallel_function_calling = false
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = false
