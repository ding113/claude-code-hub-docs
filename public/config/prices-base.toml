# Generated by scripts/convert-litellm-to-toml.ts

[metadata]
version = "2025.12.31"
checksum = "7072cf486a529f6d5749fcfdd2ee77fd11594fdedf8f886025e060439477f65b"

[models."1024-x-1024/50-steps/bedrock/amazon.nova-canvas-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 2600
mode = "image_generation"
output_cost_per_image = 0.06

[models."1024-x-1024/50-steps/stability.stable-diffusion-xl-v1"]
litellm_provider = "bedrock"
max_input_tokens = 77
max_tokens = 77
mode = "image_generation"
output_cost_per_image = 0.04

[models."1024-x-1024/dall-e-2"]
input_cost_per_pixel = 1.9e-8
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0

[models."1024-x-1024/max-steps/stability.stable-diffusion-xl-v1"]
litellm_provider = "bedrock"
max_input_tokens = 77
max_tokens = 77
mode = "image_generation"
output_cost_per_image = 0.08

[models."256-x-256/dall-e-2"]
input_cost_per_pixel = 2.4414e-7
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0

[models."512-x-512/50-steps/stability.stable-diffusion-xl-v0"]
litellm_provider = "bedrock"
max_input_tokens = 77
max_tokens = 77
mode = "image_generation"
output_cost_per_image = 0.018

[models."512-x-512/dall-e-2"]
input_cost_per_pixel = 6.86e-8
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0

[models."512-x-512/max-steps/stability.stable-diffusion-xl-v0"]
litellm_provider = "bedrock"
max_input_tokens = 77
max_tokens = 77
mode = "image_generation"
output_cost_per_image = 0.036

[models."ai21.j2-mid-v1"]
input_cost_per_token = 0.0000125
litellm_provider = "bedrock"
max_input_tokens = 8191
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.0000125

[models."ai21.j2-ultra-v1"]
input_cost_per_token = 0.0000188
litellm_provider = "bedrock"
max_input_tokens = 8191
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.0000188

[models."ai21.jamba-1-5-large-v1:0"]
input_cost_per_token = 0.000002
litellm_provider = "bedrock"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.000008

[models."ai21.jamba-1-5-mini-v1:0"]
input_cost_per_token = 2e-7
litellm_provider = "bedrock"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 4e-7

[models."ai21.jamba-instruct-v1:0"]
input_cost_per_token = 5e-7
litellm_provider = "bedrock"
max_input_tokens = 70000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 7e-7
supports_system_messages = true

[models."aiml/dall-e-2"]
litellm_provider = "aiml"
mode = "image_generation"
output_cost_per_image = 0.021
source = "https://docs.aimlapi.com/"
supported_endpoints = ["/v1/images/generations"]

[models."aiml/dall-e-2".metadata]
notes = "DALL-E 2 via AI/ML API - Reliable text-to-image generation"

[models."aiml/dall-e-3"]
litellm_provider = "aiml"
mode = "image_generation"
output_cost_per_image = 0.042
source = "https://docs.aimlapi.com/"
supported_endpoints = ["/v1/images/generations"]

[models."aiml/dall-e-3".metadata]
notes = "DALL-E 3 via AI/ML API - High-quality text-to-image generation"

[models."aiml/flux-pro"]
litellm_provider = "aiml"
mode = "image_generation"
output_cost_per_image = 0.053
source = "https://docs.aimlapi.com/"
supported_endpoints = ["/v1/images/generations"]

[models."aiml/flux-pro".metadata]
notes = "Flux Dev - Development version optimized for experimentation"

[models."aiml/flux-pro/v1.1"]
litellm_provider = "aiml"
mode = "image_generation"
output_cost_per_image = 0.042
supported_endpoints = ["/v1/images/generations"]

[models."aiml/flux-pro/v1.1-ultra"]
litellm_provider = "aiml"
mode = "image_generation"
output_cost_per_image = 0.063
supported_endpoints = ["/v1/images/generations"]

[models."aiml/flux-realism"]
litellm_provider = "aiml"
mode = "image_generation"
output_cost_per_image = 0.037
source = "https://docs.aimlapi.com/"
supported_endpoints = ["/v1/images/generations"]

[models."aiml/flux-realism".metadata]
notes = "Flux Pro - Professional-grade image generation model"

[models."aiml/flux/dev"]
litellm_provider = "aiml"
mode = "image_generation"
output_cost_per_image = 0.026
source = "https://docs.aimlapi.com/"
supported_endpoints = ["/v1/images/generations"]

[models."aiml/flux/dev".metadata]
notes = "Flux Dev - Development version optimized for experimentation"

[models."aiml/flux/kontext-max/text-to-image"]
litellm_provider = "aiml"
mode = "image_generation"
output_cost_per_image = 0.084
source = "https://docs.aimlapi.com/"
supported_endpoints = ["/v1/images/generations"]

[models."aiml/flux/kontext-max/text-to-image".metadata]
notes = "Flux Pro v1.1 - Enhanced version with improved capabilities and 6x faster inference speed"

[models."aiml/flux/kontext-pro/text-to-image"]
litellm_provider = "aiml"
mode = "image_generation"
output_cost_per_image = 0.042
source = "https://docs.aimlapi.com/"
supported_endpoints = ["/v1/images/generations"]

[models."aiml/flux/kontext-pro/text-to-image".metadata]
notes = "Flux Pro v1.1 - Enhanced version with improved capabilities and 6x faster inference speed"

[models."aiml/flux/schnell"]
litellm_provider = "aiml"
mode = "image_generation"
output_cost_per_image = 0.003
source = "https://docs.aimlapi.com/"
supported_endpoints = ["/v1/images/generations"]

[models."aiml/flux/schnell".metadata]
notes = "Flux Schnell - Fast generation model optimized for speed"

[models."aiml/google/imagen-4.0-ultra-generate-001"]
litellm_provider = "aiml"
mode = "image_generation"
output_cost_per_image = 0.063
source = "https://docs.aimlapi.com/api-references/image-models/google/imagen-4-ultra-generate"
supported_endpoints = ["/v1/images/generations"]

[models."aiml/google/imagen-4.0-ultra-generate-001".metadata]
notes = "Imagen 4.0 Ultra Generate API - Photorealistic image generation with precise text rendering"

[models."aiml/google/nano-banana-pro"]
litellm_provider = "aiml"
mode = "image_generation"
output_cost_per_image = 0.1575
source = "https://docs.aimlapi.com/api-references/image-models/google/gemini-3-pro-image-preview"
supported_endpoints = ["/v1/images/generations"]

[models."aiml/google/nano-banana-pro".metadata]
notes = "Gemini 3 Pro Image (Nano Banana Pro) - Advanced text-to-image generation with reasoning and 4K resolution support"

[models."amazon-nova/nova-lite-v1"]
input_cost_per_token = 6e-8
litellm_provider = "amazon_nova"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 2.4e-7
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_vision = true

[models."amazon-nova/nova-micro-v1"]
input_cost_per_token = 3.5e-8
litellm_provider = "amazon_nova"
max_input_tokens = 128000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 1.4e-7
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true

[models."amazon-nova/nova-premier-v1"]
input_cost_per_token = 0.0000025
litellm_provider = "amazon_nova"
max_input_tokens = 1000000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 0.0000125
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = false
supports_response_schema = true
supports_vision = true

[models."amazon-nova/nova-pro-v1"]
input_cost_per_token = 8e-7
litellm_provider = "amazon_nova"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 0.0000032
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_vision = true

[models."amazon.nova-2-lite-v1:0"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.0000025
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_video_input = true
supports_vision = true

[models."amazon.nova-canvas-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 2600
mode = "image_generation"
output_cost_per_image = 0.06

[models."amazon.nova-lite-v1:0"]
input_cost_per_token = 6e-8
litellm_provider = "bedrock_converse"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 2.4e-7
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_vision = true

[models."amazon.nova-micro-v1:0"]
input_cost_per_token = 3.5e-8
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 1.4e-7
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true

[models."amazon.nova-pro-v1:0"]
input_cost_per_token = 8e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 0.0000032
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_vision = true

[models."amazon.rerank-v1:0"]
input_cost_per_query = 0.001
input_cost_per_token = 0
litellm_provider = "bedrock"
max_document_chunks_per_query = 100
max_input_tokens = 32000
max_output_tokens = 32000
max_query_tokens = 32000
max_tokens = 32000
max_tokens_per_document_chunk = 512
mode = "rerank"
output_cost_per_token = 0

[models."amazon.titan-embed-image-v1"]
input_cost_per_image = 0.00006
input_cost_per_token = 8e-7
litellm_provider = "bedrock"
max_input_tokens = 128
max_tokens = 128
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 1024
source = "https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/providers?model=amazon.titan-image-generator-v1"
supports_embedding_image_input = true
supports_image_input = true

[models."amazon.titan-embed-image-v1".metadata]
notes = "'supports_image_input' is a deprecated field. Use 'supports_embedding_image_input' instead."

[models."amazon.titan-embed-text-v1"]
input_cost_per_token = 1e-7
litellm_provider = "bedrock"
max_input_tokens = 8192
max_tokens = 8192
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 1536

[models."amazon.titan-embed-text-v2:0"]
input_cost_per_token = 2e-7
litellm_provider = "bedrock"
max_input_tokens = 8192
max_tokens = 8192
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 1024

[models."amazon.titan-image-generator-v1"]
input_cost_per_image = 0
litellm_provider = "bedrock"
mode = "image_generation"
output_cost_per_image = 0.008
output_cost_per_image_above_512_and_512_pixels = 0.01
output_cost_per_image_above_512_and_512_pixels_and_premium_image = 0.012
output_cost_per_image_premium_image = 0.01

[models."amazon.titan-image-generator-v2"]
input_cost_per_image = 0
litellm_provider = "bedrock"
mode = "image_generation"
output_cost_per_image = 0.008
output_cost_per_image_above_1024_and_1024_pixels = 0.01
output_cost_per_image_above_1024_and_1024_pixels_and_premium_image = 0.012
output_cost_per_image_premium_image = 0.01

[models."amazon.titan-image-generator-v2:0"]
input_cost_per_image = 0
litellm_provider = "bedrock"
mode = "image_generation"
output_cost_per_image = 0.008
output_cost_per_image_above_1024_and_1024_pixels = 0.01
output_cost_per_image_above_1024_and_1024_pixels_and_premium_image = 0.012
output_cost_per_image_premium_image = 0.01

[models."amazon.titan-text-express-v1"]
input_cost_per_token = 0.0000013
litellm_provider = "bedrock"
max_input_tokens = 42000
max_output_tokens = 8000
max_tokens = 8000
mode = "chat"
output_cost_per_token = 0.0000017

[models."amazon.titan-text-lite-v1"]
input_cost_per_token = 3e-7
litellm_provider = "bedrock"
max_input_tokens = 42000
max_output_tokens = 4000
max_tokens = 4000
mode = "chat"
output_cost_per_token = 4e-7

[models."amazon.titan-text-premier-v1:0"]
input_cost_per_token = 5e-7
litellm_provider = "bedrock"
max_input_tokens = 42000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.0000015

[models."anthropic.claude-3-5-haiku-20241022-v1:0"]
cache_creation_input_token_cost = 0.000001
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8e-7
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000004
supports_assistant_prefill = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true

[models."anthropic.claude-3-5-sonnet-20240620-v1:0"]
input_cost_per_token = 0.000003
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."anthropic.claude-3-5-sonnet-20241022-v2:0"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."anthropic.claude-3-7-sonnet-20240620-v1:0"]
cache_creation_input_token_cost = 0.0000045
cache_read_input_token_cost = 3.6e-7
input_cost_per_token = 0.0000036
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000018
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."anthropic.claude-3-7-sonnet-20250219-v1:0"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."anthropic.claude-3-haiku-20240307-v1:0"]
input_cost_per_token = 2.5e-7
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00000125
supports_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."anthropic.claude-3-opus-20240229-v1:0"]
input_cost_per_token = 0.000015
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000075
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."anthropic.claude-3-sonnet-20240229-v1:0"]
input_cost_per_token = 0.000003
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."anthropic.claude-haiku-4-5-20251001-v1:0"]
cache_creation_input_token_cost = 0.00000125
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000005
source = "https://aws.amazon.com/about-aws/whats-new/2025/10/claude-4-5-haiku-anthropic-amazon-bedrock"
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 346

[models."anthropic.claude-haiku-4-5@20251001"]
cache_creation_input_token_cost = 0.00000125
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000005
source = "https://aws.amazon.com/about-aws/whats-new/2025/10/claude-4-5-haiku-anthropic-amazon-bedrock"
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 346

[models."anthropic.claude-instant-v1"]
input_cost_per_token = 8e-7
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.0000024
supports_tool_choice = true

[models."anthropic.claude-opus-4-1-20250805-v1:0"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."anthropic.claude-opus-4-1-20250805-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."anthropic.claude-opus-4-20250514-v1:0"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."anthropic.claude-opus-4-20250514-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."anthropic.claude-opus-4-5-20251101-v1:0"]
cache_creation_input_token_cost = 0.00000625
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000025
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."anthropic.claude-opus-4-5-20251101-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."anthropic.claude-sonnet-4-20250514-v1:0"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "bedrock_converse"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."anthropic.claude-sonnet-4-20250514-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."anthropic.claude-sonnet-4-5-20250929-v1:0"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."anthropic.claude-sonnet-4-5-20250929-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."anthropic.claude-v1"]
input_cost_per_token = 0.000008
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000024

[models."anthropic.claude-v2:1"]
input_cost_per_token = 0.000008
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000024
supports_tool_choice = true

[models."anyscale/HuggingFaceH4/zephyr-7b-beta"]
input_cost_per_token = 1.5e-7
litellm_provider = "anyscale"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 1.5e-7

[models."anyscale/codellama/CodeLlama-34b-Instruct-hf"]
input_cost_per_token = 0.000001
litellm_provider = "anyscale"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000001

[models."anyscale/codellama/CodeLlama-70b-Instruct-hf"]
input_cost_per_token = 0.000001
litellm_provider = "anyscale"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000001
source = "https://docs.anyscale.com/preview/endpoints/text-generation/supported-models/codellama-CodeLlama-70b-Instruct-hf"

[models."anyscale/google/gemma-7b-it"]
input_cost_per_token = 1.5e-7
litellm_provider = "anyscale"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 1.5e-7
source = "https://docs.anyscale.com/preview/endpoints/text-generation/supported-models/google-gemma-7b-it"

[models."anyscale/meta-llama/Llama-2-13b-chat-hf"]
input_cost_per_token = 2.5e-7
litellm_provider = "anyscale"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2.5e-7

[models."anyscale/meta-llama/Llama-2-70b-chat-hf"]
input_cost_per_token = 0.000001
litellm_provider = "anyscale"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000001

[models."anyscale/meta-llama/Llama-2-7b-chat-hf"]
input_cost_per_token = 1.5e-7
litellm_provider = "anyscale"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 1.5e-7

[models."anyscale/meta-llama/Meta-Llama-3-70B-Instruct"]
input_cost_per_token = 0.000001
litellm_provider = "anyscale"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000001
source = "https://docs.anyscale.com/preview/endpoints/text-generation/supported-models/meta-llama-Meta-Llama-3-70B-Instruct"

[models."anyscale/meta-llama/Meta-Llama-3-8B-Instruct"]
input_cost_per_token = 1.5e-7
litellm_provider = "anyscale"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 1.5e-7
source = "https://docs.anyscale.com/preview/endpoints/text-generation/supported-models/meta-llama-Meta-Llama-3-8B-Instruct"

[models."anyscale/mistralai/Mistral-7B-Instruct-v0.1"]
input_cost_per_token = 1.5e-7
litellm_provider = "anyscale"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 1.5e-7
source = "https://docs.anyscale.com/preview/endpoints/text-generation/supported-models/mistralai-Mistral-7B-Instruct-v0.1"
supports_function_calling = true

[models."anyscale/mistralai/Mixtral-8x22B-Instruct-v0.1"]
input_cost_per_token = 9e-7
litellm_provider = "anyscale"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 9e-7
source = "https://docs.anyscale.com/preview/endpoints/text-generation/supported-models/mistralai-Mixtral-8x22B-Instruct-v0.1"
supports_function_calling = true

[models."anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1"]
input_cost_per_token = 1.5e-7
litellm_provider = "anyscale"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 1.5e-7
source = "https://docs.anyscale.com/preview/endpoints/text-generation/supported-models/mistralai-Mixtral-8x7B-Instruct-v0.1"
supports_function_calling = true

[models."apac.amazon.nova-2-lite-v1:0"]
cache_read_input_token_cost = 8.25e-8
input_cost_per_token = 3.3e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.00000275
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_video_input = true
supports_vision = true

[models."apac.amazon.nova-lite-v1:0"]
input_cost_per_token = 6.3e-8
litellm_provider = "bedrock_converse"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 2.52e-7
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_vision = true

[models."apac.amazon.nova-micro-v1:0"]
input_cost_per_token = 3.7e-8
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 1.48e-7
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true

[models."apac.amazon.nova-pro-v1:0"]
input_cost_per_token = 8.4e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 0.00000336
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_vision = true

[models."apac.anthropic.claude-3-5-sonnet-20240620-v1:0"]
input_cost_per_token = 0.000003
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."apac.anthropic.claude-3-5-sonnet-20241022-v2:0"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."apac.anthropic.claude-3-haiku-20240307-v1:0"]
input_cost_per_token = 2.5e-7
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00000125
supports_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."apac.anthropic.claude-3-sonnet-20240229-v1:0"]
input_cost_per_token = 0.000003
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."apac.anthropic.claude-haiku-4-5-20251001-v1:0"]
cache_creation_input_token_cost = 0.000001375
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.0000055
source = "https://aws.amazon.com/about-aws/whats-new/2025/10/claude-4-5-haiku-anthropic-amazon-bedrock"
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 346

[models."apac.anthropic.claude-sonnet-4-20250514-v1:0"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "bedrock_converse"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."apac.anthropic.claude-sonnet-4-20250514-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."assemblyai/best"]
input_cost_per_second = 0.00003333
litellm_provider = "assemblyai"
mode = "audio_transcription"
output_cost_per_second = 0

[models."assemblyai/nano"]
input_cost_per_second = 0.00010278
litellm_provider = "assemblyai"
mode = "audio_transcription"
output_cost_per_second = 0

[models."au.anthropic.claude-haiku-4-5-20251001-v1:0"]
cache_creation_input_token_cost = 0.000001375
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.0000055
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 346

[models."au.anthropic.claude-sonnet-4-5-20250929-v1:0"]
cache_creation_input_token_cost = 0.000004125
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost = 3.3e-7
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token = 0.0000033
input_cost_per_token_above_200k_tokens = 0.0000066
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.0000165
output_cost_per_token_above_200k_tokens = 0.00002475
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 346

[models."au.anthropic.claude-sonnet-4-5-20250929-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."aws_polly/generative"]
input_cost_per_character = 0.00003
litellm_provider = "aws_polly"
mode = "audio_speech"
source = "https://aws.amazon.com/polly/pricing/"
supported_endpoints = ["/v1/audio/speech"]

[models."aws_polly/long-form"]
input_cost_per_character = 0.0001
litellm_provider = "aws_polly"
mode = "audio_speech"
source = "https://aws.amazon.com/polly/pricing/"
supported_endpoints = ["/v1/audio/speech"]

[models."aws_polly/neural"]
input_cost_per_character = 0.000016
litellm_provider = "aws_polly"
mode = "audio_speech"
source = "https://aws.amazon.com/polly/pricing/"
supported_endpoints = ["/v1/audio/speech"]

[models."aws_polly/standard"]
input_cost_per_character = 0.000004
litellm_provider = "aws_polly"
mode = "audio_speech"
source = "https://aws.amazon.com/polly/pricing/"
supported_endpoints = ["/v1/audio/speech"]

[models."azure/ada"]
input_cost_per_token = 1e-7
litellm_provider = "azure"
max_input_tokens = 8191
max_tokens = 8191
mode = "embedding"
output_cost_per_token = 0

[models."azure/codex-mini"]
cache_read_input_token_cost = 3.75e-7
input_cost_per_token = 0.0000015
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "responses"
output_cost_per_token = 0.000006
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/command-r-plus"]
input_cost_per_token = 0.000003
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true

[models."azure/computer-use-preview"]
input_cost_per_token = 0.000003
litellm_provider = "azure"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
mode = "chat"
output_cost_per_token = 0.000012
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = false
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/container"]
code_interpreter_cost_per_session = 0.03
litellm_provider = "azure"
mode = "chat"

[models."azure/eu/gpt-4o-2024-08-06"]
cache_read_input_token_cost = 0.000001375
deprecation_date = "2026-02-27"
input_cost_per_token = 0.00000275
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.000011
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/eu/gpt-4o-2024-11-20"]
cache_creation_input_token_cost = 0.00000138
deprecation_date = "2026-03-01"
input_cost_per_token = 0.00000275
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.000011
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/eu/gpt-4o-mini-2024-07-18"]
cache_read_input_token_cost = 8.3e-8
input_cost_per_token = 1.65e-7
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 6.6e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/eu/gpt-4o-mini-realtime-preview-2024-12-17"]
cache_creation_input_audio_token_cost = 3.3e-7
cache_read_input_token_cost = 3.3e-7
input_cost_per_audio_token = 0.000011
input_cost_per_token = 6.6e-7
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.000022
output_cost_per_token = 0.00000264
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."azure/eu/gpt-4o-realtime-preview-2024-10-01"]
cache_creation_input_audio_token_cost = 0.000022
cache_read_input_token_cost = 0.00000275
input_cost_per_audio_token = 0.00011
input_cost_per_token = 0.0000055
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00022
output_cost_per_token = 0.000022
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."azure/eu/gpt-4o-realtime-preview-2024-12-17"]
cache_read_input_audio_token_cost = 0.0000025
cache_read_input_token_cost = 0.00000275
input_cost_per_audio_token = 0.000044
input_cost_per_token = 0.0000055
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.000022
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."azure/eu/gpt-5-2025-08-07"]
cache_read_input_token_cost = 1.375e-7
input_cost_per_token = 0.000001375
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000011
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/eu/gpt-5-mini-2025-08-07"]
cache_read_input_token_cost = 2.75e-8
input_cost_per_token = 2.75e-7
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.0000022
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/eu/gpt-5-nano-2025-08-07"]
cache_read_input_token_cost = 5.5e-9
input_cost_per_token = 5.5e-8
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 4.4e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/eu/gpt-5.1"]
cache_read_input_token_cost = 1.4e-7
input_cost_per_token = 0.00000138
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000011
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/eu/gpt-5.1-chat"]
cache_read_input_token_cost = 1.4e-7
input_cost_per_token = 0.00000138
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000011
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/eu/gpt-5.1-codex"]
cache_read_input_token_cost = 1.4e-7
input_cost_per_token = 0.00000138
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.000011
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."azure/eu/gpt-5.1-codex-mini"]
cache_read_input_token_cost = 2.8e-8
input_cost_per_token = 2.75e-7
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.0000022
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."azure/eu/o1-2024-12-17"]
cache_read_input_token_cost = 0.00000825
input_cost_per_token = 0.0000165
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.000066
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_tool_choice = true
supports_vision = true

[models."azure/eu/o1-mini-2024-09-12"]
cache_read_input_token_cost = 6.05e-7
input_cost_per_token = 0.00000121
input_cost_per_token_batches = 6.05e-7
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.00000484
output_cost_per_token_batches = 0.00000242
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_vision = false

[models."azure/eu/o1-preview-2024-09-12"]
cache_read_input_token_cost = 0.00000825
input_cost_per_token = 0.0000165
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000066
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_vision = false

[models."azure/eu/o3-mini-2025-01-31"]
cache_read_input_token_cost = 6.05e-7
input_cost_per_token = 0.00000121
input_cost_per_token_batches = 6.05e-7
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.00000484
output_cost_per_token_batches = 0.00000242
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true
supports_vision = false

[models."azure/global-standard/gpt-4o-2024-08-06"]
cache_read_input_token_cost = 0.00000125
deprecation_date = "2026-02-27"
input_cost_per_token = 0.0000025
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/global-standard/gpt-4o-2024-11-20"]
cache_read_input_token_cost = 0.00000125
deprecation_date = "2026-03-01"
input_cost_per_token = 0.0000025
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/global-standard/gpt-4o-mini"]
input_cost_per_token = 1.5e-7
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/global/gpt-4o-2024-08-06"]
cache_read_input_token_cost = 0.00000125
deprecation_date = "2026-02-27"
input_cost_per_token = 0.0000025
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/global/gpt-4o-2024-11-20"]
cache_read_input_token_cost = 0.00000125
deprecation_date = "2026-03-01"
input_cost_per_token = 0.0000025
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/global/gpt-5.1"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/global/gpt-5.1-chat"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/global/gpt-5.1-codex"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."azure/global/gpt-5.1-codex-mini"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 2.5e-7
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.000002
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-3.5-turbo"]
input_cost_per_token = 5e-7
litellm_provider = "azure"
max_input_tokens = 4097
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0000015
supports_function_calling = true
supports_tool_choice = true

[models."azure/gpt-3.5-turbo-0125"]
deprecation_date = "2025-03-31"
input_cost_per_token = 5e-7
litellm_provider = "azure"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0000015
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."azure/gpt-3.5-turbo-instruct-0914"]
input_cost_per_token = 0.0000015
litellm_provider = "azure_text"
max_input_tokens = 4097
max_tokens = 4097
mode = "completion"
output_cost_per_token = 0.000002

[models."azure/gpt-35-turbo"]
input_cost_per_token = 5e-7
litellm_provider = "azure"
max_input_tokens = 4097
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0000015
supports_function_calling = true
supports_tool_choice = true

[models."azure/gpt-35-turbo-0125"]
deprecation_date = "2025-05-31"
input_cost_per_token = 5e-7
litellm_provider = "azure"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0000015
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."azure/gpt-35-turbo-0301"]
deprecation_date = "2025-02-13"
input_cost_per_token = 2e-7
litellm_provider = "azure"
max_input_tokens = 4097
max_output_tokens = 4096
max_tokens = 4097
mode = "chat"
output_cost_per_token = 0.000002
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."azure/gpt-35-turbo-0613"]
deprecation_date = "2025-02-13"
input_cost_per_token = 0.0000015
litellm_provider = "azure"
max_input_tokens = 4097
max_output_tokens = 4096
max_tokens = 4097
mode = "chat"
output_cost_per_token = 0.000002
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."azure/gpt-35-turbo-1106"]
deprecation_date = "2025-03-31"
input_cost_per_token = 0.000001
litellm_provider = "azure"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000002
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."azure/gpt-35-turbo-16k"]
input_cost_per_token = 0.000003
litellm_provider = "azure"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000004
supports_tool_choice = true

[models."azure/gpt-35-turbo-16k-0613"]
input_cost_per_token = 0.000003
litellm_provider = "azure"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000004
supports_function_calling = true
supports_tool_choice = true

[models."azure/gpt-35-turbo-instruct"]
input_cost_per_token = 0.0000015
litellm_provider = "azure_text"
max_input_tokens = 4097
max_tokens = 4097
mode = "completion"
output_cost_per_token = 0.000002

[models."azure/gpt-35-turbo-instruct-0914"]
input_cost_per_token = 0.0000015
litellm_provider = "azure_text"
max_input_tokens = 4097
max_tokens = 4097
mode = "completion"
output_cost_per_token = 0.000002

[models."azure/gpt-4"]
input_cost_per_token = 0.00003
litellm_provider = "azure"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_tool_choice = true

[models."azure/gpt-4-0125-preview"]
input_cost_per_token = 0.00001
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."azure/gpt-4-0613"]
input_cost_per_token = 0.00003
litellm_provider = "azure"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_tool_choice = true

[models."azure/gpt-4-1106-preview"]
input_cost_per_token = 0.00001
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."azure/gpt-4-32k"]
input_cost_per_token = 0.00006
litellm_provider = "azure"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00012
supports_tool_choice = true

[models."azure/gpt-4-32k-0613"]
input_cost_per_token = 0.00006
litellm_provider = "azure"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00012
supports_tool_choice = true

[models."azure/gpt-4-turbo"]
input_cost_per_token = 0.00001
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."azure/gpt-4-turbo-2024-04-09"]
input_cost_per_token = 0.00001
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-4-turbo-vision-preview"]
input_cost_per_token = 0.00001
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-4.1"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
litellm_provider = "azure"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000008
output_cost_per_token_batches = 0.000004
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = false

[models."azure/gpt-4.1-2025-04-14"]
cache_read_input_token_cost = 5e-7
deprecation_date = "2026-11-04"
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
litellm_provider = "azure"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000008
output_cost_per_token_batches = 0.000004
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = false

[models."azure/gpt-4.1-mini"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 4e-7
input_cost_per_token_batches = 2e-7
litellm_provider = "azure"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000016
output_cost_per_token_batches = 8e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = false

[models."azure/gpt-4.1-mini-2025-04-14"]
cache_read_input_token_cost = 1e-7
deprecation_date = "2026-11-04"
input_cost_per_token = 4e-7
input_cost_per_token_batches = 2e-7
litellm_provider = "azure"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000016
output_cost_per_token_batches = 8e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = false

[models."azure/gpt-4.1-nano"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 1e-7
input_cost_per_token_batches = 5e-8
litellm_provider = "azure"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 4e-7
output_cost_per_token_batches = 2e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-4.1-nano-2025-04-14"]
cache_read_input_token_cost = 2.5e-8
deprecation_date = "2026-11-04"
input_cost_per_token = 1e-7
input_cost_per_token_batches = 5e-8
litellm_provider = "azure"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 4e-7
output_cost_per_token_batches = 2e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-4.5-preview"]
cache_read_input_token_cost = 0.0000375
input_cost_per_token = 0.000075
input_cost_per_token_batches = 0.0000375
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00015
output_cost_per_token_batches = 0.000075
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-4o"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-4o-2024-05-13"]
input_cost_per_token = 0.000005
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-4o-2024-08-06"]
cache_read_input_token_cost = 0.00000125
deprecation_date = "2026-02-27"
input_cost_per_token = 0.0000025
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-4o-2024-11-20"]
cache_read_input_token_cost = 0.00000125
deprecation_date = "2026-03-01"
input_cost_per_token = 0.00000275
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.000011
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-4o-audio-preview-2024-12-17"]
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.0000025
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_prompt_caching = false
supports_reasoning = false
supports_response_schema = false
supports_system_messages = true
supports_tool_choice = true
supports_vision = false

[models."azure/gpt-4o-mini"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.65e-7
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 6.6e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-4o-mini-2024-07-18"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.65e-7
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 6.6e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-4o-mini-audio-preview-2024-12-17"]
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.0000025
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_prompt_caching = false
supports_reasoning = false
supports_response_schema = false
supports_system_messages = true
supports_tool_choice = true
supports_vision = false

[models."azure/gpt-4o-mini-realtime-preview-2024-12-17"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_token_cost = 3e-7
input_cost_per_audio_token = 0.00001
input_cost_per_token = 6e-7
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."azure/gpt-4o-mini-transcribe"]
input_cost_per_audio_token = 0.000003
input_cost_per_token = 0.00000125
litellm_provider = "azure"
max_input_tokens = 16000
max_output_tokens = 2000
mode = "audio_transcription"
output_cost_per_token = 0.000005
supported_endpoints = ["/v1/audio/transcriptions"]

[models."azure/gpt-4o-mini-tts"]
input_cost_per_token = 0.0000025
litellm_provider = "azure"
mode = "audio_speech"
output_cost_per_audio_token = 0.000012
output_cost_per_second = 0.00025
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/audio/speech"]
supported_modalities = ["text", "audio"]
supported_output_modalities = ["audio"]

[models."azure/gpt-4o-realtime-preview-2024-10-01"]
cache_creation_input_audio_token_cost = 0.00002
cache_read_input_token_cost = 0.0000025
input_cost_per_audio_token = 0.0001
input_cost_per_token = 0.000005
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.0002
output_cost_per_token = 0.00002
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."azure/gpt-4o-realtime-preview-2024-12-17"]
cache_read_input_token_cost = 0.0000025
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.000005
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00002
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."azure/gpt-4o-transcribe"]
input_cost_per_audio_token = 0.000006
input_cost_per_token = 0.0000025
litellm_provider = "azure"
max_input_tokens = 16000
max_output_tokens = 2000
mode = "audio_transcription"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/audio/transcriptions"]

[models."azure/gpt-4o-transcribe-diarize"]
input_cost_per_audio_token = 0.000006
input_cost_per_token = 0.0000025
litellm_provider = "azure"
max_input_tokens = 16000
max_output_tokens = 2000
mode = "audio_transcription"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/audio/transcriptions"]

[models."azure/gpt-5"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5-2025-08-07"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5-chat"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
source = "https://azure.microsoft.com/en-us/blog/gpt-5-in-azure-ai-foundry-the-future-of-ai-apps-and-agents-starts-here/"
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = false
supports_vision = true

[models."azure/gpt-5-chat-latest"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = false
supports_vision = true

[models."azure/gpt-5-codex"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5-mini"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 2.5e-7
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000002
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5-mini-2025-08-07"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 2.5e-7
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000002
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5-nano"]
cache_read_input_token_cost = 5e-9
input_cost_per_token = 5e-8
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 4e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5-nano-2025-08-07"]
cache_read_input_token_cost = 5e-9
input_cost_per_token = 5e-8
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 4e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5-pro"]
input_cost_per_token = 0.000015
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 400000
mode = "responses"
output_cost_per_token = 0.00012
source = "https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/concepts/models-sold-directly-by-azure?pivots=azure-openai&tabs=global-standard-aoai%2Cstandard-chat-completions%2Cglobal-standard#gpt-5"
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5.1"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5.1-2025-11-13"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5.1-chat"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5.1-chat-2025-11-13"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = false
supports_native_streaming = true
supports_parallel_function_calling = false
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = false
supports_vision = true

[models."azure/gpt-5.1-codex"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5.1-codex-2025-11-13"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5.1-codex-max"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "azure"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5.1-codex-mini"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 2.5e-7
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.000002
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5.1-codex-mini-2025-11-13"]
cache_read_input_token_cost = 2.5e-8
cache_read_input_token_cost_priority = 4.5e-8
input_cost_per_token = 2.5e-7
input_cost_per_token_priority = 4.5e-7
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.000002
output_cost_per_token_priority = 0.0000036
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5.2"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
litellm_provider = "azure"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000014
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5.2-2025-12-11"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
litellm_provider = "azure"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5.2-chat"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5.2-chat-2025-12-11"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/gpt-5.2-pro"]
input_cost_per_token = 0.000021
litellm_provider = "azure"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.000168
supported_endpoints = ["/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."azure/gpt-5.2-pro-2025-12-11"]
input_cost_per_token = 0.000021
litellm_provider = "azure"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.000168
supported_endpoints = ["/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."azure/gpt-audio-2025-08-28"]
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.0000025
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_prompt_caching = false
supports_reasoning = false
supports_response_schema = false
supports_system_messages = true
supports_tool_choice = true
supports_vision = false

[models."azure/gpt-audio-mini-2025-10-06"]
input_cost_per_audio_token = 0.00001
input_cost_per_token = 6e-7
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_prompt_caching = false
supports_reasoning = false
supports_response_schema = false
supports_system_messages = true
supports_tool_choice = true
supports_vision = false

[models."azure/gpt-image-1"]
input_cost_per_pixel = 4.0054321e-8
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/gpt-image-1-mini"]
input_cost_per_pixel = 8.0566406e-9
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/gpt-image-1.5"]
cache_read_input_image_token_cost = 0.000002
cache_read_input_token_cost = 0.00000125
input_cost_per_image_token = 0.000008
input_cost_per_token = 0.000005
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_image_token = 0.000032
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."azure/gpt-image-1.5-2025-12-16"]
cache_read_input_image_token_cost = 0.000002
cache_read_input_token_cost = 0.00000125
input_cost_per_image_token = 0.000008
input_cost_per_token = 0.000005
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_image_token = 0.000032
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."azure/gpt-realtime-2025-08-28"]
cache_creation_input_audio_token_cost = 0.000004
cache_read_input_token_cost = 0.000004
input_cost_per_audio_token = 0.000032
input_cost_per_image = 0.000005
input_cost_per_token = 0.000004
litellm_provider = "azure"
max_input_tokens = 32000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.000064
output_cost_per_token = 0.000016
supported_endpoints = ["/v1/realtime"]
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text", "audio"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."azure/gpt-realtime-mini-2025-10-06"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_token_cost = 6e-8
input_cost_per_audio_token = 0.00001
input_cost_per_image = 8e-7
input_cost_per_token = 6e-7
litellm_provider = "azure"
max_input_tokens = 32000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024
supported_endpoints = ["/v1/realtime"]
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text", "audio"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."azure/hd/1024-x-1024/dall-e-3"]
input_cost_per_pixel = 7.629e-8
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_token = 0

[models."azure/hd/1024-x-1792/dall-e-3"]
input_cost_per_pixel = 6.539e-8
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_token = 0

[models."azure/hd/1792-x-1024/dall-e-3"]
input_cost_per_pixel = 6.539e-8
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_token = 0

[models."azure/high/1024-x-1024/gpt-image-1"]
input_cost_per_pixel = 1.59263611e-7
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/high/1024-x-1024/gpt-image-1-mini"]
input_cost_per_pixel = 3.173828125e-8
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/high/1024-x-1536/gpt-image-1"]
input_cost_per_pixel = 1.58945719e-7
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/high/1024-x-1536/gpt-image-1-mini"]
input_cost_per_pixel = 3.173828125e-8
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/high/1536-x-1024/gpt-image-1"]
input_cost_per_pixel = 1.58945719e-7
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/high/1536-x-1024/gpt-image-1-mini"]
input_cost_per_pixel = 3.1575520833e-8
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/low/1024-x-1024/gpt-image-1"]
input_cost_per_pixel = 1.0490417e-8
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/low/1024-x-1024/gpt-image-1-mini"]
input_cost_per_pixel = 2.0751953125e-9
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/low/1024-x-1536/gpt-image-1"]
input_cost_per_pixel = 1.0172526e-8
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/low/1024-x-1536/gpt-image-1-mini"]
input_cost_per_pixel = 2.0751953125e-9
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/low/1536-x-1024/gpt-image-1"]
input_cost_per_pixel = 1.0172526e-8
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/low/1536-x-1024/gpt-image-1-mini"]
input_cost_per_pixel = 2.0345052083e-9
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/medium/1024-x-1024/gpt-image-1"]
input_cost_per_pixel = 4.0054321e-8
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/medium/1024-x-1024/gpt-image-1-mini"]
input_cost_per_pixel = 8.056640625e-9
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/medium/1024-x-1536/gpt-image-1"]
input_cost_per_pixel = 4.0054321e-8
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/medium/1024-x-1536/gpt-image-1-mini"]
input_cost_per_pixel = 8.056640625e-9
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/medium/1536-x-1024/gpt-image-1"]
input_cost_per_pixel = 4.0054321e-8
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/medium/1536-x-1024/gpt-image-1-mini"]
input_cost_per_pixel = 7.9752604167e-9
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."azure/mistral-large-2402"]
input_cost_per_token = 0.000008
litellm_provider = "azure"
max_input_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000024
supports_function_calling = true

[models."azure/mistral-large-latest"]
input_cost_per_token = 0.000008
litellm_provider = "azure"
max_input_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000024
supports_function_calling = true

[models."azure/o1"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true
supports_vision = true

[models."azure/o1-2024-12-17"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true
supports_vision = true

[models."azure/o1-mini"]
cache_read_input_token_cost = 6.05e-7
input_cost_per_token = 0.00000121
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.00000484
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_vision = false

[models."azure/o1-mini-2024-09-12"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.0000044
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_vision = false

[models."azure/o1-preview"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_vision = false

[models."azure/o1-preview-2024-09-12"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_vision = false

[models."azure/o3"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.000008
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = false
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/o3-2025-04-16"]
cache_read_input_token_cost = 5e-7
deprecation_date = "2026-04-16"
input_cost_per_token = 0.000002
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.000008
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = false
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/o3-deep-research"]
cache_read_input_token_cost = 0.0000025
input_cost_per_token = 0.00001
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "responses"
output_cost_per_token = 0.00004
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."azure/o3-mini"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.0000044
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = false

[models."azure/o3-mini-2025-01-31"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.0000044
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true
supports_vision = false

[models."azure/o3-pro"]
input_cost_per_token = 0.00002
input_cost_per_token_batches = 0.00001
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "responses"
output_cost_per_token = 0.00008
output_cost_per_token_batches = 0.00004
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = false
supports_prompt_caching = false
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/o3-pro-2025-06-10"]
input_cost_per_token = 0.00002
input_cost_per_token_batches = 0.00001
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "responses"
output_cost_per_token = 0.00008
output_cost_per_token_batches = 0.00004
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = false
supports_prompt_caching = false
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/o4-mini"]
cache_read_input_token_cost = 2.75e-7
input_cost_per_token = 0.0000011
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.0000044
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = false
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/o4-mini-2025-04-16"]
cache_read_input_token_cost = 2.75e-7
input_cost_per_token = 0.0000011
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.0000044
supports_function_calling = true
supports_parallel_function_calling = false
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/sora-2"]
litellm_provider = "azure"
mode = "video_generation"
output_cost_per_video_per_second = 0.1
source = "https://azure.microsoft.com/en-us/products/ai-services/video-generation"
supported_modalities = ["text"]
supported_output_modalities = ["video"]
supported_resolutions = ["720x1280", "1280x720"]

[models."azure/sora-2-pro"]
litellm_provider = "azure"
mode = "video_generation"
output_cost_per_video_per_second = 0.3
source = "https://azure.microsoft.com/en-us/products/ai-services/video-generation"
supported_modalities = ["text"]
supported_output_modalities = ["video"]
supported_resolutions = ["720x1280", "1280x720"]

[models."azure/sora-2-pro-high-res"]
litellm_provider = "azure"
mode = "video_generation"
output_cost_per_video_per_second = 0.5
source = "https://azure.microsoft.com/en-us/products/ai-services/video-generation"
supported_modalities = ["text"]
supported_output_modalities = ["video"]
supported_resolutions = ["1024x1792", "1792x1024"]

[models."azure/speech/azure-tts"]
input_cost_per_character = 0.000015
litellm_provider = "azure"
mode = "audio_speech"
source = "https://azure.microsoft.com/en-us/pricing/calculator/"

[models."azure/speech/azure-tts-hd"]
input_cost_per_character = 0.00003
litellm_provider = "azure"
mode = "audio_speech"
source = "https://azure.microsoft.com/en-us/pricing/calculator/"

[models."azure/standard/1024-x-1024/dall-e-2"]
input_cost_per_pixel = 0
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_token = 0

[models."azure/standard/1024-x-1024/dall-e-3"]
input_cost_per_pixel = 3.81469e-8
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_token = 0

[models."azure/standard/1024-x-1792/dall-e-3"]
input_cost_per_pixel = 4.359e-8
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_token = 0

[models."azure/standard/1792-x-1024/dall-e-3"]
input_cost_per_pixel = 4.359e-8
litellm_provider = "azure"
mode = "image_generation"
output_cost_per_token = 0

[models."azure/text-embedding-3-large"]
input_cost_per_token = 1.3e-7
litellm_provider = "azure"
max_input_tokens = 8191
max_tokens = 8191
mode = "embedding"
output_cost_per_token = 0

[models."azure/text-embedding-3-small"]
deprecation_date = "2026-04-30"
input_cost_per_token = 2e-8
litellm_provider = "azure"
max_input_tokens = 8191
max_tokens = 8191
mode = "embedding"
output_cost_per_token = 0

[models."azure/text-embedding-ada-002"]
input_cost_per_token = 1e-7
litellm_provider = "azure"
max_input_tokens = 8191
max_tokens = 8191
mode = "embedding"
output_cost_per_token = 0

[models."azure/tts-1"]
input_cost_per_character = 0.000015
litellm_provider = "azure"
mode = "audio_speech"

[models."azure/tts-1-hd"]
input_cost_per_character = 0.00003
litellm_provider = "azure"
mode = "audio_speech"

[models."azure/us/gpt-4.1-2025-04-14"]
cache_read_input_token_cost = 5.5e-7
deprecation_date = "2026-11-04"
input_cost_per_token = 0.0000022
input_cost_per_token_batches = 0.0000011
litellm_provider = "azure"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000088
output_cost_per_token_batches = 0.0000044
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = false

[models."azure/us/gpt-4.1-mini-2025-04-14"]
cache_read_input_token_cost = 1.1e-7
deprecation_date = "2026-11-04"
input_cost_per_token = 4.4e-7
input_cost_per_token_batches = 2.2e-7
litellm_provider = "azure"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.00000176
output_cost_per_token_batches = 8.8e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = false

[models."azure/us/gpt-4.1-nano-2025-04-14"]
cache_read_input_token_cost = 2.5e-8
deprecation_date = "2026-11-04"
input_cost_per_token = 1.1e-7
input_cost_per_token_batches = 6e-8
litellm_provider = "azure"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 4.4e-7
output_cost_per_token_batches = 2.2e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/us/gpt-4o-2024-08-06"]
cache_read_input_token_cost = 0.000001375
deprecation_date = "2026-02-27"
input_cost_per_token = 0.00000275
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.000011
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/us/gpt-4o-2024-11-20"]
cache_creation_input_token_cost = 0.00000138
deprecation_date = "2026-03-01"
input_cost_per_token = 0.00000275
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.000011
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/us/gpt-4o-mini-2024-07-18"]
cache_read_input_token_cost = 8.3e-8
input_cost_per_token = 1.65e-7
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 6.6e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/us/gpt-4o-mini-realtime-preview-2024-12-17"]
cache_creation_input_audio_token_cost = 3.3e-7
cache_read_input_token_cost = 3.3e-7
input_cost_per_audio_token = 0.000011
input_cost_per_token = 6.6e-7
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.000022
output_cost_per_token = 0.00000264
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."azure/us/gpt-4o-realtime-preview-2024-10-01"]
cache_creation_input_audio_token_cost = 0.000022
cache_read_input_token_cost = 0.00000275
input_cost_per_audio_token = 0.00011
input_cost_per_token = 0.0000055
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00022
output_cost_per_token = 0.000022
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."azure/us/gpt-4o-realtime-preview-2024-12-17"]
cache_read_input_audio_token_cost = 0.0000025
cache_read_input_token_cost = 0.00000275
input_cost_per_audio_token = 0.000044
input_cost_per_token = 0.0000055
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.000022
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."azure/us/gpt-5-2025-08-07"]
cache_read_input_token_cost = 1.375e-7
input_cost_per_token = 0.000001375
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000011
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/us/gpt-5-mini-2025-08-07"]
cache_read_input_token_cost = 2.75e-8
input_cost_per_token = 2.75e-7
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.0000022
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/us/gpt-5-nano-2025-08-07"]
cache_read_input_token_cost = 5.5e-9
input_cost_per_token = 5.5e-8
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 4.4e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/us/gpt-5.1"]
cache_read_input_token_cost = 1.4e-7
input_cost_per_token = 0.00000138
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000011
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/us/gpt-5.1-chat"]
cache_read_input_token_cost = 1.4e-7
input_cost_per_token = 0.00000138
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000011
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."azure/us/gpt-5.1-codex"]
cache_read_input_token_cost = 1.4e-7
input_cost_per_token = 0.00000138
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.000011
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."azure/us/gpt-5.1-codex-mini"]
cache_read_input_token_cost = 2.8e-8
input_cost_per_token = 2.75e-7
litellm_provider = "azure"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.0000022
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."azure/us/o1-2024-12-17"]
cache_read_input_token_cost = 0.00000825
input_cost_per_token = 0.0000165
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.000066
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_tool_choice = true
supports_vision = true

[models."azure/us/o1-mini-2024-09-12"]
cache_read_input_token_cost = 6.05e-7
input_cost_per_token = 0.00000121
input_cost_per_token_batches = 6.05e-7
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.00000484
output_cost_per_token_batches = 0.00000242
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_vision = false

[models."azure/us/o1-preview-2024-09-12"]
cache_read_input_token_cost = 0.00000825
input_cost_per_token = 0.0000165
litellm_provider = "azure"
max_input_tokens = 128000
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000066
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_vision = false

[models."azure/us/o3-2025-04-16"]
cache_read_input_token_cost = 5.5e-7
deprecation_date = "2026-04-16"
input_cost_per_token = 0.0000022
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.0000088
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = false
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/us/o3-mini-2025-01-31"]
cache_read_input_token_cost = 6.05e-7
input_cost_per_token = 0.00000121
input_cost_per_token_batches = 6.05e-7
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.00000484
output_cost_per_token_batches = 0.00000242
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true
supports_vision = false

[models."azure/us/o4-mini-2025-04-16"]
cache_read_input_token_cost = 3.1e-7
input_cost_per_token = 0.00000121
litellm_provider = "azure"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.00000484
supports_function_calling = true
supports_parallel_function_calling = false
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure/whisper-1"]
input_cost_per_second = 0.0001
litellm_provider = "azure"
mode = "audio_transcription"
output_cost_per_second = 0.0001

[models."azure_ai/Cohere-embed-v3-english"]
input_cost_per_token = 1e-7
litellm_provider = "azure_ai"
max_input_tokens = 512
max_tokens = 512
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 1024
source = "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/cohere.cohere-embed-v3-english-offer?tab=PlansAndPrice"
supports_embedding_image_input = true

[models."azure_ai/Cohere-embed-v3-multilingual"]
input_cost_per_token = 1e-7
litellm_provider = "azure_ai"
max_input_tokens = 512
max_tokens = 512
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 1024
source = "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/cohere.cohere-embed-v3-english-offer?tab=PlansAndPrice"
supports_embedding_image_input = true

[models."azure_ai/FLUX-1.1-pro"]
litellm_provider = "azure_ai"
mode = "image_generation"
output_cost_per_image = 0.04
source = "https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/black-forest-labs-flux-1-kontext-pro-and-flux1-1-pro-now-available-in-azure-ai-f/4434659"
supported_endpoints = ["/v1/images/generations"]

[models."azure_ai/FLUX.1-Kontext-pro"]
litellm_provider = "azure_ai"
mode = "image_generation"
output_cost_per_image = 0.04
source = "https://azuremarketplace.microsoft.com/pt-br/marketplace/apps/cohere.cohere-embed-4-offer?tab=PlansAndPrice"
supported_endpoints = ["/v1/images/generations"]

[models."azure_ai/Llama-3.2-11B-Vision-Instruct"]
input_cost_per_token = 3.7e-7
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 3.7e-7
source = "https://azuremarketplace.microsoft.com/en/marketplace/apps/metagenai.meta-llama-3-2-11b-vision-instruct-offer?tab=Overview"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."azure_ai/Llama-3.2-90B-Vision-Instruct"]
input_cost_per_token = 0.00000204
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 0.00000204
source = "https://azuremarketplace.microsoft.com/en/marketplace/apps/metagenai.meta-llama-3-2-90b-vision-instruct-offer?tab=Overview"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."azure_ai/Llama-3.3-70B-Instruct"]
input_cost_per_token = 7.1e-7
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 7.1e-7
source = "https://azuremarketplace.microsoft.com/en/marketplace/apps/metagenai.llama-3-3-70b-instruct-offer?tab=Overview"
supports_function_calling = true
supports_tool_choice = true

[models."azure_ai/Llama-4-Maverick-17B-128E-Instruct-FP8"]
input_cost_per_token = 0.00000141
litellm_provider = "azure_ai"
max_input_tokens = 1000000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 3.5e-7
source = "https://azure.microsoft.com/en-us/blog/introducing-the-llama-4-herd-in-azure-ai-foundry-and-azure-databricks/"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."azure_ai/Llama-4-Scout-17B-16E-Instruct"]
input_cost_per_token = 2e-7
litellm_provider = "azure_ai"
max_input_tokens = 10000000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 7.8e-7
source = "https://azure.microsoft.com/en-us/blog/introducing-the-llama-4-herd-in-azure-ai-foundry-and-azure-databricks/"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."azure_ai/MAI-DS-R1"]
input_cost_per_token = 0.00000135
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000054
source = "https://azure.microsoft.com/en-us/pricing/details/ai-foundry-models/microsoft/"
supports_reasoning = true
supports_tool_choice = true

[models."azure_ai/Meta-Llama-3-70B-Instruct"]
input_cost_per_token = 0.0000011
litellm_provider = "azure_ai"
max_input_tokens = 8192
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 3.7e-7
supports_tool_choice = true

[models."azure_ai/Meta-Llama-3.1-405B-Instruct"]
input_cost_per_token = 0.00000533
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 0.000016
source = "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/metagenai.meta-llama-3-1-405b-instruct-offer?tab=PlansAndPrice"
supports_tool_choice = true

[models."azure_ai/Meta-Llama-3.1-70B-Instruct"]
input_cost_per_token = 0.00000268
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 0.00000354
source = "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/metagenai.meta-llama-3-1-70b-instruct-offer?tab=PlansAndPrice"
supports_tool_choice = true

[models."azure_ai/Meta-Llama-3.1-8B-Instruct"]
input_cost_per_token = 3e-7
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 6.1e-7
source = "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/metagenai.meta-llama-3-1-8b-instruct-offer?tab=PlansAndPrice"
supports_tool_choice = true

[models."azure_ai/Phi-3-medium-128k-instruct"]
input_cost_per_token = 1.7e-7
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 6.8e-7
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true
supports_vision = false

[models."azure_ai/Phi-3-medium-4k-instruct"]
input_cost_per_token = 1.7e-7
litellm_provider = "azure_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 6.8e-7
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true
supports_vision = false

[models."azure_ai/Phi-3-mini-128k-instruct"]
input_cost_per_token = 1.3e-7
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 5.2e-7
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true
supports_vision = false

[models."azure_ai/Phi-3-mini-4k-instruct"]
input_cost_per_token = 1.3e-7
litellm_provider = "azure_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 5.2e-7
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true
supports_vision = false

[models."azure_ai/Phi-3-small-128k-instruct"]
input_cost_per_token = 1.5e-7
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 6e-7
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true
supports_vision = false

[models."azure_ai/Phi-3-small-8k-instruct"]
input_cost_per_token = 1.5e-7
litellm_provider = "azure_ai"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 6e-7
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true
supports_vision = false

[models."azure_ai/Phi-3.5-MoE-instruct"]
input_cost_per_token = 1.6e-7
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 6.4e-7
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true
supports_vision = false

[models."azure_ai/Phi-3.5-mini-instruct"]
input_cost_per_token = 1.3e-7
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 5.2e-7
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true
supports_vision = false

[models."azure_ai/Phi-3.5-vision-instruct"]
input_cost_per_token = 1.3e-7
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 5.2e-7
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true
supports_vision = true

[models."azure_ai/Phi-4"]
input_cost_per_token = 1.25e-7
litellm_provider = "azure_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 5e-7
source = "https://techcommunity.microsoft.com/blog/machinelearningblog/affordable-innovation-unveiling-the-pricing-of-phi-3-slms-on-models-as-a-service/4156495"
supports_function_calling = true
supports_tool_choice = true
supports_vision = false

[models."azure_ai/Phi-4-mini-instruct"]
input_cost_per_token = 7.5e-8
litellm_provider = "azure_ai"
max_input_tokens = 131072
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 3e-7
source = "https://techcommunity.microsoft.com/blog/Azure-AI-Services-blog/announcing-new-phi-pricing-empowering-your-business-with-small-language-models/4395112"
supports_function_calling = true

[models."azure_ai/Phi-4-mini-reasoning"]
input_cost_per_token = 8e-8
litellm_provider = "azure_ai"
max_input_tokens = 131072
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 3.2e-7
source = "https://azure.microsoft.com/en-us/pricing/details/ai-foundry-models/microsoft/"
supports_function_calling = true

[models."azure_ai/Phi-4-multimodal-instruct"]
input_cost_per_audio_token = 0.000004
input_cost_per_token = 8e-8
litellm_provider = "azure_ai"
max_input_tokens = 131072
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 3.2e-7
source = "https://techcommunity.microsoft.com/blog/Azure-AI-Services-blog/announcing-new-phi-pricing-empowering-your-business-with-small-language-models/4395112"
supports_audio_input = true
supports_function_calling = true
supports_vision = true

[models."azure_ai/Phi-4-reasoning"]
input_cost_per_token = 1.25e-7
litellm_provider = "azure_ai"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 5e-7
source = "https://azure.microsoft.com/en-us/pricing/details/ai-foundry-models/microsoft/"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."azure_ai/claude-haiku-4-5"]
input_cost_per_token = 0.000001
litellm_provider = "azure_ai"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000005
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure_ai/claude-opus-4-1"]
input_cost_per_token = 0.000015
litellm_provider = "azure_ai"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure_ai/claude-sonnet-4-5"]
input_cost_per_token = 0.000003
litellm_provider = "azure_ai"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."azure_ai/cohere-rerank-v3-english"]
input_cost_per_query = 0.002
input_cost_per_token = 0
litellm_provider = "azure_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_query_tokens = 2048
max_tokens = 4096
mode = "rerank"
output_cost_per_token = 0

[models."azure_ai/cohere-rerank-v3-multilingual"]
input_cost_per_query = 0.002
input_cost_per_token = 0
litellm_provider = "azure_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_query_tokens = 2048
max_tokens = 4096
mode = "rerank"
output_cost_per_token = 0

[models."azure_ai/cohere-rerank-v3.5"]
input_cost_per_query = 0.002
input_cost_per_token = 0
litellm_provider = "azure_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_query_tokens = 2048
max_tokens = 4096
mode = "rerank"
output_cost_per_token = 0

[models."azure_ai/cohere-rerank-v4.0-fast"]
input_cost_per_query = 0.002
input_cost_per_token = 0
litellm_provider = "azure_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_query_tokens = 4096
max_tokens = 32768
mode = "rerank"
output_cost_per_token = 0

[models."azure_ai/cohere-rerank-v4.0-pro"]
input_cost_per_query = 0.0025
input_cost_per_token = 0
litellm_provider = "azure_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_query_tokens = 4096
max_tokens = 32768
mode = "rerank"
output_cost_per_token = 0

[models."azure_ai/deepseek-r1"]
input_cost_per_token = 0.00000135
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000054
source = "https://techcommunity.microsoft.com/blog/machinelearningblog/deepseek-r1-improved-performance-higher-limits-and-transparent-pricing/4386367"
supports_reasoning = true
supports_tool_choice = true

[models."azure_ai/deepseek-v3"]
input_cost_per_token = 0.00000114
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000456
source = "https://techcommunity.microsoft.com/blog/machinelearningblog/announcing-deepseek-v3-on-azure-ai-foundry-and-github/4390438"
supports_tool_choice = true

[models."azure_ai/deepseek-v3-0324"]
input_cost_per_token = 0.00000114
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000456
source = "https://techcommunity.microsoft.com/blog/machinelearningblog/announcing-deepseek-v3-on-azure-ai-foundry-and-github/4390438"
supports_function_calling = true
supports_tool_choice = true

[models."azure_ai/deepseek-v3.2"]
input_cost_per_token = 5.8e-7
litellm_provider = "azure_ai"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000168
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true

[models."azure_ai/deepseek-v3.2-speciale"]
input_cost_per_token = 5.8e-7
litellm_provider = "azure_ai"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000168
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true

[models."azure_ai/doc-intelligence/prebuilt-document"]
litellm_provider = "azure_ai"
mode = "ocr"
ocr_cost_per_page = 0.01
source = "https://azure.microsoft.com/en-us/pricing/details/ai-document-intelligence/"
supported_endpoints = ["/v1/ocr"]

[models."azure_ai/doc-intelligence/prebuilt-layout"]
litellm_provider = "azure_ai"
mode = "ocr"
ocr_cost_per_page = 0.01
source = "https://azure.microsoft.com/en-us/pricing/details/ai-document-intelligence/"
supported_endpoints = ["/v1/ocr"]

[models."azure_ai/doc-intelligence/prebuilt-read"]
litellm_provider = "azure_ai"
mode = "ocr"
ocr_cost_per_page = 0.0015
source = "https://azure.microsoft.com/en-us/pricing/details/ai-document-intelligence/"
supported_endpoints = ["/v1/ocr"]

[models."azure_ai/embed-v-4-0"]
input_cost_per_token = 1.2e-7
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_tokens = 128000
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 3072
source = "https://azuremarketplace.microsoft.com/pt-br/marketplace/apps/cohere.cohere-embed-4-offer?tab=PlansAndPrice"
supported_endpoints = ["/v1/embeddings"]
supported_modalities = ["text", "image"]
supports_embedding_image_input = true

[models."azure_ai/global/grok-3"]
input_cost_per_token = 0.000003
litellm_provider = "azure_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000015
source = "https://devblogs.microsoft.com/foundry/announcing-grok-3-and-grok-3-mini-on-azure-ai-foundry/"
supports_function_calling = true
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."azure_ai/global/grok-3-mini"]
input_cost_per_token = 2.5e-7
litellm_provider = "azure_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.00000127
source = "https://devblogs.microsoft.com/foundry/announcing-grok-3-and-grok-3-mini-on-azure-ai-foundry/"
supports_function_calling = true
supports_reasoning = true
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."azure_ai/gpt-oss-120b"]
input_cost_per_token = 1.5e-7
litellm_provider = "azure_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 6e-7
source = "https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."azure_ai/grok-3"]
input_cost_per_token = 0.0000033
litellm_provider = "azure_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000165
source = "https://devblogs.microsoft.com/foundry/announcing-grok-3-and-grok-3-mini-on-azure-ai-foundry/"
supports_function_calling = true
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."azure_ai/grok-3-mini"]
input_cost_per_token = 2.75e-7
litellm_provider = "azure_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.00000138
source = "https://devblogs.microsoft.com/foundry/announcing-grok-3-and-grok-3-mini-on-azure-ai-foundry/"
supports_function_calling = true
supports_reasoning = true
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."azure_ai/grok-4"]
input_cost_per_token = 0.0000055
litellm_provider = "azure_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000275
source = "https://azure.microsoft.com/en-us/blog/grok-4-is-now-available-in-azure-ai-foundry-unlock-frontier-intelligence-and-business-ready-capabilities/"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."azure_ai/grok-4-fast-non-reasoning"]
input_cost_per_token = 4.3e-7
litellm_provider = "azure_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.00000173
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."azure_ai/grok-4-fast-reasoning"]
input_cost_per_token = 4.3e-7
litellm_provider = "azure_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.00000173
source = "https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/announcing-the-grok-4-fast-models-from-xai-now-available-in-azure-ai-foundry/4456701"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."azure_ai/grok-code-fast-1"]
input_cost_per_token = 0.0000035
litellm_provider = "azure_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000175
source = "https://azure.microsoft.com/en-us/blog/grok-4-is-now-available-in-azure-ai-foundry-unlock-frontier-intelligence-and-business-ready-capabilities/"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."azure_ai/jais-30b-chat"]
input_cost_per_token = 0.0032
litellm_provider = "azure_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00971
source = "https://azure.microsoft.com/en-us/products/ai-services/ai-foundry/models/jais-30b-chat"

[models."azure_ai/jamba-instruct"]
input_cost_per_token = 5e-7
litellm_provider = "azure_ai"
max_input_tokens = 70000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 7e-7
supports_tool_choice = true

[models."azure_ai/ministral-3b"]
input_cost_per_token = 4e-8
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 4e-8
source = "https://azuremarketplace.microsoft.com/en/marketplace/apps/000-000.ministral-3b-2410-offer?tab=Overview"
supports_function_calling = true
supports_tool_choice = true

[models."azure_ai/mistral-document-ai-2505"]
litellm_provider = "azure_ai"
mode = "ocr"
ocr_cost_per_page = 0.003
source = "https://devblogs.microsoft.com/foundry/whats-new-in-azure-ai-foundry-august-2025/#mistral-document-ai-(ocr)-%E2%80%94-serverless-in-foundry"
supported_endpoints = ["/v1/ocr"]

[models."azure_ai/mistral-large"]
input_cost_per_token = 0.000004
litellm_provider = "azure_ai"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000012
supports_function_calling = true
supports_tool_choice = true

[models."azure_ai/mistral-large-2407"]
input_cost_per_token = 0.000002
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000006
source = "https://azuremarketplace.microsoft.com/en/marketplace/apps/000-000.mistral-ai-large-2407-offer?tab=Overview"
supports_function_calling = true
supports_tool_choice = true

[models."azure_ai/mistral-large-3"]
input_cost_per_token = 5e-7
litellm_provider = "azure_ai"
max_input_tokens = 256000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.0000015
source = "https://azure.microsoft.com/en-us/blog/introducing-mistral-large-3-in-microsoft-foundry-open-capable-and-ready-for-production-workloads/"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."azure_ai/mistral-large-latest"]
input_cost_per_token = 0.000002
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000006
source = "https://azuremarketplace.microsoft.com/en/marketplace/apps/000-000.mistral-ai-large-2407-offer?tab=Overview"
supports_function_calling = true
supports_tool_choice = true

[models."azure_ai/mistral-medium-2505"]
input_cost_per_token = 4e-7
litellm_provider = "azure_ai"
max_input_tokens = 131072
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000002
supports_assistant_prefill = true
supports_function_calling = true
supports_tool_choice = true

[models."azure_ai/mistral-nemo"]
input_cost_per_token = 1.5e-7
litellm_provider = "azure_ai"
max_input_tokens = 131072
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 1.5e-7
source = "https://azuremarketplace.microsoft.com/en/marketplace/apps/000-000.mistral-nemo-12b-2407?tab=PlansAndPrice"
supports_function_calling = true

[models."azure_ai/mistral-small"]
input_cost_per_token = 0.000001
litellm_provider = "azure_ai"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000003
supports_function_calling = true
supports_tool_choice = true

[models."azure_ai/mistral-small-2503"]
input_cost_per_token = 0.000001
litellm_provider = "azure_ai"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000003
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."babbage-002"]
input_cost_per_token = 4e-7
litellm_provider = "text-completion-openai"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 16384
mode = "completion"
output_cost_per_token = 4e-7

[models."bedrock/*/1-month-commitment/cohere.command-light-text-v14"]
input_cost_per_second = 0.001902
litellm_provider = "bedrock"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_second = 0.001902
supports_tool_choice = true

[models."bedrock/*/1-month-commitment/cohere.command-text-v14"]
input_cost_per_second = 0.011
litellm_provider = "bedrock"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_second = 0.011
supports_tool_choice = true

[models."bedrock/*/6-month-commitment/cohere.command-light-text-v14"]
input_cost_per_second = 0.0011416
litellm_provider = "bedrock"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_second = 0.0011416
supports_tool_choice = true

[models."bedrock/*/6-month-commitment/cohere.command-text-v14"]
input_cost_per_second = 0.0066027
litellm_provider = "bedrock"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_second = 0.0066027
supports_tool_choice = true

[models."bedrock/ap-northeast-1/1-month-commitment/anthropic.claude-instant-v1"]
input_cost_per_second = 0.01475
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.01475
supports_tool_choice = true

[models."bedrock/ap-northeast-1/1-month-commitment/anthropic.claude-v1"]
input_cost_per_second = 0.0455
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.0455

[models."bedrock/ap-northeast-1/1-month-commitment/anthropic.claude-v2:1"]
input_cost_per_second = 0.0455
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.0455
supports_tool_choice = true

[models."bedrock/ap-northeast-1/6-month-commitment/anthropic.claude-instant-v1"]
input_cost_per_second = 0.008194
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.008194
supports_tool_choice = true

[models."bedrock/ap-northeast-1/6-month-commitment/anthropic.claude-v1"]
input_cost_per_second = 0.02527
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.02527

[models."bedrock/ap-northeast-1/6-month-commitment/anthropic.claude-v2:1"]
input_cost_per_second = 0.02527
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.02527
supports_tool_choice = true

[models."bedrock/ap-northeast-1/anthropic.claude-instant-v1"]
input_cost_per_token = 0.00000223
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.00000755
supports_tool_choice = true

[models."bedrock/ap-northeast-1/anthropic.claude-v1"]
input_cost_per_token = 0.000008
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000024
supports_tool_choice = true

[models."bedrock/ap-northeast-1/anthropic.claude-v2:1"]
input_cost_per_token = 0.000008
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000024
supports_tool_choice = true

[models."bedrock/ap-south-1/meta.llama3-70b-instruct-v1:0"]
input_cost_per_token = 0.00000318
litellm_provider = "bedrock"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000042

[models."bedrock/ap-south-1/meta.llama3-8b-instruct-v1:0"]
input_cost_per_token = 3.6e-7
litellm_provider = "bedrock"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 7.2e-7

[models."bedrock/ca-central-1/meta.llama3-70b-instruct-v1:0"]
input_cost_per_token = 0.00000305
litellm_provider = "bedrock"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000403

[models."bedrock/ca-central-1/meta.llama3-8b-instruct-v1:0"]
input_cost_per_token = 3.5e-7
litellm_provider = "bedrock"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 6.9e-7

[models."bedrock/eu-central-1/1-month-commitment/anthropic.claude-instant-v1"]
input_cost_per_second = 0.01635
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.01635
supports_tool_choice = true

[models."bedrock/eu-central-1/1-month-commitment/anthropic.claude-v1"]
input_cost_per_second = 0.0415
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.0415

[models."bedrock/eu-central-1/1-month-commitment/anthropic.claude-v2:1"]
input_cost_per_second = 0.0415
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.0415
supports_tool_choice = true

[models."bedrock/eu-central-1/6-month-commitment/anthropic.claude-instant-v1"]
input_cost_per_second = 0.009083
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.009083
supports_tool_choice = true

[models."bedrock/eu-central-1/6-month-commitment/anthropic.claude-v1"]
input_cost_per_second = 0.02305
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.02305

[models."bedrock/eu-central-1/6-month-commitment/anthropic.claude-v2:1"]
input_cost_per_second = 0.02305
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.02305
supports_tool_choice = true

[models."bedrock/eu-central-1/anthropic.claude-instant-v1"]
input_cost_per_token = 0.00000248
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.00000838
supports_tool_choice = true

[models."bedrock/eu-central-1/anthropic.claude-v1"]
input_cost_per_token = 0.000008
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000024

[models."bedrock/eu-central-1/anthropic.claude-v2:1"]
input_cost_per_token = 0.000008
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000024
supports_tool_choice = true

[models."bedrock/eu-west-1/meta.llama3-70b-instruct-v1:0"]
input_cost_per_token = 0.00000286
litellm_provider = "bedrock"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000378

[models."bedrock/eu-west-1/meta.llama3-8b-instruct-v1:0"]
input_cost_per_token = 3.2e-7
litellm_provider = "bedrock"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 6.5e-7

[models."bedrock/eu-west-2/meta.llama3-70b-instruct-v1:0"]
input_cost_per_token = 0.00000345
litellm_provider = "bedrock"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000455

[models."bedrock/eu-west-2/meta.llama3-8b-instruct-v1:0"]
input_cost_per_token = 3.9e-7
litellm_provider = "bedrock"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 7.8e-7

[models."bedrock/eu-west-3/mistral.mistral-7b-instruct-v0:2"]
input_cost_per_token = 2e-7
litellm_provider = "bedrock"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 2.6e-7
supports_tool_choice = true

[models."bedrock/eu-west-3/mistral.mistral-large-2402-v1:0"]
input_cost_per_token = 0.0000104
litellm_provider = "bedrock"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.0000312
supports_function_calling = true

[models."bedrock/eu-west-3/mistral.mixtral-8x7b-instruct-v0:1"]
input_cost_per_token = 5.9e-7
litellm_provider = "bedrock"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 9.1e-7
supports_tool_choice = true

[models."bedrock/invoke/anthropic.claude-3-5-sonnet-20240620-v1:0"]
input_cost_per_token = 0.000003
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."bedrock/invoke/anthropic.claude-3-5-sonnet-20240620-v1:0".metadata]
notes = "Anthropic via Invoke route does not currently support pdf input."

[models."bedrock/sa-east-1/meta.llama3-70b-instruct-v1:0"]
input_cost_per_token = 0.00000445
litellm_provider = "bedrock"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000588

[models."bedrock/sa-east-1/meta.llama3-8b-instruct-v1:0"]
input_cost_per_token = 5e-7
litellm_provider = "bedrock"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000101

[models."bedrock/us-east-1/1-month-commitment/anthropic.claude-instant-v1"]
input_cost_per_second = 0.011
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.011
supports_tool_choice = true

[models."bedrock/us-east-1/1-month-commitment/anthropic.claude-v1"]
input_cost_per_second = 0.0175
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.0175

[models."bedrock/us-east-1/1-month-commitment/anthropic.claude-v2:1"]
input_cost_per_second = 0.0175
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.0175
supports_tool_choice = true

[models."bedrock/us-east-1/6-month-commitment/anthropic.claude-instant-v1"]
input_cost_per_second = 0.00611
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.00611
supports_tool_choice = true

[models."bedrock/us-east-1/6-month-commitment/anthropic.claude-v1"]
input_cost_per_second = 0.00972
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.00972

[models."bedrock/us-east-1/6-month-commitment/anthropic.claude-v2:1"]
input_cost_per_second = 0.00972
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.00972
supports_tool_choice = true

[models."bedrock/us-east-1/anthropic.claude-instant-v1"]
input_cost_per_token = 8e-7
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.0000024
supports_tool_choice = true

[models."bedrock/us-east-1/anthropic.claude-v1"]
input_cost_per_token = 0.000008
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000024
supports_tool_choice = true

[models."bedrock/us-east-1/anthropic.claude-v2:1"]
input_cost_per_token = 0.000008
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000024
supports_tool_choice = true

[models."bedrock/us-east-1/meta.llama3-70b-instruct-v1:0"]
input_cost_per_token = 0.00000265
litellm_provider = "bedrock"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000035

[models."bedrock/us-east-1/meta.llama3-8b-instruct-v1:0"]
input_cost_per_token = 3e-7
litellm_provider = "bedrock"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 6e-7

[models."bedrock/us-east-1/mistral.mistral-7b-instruct-v0:2"]
input_cost_per_token = 1.5e-7
litellm_provider = "bedrock"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 2e-7
supports_tool_choice = true

[models."bedrock/us-east-1/mistral.mistral-large-2402-v1:0"]
input_cost_per_token = 0.000008
litellm_provider = "bedrock"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000024
supports_function_calling = true

[models."bedrock/us-east-1/mistral.mixtral-8x7b-instruct-v0:1"]
input_cost_per_token = 4.5e-7
litellm_provider = "bedrock"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 7e-7
supports_tool_choice = true

[models."bedrock/us-gov-east-1/amazon.nova-pro-v1:0"]
input_cost_per_token = 9.6e-7
litellm_provider = "bedrock"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 0.00000384
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_vision = true

[models."bedrock/us-gov-east-1/amazon.titan-embed-text-v1"]
input_cost_per_token = 1e-7
litellm_provider = "bedrock"
max_input_tokens = 8192
max_tokens = 8192
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 1536

[models."bedrock/us-gov-east-1/amazon.titan-embed-text-v2:0"]
input_cost_per_token = 2e-7
litellm_provider = "bedrock"
max_input_tokens = 8192
max_tokens = 8192
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 1024

[models."bedrock/us-gov-east-1/amazon.titan-text-express-v1"]
input_cost_per_token = 0.0000013
litellm_provider = "bedrock"
max_input_tokens = 42000
max_output_tokens = 8000
max_tokens = 8000
mode = "chat"
output_cost_per_token = 0.0000017

[models."bedrock/us-gov-east-1/amazon.titan-text-lite-v1"]
input_cost_per_token = 3e-7
litellm_provider = "bedrock"
max_input_tokens = 42000
max_output_tokens = 4000
max_tokens = 4000
mode = "chat"
output_cost_per_token = 4e-7

[models."bedrock/us-gov-east-1/amazon.titan-text-premier-v1:0"]
input_cost_per_token = 5e-7
litellm_provider = "bedrock"
max_input_tokens = 42000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.0000015

[models."bedrock/us-gov-east-1/anthropic.claude-3-5-sonnet-20240620-v1:0"]
input_cost_per_token = 0.0000036
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000018
supports_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."bedrock/us-gov-east-1/anthropic.claude-3-haiku-20240307-v1:0"]
input_cost_per_token = 3e-7
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0000015
supports_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."bedrock/us-gov-east-1/claude-sonnet-4-5-20250929-v1:0"]
input_cost_per_token = 0.0000033
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0000165
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."bedrock/us-gov-east-1/meta.llama3-70b-instruct-v1:0"]
input_cost_per_token = 0.00000265
litellm_provider = "bedrock"
max_input_tokens = 8000
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 0.0000035
supports_pdf_input = true

[models."bedrock/us-gov-east-1/meta.llama3-8b-instruct-v1:0"]
input_cost_per_token = 3e-7
litellm_provider = "bedrock"
max_input_tokens = 8000
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 0.00000265
supports_pdf_input = true

[models."bedrock/us-gov-west-1/amazon.nova-pro-v1:0"]
input_cost_per_token = 9.6e-7
litellm_provider = "bedrock"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 0.00000384
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_vision = true

[models."bedrock/us-gov-west-1/amazon.titan-embed-text-v1"]
input_cost_per_token = 1e-7
litellm_provider = "bedrock"
max_input_tokens = 8192
max_tokens = 8192
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 1536

[models."bedrock/us-gov-west-1/amazon.titan-embed-text-v2:0"]
input_cost_per_token = 2e-7
litellm_provider = "bedrock"
max_input_tokens = 8192
max_tokens = 8192
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 1024

[models."bedrock/us-gov-west-1/amazon.titan-text-express-v1"]
input_cost_per_token = 0.0000013
litellm_provider = "bedrock"
max_input_tokens = 42000
max_output_tokens = 8000
max_tokens = 8000
mode = "chat"
output_cost_per_token = 0.0000017

[models."bedrock/us-gov-west-1/amazon.titan-text-lite-v1"]
input_cost_per_token = 3e-7
litellm_provider = "bedrock"
max_input_tokens = 42000
max_output_tokens = 4000
max_tokens = 4000
mode = "chat"
output_cost_per_token = 4e-7

[models."bedrock/us-gov-west-1/amazon.titan-text-premier-v1:0"]
input_cost_per_token = 5e-7
litellm_provider = "bedrock"
max_input_tokens = 42000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.0000015

[models."bedrock/us-gov-west-1/anthropic.claude-3-5-sonnet-20240620-v1:0"]
input_cost_per_token = 0.0000036
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000018
supports_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."bedrock/us-gov-west-1/anthropic.claude-3-7-sonnet-20250219-v1:0"]
cache_creation_input_token_cost = 0.0000045
cache_read_input_token_cost = 3.6e-7
input_cost_per_token = 0.0000036
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000018
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."bedrock/us-gov-west-1/anthropic.claude-3-haiku-20240307-v1:0"]
input_cost_per_token = 3e-7
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0000015
supports_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."bedrock/us-gov-west-1/claude-sonnet-4-5-20250929-v1:0"]
input_cost_per_token = 0.0000033
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0000165
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."bedrock/us-gov-west-1/meta.llama3-70b-instruct-v1:0"]
input_cost_per_token = 0.00000265
litellm_provider = "bedrock"
max_input_tokens = 8000
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 0.0000035
supports_pdf_input = true

[models."bedrock/us-gov-west-1/meta.llama3-8b-instruct-v1:0"]
input_cost_per_token = 3e-7
litellm_provider = "bedrock"
max_input_tokens = 8000
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 0.00000265
supports_pdf_input = true

[models."bedrock/us-west-1/meta.llama3-70b-instruct-v1:0"]
input_cost_per_token = 0.00000265
litellm_provider = "bedrock"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000035

[models."bedrock/us-west-1/meta.llama3-8b-instruct-v1:0"]
input_cost_per_token = 3e-7
litellm_provider = "bedrock"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 6e-7

[models."bedrock/us-west-2/1-month-commitment/anthropic.claude-instant-v1"]
input_cost_per_second = 0.011
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.011
supports_tool_choice = true

[models."bedrock/us-west-2/1-month-commitment/anthropic.claude-v1"]
input_cost_per_second = 0.0175
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.0175

[models."bedrock/us-west-2/1-month-commitment/anthropic.claude-v2:1"]
input_cost_per_second = 0.0175
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.0175
supports_tool_choice = true

[models."bedrock/us-west-2/6-month-commitment/anthropic.claude-instant-v1"]
input_cost_per_second = 0.00611
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.00611
supports_tool_choice = true

[models."bedrock/us-west-2/6-month-commitment/anthropic.claude-v1"]
input_cost_per_second = 0.00972
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.00972

[models."bedrock/us-west-2/6-month-commitment/anthropic.claude-v2:1"]
input_cost_per_second = 0.00972
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_second = 0.00972
supports_tool_choice = true

[models."bedrock/us-west-2/anthropic.claude-instant-v1"]
input_cost_per_token = 8e-7
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.0000024
supports_tool_choice = true

[models."bedrock/us-west-2/anthropic.claude-v1"]
input_cost_per_token = 0.000008
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000024
supports_tool_choice = true

[models."bedrock/us-west-2/anthropic.claude-v2:1"]
input_cost_per_token = 0.000008
litellm_provider = "bedrock"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000024
supports_tool_choice = true

[models."bedrock/us-west-2/mistral.mistral-7b-instruct-v0:2"]
input_cost_per_token = 1.5e-7
litellm_provider = "bedrock"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 2e-7
supports_tool_choice = true

[models."bedrock/us-west-2/mistral.mistral-large-2402-v1:0"]
input_cost_per_token = 0.000008
litellm_provider = "bedrock"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000024
supports_function_calling = true

[models."bedrock/us-west-2/mistral.mixtral-8x7b-instruct-v0:1"]
input_cost_per_token = 4.5e-7
litellm_provider = "bedrock"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 7e-7
supports_tool_choice = true

[models."bedrock/us.anthropic.claude-3-5-haiku-20241022-v1:0"]
cache_creation_input_token_cost = 0.000001
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8e-7
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000004
supports_assistant_prefill = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true

[models."cerebras/gpt-oss-120b"]
input_cost_per_token = 2.5e-7
litellm_provider = "cerebras"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 6.9e-7
source = "https://www.cerebras.ai/blog/openai-gpt-oss-120b-runs-fastest-on-cerebras"
supports_function_calling = true
supports_parallel_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."cerebras/llama-3.3-70b"]
input_cost_per_token = 8.5e-7
litellm_provider = "cerebras"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.0000012
supports_function_calling = true
supports_tool_choice = true

[models."cerebras/llama3.1-70b"]
input_cost_per_token = 6e-7
litellm_provider = "cerebras"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_tool_choice = true

[models."cerebras/llama3.1-8b"]
input_cost_per_token = 1e-7
litellm_provider = "cerebras"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1e-7
supports_function_calling = true
supports_tool_choice = true

[models."cerebras/qwen-3-32b"]
input_cost_per_token = 4e-7
litellm_provider = "cerebras"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 8e-7
source = "https://inference-docs.cerebras.ai/support/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."cerebras/zai-glm-4.6"]
input_cost_per_token = 0.00000225
litellm_provider = "cerebras"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00000275
source = "https://www.cerebras.ai/pricing"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."chat-bison"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-chat-models"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_tool_choice = true

[models."chat-bison-32k"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-chat-models"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_tool_choice = true

[models."chat-bison-32k@002"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-chat-models"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_tool_choice = true

[models."chat-bison@001"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-chat-models"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_tool_choice = true

[models."chat-bison@002"]
deprecation_date = "2025-04-09"
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-chat-models"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_tool_choice = true

[models."chatdolphin"]
input_cost_per_token = 5e-7
litellm_provider = "nlp_cloud"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 5e-7

[models."chatgpt-4o-latest"]
input_cost_per_token = 0.000005
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."claude-3-5-haiku-20241022"]
cache_creation_input_token_cost = 0.000001
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 8e-8
deprecation_date = "2025-10-01"
input_cost_per_token = 8e-7
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000004
supports_assistant_prefill = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tool_use_system_prompt_tokens = 264

[models."claude-3-5-haiku-20241022".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-5-haiku-latest"]
cache_creation_input_token_cost = 0.00000125
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 1e-7
deprecation_date = "2025-10-01"
input_cost_per_token = 0.000001
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000005
supports_assistant_prefill = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tool_use_system_prompt_tokens = 264

[models."claude-3-5-haiku-latest".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-5-sonnet-20240620"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
deprecation_date = "2025-06-01"
input_cost_per_token = 0.000003
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-3-5-sonnet-20241022"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
deprecation_date = "2025-10-01"
input_cost_per_token = 0.000003
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tool_use_system_prompt_tokens = 159

[models."claude-3-5-sonnet-20241022".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-5-sonnet-latest"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
deprecation_date = "2025-06-01"
input_cost_per_token = 0.000003
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tool_use_system_prompt_tokens = 159

[models."claude-3-5-sonnet-latest".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-7-sonnet-20250219"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
deprecation_date = "2026-02-19"
input_cost_per_token = 0.000003
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tool_use_system_prompt_tokens = 159

[models."claude-3-7-sonnet-20250219".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-7-sonnet-latest"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
deprecation_date = "2025-06-01"
input_cost_per_token = 0.000003
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-3-7-sonnet-latest".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-haiku-20240307"]
cache_creation_input_token_cost = 3e-7
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00000125
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 264

[models."claude-3-opus-20240229"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 0.0000015
deprecation_date = "2026-05-01"
input_cost_per_token = 0.000015
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 395

[models."claude-3-opus-latest"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 0.0000015
deprecation_date = "2025-03-01"
input_cost_per_token = 0.000015
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 395

[models."claude-4-opus-20250514"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-4-opus-20250514".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-4-sonnet-20250514"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "anthropic"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 1000000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-4-sonnet-20250514".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-haiku-4-5"]
cache_creation_input_token_cost = 0.00000125
cache_creation_input_token_cost_above_1hr = 0.000002
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000005
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."claude-haiku-4-5-20251001"]
cache_creation_input_token_cost = 0.00000125
cache_creation_input_token_cost_above_1hr = 0.000002
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000005
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."claude-opus-4-1"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.00003
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4-1".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-opus-4-1-20250805"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.00003
cache_read_input_token_cost = 0.0000015
deprecation_date = "2026-08-05"
input_cost_per_token = 0.000015
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4-1-20250805".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-opus-4-20250514"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.00003
cache_read_input_token_cost = 0.0000015
deprecation_date = "2026-05-14"
input_cost_per_token = 0.000015
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4-20250514".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-opus-4-5"]
cache_creation_input_token_cost = 0.00000625
cache_creation_input_token_cost_above_1hr = 0.00001
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000025
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4-5".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-opus-4-5-20251101"]
cache_creation_input_token_cost = 0.00000625
cache_creation_input_token_cost_above_1hr = 0.00001
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000025
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4-5-20251101".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-sonnet-4-20250514"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
deprecation_date = "2026-05-14"
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "anthropic"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."claude-sonnet-4-20250514".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-sonnet-4-5"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 346

[models."claude-sonnet-4-5".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-sonnet-4-5-20250929"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "anthropic"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tool_use_system_prompt_tokens = 346

[models."claude-sonnet-4-5-20250929".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-sonnet-4-5-20250929-v1:0"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."cloudflare/@cf/meta/llama-2-7b-chat-fp16"]
input_cost_per_token = 0.000001923
litellm_provider = "cloudflare"
max_input_tokens = 3072
max_output_tokens = 3072
max_tokens = 3072
mode = "chat"
output_cost_per_token = 0.000001923

[models."cloudflare/@cf/meta/llama-2-7b-chat-int8"]
input_cost_per_token = 0.000001923
litellm_provider = "cloudflare"
max_input_tokens = 2048
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 0.000001923

[models."cloudflare/@cf/mistral/mistral-7b-instruct-v0.1"]
input_cost_per_token = 0.000001923
litellm_provider = "cloudflare"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000001923

[models."cloudflare/@hf/thebloke/codellama-7b-instruct-awq"]
input_cost_per_token = 0.000001923
litellm_provider = "cloudflare"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000001923

[models."code-bison"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
mode = "chat"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_tool_choice = true

[models."code-bison-32k@002"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
mode = "completion"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."code-bison32k"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
mode = "completion"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."code-bison@001"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
mode = "completion"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."code-bison@002"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
mode = "completion"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."code-gecko"]
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
max_input_tokens = 2048
max_output_tokens = 64
max_tokens = 64
mode = "completion"
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."code-gecko-latest"]
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
max_input_tokens = 2048
max_output_tokens = 64
max_tokens = 64
mode = "completion"
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."code-gecko@001"]
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
max_input_tokens = 2048
max_output_tokens = 64
max_tokens = 64
mode = "completion"
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."code-gecko@002"]
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
max_input_tokens = 2048
max_output_tokens = 64
max_tokens = 64
mode = "completion"
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."codechat-bison"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-chat-models"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
mode = "chat"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_tool_choice = true

[models."codechat-bison-32k"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-chat-models"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_tool_choice = true

[models."codechat-bison-32k@002"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-chat-models"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_tool_choice = true

[models."codechat-bison@001"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-chat-models"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
mode = "chat"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_tool_choice = true

[models."codechat-bison@002"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-chat-models"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
mode = "chat"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_tool_choice = true

[models."codechat-bison@latest"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-chat-models"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
mode = "chat"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_tool_choice = true

[models."codestral/codestral-2405"]
input_cost_per_token = 0
litellm_provider = "codestral"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0
source = "https://docs.mistral.ai/capabilities/code_generation/"
supports_assistant_prefill = true
supports_tool_choice = true

[models."codestral/codestral-latest"]
input_cost_per_token = 0
litellm_provider = "codestral"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0
source = "https://docs.mistral.ai/capabilities/code_generation/"
supports_assistant_prefill = true
supports_tool_choice = true

[models."codex-mini-latest"]
cache_read_input_token_cost = 3.75e-7
input_cost_per_token = 0.0000015
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "responses"
output_cost_per_token = 0.000006
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."cohere.command-light-text-v14"]
input_cost_per_token = 3e-7
litellm_provider = "bedrock"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 6e-7
supports_tool_choice = true

[models."cohere.command-r-plus-v1:0"]
input_cost_per_token = 0.000003
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
supports_tool_choice = true

[models."cohere.command-r-v1:0"]
input_cost_per_token = 5e-7
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0000015
supports_tool_choice = true

[models."cohere.command-text-v14"]
input_cost_per_token = 0.0000015
litellm_provider = "bedrock"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000002
supports_tool_choice = true

[models."cohere.embed-english-v3"]
input_cost_per_token = 1e-7
litellm_provider = "bedrock"
max_input_tokens = 512
max_tokens = 512
mode = "embedding"
output_cost_per_token = 0
supports_embedding_image_input = true

[models."cohere.embed-multilingual-v3"]
input_cost_per_token = 1e-7
litellm_provider = "bedrock"
max_input_tokens = 512
max_tokens = 512
mode = "embedding"
output_cost_per_token = 0
supports_embedding_image_input = true

[models."cohere.embed-v4:0"]
input_cost_per_token = 1.2e-7
litellm_provider = "bedrock"
max_input_tokens = 128000
max_tokens = 128000
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 1536
supports_embedding_image_input = true

[models."cohere.rerank-v3-5:0"]
input_cost_per_query = 0.002
input_cost_per_token = 0
litellm_provider = "bedrock"
max_document_chunks_per_query = 100
max_input_tokens = 32000
max_output_tokens = 32000
max_query_tokens = 32000
max_tokens = 32000
max_tokens_per_document_chunk = 512
mode = "rerank"
output_cost_per_token = 0

[models."cohere/embed-v4.0"]
input_cost_per_token = 1.2e-7
litellm_provider = "cohere"
max_input_tokens = 128000
max_tokens = 128000
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 1536
supports_embedding_image_input = true

[models."command"]
input_cost_per_token = 0.000001
litellm_provider = "cohere"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "completion"
output_cost_per_token = 0.000002

[models."command-a-03-2025"]
input_cost_per_token = 0.0000025
litellm_provider = "cohere_chat"
max_input_tokens = 256000
max_output_tokens = 8000
max_tokens = 8000
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_tool_choice = true

[models."command-light"]
input_cost_per_token = 3e-7
litellm_provider = "cohere_chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 6e-7
supports_tool_choice = true

[models."command-nightly"]
input_cost_per_token = 0.000001
litellm_provider = "cohere"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "completion"
output_cost_per_token = 0.000002

[models."command-r"]
input_cost_per_token = 1.5e-7
litellm_provider = "cohere_chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_tool_choice = true

[models."command-r-08-2024"]
input_cost_per_token = 1.5e-7
litellm_provider = "cohere_chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_tool_choice = true

[models."command-r-plus"]
input_cost_per_token = 0.0000025
litellm_provider = "cohere_chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_tool_choice = true

[models."command-r-plus-08-2024"]
input_cost_per_token = 0.0000025
litellm_provider = "cohere_chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_tool_choice = true

[models."command-r7b-12-2024"]
input_cost_per_token = 1.5e-7
litellm_provider = "cohere_chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 3.75e-8
source = "https://docs.cohere.com/v2/docs/command-r7b"
supports_function_calling = true
supports_tool_choice = true

[models."computer-use-preview"]
input_cost_per_token = 0.000003
litellm_provider = "azure"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
mode = "chat"
output_cost_per_token = 0.000012
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = false
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."dashscope/qwen-coder"]
input_cost_per_token = 3e-7
litellm_provider = "dashscope"
max_input_tokens = 1000000
max_output_tokens = 16384
max_tokens = 1000000
mode = "chat"
output_cost_per_token = 0.0000015
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."dashscope/qwen-flash"]
litellm_provider = "dashscope"
max_input_tokens = 997952
max_output_tokens = 32768
max_tokens = 1000000
mode = "chat"
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 5e-8, output_cost_per_token = 4e-7, range = [0, 256000] }, { input_cost_per_token = 2.5e-7, output_cost_per_token = 0.000002, range = [256000, 1000000] }]

[models."dashscope/qwen-flash-2025-07-28"]
litellm_provider = "dashscope"
max_input_tokens = 997952
max_output_tokens = 32768
max_tokens = 1000000
mode = "chat"
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 5e-8, output_cost_per_token = 4e-7, range = [0, 256000] }, { input_cost_per_token = 2.5e-7, output_cost_per_token = 0.000002, range = [256000, 1000000] }]

[models."dashscope/qwen-max"]
input_cost_per_token = 0.0000016
litellm_provider = "dashscope"
max_input_tokens = 30720
max_output_tokens = 8192
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000064
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."dashscope/qwen-plus"]
input_cost_per_token = 4e-7
litellm_provider = "dashscope"
max_input_tokens = 129024
max_output_tokens = 16384
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000012
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."dashscope/qwen-plus-2025-01-25"]
input_cost_per_token = 4e-7
litellm_provider = "dashscope"
max_input_tokens = 129024
max_output_tokens = 8192
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000012
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."dashscope/qwen-plus-2025-04-28"]
input_cost_per_token = 4e-7
litellm_provider = "dashscope"
max_input_tokens = 129024
max_output_tokens = 16384
max_tokens = 131072
mode = "chat"
output_cost_per_reasoning_token = 0.000004
output_cost_per_token = 0.0000012
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."dashscope/qwen-plus-2025-07-14"]
input_cost_per_token = 4e-7
litellm_provider = "dashscope"
max_input_tokens = 129024
max_output_tokens = 16384
max_tokens = 131072
mode = "chat"
output_cost_per_reasoning_token = 0.000004
output_cost_per_token = 0.0000012
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."dashscope/qwen-plus-2025-07-28"]
litellm_provider = "dashscope"
max_input_tokens = 997952
max_output_tokens = 32768
max_tokens = 1000000
mode = "chat"
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 4e-7, output_cost_per_reasoning_token = 0.000004, output_cost_per_token = 0.0000012, range = [0, 256000] }, { input_cost_per_token = 0.0000012, output_cost_per_reasoning_token = 0.000012, output_cost_per_token = 0.0000036, range = [256000, 1000000] }]

[models."dashscope/qwen-plus-2025-09-11"]
litellm_provider = "dashscope"
max_input_tokens = 997952
max_output_tokens = 32768
max_tokens = 1000000
mode = "chat"
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 4e-7, output_cost_per_reasoning_token = 0.000004, output_cost_per_token = 0.0000012, range = [0, 256000] }, { input_cost_per_token = 0.0000012, output_cost_per_reasoning_token = 0.000012, output_cost_per_token = 0.0000036, range = [256000, 1000000] }]

[models."dashscope/qwen-plus-latest"]
litellm_provider = "dashscope"
max_input_tokens = 997952
max_output_tokens = 32768
max_tokens = 1000000
mode = "chat"
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 4e-7, output_cost_per_reasoning_token = 0.000004, output_cost_per_token = 0.0000012, range = [0, 256000] }, { input_cost_per_token = 0.0000012, output_cost_per_reasoning_token = 0.000012, output_cost_per_token = 0.0000036, range = [256000, 1000000] }]

[models."dashscope/qwen-turbo"]
input_cost_per_token = 5e-8
litellm_provider = "dashscope"
max_input_tokens = 129024
max_output_tokens = 16384
max_tokens = 131072
mode = "chat"
output_cost_per_reasoning_token = 5e-7
output_cost_per_token = 2e-7
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."dashscope/qwen-turbo-2024-11-01"]
input_cost_per_token = 5e-8
litellm_provider = "dashscope"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 1000000
mode = "chat"
output_cost_per_token = 2e-7
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."dashscope/qwen-turbo-2025-04-28"]
input_cost_per_token = 5e-8
litellm_provider = "dashscope"
max_input_tokens = 1000000
max_output_tokens = 16384
max_tokens = 1000000
mode = "chat"
output_cost_per_reasoning_token = 5e-7
output_cost_per_token = 2e-7
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."dashscope/qwen-turbo-latest"]
input_cost_per_token = 5e-8
litellm_provider = "dashscope"
max_input_tokens = 1000000
max_output_tokens = 16384
max_tokens = 1000000
mode = "chat"
output_cost_per_reasoning_token = 5e-7
output_cost_per_token = 2e-7
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."dashscope/qwen3-30b-a3b"]
litellm_provider = "dashscope"
max_input_tokens = 129024
max_output_tokens = 16384
max_tokens = 131072
mode = "chat"
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."dashscope/qwen3-coder-flash"]
litellm_provider = "dashscope"
max_input_tokens = 997952
max_output_tokens = 65536
max_tokens = 1000000
mode = "chat"
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true
tiered_pricing = [{ cache_read_input_token_cost = 8e-8, input_cost_per_token = 3e-7, output_cost_per_token = 0.0000015, range = [0, 32000] }, { cache_read_input_token_cost = 1.2e-7, input_cost_per_token = 5e-7, output_cost_per_token = 0.0000025, range = [32000, 128000] }, { cache_read_input_token_cost = 2e-7, input_cost_per_token = 8e-7, output_cost_per_token = 0.000004, range = [128000, 256000] }, { cache_read_input_token_cost = 4e-7, input_cost_per_token = 0.0000016, output_cost_per_token = 0.0000096, range = [256000, 1000000] }]

[models."dashscope/qwen3-coder-flash-2025-07-28"]
litellm_provider = "dashscope"
max_input_tokens = 997952
max_output_tokens = 65536
max_tokens = 1000000
mode = "chat"
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 3e-7, output_cost_per_token = 0.0000015, range = [0, 32000] }, { input_cost_per_token = 5e-7, output_cost_per_token = 0.0000025, range = [32000, 128000] }, { input_cost_per_token = 8e-7, output_cost_per_token = 0.000004, range = [128000, 256000] }, { input_cost_per_token = 0.0000016, output_cost_per_token = 0.0000096, range = [256000, 1000000] }]

[models."dashscope/qwen3-coder-plus"]
litellm_provider = "dashscope"
max_input_tokens = 997952
max_output_tokens = 65536
max_tokens = 1000000
mode = "chat"
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true
tiered_pricing = [{ cache_read_input_token_cost = 1e-7, input_cost_per_token = 0.000001, output_cost_per_token = 0.000005, range = [0, 32000] }, { cache_read_input_token_cost = 1.8e-7, input_cost_per_token = 0.0000018, output_cost_per_token = 0.000009, range = [32000, 128000] }, { cache_read_input_token_cost = 3e-7, input_cost_per_token = 0.000003, output_cost_per_token = 0.000015, range = [128000, 256000] }, { cache_read_input_token_cost = 6e-7, input_cost_per_token = 0.000006, output_cost_per_token = 0.00006, range = [256000, 1000000] }]

[models."dashscope/qwen3-coder-plus-2025-07-22"]
litellm_provider = "dashscope"
max_input_tokens = 997952
max_output_tokens = 65536
max_tokens = 1000000
mode = "chat"
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 0.000001, output_cost_per_token = 0.000005, range = [0, 32000] }, { input_cost_per_token = 0.0000018, output_cost_per_token = 0.000009, range = [32000, 128000] }, { input_cost_per_token = 0.000003, output_cost_per_token = 0.000015, range = [128000, 256000] }, { input_cost_per_token = 0.000006, output_cost_per_token = 0.00006, range = [256000, 1000000] }]

[models."dashscope/qwen3-max-preview"]
litellm_provider = "dashscope"
max_input_tokens = 258048
max_output_tokens = 65536
max_tokens = 262144
mode = "chat"
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 0.0000012, output_cost_per_token = 0.000006, range = [0, 32000] }, { input_cost_per_token = 0.0000024, output_cost_per_token = 0.000012, range = [32000, 128000] }, { input_cost_per_token = 0.000003, output_cost_per_token = 0.000015, range = [128000, 252000] }]

[models."dashscope/qwq-plus"]
input_cost_per_token = 8e-7
litellm_provider = "dashscope"
max_input_tokens = 98304
max_output_tokens = 8192
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000024
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."databricks/databricks-bge-large-en"]
input_cost_per_token = 1.0003e-7
input_dbu_cost_per_token = 0.000001429
litellm_provider = "databricks"
max_input_tokens = 512
max_tokens = 512
mode = "embedding"
output_cost_per_token = 0
output_dbu_cost_per_token = 0
output_vector_size = 1024
source = "https://www.databricks.com/product/pricing/foundation-model-serving"

[models."databricks/databricks-bge-large-en".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-claude-3-7-sonnet"]
input_cost_per_token = 0.0000029999900000000002
input_dbu_cost_per_token = 0.000042857
litellm_provider = "databricks"
max_input_tokens = 200000
max_output_tokens = 128000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.000015000020000000002
output_dbu_cost_per_token = 0.000214286
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
supports_assistant_prefill = true
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."databricks/databricks-claude-3-7-sonnet".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-claude-haiku-4-5"]
input_cost_per_token = 0.00000100002
input_dbu_cost_per_token = 0.000014286
litellm_provider = "databricks"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.00000500003
output_dbu_cost_per_token = 0.000071429
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
supports_assistant_prefill = true
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."databricks/databricks-claude-haiku-4-5".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-claude-opus-4"]
input_cost_per_token = 0.000015000020000000002
input_dbu_cost_per_token = 0.000214286
litellm_provider = "databricks"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.00007500003000000001
output_dbu_cost_per_token = 0.001071429
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
supports_assistant_prefill = true
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."databricks/databricks-claude-opus-4".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-claude-opus-4-1"]
input_cost_per_token = 0.000015000020000000002
input_dbu_cost_per_token = 0.000214286
litellm_provider = "databricks"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.00007500003000000001
output_dbu_cost_per_token = 0.001071429
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
supports_assistant_prefill = true
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."databricks/databricks-claude-opus-4-1".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-claude-opus-4-5"]
input_cost_per_token = 0.00000500003
input_dbu_cost_per_token = 0.000071429
litellm_provider = "databricks"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.000025000010000000002
output_dbu_cost_per_token = 0.000357143
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
supports_assistant_prefill = true
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."databricks/databricks-claude-opus-4-5".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-claude-sonnet-4"]
input_cost_per_token = 0.0000029999900000000002
input_dbu_cost_per_token = 0.000042857
litellm_provider = "databricks"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.000015000020000000002
output_dbu_cost_per_token = 0.000214286
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
supports_assistant_prefill = true
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."databricks/databricks-claude-sonnet-4".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-claude-sonnet-4-1"]
input_cost_per_token = 0.0000029999900000000002
input_dbu_cost_per_token = 0.000042857
litellm_provider = "databricks"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.000015000020000000002
output_dbu_cost_per_token = 0.000214286
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
supports_assistant_prefill = true
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."databricks/databricks-claude-sonnet-4-1".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-claude-sonnet-4-5"]
input_cost_per_token = 0.0000029999900000000002
input_dbu_cost_per_token = 0.000042857
litellm_provider = "databricks"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.000015000020000000002
output_dbu_cost_per_token = 0.000214286
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
supports_assistant_prefill = true
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."databricks/databricks-claude-sonnet-4-5".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-gemini-2-5-flash"]
input_cost_per_token = 3.0001999999999996e-7
input_dbu_cost_per_token = 0.000004285999999999999
litellm_provider = "databricks"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 1048576
mode = "chat"
output_cost_per_token = 0.00000249998
output_dbu_cost_per_token = 0.000035714
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
supports_function_calling = true
supports_tool_choice = true

[models."databricks/databricks-gemini-2-5-flash".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-gemini-2-5-pro"]
input_cost_per_token = 0.00000124999
input_dbu_cost_per_token = 0.000017857
litellm_provider = "databricks"
max_input_tokens = 1048576
max_output_tokens = 65536
max_tokens = 1048576
mode = "chat"
output_cost_per_token = 0.000009999990000000002
output_dbu_cost_per_token = 0.000142857
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
supports_function_calling = true
supports_tool_choice = true

[models."databricks/databricks-gemini-2-5-pro".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-gemma-3-12b"]
input_cost_per_token = 1.5000999999999998e-7
input_dbu_cost_per_token = 0.0000021429999999999996
litellm_provider = "databricks"
max_input_tokens = 128000
max_output_tokens = 32000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 5.0001e-7
output_dbu_cost_per_token = 0.000007143
source = "https://www.databricks.com/product/pricing/foundation-model-serving"

[models."databricks/databricks-gemma-3-12b".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-gpt-5"]
input_cost_per_token = 0.00000124999
input_dbu_cost_per_token = 0.000017857
litellm_provider = "databricks"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 400000
mode = "chat"
output_cost_per_token = 0.000009999990000000002
output_dbu_cost_per_token = 0.000142857
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"

[models."databricks/databricks-gpt-5".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-gpt-5-1"]
input_cost_per_token = 0.00000124999
input_dbu_cost_per_token = 0.000017857
litellm_provider = "databricks"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 400000
mode = "chat"
output_cost_per_token = 0.000009999990000000002
output_dbu_cost_per_token = 0.000142857
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"

[models."databricks/databricks-gpt-5-1".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-gpt-5-mini"]
input_cost_per_token = 2.4997000000000006e-7
input_dbu_cost_per_token = 0.000003571
litellm_provider = "databricks"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 400000
mode = "chat"
output_cost_per_token = 0.0000019999700000000004
output_dbu_cost_per_token = 0.000028571
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"

[models."databricks/databricks-gpt-5-mini".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-gpt-5-nano"]
input_cost_per_token = 4.998e-8
input_dbu_cost_per_token = 7.14e-7
litellm_provider = "databricks"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 400000
mode = "chat"
output_cost_per_token = 3.9998000000000007e-7
output_dbu_cost_per_token = 0.000005714000000000001
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"

[models."databricks/databricks-gpt-5-nano".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-gpt-oss-120b"]
input_cost_per_token = 1.5000999999999998e-7
input_dbu_cost_per_token = 0.0000021429999999999996
litellm_provider = "databricks"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 5.9997e-7
output_dbu_cost_per_token = 0.000008571
source = "https://www.databricks.com/product/pricing/foundation-model-serving"

[models."databricks/databricks-gpt-oss-120b".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-gpt-oss-20b"]
input_cost_per_token = 7e-8
input_dbu_cost_per_token = 0.000001
litellm_provider = "databricks"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 3.0001999999999996e-7
output_dbu_cost_per_token = 0.000004285999999999999
source = "https://www.databricks.com/product/pricing/foundation-model-serving"

[models."databricks/databricks-gpt-oss-20b".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-gte-large-en"]
input_cost_per_token = 1.2999000000000001e-7
input_dbu_cost_per_token = 0.000001857
litellm_provider = "databricks"
max_input_tokens = 8192
max_tokens = 8192
mode = "embedding"
output_cost_per_token = 0
output_dbu_cost_per_token = 0
output_vector_size = 1024
source = "https://www.databricks.com/product/pricing/foundation-model-serving"

[models."databricks/databricks-gte-large-en".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-llama-2-70b-chat"]
input_cost_per_token = 5.0001e-7
input_dbu_cost_per_token = 0.000007143
litellm_provider = "databricks"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0000015000300000000002
output_dbu_cost_per_token = 0.000021429
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
supports_tool_choice = true

[models."databricks/databricks-llama-2-70b-chat".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-llama-4-maverick"]
input_cost_per_token = 5.0001e-7
input_dbu_cost_per_token = 0.000007143
litellm_provider = "databricks"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.0000015000300000000002
output_dbu_cost_per_token = 0.000021429
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
supports_tool_choice = true

[models."databricks/databricks-llama-4-maverick".metadata]
notes = "Databricks documentation now provides both DBU costs (_dbu_cost_per_token) and dollar costs(_cost_per_token)."

[models."databricks/databricks-meta-llama-3-1-405b-instruct"]
input_cost_per_token = 0.00000500003
input_dbu_cost_per_token = 0.000071429
litellm_provider = "databricks"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000015000020000000002
output_dbu_cost_per_token = 0.000214286
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
supports_tool_choice = true

[models."databricks/databricks-meta-llama-3-1-405b-instruct".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-meta-llama-3-1-8b-instruct"]
input_cost_per_token = 1.5000999999999998e-7
input_dbu_cost_per_token = 0.0000021429999999999996
litellm_provider = "databricks"
max_input_tokens = 200000
max_output_tokens = 128000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 4.5003000000000007e-7
output_dbu_cost_per_token = 0.000006429000000000001
source = "https://www.databricks.com/product/pricing/foundation-model-serving"

[models."databricks/databricks-meta-llama-3-1-8b-instruct".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-meta-llama-3-3-70b-instruct"]
input_cost_per_token = 5.0001e-7
input_dbu_cost_per_token = 0.000007143
litellm_provider = "databricks"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.0000015000300000000002
output_dbu_cost_per_token = 0.000021429
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
supports_tool_choice = true

[models."databricks/databricks-meta-llama-3-3-70b-instruct".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-meta-llama-3-70b-instruct"]
input_cost_per_token = 0.00000100002
input_dbu_cost_per_token = 0.000014286
litellm_provider = "databricks"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.0000029999900000000002
output_dbu_cost_per_token = 0.000042857
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
supports_tool_choice = true

[models."databricks/databricks-meta-llama-3-70b-instruct".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-mixtral-8x7b-instruct"]
input_cost_per_token = 5.0001e-7
input_dbu_cost_per_token = 0.000007143
litellm_provider = "databricks"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00000100002
output_dbu_cost_per_token = 0.000014286
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
supports_tool_choice = true

[models."databricks/databricks-mixtral-8x7b-instruct".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-mpt-30b-instruct"]
input_cost_per_token = 0.00000100002
input_dbu_cost_per_token = 0.000014286
litellm_provider = "databricks"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000100002
output_dbu_cost_per_token = 0.000014286
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
supports_tool_choice = true

[models."databricks/databricks-mpt-30b-instruct".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks/databricks-mpt-7b-instruct"]
input_cost_per_token = 5.0001e-7
input_dbu_cost_per_token = 0.000007143
litellm_provider = "databricks"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0
output_dbu_cost_per_token = 0
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
supports_tool_choice = true

[models."databricks/databricks-mpt-7b-instruct".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."dataforseo/search"]
input_cost_per_query = 0.003
litellm_provider = "dataforseo"
mode = "search"

[models."davinci-002"]
input_cost_per_token = 0.000002
litellm_provider = "text-completion-openai"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 16384
mode = "completion"
output_cost_per_token = 0.000002

[models."deepgram/base"]
input_cost_per_second = 0.00020833
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/base".metadata]
calculation = "$0.0125/60 seconds = $0.00020833 per second"
original_pricing_per_minute = 0.0125

[models."deepgram/base-conversationalai"]
input_cost_per_second = 0.00020833
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/base-conversationalai".metadata]
calculation = "$0.0125/60 seconds = $0.00020833 per second"
original_pricing_per_minute = 0.0125

[models."deepgram/base-finance"]
input_cost_per_second = 0.00020833
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/base-finance".metadata]
calculation = "$0.0125/60 seconds = $0.00020833 per second"
original_pricing_per_minute = 0.0125

[models."deepgram/base-general"]
input_cost_per_second = 0.00020833
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/base-general".metadata]
calculation = "$0.0125/60 seconds = $0.00020833 per second"
original_pricing_per_minute = 0.0125

[models."deepgram/base-meeting"]
input_cost_per_second = 0.00020833
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/base-meeting".metadata]
calculation = "$0.0125/60 seconds = $0.00020833 per second"
original_pricing_per_minute = 0.0125

[models."deepgram/base-phonecall"]
input_cost_per_second = 0.00020833
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/base-phonecall".metadata]
calculation = "$0.0125/60 seconds = $0.00020833 per second"
original_pricing_per_minute = 0.0125

[models."deepgram/base-video"]
input_cost_per_second = 0.00020833
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/base-video".metadata]
calculation = "$0.0125/60 seconds = $0.00020833 per second"
original_pricing_per_minute = 0.0125

[models."deepgram/base-voicemail"]
input_cost_per_second = 0.00020833
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/base-voicemail".metadata]
calculation = "$0.0125/60 seconds = $0.00020833 per second"
original_pricing_per_minute = 0.0125

[models."deepgram/enhanced"]
input_cost_per_second = 0.00024167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/enhanced".metadata]
calculation = "$0.0145/60 seconds = $0.00024167 per second"
original_pricing_per_minute = 0.0145

[models."deepgram/enhanced-finance"]
input_cost_per_second = 0.00024167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/enhanced-finance".metadata]
calculation = "$0.0145/60 seconds = $0.00024167 per second"
original_pricing_per_minute = 0.0145

[models."deepgram/enhanced-general"]
input_cost_per_second = 0.00024167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/enhanced-general".metadata]
calculation = "$0.0145/60 seconds = $0.00024167 per second"
original_pricing_per_minute = 0.0145

[models."deepgram/enhanced-meeting"]
input_cost_per_second = 0.00024167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/enhanced-meeting".metadata]
calculation = "$0.0145/60 seconds = $0.00024167 per second"
original_pricing_per_minute = 0.0145

[models."deepgram/enhanced-phonecall"]
input_cost_per_second = 0.00024167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/enhanced-phonecall".metadata]
calculation = "$0.0145/60 seconds = $0.00024167 per second"
original_pricing_per_minute = 0.0145

[models."deepgram/nova"]
input_cost_per_second = 0.00007167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."deepgram/nova-2"]
input_cost_per_second = 0.00007167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova-2".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."deepgram/nova-2-atc"]
input_cost_per_second = 0.00007167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova-2-atc".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."deepgram/nova-2-automotive"]
input_cost_per_second = 0.00007167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova-2-automotive".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."deepgram/nova-2-conversationalai"]
input_cost_per_second = 0.00007167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova-2-conversationalai".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."deepgram/nova-2-drivethru"]
input_cost_per_second = 0.00007167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova-2-drivethru".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."deepgram/nova-2-finance"]
input_cost_per_second = 0.00007167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova-2-finance".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."deepgram/nova-2-general"]
input_cost_per_second = 0.00007167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova-2-general".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."deepgram/nova-2-meeting"]
input_cost_per_second = 0.00007167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova-2-meeting".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."deepgram/nova-2-phonecall"]
input_cost_per_second = 0.00007167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova-2-phonecall".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."deepgram/nova-2-video"]
input_cost_per_second = 0.00007167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova-2-video".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."deepgram/nova-2-voicemail"]
input_cost_per_second = 0.00007167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova-2-voicemail".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."deepgram/nova-3"]
input_cost_per_second = 0.00007167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova-3".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."deepgram/nova-3-general"]
input_cost_per_second = 0.00007167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova-3-general".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."deepgram/nova-3-medical"]
input_cost_per_second = 0.00008667
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova-3-medical".metadata]
calculation = "$0.0052/60 seconds = $0.00008667 per second (multilingual)"
original_pricing_per_minute = 0.0052

[models."deepgram/nova-general"]
input_cost_per_second = 0.00007167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova-general".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."deepgram/nova-phonecall"]
input_cost_per_second = 0.00007167
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/nova-phonecall".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."deepgram/whisper"]
input_cost_per_second = 0.0001
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/whisper".metadata]
notes = "Deepgram's hosted OpenAI Whisper models - pricing may differ from native Deepgram models"

[models."deepgram/whisper-base"]
input_cost_per_second = 0.0001
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/whisper-base".metadata]
notes = "Deepgram's hosted OpenAI Whisper models - pricing may differ from native Deepgram models"

[models."deepgram/whisper-large"]
input_cost_per_second = 0.0001
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/whisper-large".metadata]
notes = "Deepgram's hosted OpenAI Whisper models - pricing may differ from native Deepgram models"

[models."deepgram/whisper-medium"]
input_cost_per_second = 0.0001
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/whisper-medium".metadata]
notes = "Deepgram's hosted OpenAI Whisper models - pricing may differ from native Deepgram models"

[models."deepgram/whisper-small"]
input_cost_per_second = 0.0001
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/whisper-small".metadata]
notes = "Deepgram's hosted OpenAI Whisper models - pricing may differ from native Deepgram models"

[models."deepgram/whisper-tiny"]
input_cost_per_second = 0.0001
litellm_provider = "deepgram"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://deepgram.com/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."deepgram/whisper-tiny".metadata]
notes = "Deepgram's hosted OpenAI Whisper models - pricing may differ from native Deepgram models"

[models."deepinfra/Gryphe/MythoMax-L2-13b"]
input_cost_per_token = 8e-8
litellm_provider = "deepinfra"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9e-8
supports_tool_choice = true

[models."deepinfra/NousResearch/Hermes-3-Llama-3.1-405B"]
input_cost_per_token = 0.000001
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000001
supports_tool_choice = true

[models."deepinfra/NousResearch/Hermes-3-Llama-3.1-70B"]
input_cost_per_token = 3e-7
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 3e-7
supports_tool_choice = false

[models."deepinfra/Qwen/QwQ-32B"]
input_cost_per_token = 1.5e-7
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 4e-7
supports_tool_choice = true

[models."deepinfra/Qwen/Qwen2.5-72B-Instruct"]
input_cost_per_token = 1.2e-7
litellm_provider = "deepinfra"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 3.9e-7
supports_tool_choice = true

[models."deepinfra/Qwen/Qwen2.5-7B-Instruct"]
input_cost_per_token = 4e-8
litellm_provider = "deepinfra"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 1e-7
supports_tool_choice = false

[models."deepinfra/Qwen/Qwen2.5-VL-32B-Instruct"]
input_cost_per_token = 2e-7
litellm_provider = "deepinfra"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 6e-7
supports_tool_choice = true
supports_vision = true

[models."deepinfra/Qwen/Qwen3-14B"]
input_cost_per_token = 6e-8
litellm_provider = "deepinfra"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
mode = "chat"
output_cost_per_token = 2.4e-7
supports_tool_choice = true

[models."deepinfra/Qwen/Qwen3-235B-A22B"]
input_cost_per_token = 1.8e-7
litellm_provider = "deepinfra"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
mode = "chat"
output_cost_per_token = 5.4e-7
supports_tool_choice = true

[models."deepinfra/Qwen/Qwen3-235B-A22B-Instruct-2507"]
input_cost_per_token = 9e-8
litellm_provider = "deepinfra"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 6e-7
supports_tool_choice = true

[models."deepinfra/Qwen/Qwen3-235B-A22B-Thinking-2507"]
input_cost_per_token = 3e-7
litellm_provider = "deepinfra"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000029
supports_tool_choice = true

[models."deepinfra/Qwen/Qwen3-30B-A3B"]
input_cost_per_token = 8e-8
litellm_provider = "deepinfra"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
mode = "chat"
output_cost_per_token = 2.9e-7
supports_tool_choice = true

[models."deepinfra/Qwen/Qwen3-32B"]
input_cost_per_token = 1e-7
litellm_provider = "deepinfra"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
mode = "chat"
output_cost_per_token = 2.8e-7
supports_tool_choice = true

[models."deepinfra/Qwen/Qwen3-Coder-480B-A35B-Instruct"]
input_cost_per_token = 4e-7
litellm_provider = "deepinfra"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000016
supports_tool_choice = true

[models."deepinfra/Qwen/Qwen3-Coder-480B-A35B-Instruct-Turbo"]
input_cost_per_token = 2.9e-7
litellm_provider = "deepinfra"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000012
supports_tool_choice = true

[models."deepinfra/Qwen/Qwen3-Next-80B-A3B-Instruct"]
input_cost_per_token = 1.4e-7
litellm_provider = "deepinfra"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000014
supports_tool_choice = true

[models."deepinfra/Qwen/Qwen3-Next-80B-A3B-Thinking"]
input_cost_per_token = 1.4e-7
litellm_provider = "deepinfra"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000014
supports_tool_choice = true

[models."deepinfra/Sao10K/L3-8B-Lunaris-v1-Turbo"]
input_cost_per_token = 4e-8
litellm_provider = "deepinfra"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 5e-8
supports_tool_choice = false

[models."deepinfra/Sao10K/L3.1-70B-Euryale-v2.2"]
input_cost_per_token = 6.5e-7
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 7.5e-7
supports_tool_choice = false

[models."deepinfra/Sao10K/L3.3-70B-Euryale-v2.3"]
input_cost_per_token = 6.5e-7
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 7.5e-7
supports_tool_choice = false

[models."deepinfra/allenai/olmOCR-7B-0725-FP8"]
input_cost_per_token = 2.7e-7
litellm_provider = "deepinfra"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.0000015
supports_tool_choice = false

[models."deepinfra/anthropic/claude-3-7-sonnet-latest"]
cache_read_input_token_cost = 3.3e-7
input_cost_per_token = 0.0000033
litellm_provider = "deepinfra"
max_input_tokens = 200000
max_output_tokens = 200000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.0000165
supports_tool_choice = true

[models."deepinfra/anthropic/claude-4-opus"]
input_cost_per_token = 0.0000165
litellm_provider = "deepinfra"
max_input_tokens = 200000
max_output_tokens = 200000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.0000825
supports_tool_choice = true

[models."deepinfra/anthropic/claude-4-sonnet"]
input_cost_per_token = 0.0000033
litellm_provider = "deepinfra"
max_input_tokens = 200000
max_output_tokens = 200000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.0000165
supports_tool_choice = true

[models."deepinfra/deepseek-ai/DeepSeek-R1"]
input_cost_per_token = 7e-7
litellm_provider = "deepinfra"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
mode = "chat"
output_cost_per_token = 0.0000024
supports_tool_choice = true

[models."deepinfra/deepseek-ai/DeepSeek-R1-0528"]
cache_read_input_token_cost = 4e-7
input_cost_per_token = 5e-7
litellm_provider = "deepinfra"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
mode = "chat"
output_cost_per_token = 0.00000215
supports_tool_choice = true

[models."deepinfra/deepseek-ai/DeepSeek-R1-0528-Turbo"]
input_cost_per_token = 0.000001
litellm_provider = "deepinfra"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000003
supports_tool_choice = true

[models."deepinfra/deepseek-ai/DeepSeek-R1-Distill-Llama-70B"]
input_cost_per_token = 2e-7
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 6e-7
supports_tool_choice = false

[models."deepinfra/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"]
input_cost_per_token = 2.7e-7
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2.7e-7
supports_tool_choice = true

[models."deepinfra/deepseek-ai/DeepSeek-R1-Turbo"]
input_cost_per_token = 0.000001
litellm_provider = "deepinfra"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
mode = "chat"
output_cost_per_token = 0.000003
supports_tool_choice = true

[models."deepinfra/deepseek-ai/DeepSeek-V3"]
input_cost_per_token = 3.8e-7
litellm_provider = "deepinfra"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
mode = "chat"
output_cost_per_token = 8.9e-7
supports_tool_choice = true

[models."deepinfra/deepseek-ai/DeepSeek-V3-0324"]
input_cost_per_token = 2.5e-7
litellm_provider = "deepinfra"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
mode = "chat"
output_cost_per_token = 8.8e-7
supports_tool_choice = true

[models."deepinfra/deepseek-ai/DeepSeek-V3.1"]
cache_read_input_token_cost = 2.16e-7
input_cost_per_token = 2.7e-7
litellm_provider = "deepinfra"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
mode = "chat"
output_cost_per_token = 0.000001
supports_reasoning = true
supports_tool_choice = true

[models."deepinfra/deepseek-ai/DeepSeek-V3.1-Terminus"]
cache_read_input_token_cost = 2.16e-7
input_cost_per_token = 2.7e-7
litellm_provider = "deepinfra"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
mode = "chat"
output_cost_per_token = 0.000001
supports_tool_choice = true

[models."deepinfra/google/gemini-2.0-flash-001"]
input_cost_per_token = 1e-7
litellm_provider = "deepinfra"
max_input_tokens = 1000000
max_output_tokens = 1000000
max_tokens = 1000000
mode = "chat"
output_cost_per_token = 4e-7
supports_tool_choice = true

[models."deepinfra/google/gemini-2.5-flash"]
input_cost_per_token = 3e-7
litellm_provider = "deepinfra"
max_input_tokens = 1000000
max_output_tokens = 1000000
max_tokens = 1000000
mode = "chat"
output_cost_per_token = 0.0000025
supports_tool_choice = true

[models."deepinfra/google/gemini-2.5-pro"]
input_cost_per_token = 0.00000125
litellm_provider = "deepinfra"
max_input_tokens = 1000000
max_output_tokens = 1000000
max_tokens = 1000000
mode = "chat"
output_cost_per_token = 0.00001
supports_tool_choice = true

[models."deepinfra/google/gemma-3-12b-it"]
input_cost_per_token = 5e-8
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1e-7
supports_tool_choice = true

[models."deepinfra/google/gemma-3-27b-it"]
input_cost_per_token = 9e-8
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1.6e-7
supports_tool_choice = true

[models."deepinfra/google/gemma-3-4b-it"]
input_cost_per_token = 4e-8
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 8e-8
supports_tool_choice = true

[models."deepinfra/meta-llama/Llama-3.2-11B-Vision-Instruct"]
input_cost_per_token = 4.9e-8
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 4.9e-8
supports_tool_choice = false

[models."deepinfra/meta-llama/Llama-3.2-3B-Instruct"]
input_cost_per_token = 2e-8
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-8
supports_tool_choice = true

[models."deepinfra/meta-llama/Llama-3.3-70B-Instruct"]
input_cost_per_token = 2.3e-7
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 4e-7
supports_tool_choice = true

[models."deepinfra/meta-llama/Llama-3.3-70B-Instruct-Turbo"]
input_cost_per_token = 1.3e-7
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 3.9e-7
supports_tool_choice = true

[models."deepinfra/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"]
input_cost_per_token = 1.5e-7
litellm_provider = "deepinfra"
max_input_tokens = 1048576
max_output_tokens = 1048576
max_tokens = 1048576
mode = "chat"
output_cost_per_token = 6e-7
supports_tool_choice = true

[models."deepinfra/meta-llama/Llama-4-Scout-17B-16E-Instruct"]
input_cost_per_token = 8e-8
litellm_provider = "deepinfra"
max_input_tokens = 327680
max_output_tokens = 327680
max_tokens = 327680
mode = "chat"
output_cost_per_token = 3e-7
supports_tool_choice = true

[models."deepinfra/meta-llama/Llama-Guard-3-8B"]
input_cost_per_token = 5.5e-8
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 5.5e-8
supports_tool_choice = false

[models."deepinfra/meta-llama/Llama-Guard-4-12B"]
input_cost_per_token = 1.8e-7
litellm_provider = "deepinfra"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
mode = "chat"
output_cost_per_token = 1.8e-7
supports_tool_choice = false

[models."deepinfra/meta-llama/Meta-Llama-3-8B-Instruct"]
input_cost_per_token = 3e-8
litellm_provider = "deepinfra"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 6e-8
supports_tool_choice = true

[models."deepinfra/meta-llama/Meta-Llama-3.1-70B-Instruct"]
input_cost_per_token = 4e-7
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 4e-7
supports_tool_choice = true

[models."deepinfra/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"]
input_cost_per_token = 1e-7
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2.8e-7
supports_tool_choice = true

[models."deepinfra/meta-llama/Meta-Llama-3.1-8B-Instruct"]
input_cost_per_token = 3e-8
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 5e-8
supports_tool_choice = true

[models."deepinfra/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"]
input_cost_per_token = 2e-8
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 3e-8
supports_tool_choice = true

[models."deepinfra/microsoft/WizardLM-2-8x22B"]
input_cost_per_token = 4.8e-7
litellm_provider = "deepinfra"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 4.8e-7
supports_tool_choice = false

[models."deepinfra/microsoft/phi-4"]
input_cost_per_token = 7e-8
litellm_provider = "deepinfra"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 1.4e-7
supports_tool_choice = true

[models."deepinfra/mistralai/Mistral-Nemo-Instruct-2407"]
input_cost_per_token = 2e-8
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 4e-8
supports_tool_choice = true

[models."deepinfra/mistralai/Mistral-Small-24B-Instruct-2501"]
input_cost_per_token = 5e-8
litellm_provider = "deepinfra"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 8e-8
supports_tool_choice = true

[models."deepinfra/mistralai/Mistral-Small-3.2-24B-Instruct-2506"]
input_cost_per_token = 7.5e-8
litellm_provider = "deepinfra"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 2e-7
supports_tool_choice = true

[models."deepinfra/mistralai/Mixtral-8x7B-Instruct-v0.1"]
input_cost_per_token = 4e-7
litellm_provider = "deepinfra"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 4e-7
supports_tool_choice = true

[models."deepinfra/moonshotai/Kimi-K2-Instruct"]
input_cost_per_token = 5e-7
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000002
supports_tool_choice = true

[models."deepinfra/moonshotai/Kimi-K2-Instruct-0905"]
cache_read_input_token_cost = 4e-7
input_cost_per_token = 5e-7
litellm_provider = "deepinfra"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.000002
supports_tool_choice = true

[models."deepinfra/nvidia/Llama-3.1-Nemotron-70B-Instruct"]
input_cost_per_token = 6e-7
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 6e-7
supports_tool_choice = true

[models."deepinfra/nvidia/Llama-3.3-Nemotron-Super-49B-v1.5"]
input_cost_per_token = 1e-7
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 4e-7
supports_tool_choice = true

[models."deepinfra/nvidia/NVIDIA-Nemotron-Nano-9B-v2"]
input_cost_per_token = 4e-8
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1.6e-7
supports_tool_choice = true

[models."deepinfra/openai/gpt-oss-120b"]
input_cost_per_token = 5e-8
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 4.5e-7
supports_tool_choice = true

[models."deepinfra/openai/gpt-oss-20b"]
input_cost_per_token = 4e-8
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1.5e-7
supports_tool_choice = true

[models."deepinfra/zai-org/GLM-4.5"]
input_cost_per_token = 4e-7
litellm_provider = "deepinfra"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000016
supports_tool_choice = true

[models."deepseek-chat"]
cache_read_input_token_cost = 6e-8
input_cost_per_token = 6e-7
litellm_provider = "deepseek"
max_input_tokens = 131072
max_output_tokens = 8192
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000017
source = "https://api-docs.deepseek.com/quick_start/pricing"
supported_endpoints = ["/v1/chat/completions"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek-reasoner"]
cache_read_input_token_cost = 6e-8
input_cost_per_token = 6e-7
litellm_provider = "deepseek"
max_input_tokens = 131072
max_output_tokens = 65536
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000017
source = "https://api-docs.deepseek.com/quick_start/pricing"
supported_endpoints = ["/v1/chat/completions"]
supports_function_calling = false
supports_native_streaming = true
supports_parallel_function_calling = false
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = false

[models."deepseek.v3-v1:0"]
input_cost_per_token = 5.8e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 163840
max_output_tokens = 81920
max_tokens = 163840
mode = "chat"
output_cost_per_token = 0.00000168
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."deepseek/deepseek-chat"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 7e-8
input_cost_per_token = 2.7e-7
input_cost_per_token_cache_hit = 7e-8
litellm_provider = "deepseek"
max_input_tokens = 65536
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000011
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_tool_choice = true

[models."deepseek/deepseek-coder"]
input_cost_per_token = 1.4e-7
input_cost_per_token_cache_hit = 1.4e-8
litellm_provider = "deepseek"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2.8e-7
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_tool_choice = true

[models."deepseek/deepseek-r1"]
input_cost_per_token = 5.5e-7
input_cost_per_token_cache_hit = 1.4e-7
litellm_provider = "deepseek"
max_input_tokens = 65536
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000219
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true

[models."deepseek/deepseek-reasoner"]
input_cost_per_token = 5.5e-7
input_cost_per_token_cache_hit = 1.4e-7
litellm_provider = "deepseek"
max_input_tokens = 65536
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000219
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true

[models."deepseek/deepseek-v3"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 7e-8
input_cost_per_token = 2.7e-7
input_cost_per_token_cache_hit = 7e-8
litellm_provider = "deepseek"
max_input_tokens = 65536
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000011
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_tool_choice = true

[models."deepseek/deepseek-v3.2"]
input_cost_per_token = 2.8e-7
input_cost_per_token_cache_hit = 2.8e-8
litellm_provider = "deepseek"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 8192
mode = "chat"
output_cost_per_token = 4e-7
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true

[models."dolphin"]
input_cost_per_token = 5e-7
litellm_provider = "nlp_cloud"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "completion"
output_cost_per_token = 5e-7

[models."doubao-embedding"]
input_cost_per_token = 0
litellm_provider = "volcengine"
max_input_tokens = 4096
max_tokens = 4096
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 2560

[models."doubao-embedding".metadata]
notes = "Volcengine Doubao embedding model - standard version with 2560 dimensions"

[models."doubao-embedding-large"]
input_cost_per_token = 0
litellm_provider = "volcengine"
max_input_tokens = 4096
max_tokens = 4096
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 2048

[models."doubao-embedding-large".metadata]
notes = "Volcengine Doubao embedding model - large version with 2048 dimensions"

[models."doubao-embedding-large-text-240915"]
input_cost_per_token = 0
litellm_provider = "volcengine"
max_input_tokens = 4096
max_tokens = 4096
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 4096

[models."doubao-embedding-large-text-240915".metadata]
notes = "Volcengine Doubao embedding model - text-240915 version with 4096 dimensions"

[models."doubao-embedding-large-text-250515"]
input_cost_per_token = 0
litellm_provider = "volcengine"
max_input_tokens = 4096
max_tokens = 4096
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 2048

[models."doubao-embedding-large-text-250515".metadata]
notes = "Volcengine Doubao embedding model - text-250515 version with 2048 dimensions"

[models."doubao-embedding-text-240715"]
input_cost_per_token = 0
litellm_provider = "volcengine"
max_input_tokens = 4096
max_tokens = 4096
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 2560

[models."doubao-embedding-text-240715".metadata]
notes = "Volcengine Doubao embedding model - text-240715 version with 2560 dimensions"

[models."elevenlabs/scribe_v1"]
input_cost_per_second = 0.0000611
litellm_provider = "elevenlabs"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://elevenlabs.io/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."elevenlabs/scribe_v1".metadata]
calculation = "$0.22/hour = $0.00366/minute = $0.0000611 per second (enterprise pricing)"
notes = "ElevenLabs Scribe v1 - state-of-the-art speech recognition model with 99 language support"
original_pricing_per_hour = 0.22

[models."elevenlabs/scribe_v1_experimental"]
input_cost_per_second = 0.0000611
litellm_provider = "elevenlabs"
mode = "audio_transcription"
output_cost_per_second = 0
source = "https://elevenlabs.io/pricing"
supported_endpoints = ["/v1/audio/transcriptions"]

[models."elevenlabs/scribe_v1_experimental".metadata]
calculation = "$0.22/hour = $0.00366/minute = $0.0000611 per second (enterprise pricing)"
notes = "ElevenLabs Scribe v1 experimental - enhanced version of the main Scribe model"
original_pricing_per_hour = 0.22

[models."embed-english-light-v2.0"]
input_cost_per_token = 1e-7
litellm_provider = "cohere"
max_input_tokens = 1024
max_tokens = 1024
mode = "embedding"
output_cost_per_token = 0

[models."embed-english-light-v3.0"]
input_cost_per_token = 1e-7
litellm_provider = "cohere"
max_input_tokens = 1024
max_tokens = 1024
mode = "embedding"
output_cost_per_token = 0

[models."embed-english-v2.0"]
input_cost_per_token = 1e-7
litellm_provider = "cohere"
max_input_tokens = 4096
max_tokens = 4096
mode = "embedding"
output_cost_per_token = 0

[models."embed-english-v3.0"]
input_cost_per_image = 0.0001
input_cost_per_token = 1e-7
litellm_provider = "cohere"
max_input_tokens = 1024
max_tokens = 1024
mode = "embedding"
output_cost_per_token = 0
supports_embedding_image_input = true
supports_image_input = true

[models."embed-english-v3.0".metadata]
notes = "'supports_image_input' is a deprecated field. Use 'supports_embedding_image_input' instead."

[models."embed-multilingual-light-v3.0"]
input_cost_per_token = 0.0001
litellm_provider = "cohere"
max_input_tokens = 1024
max_tokens = 1024
mode = "embedding"
output_cost_per_token = 0
supports_embedding_image_input = true

[models."embed-multilingual-v2.0"]
input_cost_per_token = 1e-7
litellm_provider = "cohere"
max_input_tokens = 768
max_tokens = 768
mode = "embedding"
output_cost_per_token = 0

[models."embed-multilingual-v3.0"]
input_cost_per_token = 1e-7
litellm_provider = "cohere"
max_input_tokens = 1024
max_tokens = 1024
mode = "embedding"
output_cost_per_token = 0
supports_embedding_image_input = true

[models."eu.amazon.nova-2-lite-v1:0"]
cache_read_input_token_cost = 8.25e-8
input_cost_per_token = 3.3e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.00000275
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_video_input = true
supports_vision = true

[models."eu.amazon.nova-lite-v1:0"]
input_cost_per_token = 7.8e-8
litellm_provider = "bedrock_converse"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 3.12e-7
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_vision = true

[models."eu.amazon.nova-micro-v1:0"]
input_cost_per_token = 4.6e-8
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 1.84e-7
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true

[models."eu.amazon.nova-pro-v1:0"]
input_cost_per_token = 0.00000105
litellm_provider = "bedrock_converse"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 0.0000042
source = "https://aws.amazon.com/bedrock/pricing/"
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_vision = true

[models."eu.anthropic.claude-3-5-haiku-20241022-v1:0"]
input_cost_per_token = 2.5e-7
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000125
supports_assistant_prefill = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true

[models."eu.anthropic.claude-3-5-sonnet-20240620-v1:0"]
input_cost_per_token = 0.000003
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."eu.anthropic.claude-3-5-sonnet-20241022-v2:0"]
input_cost_per_token = 0.000003
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."eu.anthropic.claude-3-7-sonnet-20250219-v1:0"]
input_cost_per_token = 0.000003
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."eu.anthropic.claude-3-haiku-20240307-v1:0"]
input_cost_per_token = 2.5e-7
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00000125
supports_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."eu.anthropic.claude-3-opus-20240229-v1:0"]
input_cost_per_token = 0.000015
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000075
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."eu.anthropic.claude-3-sonnet-20240229-v1:0"]
input_cost_per_token = 0.000003
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."eu.anthropic.claude-haiku-4-5-20251001-v1:0"]
cache_creation_input_token_cost = 0.000001375
cache_read_input_token_cost = 1.1e-7
deprecation_date = "2026-10-15"
input_cost_per_token = 0.0000011
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.0000055
source = "https://aws.amazon.com/about-aws/whats-new/2025/10/claude-4-5-haiku-anthropic-amazon-bedrock"
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 346

[models."eu.anthropic.claude-opus-4-1-20250805-v1:0"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."eu.anthropic.claude-opus-4-1-20250805-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."eu.anthropic.claude-opus-4-20250514-v1:0"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."eu.anthropic.claude-opus-4-20250514-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."eu.anthropic.claude-opus-4-5-20251101-v1:0"]
cache_creation_input_token_cost = 0.00000625
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000025
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."eu.anthropic.claude-opus-4-5-20251101-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."eu.anthropic.claude-sonnet-4-20250514-v1:0"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "bedrock_converse"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."eu.anthropic.claude-sonnet-4-20250514-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."eu.anthropic.claude-sonnet-4-5-20250929-v1:0"]
cache_creation_input_token_cost = 0.000004125
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost = 3.3e-7
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token = 0.0000033
input_cost_per_token_above_200k_tokens = 0.0000066
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.0000165
output_cost_per_token_above_200k_tokens = 0.00002475
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 346

[models."eu.anthropic.claude-sonnet-4-5-20250929-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."eu.meta.llama3-2-1b-instruct-v1:0"]
input_cost_per_token = 1.3e-7
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1.3e-7
supports_function_calling = true
supports_tool_choice = false

[models."eu.meta.llama3-2-3b-instruct-v1:0"]
input_cost_per_token = 1.9e-7
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1.9e-7
supports_function_calling = true
supports_tool_choice = false

[models."eu.mistral.pixtral-large-2502-v1:0"]
input_cost_per_token = 0.000002
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000006
supports_function_calling = true
supports_tool_choice = false

[models."eu.twelvelabs.marengo-embed-2-7-v1:0"]
input_cost_per_audio_per_second = 0.00014
input_cost_per_image = 0.0001
input_cost_per_token = 0.00007
input_cost_per_video_per_second = 0.0007
litellm_provider = "bedrock"
max_input_tokens = 77
max_tokens = 77
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 1024
supports_embedding_image_input = true
supports_image_input = true

[models."eu.twelvelabs.pegasus-1-2-v1:0"]
input_cost_per_video_per_second = 0.00049
litellm_provider = "bedrock"
mode = "chat"
output_cost_per_token = 0.0000075
supports_video_input = true

[models."exa_ai/search"]
litellm_provider = "exa_ai"
mode = "search"
tiered_pricing = [{ input_cost_per_query = 0.005, max_results_range = [0, 25] }, { input_cost_per_query = 0.025, max_results_range = [26, 100] }]

[models."fal_ai/bria/text-to-image/3.2"]
litellm_provider = "fal_ai"
mode = "image_generation"
output_cost_per_image = 0.0398
supported_endpoints = ["/v1/images/generations"]

[models."fal_ai/fal-ai/bytedance/dreamina/v3.1/text-to-image"]
litellm_provider = "fal_ai"
mode = "image_generation"
output_cost_per_image = 0.03
supported_endpoints = ["/v1/images/generations"]

[models."fal_ai/fal-ai/bytedance/seedream/v3/text-to-image"]
litellm_provider = "fal_ai"
mode = "image_generation"
output_cost_per_image = 0.03
supported_endpoints = ["/v1/images/generations"]

[models."fal_ai/fal-ai/flux-pro/v1.1"]
litellm_provider = "fal_ai"
mode = "image_generation"
output_cost_per_image = 0.04
supported_endpoints = ["/v1/images/generations"]

[models."fal_ai/fal-ai/flux-pro/v1.1-ultra"]
litellm_provider = "fal_ai"
mode = "image_generation"
output_cost_per_image = 0.06
supported_endpoints = ["/v1/images/generations"]

[models."fal_ai/fal-ai/flux/schnell"]
litellm_provider = "fal_ai"
mode = "image_generation"
output_cost_per_image = 0.003
supported_endpoints = ["/v1/images/generations"]

[models."fal_ai/fal-ai/ideogram/v3"]
litellm_provider = "fal_ai"
mode = "image_generation"
output_cost_per_image = 0.06
supported_endpoints = ["/v1/images/generations"]

[models."fal_ai/fal-ai/imagen4/preview"]
litellm_provider = "fal_ai"
mode = "image_generation"
output_cost_per_image = 0.0398
supported_endpoints = ["/v1/images/generations"]

[models."fal_ai/fal-ai/imagen4/preview/fast"]
litellm_provider = "fal_ai"
mode = "image_generation"
output_cost_per_image = 0.02
supported_endpoints = ["/v1/images/generations"]

[models."fal_ai/fal-ai/imagen4/preview/ultra"]
litellm_provider = "fal_ai"
mode = "image_generation"
output_cost_per_image = 0.06
supported_endpoints = ["/v1/images/generations"]

[models."fal_ai/fal-ai/recraft/v3/text-to-image"]
litellm_provider = "fal_ai"
mode = "image_generation"
output_cost_per_image = 0.0398
supported_endpoints = ["/v1/images/generations"]

[models."fal_ai/fal-ai/stable-diffusion-v35-medium"]
litellm_provider = "fal_ai"
mode = "image_generation"
output_cost_per_image = 0.0398
supported_endpoints = ["/v1/images/generations"]

[models."featherless_ai/featherless-ai/Qwerky-72B"]
litellm_provider = "featherless_ai"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 32768
mode = "chat"

[models."featherless_ai/featherless-ai/Qwerky-QwQ-32B"]
litellm_provider = "featherless_ai"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 32768
mode = "chat"

[models."firecrawl/search"]
litellm_provider = "firecrawl"
mode = "search"
tiered_pricing = [{ input_cost_per_query = 0.00166, max_results_range = [1, 10] }, { input_cost_per_query = 0.00332, max_results_range = [11, 20] }, { input_cost_per_query = 0.00498, max_results_range = [21, 30] }, { input_cost_per_query = 0.00664, max_results_range = [31, 40] }, { input_cost_per_query = 0.0083, max_results_range = [41, 50] }, { input_cost_per_query = 0.00996, max_results_range = [51, 60] }, { input_cost_per_query = 0.01162, max_results_range = [61, 70] }, { input_cost_per_query = 0.01328, max_results_range = [71, 80] }, { input_cost_per_query = 0.01494, max_results_range = [81, 90] }, { input_cost_per_query = 0.0166, max_results_range = [91, 100] }]

[models."firecrawl/search".metadata]
notes = "Firecrawl search pricing: $83 for 100,000 credits, 2 credits per 10 results. Cost = ceiling(limit/10) * 2 * $0.00083"

[models."fireworks-ai-4.1b-to-16b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
output_cost_per_token = 2e-7

[models."fireworks-ai-56b-to-176b"]
input_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
output_cost_per_token = 0.0000012

[models."fireworks-ai-above-16b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
output_cost_per_token = 9e-7

[models."fireworks-ai-default"]
input_cost_per_token = 0
litellm_provider = "fireworks_ai"
output_cost_per_token = 0

[models."fireworks-ai-embedding-150m-to-350m"]
input_cost_per_token = 1.6e-8
litellm_provider = "fireworks_ai-embedding-models"
output_cost_per_token = 0

[models."fireworks-ai-embedding-up-to-150m"]
input_cost_per_token = 8e-9
litellm_provider = "fireworks_ai-embedding-models"
output_cost_per_token = 0

[models."fireworks-ai-moe-up-to-56b"]
input_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
output_cost_per_token = 5e-7

[models."fireworks-ai-up-to-4b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
output_cost_per_token = 2e-7

[models."fireworks_ai/WhereIsAI/UAE-Large-V1"]
input_cost_per_token = 1.6e-8
litellm_provider = "fireworks_ai-embedding-models"
max_input_tokens = 512
max_tokens = 512
mode = "embedding"
output_cost_per_token = 0
source = "https://fireworks.ai/pricing"

[models."fireworks_ai/accounts/fireworks/models/"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
mode = "embedding"
output_cost_per_token = 0

[models."fireworks_ai/accounts/fireworks/models/SSD-1B"]
input_cost_per_token = 1.3e-10
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "image_generation"
output_cost_per_token = 1.3e-10

[models."fireworks_ai/accounts/fireworks/models/chronos-hermes-13b-v2"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/code-llama-13b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/code-llama-13b-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/code-llama-13b-python"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/code-llama-34b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/code-llama-34b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/code-llama-34b-python"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/code-llama-70b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/code-llama-70b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/code-llama-70b-python"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/code-llama-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/code-llama-7b-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/code-llama-7b-python"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/code-qwen-1p5-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/codegemma-2b"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/codegemma-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/cogito-671b-v2-p1"]
input_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
mode = "chat"
output_cost_per_token = 0.0000012

[models."fireworks_ai/accounts/fireworks/models/cogito-v1-preview-llama-3b"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/cogito-v1-preview-llama-70b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/cogito-v1-preview-llama-8b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/cogito-v1-preview-qwen-14b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/cogito-v1-preview-qwen-32b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/dbrx-instruct"]
input_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000012

[models."fireworks_ai/accounts/fireworks/models/deepseek-coder-1b-base"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/deepseek-coder-33b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/deepseek-coder-7b-base"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/deepseek-coder-7b-base-v1p5"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/deepseek-coder-7b-instruct-v1p5"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/deepseek-coder-v2-instruct"]
input_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.0000012
source = "https://fireworks.ai/pricing"
supports_function_calling = false
supports_response_schema = true
supports_tool_choice = false

[models."fireworks_ai/accounts/fireworks/models/deepseek-coder-v2-lite-base"]
input_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
mode = "chat"
output_cost_per_token = 5e-7

[models."fireworks_ai/accounts/fireworks/models/deepseek-coder-v2-lite-instruct"]
input_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
mode = "chat"
output_cost_per_token = 5e-7

[models."fireworks_ai/accounts/fireworks/models/deepseek-prover-v2"]
input_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
mode = "chat"
output_cost_per_token = 0.0000012

[models."fireworks_ai/accounts/fireworks/models/deepseek-r1"]
input_cost_per_token = 0.000003
litellm_provider = "fireworks_ai"
max_input_tokens = 128000
max_output_tokens = 20480
max_tokens = 20480
mode = "chat"
output_cost_per_token = 0.000008
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."fireworks_ai/accounts/fireworks/models/deepseek-r1-0528"]
input_cost_per_token = 0.000003
litellm_provider = "fireworks_ai"
max_input_tokens = 160000
max_output_tokens = 160000
max_tokens = 160000
mode = "chat"
output_cost_per_token = 0.000008
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."fireworks_ai/accounts/fireworks/models/deepseek-r1-0528-distill-qwen3-8b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/deepseek-r1-basic"]
input_cost_per_token = 5.5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 128000
max_output_tokens = 20480
max_tokens = 20480
mode = "chat"
output_cost_per_token = 0.00000219
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-llama-70b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-llama-8b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-qwen-14b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-qwen-1p5b"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-qwen-32b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-qwen-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/deepseek-v2-lite-chat"]
input_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
mode = "chat"
output_cost_per_token = 5e-7

[models."fireworks_ai/accounts/fireworks/models/deepseek-v2p5"]
input_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000012

[models."fireworks_ai/accounts/fireworks/models/deepseek-v3"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 9e-7
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."fireworks_ai/accounts/fireworks/models/deepseek-v3-0324"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
mode = "chat"
output_cost_per_token = 9e-7
source = "https://fireworks.ai/models/fireworks/deepseek-v3-0324"
supports_response_schema = true
supports_tool_choice = false

[models."fireworks_ai/accounts/fireworks/models/deepseek-v3p1"]
input_cost_per_token = 5.6e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000168
source = "https://fireworks.ai/pricing"
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."fireworks_ai/accounts/fireworks/models/deepseek-v3p1-terminus"]
input_cost_per_token = 5.6e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000168
source = "https://fireworks.ai/pricing"
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."fireworks_ai/accounts/fireworks/models/deepseek-v3p2"]
input_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
mode = "chat"
output_cost_per_token = 0.0000012
source = "https://fireworks.ai/models/fireworks/deepseek-v3p2"
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."fireworks_ai/accounts/fireworks/models/devstral-small-2505"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/dobby-mini-unhinged-plus-llama-3-1-8b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/dobby-unhinged-llama-3-3-70b-new"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/dolphin-2-9-2-qwen2-72b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/dolphin-2p6-mixtral-8x7b"]
input_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 5e-7

[models."fireworks_ai/accounts/fireworks/models/ernie-4p5-21b-a3b-pt"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/ernie-4p5-300b-a47b-pt"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/fare-20b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/firefunction-v1"]
input_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 5e-7

[models."fireworks_ai/accounts/fireworks/models/firefunction-v2"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 9e-7
source = "https://fireworks.ai/pricing"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."fireworks_ai/accounts/fireworks/models/firellava-13b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/firesearch-ocr-v6"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/fireworks-asr-large"]
input_cost_per_token = 0
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "audio_transcription"
output_cost_per_token = 0

[models."fireworks_ai/accounts/fireworks/models/fireworks-asr-v2"]
input_cost_per_token = 0
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "audio_transcription"
output_cost_per_token = 0

[models."fireworks_ai/accounts/fireworks/models/flux-1-dev"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/flux-1-dev-controlnet-union"]
input_cost_per_token = 1e-9
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 1e-9

[models."fireworks_ai/accounts/fireworks/models/flux-1-dev-fp8"]
input_cost_per_token = 5e-10
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "image_generation"
output_cost_per_token = 5e-10

[models."fireworks_ai/accounts/fireworks/models/flux-1-schnell"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/flux-1-schnell-fp8"]
input_cost_per_token = 3.5e-10
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "image_generation"
output_cost_per_token = 3.5e-10

[models."fireworks_ai/accounts/fireworks/models/flux-kontext-max"]
input_cost_per_token = 8e-8
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "image_generation"
output_cost_per_token = 8e-8

[models."fireworks_ai/accounts/fireworks/models/flux-kontext-pro"]
input_cost_per_token = 4e-8
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "image_generation"
output_cost_per_token = 4e-8

[models."fireworks_ai/accounts/fireworks/models/gemma-2b-it"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/gemma-3-27b-it"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/gemma-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/gemma-7b-it"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/gemma2-9b-it"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/glm-4p5"]
input_cost_per_token = 5.5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 128000
max_output_tokens = 96000
max_tokens = 96000
mode = "chat"
output_cost_per_token = 0.00000219
source = "https://fireworks.ai/models/fireworks/glm-4p5"
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."fireworks_ai/accounts/fireworks/models/glm-4p5-air"]
input_cost_per_token = 2.2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 128000
max_output_tokens = 96000
max_tokens = 96000
mode = "chat"
output_cost_per_token = 8.8e-7
source = "https://artificialanalysis.ai/models/glm-4-5-air"
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."fireworks_ai/accounts/fireworks/models/glm-4p5v"]
input_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000012
supports_reasoning = true

[models."fireworks_ai/accounts/fireworks/models/glm-4p6"]
input_cost_per_token = 5.5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 202800
max_output_tokens = 202800
max_tokens = 202800
mode = "chat"
output_cost_per_token = 0.00000219
source = "https://fireworks.ai/pricing"
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."fireworks_ai/accounts/fireworks/models/gpt-oss-120b"]
input_cost_per_token = 1.5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 6e-7
source = "https://fireworks.ai/pricing"
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."fireworks_ai/accounts/fireworks/models/gpt-oss-20b"]
input_cost_per_token = 5e-8
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7
source = "https://fireworks.ai/pricing"
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."fireworks_ai/accounts/fireworks/models/gpt-oss-safeguard-120b"]
input_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000012

[models."fireworks_ai/accounts/fireworks/models/gpt-oss-safeguard-20b"]
input_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 5e-7

[models."fireworks_ai/accounts/fireworks/models/hermes-2-pro-mistral-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/internvl3-38b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/internvl3-78b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/internvl3-8b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/japanese-stable-diffusion-xl"]
input_cost_per_token = 1.3e-10
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "image_generation"
output_cost_per_token = 1.3e-10

[models."fireworks_ai/accounts/fireworks/models/kat-coder"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/kat-dev-32b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/kat-dev-72b-exp"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/kimi-k2-instruct"]
input_cost_per_token = 6e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 16384
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000025
source = "https://fireworks.ai/models/fireworks/kimi-k2-instruct"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."fireworks_ai/accounts/fireworks/models/kimi-k2-instruct-0905"]
input_cost_per_token = 6e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 262144
max_output_tokens = 32768
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000025
source = "https://app.fireworks.ai/models/fireworks/kimi-k2-instruct-0905"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."fireworks_ai/accounts/fireworks/models/kimi-k2-thinking"]
input_cost_per_token = 6e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000025
source = "https://fireworks.ai/pricing"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."fireworks_ai/accounts/fireworks/models/llama-guard-2-8b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/llama-guard-3-1b"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/llama-guard-3-8b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v2-13b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v2-13b-chat"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v2-70b"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v2-70b-chat"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 2048
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v2-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v2-7b-chat"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v3-70b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v3-70b-instruct-hf"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v3-8b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v3-8b-instruct-hf"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v3p1-405b-instruct"]
input_cost_per_token = 0.000003
litellm_provider = "fireworks_ai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.000003
source = "https://fireworks.ai/pricing"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."fireworks_ai/accounts/fireworks/models/llama-v3p1-405b-instruct-long"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v3p1-70b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v3p1-70b-instruct-1b"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v3p1-8b-instruct"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 1e-7
source = "https://fireworks.ai/pricing"
supports_function_calling = false
supports_response_schema = true
supports_tool_choice = false

[models."fireworks_ai/accounts/fireworks/models/llama-v3p1-nemotron-70b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v3p2-11b-vision-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 2e-7
source = "https://fireworks.ai/pricing"
supports_function_calling = false
supports_response_schema = true
supports_tool_choice = false
supports_vision = true

[models."fireworks_ai/accounts/fireworks/models/llama-v3p2-1b"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v3p2-1b-instruct"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 1e-7
source = "https://fireworks.ai/pricing"
supports_function_calling = false
supports_response_schema = true
supports_tool_choice = false

[models."fireworks_ai/accounts/fireworks/models/llama-v3p2-3b"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/llama-v3p2-3b-instruct"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 1e-7
source = "https://fireworks.ai/pricing"
supports_function_calling = false
supports_response_schema = true
supports_tool_choice = false

[models."fireworks_ai/accounts/fireworks/models/llama-v3p2-90b-vision-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 9e-7
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false
supports_vision = true

[models."fireworks_ai/accounts/fireworks/models/llama-v3p3-70b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/llama4-maverick-instruct-basic"]
input_cost_per_token = 2.2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 8.8e-7
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."fireworks_ai/accounts/fireworks/models/llama4-scout-instruct-basic"]
input_cost_per_token = 1.5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 6e-7
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."fireworks_ai/accounts/fireworks/models/llamaguard-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/llava-yi-34b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/minimax-m1-80k"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/minimax-m2"]
input_cost_per_token = 3e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0000012

[models."fireworks_ai/accounts/fireworks/models/ministral-3-14b-instruct-2512"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/ministral-3-3b-instruct-2512"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/ministral-3-8b-instruct-2512"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/mistral-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/mistral-7b-instruct-4k"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/mistral-7b-instruct-v0p2"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/mistral-7b-instruct-v3"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/mistral-7b-v0p2"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/mistral-large-3-fp8"]
input_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.0000012

[models."fireworks_ai/accounts/fireworks/models/mistral-nemo-base-2407"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/mistral-nemo-instruct-2407"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/mistral-small-24b-instruct-2501"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/mixtral-8x22b"]
input_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.0000012

[models."fireworks_ai/accounts/fireworks/models/mixtral-8x22b-instruct"]
input_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.0000012

[models."fireworks_ai/accounts/fireworks/models/mixtral-8x22b-instruct-hf"]
input_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.0000012
source = "https://fireworks.ai/pricing"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."fireworks_ai/accounts/fireworks/models/mixtral-8x7b"]
input_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 5e-7

[models."fireworks_ai/accounts/fireworks/models/mixtral-8x7b-instruct"]
input_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 5e-7

[models."fireworks_ai/accounts/fireworks/models/mixtral-8x7b-instruct-hf"]
input_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 5e-7

[models."fireworks_ai/accounts/fireworks/models/mythomax-l2-13b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/nemotron-nano-v2-12b-vl"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/nous-capybara-7b-v1p9"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/nous-hermes-2-mixtral-8x7b-dpo"]
input_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 5e-7

[models."fireworks_ai/accounts/fireworks/models/nous-hermes-2-yi-34b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/nous-hermes-llama2-13b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/nous-hermes-llama2-70b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/nous-hermes-llama2-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/nvidia-nemotron-nano-12b-v2"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/nvidia-nemotron-nano-9b-v2"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/openchat-3p5-0106-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/openhermes-2-mistral-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/openhermes-2p5-mistral-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/openorca-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/phi-2-3b"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 2048
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/phi-3-mini-128k-instruct"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/phi-3-vision-128k-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32064
max_output_tokens = 32064
max_tokens = 32064
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/phind-code-llama-34b-python-v1"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/phind-code-llama-34b-v1"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/phind-code-llama-34b-v2"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/playground-v2-1024px-aesthetic"]
input_cost_per_token = 1.3e-10
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "image_generation"
output_cost_per_token = 1.3e-10

[models."fireworks_ai/accounts/fireworks/models/playground-v2-5-1024px-aesthetic"]
input_cost_per_token = 1.3e-10
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "image_generation"
output_cost_per_token = 1.3e-10

[models."fireworks_ai/accounts/fireworks/models/pythia-12b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 2048
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwen-qwq-32b-preview"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen-v2p5-14b-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwen-v2p5-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwen1p5-72b-chat"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2-72b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 9e-7
source = "https://fireworks.ai/pricing"
supports_function_calling = false
supports_response_schema = true
supports_tool_choice = false

[models."fireworks_ai/accounts/fireworks/models/qwen2-7b-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2-vl-2b-instruct"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2-vl-72b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2-vl-7b-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-0p5b-instruct"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-14b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-1p5b-instruct"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-32b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-32b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-72b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-72b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-7b-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-coder-0p5b"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-coder-0p5b-instruct"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-coder-14b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-coder-14b-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-coder-1p5b"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-coder-1p5b-instruct"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-coder-32b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-coder-32b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9e-7
source = "https://fireworks.ai/pricing"
supports_function_calling = false
supports_response_schema = true
supports_tool_choice = false

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-coder-32b-instruct-128k"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-coder-32b-instruct-32k-rope"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-coder-32b-instruct-64k"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-coder-3b"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-coder-3b-instruct"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-coder-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-coder-7b-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-math-72b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-vl-32b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-vl-3b-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-vl-72b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen2p5-vl-7b-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-0p6b"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-14b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-1p7b"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-1p7b-fp8-draft"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-1p7b-fp8-draft-131072"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-1p7b-fp8-draft-40960"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-235b-a22b"]
input_cost_per_token = 2.2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 8.8e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-235b-a22b-instruct-2507"]
input_cost_per_token = 2.2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 8.8e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-235b-a22b-thinking-2507"]
input_cost_per_token = 2.2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 8.8e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-30b-a3b"]
input_cost_per_token = 1.5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 6e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-30b-a3b-instruct-2507"]
input_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 5e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-30b-a3b-thinking-2507"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-32b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7
supports_reasoning = true

[models."fireworks_ai/accounts/fireworks/models/qwen3-4b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-4b-instruct-2507"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-8b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
mode = "chat"
output_cost_per_token = 2e-7
supports_reasoning = true

[models."fireworks_ai/accounts/fireworks/models/qwen3-coder-30b-a3b-instruct"]
input_cost_per_token = 1.5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 6e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-coder-480b-a35b-instruct"]
input_cost_per_token = 4.5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000018
supports_reasoning = true

[models."fireworks_ai/accounts/fireworks/models/qwen3-coder-480b-instruct-bf16"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-embedding-0p6b"]
input_cost_per_token = 0
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "embedding"
output_cost_per_token = 0

[models."fireworks_ai/accounts/fireworks/models/qwen3-embedding-4b"]
input_cost_per_token = 0
litellm_provider = "fireworks_ai"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
mode = "embedding"
output_cost_per_token = 0

[models."fireworks_ai/accounts/fireworks/models/qwen3-next-80b-a3b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-next-80b-a3b-thinking"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-reranker-0p6b"]
input_cost_per_token = 0
litellm_provider = "fireworks_ai"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
mode = "rerank"
output_cost_per_token = 0

[models."fireworks_ai/accounts/fireworks/models/qwen3-reranker-4b"]
input_cost_per_token = 0
litellm_provider = "fireworks_ai"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
mode = "rerank"
output_cost_per_token = 0

[models."fireworks_ai/accounts/fireworks/models/qwen3-reranker-8b"]
input_cost_per_token = 0
litellm_provider = "fireworks_ai"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
mode = "rerank"
output_cost_per_token = 0

[models."fireworks_ai/accounts/fireworks/models/qwen3-vl-235b-a22b-instruct"]
input_cost_per_token = 2.2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 8.8e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-vl-235b-a22b-thinking"]
input_cost_per_token = 2.2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 8.8e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-vl-30b-a3b-instruct"]
input_cost_per_token = 1.5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 6e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-vl-30b-a3b-thinking"]
input_cost_per_token = 1.5e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 6e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-vl-32b-instruct"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/qwen3-vl-8b-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/qwq-32b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/rolm-ocr"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/snorkel-mistral-7b-pairrm-dpo"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/stable-diffusion-xl-1024-v1-0"]
input_cost_per_token = 1.3e-10
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "image_generation"
output_cost_per_token = 1.3e-10

[models."fireworks_ai/accounts/fireworks/models/stablecode-3b"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/starcoder-16b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/starcoder-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/starcoder2-15b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/starcoder2-3b"]
input_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 1e-7

[models."fireworks_ai/accounts/fireworks/models/starcoder2-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/toppy-m-7b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/whisper-v3"]
input_cost_per_token = 0
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "audio_transcription"
output_cost_per_token = 0

[models."fireworks_ai/accounts/fireworks/models/whisper-v3-turbo"]
input_cost_per_token = 0
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "audio_transcription"
output_cost_per_token = 0

[models."fireworks_ai/accounts/fireworks/models/yi-34b"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/yi-34b-200k-capybara"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 200000
max_output_tokens = 200000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/yi-34b-chat"]
input_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9e-7

[models."fireworks_ai/accounts/fireworks/models/yi-6b"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/accounts/fireworks/models/yi-large"]
input_cost_per_token = 0.000003
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000003
source = "https://fireworks.ai/pricing"
supports_function_calling = false
supports_response_schema = true
supports_tool_choice = false

[models."fireworks_ai/accounts/fireworks/models/zephyr-7b-beta"]
input_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7

[models."fireworks_ai/nomic-ai/nomic-embed-text-v1"]
input_cost_per_token = 8e-9
litellm_provider = "fireworks_ai-embedding-models"
max_input_tokens = 8192
max_tokens = 8192
mode = "embedding"
output_cost_per_token = 0
source = "https://fireworks.ai/pricing"

[models."fireworks_ai/nomic-ai/nomic-embed-text-v1.5"]
input_cost_per_token = 8e-9
litellm_provider = "fireworks_ai-embedding-models"
max_input_tokens = 8192
max_tokens = 8192
mode = "embedding"
output_cost_per_token = 0
source = "https://fireworks.ai/pricing"

[models."fireworks_ai/thenlper/gte-base"]
input_cost_per_token = 8e-9
litellm_provider = "fireworks_ai-embedding-models"
max_input_tokens = 512
max_tokens = 512
mode = "embedding"
output_cost_per_token = 0
source = "https://fireworks.ai/pricing"

[models."fireworks_ai/thenlper/gte-large"]
input_cost_per_token = 1.6e-8
litellm_provider = "fireworks_ai-embedding-models"
max_input_tokens = 512
max_tokens = 512
mode = "embedding"
output_cost_per_token = 0
source = "https://fireworks.ai/pricing"

[models."friendliai/meta-llama-3.1-70b-instruct"]
input_cost_per_token = 6e-7
litellm_provider = "friendliai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."friendliai/meta-llama-3.1-8b-instruct"]
input_cost_per_token = 1e-7
litellm_provider = "friendliai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 1e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."ft:babbage-002"]
input_cost_per_token = 0.0000016
input_cost_per_token_batches = 2e-7
litellm_provider = "text-completion-openai"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 16384
mode = "completion"
output_cost_per_token = 0.0000016
output_cost_per_token_batches = 2e-7

[models."ft:davinci-002"]
input_cost_per_token = 0.000012
input_cost_per_token_batches = 0.000001
litellm_provider = "text-completion-openai"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 16384
mode = "completion"
output_cost_per_token = 0.000012
output_cost_per_token_batches = 0.000001

[models."ft:gpt-3.5-turbo"]
input_cost_per_token = 0.000003
input_cost_per_token_batches = 0.0000015
litellm_provider = "openai"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000006
output_cost_per_token_batches = 0.000003
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-3.5-turbo-0125"]
input_cost_per_token = 0.000003
litellm_provider = "openai"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000006
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-3.5-turbo-0613"]
input_cost_per_token = 0.000003
litellm_provider = "openai"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000006
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-3.5-turbo-1106"]
input_cost_per_token = 0.000003
litellm_provider = "openai"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000006
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-4-0613"]
input_cost_per_token = 0.00003
litellm_provider = "openai"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00006
source = "OpenAI needs to add pricing for this ft model, will be updated when added by OpenAI. Defaulting to base model pricing"
supports_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-4.1-2025-04-14"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
input_cost_per_token_batches = 0.0000015
litellm_provider = "openai"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000012
output_cost_per_token_batches = 0.000006
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-4.1-mini-2025-04-14"]
cache_read_input_token_cost = 2e-7
input_cost_per_token = 8e-7
input_cost_per_token_batches = 4e-7
litellm_provider = "openai"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000032
output_cost_per_token_batches = 0.0000016
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-4.1-nano-2025-04-14"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
input_cost_per_token_batches = 1e-7
litellm_provider = "openai"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 8e-7
output_cost_per_token_batches = 4e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-4o-2024-08-06"]
cache_read_input_token_cost = 0.000001875
input_cost_per_token = 0.00000375
input_cost_per_token_batches = 0.000001875
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_batches = 0.0000075
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."ft:gpt-4o-2024-11-20"]
cache_creation_input_token_cost = 0.000001875
input_cost_per_token = 0.00000375
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-4o-mini-2024-07-18"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 3e-7
input_cost_per_token_batches = 1.5e-7
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.0000012
output_cost_per_token_batches = 6e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."ft:o4-mini-2025-04-16"]
cache_read_input_token_cost = 0.000001
input_cost_per_token = 0.000004
input_cost_per_token_batches = 0.000002
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.000016
output_cost_per_token_batches = 0.000008
supports_function_calling = true
supports_parallel_function_calling = false
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."gemini-1.0-pro"]
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 32760
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#google_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.0-pro-001"]
deprecation_date = "2025-04-09"
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 32760
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.0-pro-002"]
deprecation_date = "2025-04-09"
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 32760
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.0-pro-vision"]
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
litellm_provider = "vertex_ai-vision-models"
max_images_per_prompt = 16
max_input_tokens = 16384
max_output_tokens = 2048
max_tokens = 2048
max_video_length = 2
max_videos_per_prompt = 1
mode = "chat"
output_cost_per_token = 0.0000015
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.0-pro-vision-001"]
deprecation_date = "2025-04-09"
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
litellm_provider = "vertex_ai-vision-models"
max_images_per_prompt = 16
max_input_tokens = 16384
max_output_tokens = 2048
max_tokens = 2048
max_video_length = 2
max_videos_per_prompt = 1
mode = "chat"
output_cost_per_token = 0.0000015
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.0-ultra"]
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 8192
max_output_tokens = 2048
max_tokens = 8192
mode = "chat"
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015
source = "As of Jun, 2024. There is no available doc on vertex ai pricing gemini-1.0-ultra-001. Using gemini-1.0-pro pricing. Got max_tokens info here: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.0-ultra-001"]
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 8192
max_output_tokens = 2048
max_tokens = 8192
mode = "chat"
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015
source = "As of Jun, 2024. There is no available doc on vertex ai pricing gemini-1.0-ultra-001. Using gemini-1.0-pro pricing. Got max_tokens info here: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.5-flash"]
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1000000
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 7.5e-8
output_cost_per_character_above_128k_tokens = 1.5e-7
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.5-flash-001"]
deprecation_date = "2025-05-24"
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1000000
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 7.5e-8
output_cost_per_character_above_128k_tokens = 1.5e-7
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.5-flash-002"]
deprecation_date = "2025-09-24"
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 7.5e-8
output_cost_per_character_above_128k_tokens = 1.5e-7
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-1.5-flash"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.5-flash-exp-0827"]
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token = 4.688e-9
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1000000
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 1.875e-8
output_cost_per_character_above_128k_tokens = 3.75e-8
output_cost_per_token = 4.6875e-9
output_cost_per_token_above_128k_tokens = 9.375e-9
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.5-flash-preview-0514"]
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1000000
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 1.875e-8
output_cost_per_character_above_128k_tokens = 3.75e-8
output_cost_per_token = 4.6875e-9
output_cost_per_token_above_128k_tokens = 9.375e-9
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.5-pro"]
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 0.00000125
input_cost_per_token_above_128k_tokens = 0.0000025
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 0.000005
output_cost_per_token_above_128k_tokens = 0.00001
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.5-pro-001"]
deprecation_date = "2025-05-24"
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 0.00000125
input_cost_per_token_above_128k_tokens = 0.0000025
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 0.000005
output_cost_per_token_above_128k_tokens = 0.00001
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.5-pro-002"]
deprecation_date = "2025-09-24"
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 0.00000125
input_cost_per_token_above_128k_tokens = 0.0000025
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 0.000005
output_cost_per_token_above_128k_tokens = 0.00001
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-1.5-pro"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini-1.5-pro-preview-0215"]
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 7.8125e-8
input_cost_per_token_above_128k_tokens = 1.5625e-7
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 3.125e-7
output_cost_per_token_above_128k_tokens = 6.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gemini-1.5-pro-preview-0409"]
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 7.8125e-8
input_cost_per_token_above_128k_tokens = 1.5625e-7
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 3.125e-7
output_cost_per_token_above_128k_tokens = 6.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."gemini-1.5-pro-preview-0514"]
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 7.8125e-8
input_cost_per_token_above_128k_tokens = 1.5625e-7
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 3.125e-7
output_cost_per_token_above_128k_tokens = 6.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gemini-2.0-flash"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 4e-7
source = "https://ai.google.dev/pricing#2_0flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-2.0-flash-001"]
cache_read_input_token_cost = 3.75e-8
deprecation_date = "2026-02-05"
input_cost_per_audio_token = 0.000001
input_cost_per_token = 1.5e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 6e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.0-flash-exp"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 1.5e-7
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 6e-7
output_cost_per_token_above_128k_tokens = 0
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.0-flash-lite"]
cache_read_input_token_cost = 1.875e-8
input_cost_per_audio_token = 7.5e-8
input_cost_per_token = 7.5e-8
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 50
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 3e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.0-flash-lite-001"]
cache_read_input_token_cost = 1.875e-8
deprecation_date = "2026-02-25"
input_cost_per_audio_token = 7.5e-8
input_cost_per_token = 7.5e-8
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 50
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 3e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.0-flash-live-preview-04-09"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000003
input_cost_per_image = 0.000003
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.000003
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_audio_token = 0.000012
output_cost_per_token = 0.000002
rpm = 10
source = "https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini#gemini-2-0-flash-live-preview-04-09"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
supports_audio_output = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini-2.0-flash-preview-image-generation"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 4e-7
source = "https://ai.google.dev/pricing#2_0flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.0-flash-thinking-exp"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.0-flash-thinking-exp-01-21"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65536
max_pdf_size_mb = 30
max_tokens = 65536
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = false
supports_function_calling = false
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = false
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.0-pro-exp-02-05"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 2097152
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-flash"]
cache_read_input_token_cost = 3e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-flash-image"]
cache_read_input_token_cost = 3e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 32768
max_output_tokens = 32768
max_pdf_size_mb = 30
max_tokens = 32768
max_video_length = 1
max_videos_per_prompt = 10
mode = "image_generation"
output_cost_per_image = 0.039
output_cost_per_image_token = 0.00003
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
rpm = 100000
source = "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-flash-image"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = false
tpm = 8000000

[models."gemini-2.5-flash-image-preview"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "image_generation"
output_cost_per_image = 0.039
output_cost_per_image_token = 0.00003
output_cost_per_reasoning_token = 0.00003
output_cost_per_token = 0.00003
rpm = 100000
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 8000000

[models."gemini-2.5-flash-lite"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 5e-7
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-flash-lite-preview-06-17"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 5e-7
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-flash-lite-preview-09-2025"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 3e-7
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
source = "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-flash-preview-04-17"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 1.5e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000035
output_cost_per_token = 6e-7
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-flash-preview-05-20"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-flash-preview-09-2025"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
source = "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-pro"]
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-pro-exp-03-25"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-pro-preview-03-25"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_audio_token = 0.00000125
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-pro-preview-05-06"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_audio_token = 0.00000125
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supported_regions = ["global"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-pro-preview-06-05"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_audio_token = 0.00000125
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-2.5-pro-preview-tts"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
source = "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-pro-preview"
supported_modalities = ["text"]
supported_output_modalities = ["audio"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gemini-3-flash-preview"]
cache_read_input_token_cost = 5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 5e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.000003
output_cost_per_token = 0.000003
source = "https://ai.google.dev/pricing/gemini-3"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-3-pro-image-preview"]
input_cost_per_image = 0.0011
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 65536
max_output_tokens = 32768
max_tokens = 65536
mode = "image_generation"
output_cost_per_image = 0.134
output_cost_per_image_token = 0.00012
output_cost_per_token = 0.000012
output_cost_per_token_batches = 0.000006
source = "https://ai.google.dev/gemini-api/docs/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = false
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_vision = true
supports_web_search = true

[models."gemini-3-pro-preview"]
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost = 2e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
input_cost_per_token = 0.000002
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_batches = 0.000001
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.000012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_batches = 0.000006
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true

[models."gemini-embedding-001"]
input_cost_per_token = 1.5e-7
litellm_provider = "vertex_ai-embedding-models"
max_input_tokens = 2048
max_tokens = 2048
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 3072
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"

[models."gemini-flash-experimental"]
input_cost_per_character = 0
input_cost_per_token = 0
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0
output_cost_per_token = 0
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/gemini-experimental"
supports_function_calling = false
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-live-2.5-flash-preview-native-audio-09-2025"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000003
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_audio_token = 0.000012
output_cost_per_token = 0.000002
source = "https://ai.google.dev/gemini-api/docs/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true

[models."gemini-pro"]
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 32760
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-pro-experimental"]
input_cost_per_character = 0
input_cost_per_token = 0
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0
output_cost_per_token = 0
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/gemini-experimental"
supports_function_calling = false
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-pro-vision"]
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
litellm_provider = "vertex_ai-vision-models"
max_images_per_prompt = 16
max_input_tokens = 16384
max_output_tokens = 2048
max_tokens = 2048
max_video_length = 2
max_videos_per_prompt = 1
mode = "chat"
output_cost_per_token = 0.0000015
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."gemini/gemini-1.5-flash"]
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 1.5e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
rpm = 2000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-flash-001"]
cache_creation_input_token_cost = 0.000001
cache_read_input_token_cost = 1.875e-8
deprecation_date = "2025-05-24"
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 1.5e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
rpm = 2000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-flash-002"]
cache_creation_input_token_cost = 0.000001
cache_read_input_token_cost = 1.875e-8
deprecation_date = "2025-09-24"
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 1.5e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
rpm = 2000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-flash-8b"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 4000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-flash-8b-exp-0827"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1000000
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 4000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-flash-8b-exp-0924"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 4000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-flash-exp-0827"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 2000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-flash-latest"]
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 1.5e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
rpm = 2000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-pro"]
input_cost_per_token = 0.0000035
input_cost_per_token_above_128k_tokens = 0.000007
litellm_provider = "gemini"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000105
output_cost_per_token_above_128k_tokens = 0.000021
rpm = 1000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-pro-001"]
deprecation_date = "2025-05-24"
input_cost_per_token = 0.0000035
input_cost_per_token_above_128k_tokens = 0.000007
litellm_provider = "gemini"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000105
output_cost_per_token_above_128k_tokens = 0.000021
rpm = 1000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-pro-002"]
deprecation_date = "2025-09-24"
input_cost_per_token = 0.0000035
input_cost_per_token_above_128k_tokens = 0.000007
litellm_provider = "gemini"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000105
output_cost_per_token_above_128k_tokens = 0.000021
rpm = 1000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-pro-exp-0801"]
input_cost_per_token = 0.0000035
input_cost_per_token_above_128k_tokens = 0.000007
litellm_provider = "gemini"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000105
output_cost_per_token_above_128k_tokens = 0.000021
rpm = 1000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-pro-exp-0827"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
litellm_provider = "gemini"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 1000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-1.5-pro-latest"]
input_cost_per_token = 0.0000035
input_cost_per_token_above_128k_tokens = 0.000007
litellm_provider = "gemini"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000105
output_cost_per_token_above_128k_tokens = 0.000021
rpm = 1000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-2.0-flash"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 4e-7
rpm = 10000
source = "https://ai.google.dev/pricing#2_0flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 10000000

[models."gemini/gemini-2.0-flash-001"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 4e-7
rpm = 10000
source = "https://ai.google.dev/pricing#2_0flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = false
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 10000000

[models."gemini/gemini-2.0-flash-exp"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 10
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = true
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 4000000

[models."gemini/gemini-2.0-flash-lite"]
cache_read_input_token_cost = 1.875e-8
input_cost_per_audio_token = 7.5e-8
input_cost_per_token = 7.5e-8
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 50
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 3e-7
rpm = 4000
source = "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.0-flash-lite"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = true
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 4000000

[models."gemini/gemini-2.0-flash-lite-preview-02-05"]
cache_read_input_token_cost = 1.875e-8
input_cost_per_audio_token = 7.5e-8
input_cost_per_token = 7.5e-8
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 3e-7
rpm = 60000
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash-lite"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 10000000

[models."gemini/gemini-2.0-flash-live-001"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.0000021
input_cost_per_image = 0.0000021
input_cost_per_token = 3.5e-7
input_cost_per_video_per_second = 0.0000021
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_audio_token = 0.0000085
output_cost_per_token = 0.0000015
rpm = 10
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2-0-flash-live-001"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
supports_audio_output = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.0-flash-preview-image-generation"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 4e-7
rpm = 10000
source = "https://ai.google.dev/pricing#2_0flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 10000000

[models."gemini/gemini-2.0-flash-thinking-exp"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65536
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 10
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = true
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 4000000

[models."gemini/gemini-2.0-flash-thinking-exp-01-21"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65536
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 10
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 4000000

[models."gemini/gemini-2.0-pro-exp-02-05"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 2097152
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 2
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supports_audio_input = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true
tpm = 1000000

[models."gemini/gemini-2.5-computer-use-preview-10-2025"]
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "gemini"
max_images_per_prompt = 3000
max_input_tokens = 128000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
rpm = 2000
source = "https://ai.google.dev/gemini-api/docs/computer-use"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_computer_use = true
supports_function_calling = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 800000

[models."gemini/gemini-2.5-flash"]
cache_read_input_token_cost = 3e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
rpm = 100000
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 8000000

[models."gemini/gemini-2.5-flash-image"]
cache_read_input_token_cost = 3e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 32768
max_output_tokens = 32768
max_pdf_size_mb = 30
max_tokens = 32768
max_video_length = 1
max_videos_per_prompt = 10
mode = "image_generation"
output_cost_per_image = 0.039
output_cost_per_image_token = 0.00003
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
rpm = 100000
source = "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-flash-image"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = false
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 8000000

[models."gemini/gemini-2.5-flash-image-preview"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "image_generation"
output_cost_per_image = 0.039
output_cost_per_image_token = 0.00003
output_cost_per_reasoning_token = 0.00003
output_cost_per_token = 0.00003
rpm = 100000
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 8000000

[models."gemini/gemini-2.5-flash-lite"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 5e-7
input_cost_per_token = 1e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
rpm = 15
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-lite"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.5-flash-lite-preview-06-17"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 5e-7
input_cost_per_token = 1e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
rpm = 15
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-lite"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.5-flash-lite-preview-09-2025"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 3e-7
input_cost_per_token = 1e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
rpm = 15
source = "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.5-flash-preview-04-17"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 1.5e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000035
output_cost_per_token = 6e-7
rpm = 10
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.5-flash-preview-05-20"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
rpm = 10
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.5-flash-preview-09-2025"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
rpm = 15
source = "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.5-flash-preview-tts"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 1.5e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000035
output_cost_per_token = 6e-7
rpm = 10
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text"]
supported_output_modalities = ["audio"]
supports_audio_output = false
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.5-pro"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
rpm = 2000
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true
tpm = 800000

[models."gemini/gemini-2.5-pro-exp-03-25"]
cache_read_input_token_cost = 0
input_cost_per_token = 0
input_cost_per_token_above_200k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0
output_cost_per_token_above_200k_tokens = 0
rpm = 5
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-2.5-pro-preview-03-25"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
rpm = 10000
source = "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-pro-preview"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 10000000

[models."gemini/gemini-2.5-pro-preview-05-06"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
rpm = 10000
source = "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-pro-preview"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 10000000

[models."gemini/gemini-2.5-pro-preview-06-05"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
rpm = 10000
source = "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-pro-preview"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 10000000

[models."gemini/gemini-2.5-pro-preview-tts"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
rpm = 10000
source = "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-pro-preview"
supported_modalities = ["text"]
supported_output_modalities = ["audio"]
supports_audio_output = false
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true
tpm = 10000000

[models."gemini/gemini-3-flash-preview"]
cache_read_input_token_cost = 5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 5e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.000003
output_cost_per_token = 0.000003
rpm = 2000
source = "https://ai.google.dev/pricing/gemini-3"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 800000

[models."gemini/gemini-3-pro-image-preview"]
input_cost_per_image = 0.0011
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
litellm_provider = "gemini"
max_input_tokens = 65536
max_output_tokens = 32768
max_tokens = 65536
mode = "image_generation"
output_cost_per_image = 0.134
output_cost_per_image_token = 0.00012
output_cost_per_token = 0.000012
output_cost_per_token_batches = 0.000006
rpm = 1000
source = "https://ai.google.dev/gemini-api/docs/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = false
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_vision = true
supports_web_search = true
tpm = 4000000

[models."gemini/gemini-3-pro-preview"]
cache_read_input_token_cost = 2e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
input_cost_per_token = 0.000002
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_batches = 0.000001
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.000012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_batches = 0.000006
rpm = 2000
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true
tpm = 800000

[models."gemini/gemini-embedding-001"]
input_cost_per_token = 1.5e-7
litellm_provider = "gemini"
max_input_tokens = 2048
max_tokens = 2048
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 3072
rpm = 10000
source = "https://ai.google.dev/gemini-api/docs/embeddings#model-versions"
tpm = 10000000

[models."gemini/gemini-exp-1114"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 1000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-exp-1114".metadata]
notes = "Rate limits not documented for gemini-exp-1114. Assuming same as gemini-1.5-pro."
supports_tool_choice = true

[models."gemini/gemini-exp-1206"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 2097152
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 1000
source = "https://ai.google.dev/pricing"
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
tpm = 4000000

[models."gemini/gemini-exp-1206".metadata]
notes = "Rate limits not documented for gemini-exp-1206. Assuming same as gemini-1.5-pro."
supports_tool_choice = true

[models."gemini/gemini-flash-latest"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
rpm = 15
source = "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-flash-lite-latest"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 3e-7
input_cost_per_token = 1e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
rpm = 15
source = "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 250000

[models."gemini/gemini-gemma-2-27b-it"]
input_cost_per_token = 3.5e-7
litellm_provider = "gemini"
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000105
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."gemini/gemini-gemma-2-9b-it"]
input_cost_per_token = 3.5e-7
litellm_provider = "gemini"
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000105
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."gemini/gemini-live-2.5-flash-preview-native-audio-09-2025"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000003
input_cost_per_token = 3e-7
litellm_provider = "gemini"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_audio_token = 0.000012
output_cost_per_token = 0.000002
rpm = 100000
source = "https://ai.google.dev/gemini-api/docs/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = true
tpm = 8000000

[models."gemini/gemini-pro"]
input_cost_per_token = 3.5e-7
input_cost_per_token_above_128k_tokens = 7e-7
litellm_provider = "gemini"
max_input_tokens = 32760
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000105
output_cost_per_token_above_128k_tokens = 0.0000021
rpd = 30000
rpm = 360
source = "https://ai.google.dev/gemini-api/docs/models/gemini"
supports_function_calling = true
supports_tool_choice = true
tpm = 120000

[models."gemini/gemini-pro-vision"]
input_cost_per_token = 3.5e-7
input_cost_per_token_above_128k_tokens = 7e-7
litellm_provider = "gemini"
max_input_tokens = 30720
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 0.00000105
output_cost_per_token_above_128k_tokens = 0.0000021
rpd = 30000
rpm = 360
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true
tpm = 120000

[models."gemini/gemma-3-27b-it"]
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "gemini"
max_input_tokens = 131072
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
source = "https://aistudio.google.com"
supports_audio_output = false
supports_function_calling = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."gemini/imagen-3.0-fast-generate-001"]
litellm_provider = "gemini"
mode = "image_generation"
output_cost_per_image = 0.02
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"

[models."gemini/imagen-3.0-generate-001"]
litellm_provider = "gemini"
mode = "image_generation"
output_cost_per_image = 0.04
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"

[models."gemini/imagen-3.0-generate-002"]
litellm_provider = "gemini"
mode = "image_generation"
output_cost_per_image = 0.04
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"

[models."gemini/imagen-4.0-fast-generate-001"]
litellm_provider = "gemini"
mode = "image_generation"
output_cost_per_image = 0.02
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"

[models."gemini/imagen-4.0-generate-001"]
litellm_provider = "gemini"
mode = "image_generation"
output_cost_per_image = 0.04
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"

[models."gemini/imagen-4.0-ultra-generate-001"]
litellm_provider = "gemini"
mode = "image_generation"
output_cost_per_image = 0.06
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"

[models."gemini/learnlm-1.5-pro-experimental"]
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
litellm_provider = "gemini"
max_input_tokens = 32767
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
source = "https://aistudio.google.com"
supports_audio_output = false
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gemini/veo-2.0-generate-001"]
litellm_provider = "gemini"
max_input_tokens = 1024
max_tokens = 1024
mode = "video_generation"
output_cost_per_second = 0.35
source = "https://ai.google.dev/gemini-api/docs/video"
supported_modalities = ["text"]
supported_output_modalities = ["video"]

[models."gemini/veo-3.0-fast-generate-preview"]
litellm_provider = "gemini"
max_input_tokens = 1024
max_tokens = 1024
mode = "video_generation"
output_cost_per_second = 0.4
source = "https://ai.google.dev/gemini-api/docs/video"
supported_modalities = ["text"]
supported_output_modalities = ["video"]

[models."gemini/veo-3.0-generate-preview"]
litellm_provider = "gemini"
max_input_tokens = 1024
max_tokens = 1024
mode = "video_generation"
output_cost_per_second = 0.75
source = "https://ai.google.dev/gemini-api/docs/video"
supported_modalities = ["text"]
supported_output_modalities = ["video"]

[models."gemini/veo-3.1-fast-generate-001"]
litellm_provider = "gemini"
max_input_tokens = 1024
max_tokens = 1024
mode = "video_generation"
output_cost_per_second = 0.15
source = "https://ai.google.dev/gemini-api/docs/video"
supported_modalities = ["text"]
supported_output_modalities = ["video"]

[models."gemini/veo-3.1-fast-generate-preview"]
litellm_provider = "gemini"
max_input_tokens = 1024
max_tokens = 1024
mode = "video_generation"
output_cost_per_second = 0.15
source = "https://ai.google.dev/gemini-api/docs/video"
supported_modalities = ["text"]
supported_output_modalities = ["video"]

[models."gemini/veo-3.1-generate-001"]
litellm_provider = "gemini"
max_input_tokens = 1024
max_tokens = 1024
mode = "video_generation"
output_cost_per_second = 0.4
source = "https://ai.google.dev/gemini-api/docs/video"
supported_modalities = ["text"]
supported_output_modalities = ["video"]

[models."gemini/veo-3.1-generate-preview"]
litellm_provider = "gemini"
max_input_tokens = 1024
max_tokens = 1024
mode = "video_generation"
output_cost_per_second = 0.4
source = "https://ai.google.dev/gemini-api/docs/video"
supported_modalities = ["text"]
supported_output_modalities = ["video"]

[models."github_copilot/claude-haiku-4.5"]
litellm_provider = "github_copilot"
max_input_tokens = 128000
max_output_tokens = 16000
max_tokens = 16000
mode = "chat"
supported_endpoints = ["/v1/chat/completions"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = true

[models."github_copilot/claude-opus-4.5"]
litellm_provider = "github_copilot"
max_input_tokens = 128000
max_output_tokens = 16000
max_tokens = 16000
mode = "chat"
supported_endpoints = ["/v1/chat/completions"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = true

[models."github_copilot/claude-opus-41"]
litellm_provider = "github_copilot"
max_input_tokens = 80000
max_output_tokens = 16000
max_tokens = 16000
mode = "chat"
supported_endpoints = ["/v1/chat/completions"]
supports_vision = true

[models."github_copilot/claude-sonnet-4"]
litellm_provider = "github_copilot"
max_input_tokens = 128000
max_output_tokens = 16000
max_tokens = 16000
mode = "chat"
supported_endpoints = ["/v1/chat/completions"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = true

[models."github_copilot/claude-sonnet-4.5"]
litellm_provider = "github_copilot"
max_input_tokens = 128000
max_output_tokens = 16000
max_tokens = 16000
mode = "chat"
supported_endpoints = ["/v1/chat/completions"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = true

[models."github_copilot/gemini-2.5-pro"]
litellm_provider = "github_copilot"
max_input_tokens = 128000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = true

[models."github_copilot/gemini-3-pro-preview"]
litellm_provider = "github_copilot"
max_input_tokens = 128000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = true

[models."github_copilot/gpt-3.5-turbo"]
litellm_provider = "github_copilot"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
supports_function_calling = true

[models."github_copilot/gpt-3.5-turbo-0613"]
litellm_provider = "github_copilot"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
supports_function_calling = true

[models."github_copilot/gpt-4"]
litellm_provider = "github_copilot"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
supports_function_calling = true

[models."github_copilot/gpt-4-0613"]
litellm_provider = "github_copilot"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
supports_function_calling = true

[models."github_copilot/gpt-4-o-preview"]
litellm_provider = "github_copilot"
max_input_tokens = 64000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true

[models."github_copilot/gpt-4.1"]
litellm_provider = "github_copilot"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_vision = true

[models."github_copilot/gpt-4.1-2025-04-14"]
litellm_provider = "github_copilot"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_vision = true

[models."github_copilot/gpt-41-copilot"]
litellm_provider = "github_copilot"
mode = "completion"

[models."github_copilot/gpt-4o"]
litellm_provider = "github_copilot"
max_input_tokens = 64000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = true

[models."github_copilot/gpt-4o-2024-05-13"]
litellm_provider = "github_copilot"
max_input_tokens = 64000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = true

[models."github_copilot/gpt-4o-2024-08-06"]
litellm_provider = "github_copilot"
max_input_tokens = 64000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true

[models."github_copilot/gpt-4o-2024-11-20"]
litellm_provider = "github_copilot"
max_input_tokens = 64000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = true

[models."github_copilot/gpt-4o-mini"]
litellm_provider = "github_copilot"
max_input_tokens = 64000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true

[models."github_copilot/gpt-4o-mini-2024-07-18"]
litellm_provider = "github_copilot"
max_input_tokens = 64000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true

[models."github_copilot/gpt-5"]
litellm_provider = "github_copilot"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_vision = true

[models."github_copilot/gpt-5-mini"]
litellm_provider = "github_copilot"
max_input_tokens = 128000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_vision = true

[models."github_copilot/gpt-5.1"]
litellm_provider = "github_copilot"
max_input_tokens = 128000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_vision = true

[models."github_copilot/gpt-5.1-codex-max"]
litellm_provider = "github_copilot"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
supported_endpoints = ["/v1/responses"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_vision = true

[models."github_copilot/gpt-5.2"]
litellm_provider = "github_copilot"
max_input_tokens = 128000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_vision = true

[models."github_copilot/text-embedding-3-small"]
litellm_provider = "github_copilot"
max_input_tokens = 8191
max_tokens = 8191
mode = "embedding"

[models."github_copilot/text-embedding-3-small-inference"]
litellm_provider = "github_copilot"
max_input_tokens = 8191
max_tokens = 8191
mode = "embedding"

[models."github_copilot/text-embedding-ada-002"]
litellm_provider = "github_copilot"
max_input_tokens = 8191
max_tokens = 8191
mode = "embedding"

[models."global.amazon.nova-2-lite-v1:0"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.0000025
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_video_input = true
supports_vision = true

[models."global.anthropic.claude-haiku-4-5-20251001-v1:0"]
cache_creation_input_token_cost = 0.00000125
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000005
source = "https://aws.amazon.com/about-aws/whats-new/2025/10/claude-4-5-haiku-anthropic-amazon-bedrock"
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 346

[models."global.anthropic.claude-opus-4-5-20251101-v1:0"]
cache_creation_input_token_cost = 0.00000625
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000025
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."global.anthropic.claude-opus-4-5-20251101-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."global.anthropic.claude-sonnet-4-20250514-v1:0"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "bedrock_converse"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."global.anthropic.claude-sonnet-4-20250514-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."global.anthropic.claude-sonnet-4-5-20250929-v1:0"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 346

[models."global.anthropic.claude-sonnet-4-5-20250929-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."google.gemma-3-12b-it"]
input_cost_per_token = 9e-8
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2.9e-7
supports_system_messages = true
supports_vision = true

[models."google.gemma-3-27b-it"]
input_cost_per_token = 2.3e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 3.8e-7
supports_system_messages = true
supports_vision = true

[models."google.gemma-3-4b-it"]
input_cost_per_token = 4e-8
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 8e-8
supports_system_messages = true
supports_vision = true

[models."google_pse/search"]
input_cost_per_query = 0.005
litellm_provider = "google_pse"
mode = "search"

[models."gpt-3.5-turbo"]
input_cost_per_token = 5e-7
litellm_provider = "openai"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 4097
mode = "chat"
output_cost_per_token = 0.0000015
supports_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-0125"]
input_cost_per_token = 5e-7
litellm_provider = "openai"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 16385
mode = "chat"
output_cost_per_token = 0.0000015
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-0301"]
input_cost_per_token = 0.0000015
litellm_provider = "openai"
max_input_tokens = 4097
max_output_tokens = 4096
max_tokens = 4097
mode = "chat"
output_cost_per_token = 0.000002
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-0613"]
input_cost_per_token = 0.0000015
litellm_provider = "openai"
max_input_tokens = 4097
max_output_tokens = 4096
max_tokens = 4097
mode = "chat"
output_cost_per_token = 0.000002
supports_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-1106"]
deprecation_date = "2026-09-28"
input_cost_per_token = 0.000001
litellm_provider = "openai"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 16385
mode = "chat"
output_cost_per_token = 0.000002
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-16k"]
input_cost_per_token = 0.000003
litellm_provider = "openai"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 16385
mode = "chat"
output_cost_per_token = 0.000004
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-16k-0613"]
input_cost_per_token = 0.000003
litellm_provider = "openai"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 16385
mode = "chat"
output_cost_per_token = 0.000004
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-instruct"]
input_cost_per_token = 0.0000015
litellm_provider = "text-completion-openai"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
mode = "completion"
output_cost_per_token = 0.000002

[models."gpt-3.5-turbo-instruct-0914"]
input_cost_per_token = 0.0000015
litellm_provider = "text-completion-openai"
max_input_tokens = 8192
max_output_tokens = 4097
max_tokens = 4097
mode = "completion"
output_cost_per_token = 0.000002

[models."gpt-4"]
input_cost_per_token = 0.00003
litellm_provider = "openai"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-0125-preview"]
deprecation_date = "2026-03-26"
input_cost_per_token = 0.00001
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-0314"]
input_cost_per_token = 0.00003
litellm_provider = "openai"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00006
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-0613"]
deprecation_date = "2025-06-06"
input_cost_per_token = 0.00003
litellm_provider = "openai"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-1106-preview"]
deprecation_date = "2026-03-26"
input_cost_per_token = 0.00001
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-1106-vision-preview"]
deprecation_date = "2024-12-06"
input_cost_per_token = 0.00001
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_pdf_input = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4-32k"]
input_cost_per_token = 0.00006
litellm_provider = "openai"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00012
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-32k-0314"]
input_cost_per_token = 0.00006
litellm_provider = "openai"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00012
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-32k-0613"]
input_cost_per_token = 0.00006
litellm_provider = "openai"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00012
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-turbo"]
input_cost_per_token = 0.00001
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4-turbo-2024-04-09"]
input_cost_per_token = 0.00001
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4-turbo-preview"]
input_cost_per_token = 0.00001
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-vision-preview"]
deprecation_date = "2024-12-06"
input_cost_per_token = 0.00001
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00003
supports_pdf_input = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4.1"]
cache_read_input_token_cost = 5e-7
cache_read_input_token_cost_priority = 8.75e-7
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
input_cost_per_token_priority = 0.0000035
litellm_provider = "openai"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000008
output_cost_per_token_batches = 0.000004
output_cost_per_token_priority = 0.000014
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4.1-2025-04-14"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
litellm_provider = "openai"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000008
output_cost_per_token_batches = 0.000004
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4.1-mini"]
cache_read_input_token_cost = 1e-7
cache_read_input_token_cost_priority = 1.75e-7
input_cost_per_token = 4e-7
input_cost_per_token_batches = 2e-7
input_cost_per_token_priority = 7e-7
litellm_provider = "openai"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000016
output_cost_per_token_batches = 8e-7
output_cost_per_token_priority = 0.0000028
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4.1-mini-2025-04-14"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 4e-7
input_cost_per_token_batches = 2e-7
litellm_provider = "openai"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000016
output_cost_per_token_batches = 8e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4.1-nano"]
cache_read_input_token_cost = 2.5e-8
cache_read_input_token_cost_priority = 5e-8
input_cost_per_token = 1e-7
input_cost_per_token_batches = 5e-8
input_cost_per_token_priority = 2e-7
litellm_provider = "openai"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 4e-7
output_cost_per_token_batches = 2e-7
output_cost_per_token_priority = 8e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4.1-nano-2025-04-14"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 1e-7
input_cost_per_token_batches = 5e-8
litellm_provider = "openai"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 4e-7
output_cost_per_token_batches = 2e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4.5-preview"]
cache_read_input_token_cost = 0.0000375
input_cost_per_token = 0.000075
input_cost_per_token_batches = 0.0000375
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00015
output_cost_per_token_batches = 0.000075
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4.5-preview-2025-02-27"]
cache_read_input_token_cost = 0.0000375
deprecation_date = "2025-07-14"
input_cost_per_token = 0.000075
input_cost_per_token_batches = 0.0000375
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00015
output_cost_per_token_batches = 0.000075
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o"]
cache_read_input_token_cost = 0.00000125
cache_read_input_token_cost_priority = 0.000002125
input_cost_per_token = 0.0000025
input_cost_per_token_batches = 0.00000125
input_cost_per_token_priority = 0.00000425
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_batches = 0.000005
output_cost_per_token_priority = 0.000017
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o-2024-05-13"]
input_cost_per_token = 0.000005
input_cost_per_token_batches = 0.0000025
input_cost_per_token_priority = 0.00000875
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_batches = 0.0000075
output_cost_per_token_priority = 0.00002625
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o-2024-08-06"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
input_cost_per_token_batches = 0.00000125
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_batches = 0.000005
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o-2024-11-20"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
input_cost_per_token_batches = 0.00000125
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_batches = 0.000005
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o-audio-preview"]
input_cost_per_audio_token = 0.0001
input_cost_per_token = 0.0000025
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_audio_token = 0.0002
output_cost_per_token = 0.00001
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-audio-preview-2024-10-01"]
input_cost_per_audio_token = 0.0001
input_cost_per_token = 0.0000025
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_audio_token = 0.0002
output_cost_per_token = 0.00001
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-audio-preview-2024-12-17"]
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.0000025
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00001
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-audio-preview-2025-06-03"]
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.0000025
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00001
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini"]
cache_read_input_token_cost = 7.5e-8
cache_read_input_token_cost_priority = 1.25e-7
input_cost_per_token = 1.5e-7
input_cost_per_token_batches = 7.5e-8
input_cost_per_token_priority = 2.5e-7
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 6e-7
output_cost_per_token_batches = 3e-7
output_cost_per_token_priority = 0.000001
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o-mini-2024-07-18"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.5e-7
input_cost_per_token_batches = 7.5e-8
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 6e-7
output_cost_per_token_batches = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o-mini-2024-07-18".search_context_cost_per_query]
search_context_size_high = 0.03
search_context_size_low = 0.025
search_context_size_medium = 0.0275

[models."gpt-4o-mini-audio-preview"]
input_cost_per_audio_token = 0.00001
input_cost_per_token = 1.5e-7
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_audio_token = 0.00002
output_cost_per_token = 6e-7
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini-audio-preview-2024-12-17"]
input_cost_per_audio_token = 0.00001
input_cost_per_token = 1.5e-7
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_audio_token = 0.00002
output_cost_per_token = 6e-7
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini-realtime-preview"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_token_cost = 3e-7
input_cost_per_audio_token = 0.00001
input_cost_per_token = 6e-7
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini-realtime-preview-2024-12-17"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_token_cost = 3e-7
input_cost_per_audio_token = 0.00001
input_cost_per_token = 6e-7
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini-search-preview"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.5e-7
input_cost_per_token_batches = 7.5e-8
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 6e-7
output_cost_per_token_batches = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gpt-4o-mini-search-preview".search_context_cost_per_query]
search_context_size_high = 0.03
search_context_size_low = 0.025
search_context_size_medium = 0.0275

[models."gpt-4o-mini-search-preview-2025-03-11"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.5e-7
input_cost_per_token_batches = 7.5e-8
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 6e-7
output_cost_per_token_batches = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o-mini-transcribe"]
input_cost_per_audio_token = 0.000003
input_cost_per_token = 0.00000125
litellm_provider = "openai"
max_input_tokens = 16000
max_output_tokens = 2000
mode = "audio_transcription"
output_cost_per_token = 0.000005
supported_endpoints = ["/v1/audio/transcriptions"]

[models."gpt-4o-mini-tts"]
input_cost_per_token = 0.0000025
litellm_provider = "openai"
mode = "audio_speech"
output_cost_per_audio_token = 0.000012
output_cost_per_second = 0.00025
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/audio/speech"]
supported_modalities = ["text", "audio"]
supported_output_modalities = ["audio"]

[models."gpt-4o-realtime-preview"]
cache_read_input_token_cost = 0.0000025
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.000005
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00002
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-realtime-preview-2024-10-01"]
cache_creation_input_audio_token_cost = 0.00002
cache_read_input_token_cost = 0.0000025
input_cost_per_audio_token = 0.0001
input_cost_per_token = 0.000005
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.0002
output_cost_per_token = 0.00002
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-realtime-preview-2024-12-17"]
cache_read_input_token_cost = 0.0000025
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.000005
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00002
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-realtime-preview-2025-06-03"]
cache_read_input_token_cost = 0.0000025
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.000005
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00002
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-search-preview"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
input_cost_per_token_batches = 0.00000125
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_batches = 0.000005
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gpt-4o-search-preview".search_context_cost_per_query]
search_context_size_high = 0.05
search_context_size_low = 0.03
search_context_size_medium = 0.035

[models."gpt-4o-search-preview-2025-03-11"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
input_cost_per_token_batches = 0.00000125
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_batches = 0.000005
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-4o-transcribe"]
input_cost_per_audio_token = 0.000006
input_cost_per_token = 0.0000025
litellm_provider = "openai"
max_input_tokens = 16000
max_output_tokens = 2000
mode = "audio_transcription"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/audio/transcriptions"]

[models."gpt-4o-transcribe-diarize"]
input_cost_per_audio_token = 0.000006
input_cost_per_token = 0.0000025
litellm_provider = "openai"
max_input_tokens = 16000
max_output_tokens = 2000
mode = "audio_transcription"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/audio/transcriptions"]

[models."gpt-5"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_flex = 6.25e-8
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_flex = 6.25e-7
input_cost_per_token_priority = 0.0000025
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_flex = 0.000005
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5-2025-08-07"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_flex = 6.25e-8
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_flex = 6.25e-7
input_cost_per_token_priority = 0.0000025
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_flex = 0.000005
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5-chat"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = false
supports_native_streaming = true
supports_parallel_function_calling = false
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = false
supports_vision = true

[models."gpt-5-chat-latest"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = false
supports_native_streaming = true
supports_parallel_function_calling = false
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = false
supports_vision = true

[models."gpt-5-codex"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."gpt-5-mini"]
cache_read_input_token_cost = 2.5e-8
cache_read_input_token_cost_flex = 1.25e-8
cache_read_input_token_cost_priority = 4.5e-8
input_cost_per_token = 2.5e-7
input_cost_per_token_flex = 1.25e-7
input_cost_per_token_priority = 4.5e-7
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000002
output_cost_per_token_flex = 0.000001
output_cost_per_token_priority = 0.0000036
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5-mini-2025-08-07"]
cache_read_input_token_cost = 2.5e-8
cache_read_input_token_cost_flex = 1.25e-8
cache_read_input_token_cost_priority = 4.5e-8
input_cost_per_token = 2.5e-7
input_cost_per_token_flex = 1.25e-7
input_cost_per_token_priority = 4.5e-7
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000002
output_cost_per_token_flex = 0.000001
output_cost_per_token_priority = 0.0000036
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5-nano"]
cache_read_input_token_cost = 5e-9
cache_read_input_token_cost_flex = 2.5e-9
input_cost_per_token = 5e-8
input_cost_per_token_flex = 2.5e-8
input_cost_per_token_priority = 0.0000025
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 4e-7
output_cost_per_token_flex = 2e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5-nano-2025-08-07"]
cache_read_input_token_cost = 5e-9
cache_read_input_token_cost_flex = 2.5e-9
input_cost_per_token = 5e-8
input_cost_per_token_flex = 2.5e-8
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 4e-7
output_cost_per_token_flex = 2e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5-pro"]
input_cost_per_token = 0.000015
input_cost_per_token_batches = 0.0000075
litellm_provider = "openai"
max_input_tokens = 400000
max_output_tokens = 272000
max_tokens = 272000
mode = "responses"
output_cost_per_token = 0.00012
output_cost_per_token_batches = 0.00006
supported_endpoints = ["/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = false
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gpt-5-pro-2025-10-06"]
input_cost_per_token = 0.000015
input_cost_per_token_batches = 0.0000075
litellm_provider = "openai"
max_input_tokens = 400000
max_output_tokens = 272000
max_tokens = 272000
mode = "responses"
output_cost_per_token = 0.00012
output_cost_per_token_batches = 0.00006
supported_endpoints = ["/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = false
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gpt-5.1"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5.1-2025-11-13"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5.1-chat-latest"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = false
supports_native_streaming = true
supports_parallel_function_calling = false
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = false
supports_vision = true

[models."gpt-5.1-codex"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."gpt-5.1-codex-max"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "openai"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."gpt-5.1-codex-mini"]
cache_read_input_token_cost = 2.5e-8
cache_read_input_token_cost_priority = 4.5e-8
input_cost_per_token = 2.5e-7
input_cost_per_token_priority = 4.5e-7
litellm_provider = "openai"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.000002
output_cost_per_token_priority = 0.0000036
supported_endpoints = ["/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true
supports_vision = true

[models."gpt-5.2"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
litellm_provider = "openai"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5.2-2025-12-11"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
litellm_provider = "openai"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5.2-chat-latest"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."gpt-5.2-pro"]
input_cost_per_token = 0.000021
litellm_provider = "openai"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.000168
supported_endpoints = ["/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gpt-5.2-pro-2025-12-11"]
input_cost_per_token = 0.000021
litellm_provider = "openai"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 128000
mode = "responses"
output_cost_per_token = 0.000168
supported_endpoints = ["/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."gpt-image-1"]
input_cost_per_image = 0.042
input_cost_per_image_token = 0.00001
input_cost_per_pixel = 4.0054321e-8
input_cost_per_token = 0.000005
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0
output_cost_per_token = 0.00004
supported_endpoints = ["/v1/images/generations"]

[models."gpt-image-1-mini"]
cache_read_input_image_token_cost = 2.5e-7
cache_read_input_token_cost = 2e-7
input_cost_per_image_token = 0.0000025
input_cost_per_token = 0.000002
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_image_token = 0.000008
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."gpt-image-1.5"]
cache_read_input_image_token_cost = 0.000002
cache_read_input_token_cost = 0.00000125
input_cost_per_image_token = 0.000008
input_cost_per_token = 0.000005
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_image_token = 0.000032
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/images/generations"]
supports_pdf_input = true
supports_vision = true

[models."gpt-image-1.5-2025-12-16"]
cache_read_input_image_token_cost = 0.000002
cache_read_input_token_cost = 0.00000125
input_cost_per_image_token = 0.000008
input_cost_per_token = 0.000005
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_image_token = 0.000032
output_cost_per_token = 0.00001
supported_endpoints = ["/v1/images/generations"]
supports_pdf_input = true
supports_vision = true

[models."gpt-realtime"]
cache_creation_input_audio_token_cost = 4e-7
cache_read_input_token_cost = 4e-7
input_cost_per_audio_token = 0.000032
input_cost_per_image = 0.000005
input_cost_per_token = 0.000004
litellm_provider = "openai"
max_input_tokens = 32000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.000064
output_cost_per_token = 0.000016
supported_endpoints = ["/v1/realtime"]
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text", "audio"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-realtime-2025-08-28"]
cache_creation_input_audio_token_cost = 4e-7
cache_read_input_token_cost = 4e-7
input_cost_per_audio_token = 0.000032
input_cost_per_image = 0.000005
input_cost_per_token = 0.000004
litellm_provider = "openai"
max_input_tokens = 32000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.000064
output_cost_per_token = 0.000016
supported_endpoints = ["/v1/realtime"]
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text", "audio"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-realtime-mini"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_audio_token_cost = 3e-7
input_cost_per_audio_token = 0.00001
input_cost_per_token = 6e-7
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024
supported_endpoints = ["/v1/realtime"]
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text", "audio"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gradient_ai/alibaba-qwen3-32b"]
litellm_provider = "gradient_ai"
max_tokens = 2048
mode = "chat"
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text"]
supports_tool_choice = false

[models."gradient_ai/anthropic-claude-3-opus"]
input_cost_per_token = 0.000015
litellm_provider = "gradient_ai"
max_tokens = 1024
mode = "chat"
output_cost_per_token = 0.000075
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text"]
supports_tool_choice = false

[models."gradient_ai/anthropic-claude-3.5-haiku"]
input_cost_per_token = 8e-7
litellm_provider = "gradient_ai"
max_tokens = 1024
mode = "chat"
output_cost_per_token = 0.000004
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text"]
supports_tool_choice = false

[models."gradient_ai/anthropic-claude-3.5-sonnet"]
input_cost_per_token = 0.000003
litellm_provider = "gradient_ai"
max_tokens = 1024
mode = "chat"
output_cost_per_token = 0.000015
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text"]
supports_tool_choice = false

[models."gradient_ai/anthropic-claude-3.7-sonnet"]
input_cost_per_token = 0.000003
litellm_provider = "gradient_ai"
max_tokens = 1024
mode = "chat"
output_cost_per_token = 0.000015
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text"]
supports_tool_choice = false

[models."gradient_ai/deepseek-r1-distill-llama-70b"]
input_cost_per_token = 9.9e-7
litellm_provider = "gradient_ai"
max_tokens = 8000
mode = "chat"
output_cost_per_token = 9.9e-7
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text"]
supports_tool_choice = false

[models."gradient_ai/llama3-8b-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "gradient_ai"
max_tokens = 512
mode = "chat"
output_cost_per_token = 2e-7
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text"]
supports_tool_choice = false

[models."gradient_ai/llama3.3-70b-instruct"]
input_cost_per_token = 6.5e-7
litellm_provider = "gradient_ai"
max_tokens = 2048
mode = "chat"
output_cost_per_token = 6.5e-7
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text"]
supports_tool_choice = false

[models."gradient_ai/mistral-nemo-instruct-2407"]
input_cost_per_token = 3e-7
litellm_provider = "gradient_ai"
max_tokens = 512
mode = "chat"
output_cost_per_token = 3e-7
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text"]
supports_tool_choice = false

[models."gradient_ai/openai-gpt-4o"]
litellm_provider = "gradient_ai"
max_tokens = 16384
mode = "chat"
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text"]
supports_tool_choice = false

[models."gradient_ai/openai-gpt-4o-mini"]
litellm_provider = "gradient_ai"
max_tokens = 16384
mode = "chat"
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text"]
supports_tool_choice = false

[models."gradient_ai/openai-o3"]
input_cost_per_token = 0.000002
litellm_provider = "gradient_ai"
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.000008
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text"]
supports_tool_choice = false

[models."gradient_ai/openai-o3-mini"]
input_cost_per_token = 0.0000011
litellm_provider = "gradient_ai"
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.0000044
supported_endpoints = ["/v1/chat/completions"]
supported_modalities = ["text"]
supports_tool_choice = false

[models."groq/llama-3.1-8b-instant"]
input_cost_per_token = 5e-8
litellm_provider = "groq"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 8e-8
supports_function_calling = true
supports_response_schema = false
supports_tool_choice = true

[models."groq/llama-3.3-70b-versatile"]
input_cost_per_token = 5.9e-7
litellm_provider = "groq"
max_input_tokens = 128000
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 7.9e-7
supports_function_calling = true
supports_response_schema = false
supports_tool_choice = true

[models."groq/meta-llama/llama-4-maverick-17b-128e-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "groq"
max_input_tokens = 131072
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."groq/meta-llama/llama-4-scout-17b-16e-instruct"]
input_cost_per_token = 1.1e-7
litellm_provider = "groq"
max_input_tokens = 131072
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 3.4e-7
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."groq/meta-llama/llama-guard-4-12b"]
input_cost_per_token = 2e-7
litellm_provider = "groq"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7

[models."groq/moonshotai/kimi-k2-instruct-0905"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000001
litellm_provider = "groq"
max_input_tokens = 262144
max_output_tokens = 16384
max_tokens = 278528
mode = "chat"
output_cost_per_token = 0.000003
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."groq/openai/gpt-oss-120b"]
input_cost_per_token = 1.5e-7
litellm_provider = "groq"
max_input_tokens = 131072
max_output_tokens = 32766
max_tokens = 32766
mode = "chat"
output_cost_per_token = 7.5e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."groq/openai/gpt-oss-20b"]
input_cost_per_token = 1e-7
litellm_provider = "groq"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 5e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."groq/playai-tts"]
input_cost_per_character = 0.00005
litellm_provider = "groq"
max_input_tokens = 10000
max_output_tokens = 10000
max_tokens = 10000
mode = "audio_speech"

[models."groq/qwen/qwen3-32b"]
input_cost_per_token = 2.9e-7
litellm_provider = "groq"
max_input_tokens = 131000
max_output_tokens = 131000
max_tokens = 131000
mode = "chat"
output_cost_per_token = 5.9e-7
supports_function_calling = true
supports_reasoning = true
supports_response_schema = false
supports_tool_choice = true

[models."groq/whisper-large-v3"]
input_cost_per_second = 0.00003083
litellm_provider = "groq"
mode = "audio_transcription"
output_cost_per_second = 0

[models."groq/whisper-large-v3-turbo"]
input_cost_per_second = 0.00001111
litellm_provider = "groq"
mode = "audio_transcription"
output_cost_per_second = 0

[models."hd/1024-x-1024/dall-e-3"]
input_cost_per_pixel = 7.629e-8
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0

[models."hd/1024-x-1792/dall-e-3"]
input_cost_per_pixel = 6.539e-8
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0

[models."hd/1792-x-1024/dall-e-3"]
input_cost_per_pixel = 6.539e-8
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0

[models."heroku/claude-3-5-haiku"]
litellm_provider = "heroku"
max_tokens = 4096
mode = "chat"
supports_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."heroku/claude-3-5-sonnet-latest"]
litellm_provider = "heroku"
max_tokens = 8192
mode = "chat"
supports_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."heroku/claude-3-7-sonnet"]
litellm_provider = "heroku"
max_tokens = 8192
mode = "chat"
supports_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."heroku/claude-4-sonnet"]
litellm_provider = "heroku"
max_tokens = 8192
mode = "chat"
supports_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."high/1024-x-1024/gpt-image-1"]
input_cost_per_image = 0.167
input_cost_per_pixel = 1.59263611e-7
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."high/1024-x-1536/gpt-image-1"]
input_cost_per_image = 0.25
input_cost_per_pixel = 1.58945719e-7
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."high/1536-x-1024/gpt-image-1"]
input_cost_per_image = 0.25
input_cost_per_pixel = 1.58945719e-7
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."hyperbolic/NousResearch/Hermes-3-Llama-3.1-70B"]
input_cost_per_token = 1.2e-7
litellm_provider = "hyperbolic"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hyperbolic/Qwen/QwQ-32B"]
input_cost_per_token = 2e-7
litellm_provider = "hyperbolic"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hyperbolic/Qwen/Qwen2.5-72B-Instruct"]
input_cost_per_token = 1.2e-7
litellm_provider = "hyperbolic"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hyperbolic/Qwen/Qwen2.5-Coder-32B-Instruct"]
input_cost_per_token = 1.2e-7
litellm_provider = "hyperbolic"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hyperbolic/Qwen/Qwen3-235B-A22B"]
input_cost_per_token = 0.000002
litellm_provider = "hyperbolic"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000002
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hyperbolic/deepseek-ai/DeepSeek-R1"]
input_cost_per_token = 4e-7
litellm_provider = "hyperbolic"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 4e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hyperbolic/deepseek-ai/DeepSeek-R1-0528"]
input_cost_per_token = 2.5e-7
litellm_provider = "hyperbolic"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2.5e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hyperbolic/deepseek-ai/DeepSeek-V3"]
input_cost_per_token = 2e-7
litellm_provider = "hyperbolic"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 2e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hyperbolic/deepseek-ai/DeepSeek-V3-0324"]
input_cost_per_token = 4e-7
litellm_provider = "hyperbolic"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 4e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hyperbolic/meta-llama/Llama-3.2-3B-Instruct"]
input_cost_per_token = 1.2e-7
litellm_provider = "hyperbolic"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hyperbolic/meta-llama/Llama-3.3-70B-Instruct"]
input_cost_per_token = 1.2e-7
litellm_provider = "hyperbolic"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hyperbolic/meta-llama/Meta-Llama-3-70B-Instruct"]
input_cost_per_token = 1.2e-7
litellm_provider = "hyperbolic"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hyperbolic/meta-llama/Meta-Llama-3.1-405B-Instruct"]
input_cost_per_token = 1.2e-7
litellm_provider = "hyperbolic"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hyperbolic/meta-llama/Meta-Llama-3.1-70B-Instruct"]
input_cost_per_token = 1.2e-7
litellm_provider = "hyperbolic"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hyperbolic/meta-llama/Meta-Llama-3.1-8B-Instruct"]
input_cost_per_token = 1.2e-7
litellm_provider = "hyperbolic"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hyperbolic/moonshotai/Kimi-K2-Instruct"]
input_cost_per_token = 0.000002
litellm_provider = "hyperbolic"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000002
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."j2-light"]
input_cost_per_token = 0.000003
litellm_provider = "ai21"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "completion"
output_cost_per_token = 0.000003

[models."j2-mid"]
input_cost_per_token = 0.00001
litellm_provider = "ai21"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "completion"
output_cost_per_token = 0.00001

[models."j2-ultra"]
input_cost_per_token = 0.000015
litellm_provider = "ai21"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "completion"
output_cost_per_token = 0.000015

[models."jamba-1.5"]
input_cost_per_token = 2e-7
litellm_provider = "ai21"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 4e-7
supports_tool_choice = true

[models."jamba-1.5-large"]
input_cost_per_token = 0.000002
litellm_provider = "ai21"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.000008
supports_tool_choice = true

[models."jamba-1.5-large@001"]
input_cost_per_token = 0.000002
litellm_provider = "ai21"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.000008
supports_tool_choice = true

[models."jamba-1.5-mini"]
input_cost_per_token = 2e-7
litellm_provider = "ai21"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 4e-7
supports_tool_choice = true

[models."jamba-1.5-mini@001"]
input_cost_per_token = 2e-7
litellm_provider = "ai21"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 4e-7
supports_tool_choice = true

[models."jamba-large-1.6"]
input_cost_per_token = 0.000002
litellm_provider = "ai21"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.000008
supports_tool_choice = true

[models."jamba-large-1.7"]
input_cost_per_token = 0.000002
litellm_provider = "ai21"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.000008
supports_tool_choice = true

[models."jamba-mini-1.6"]
input_cost_per_token = 2e-7
litellm_provider = "ai21"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 4e-7
supports_tool_choice = true

[models."jamba-mini-1.7"]
input_cost_per_token = 2e-7
litellm_provider = "ai21"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 4e-7
supports_tool_choice = true

[models."jina-reranker-v2-base-multilingual"]
input_cost_per_token = 1.8e-8
litellm_provider = "jina_ai"
max_document_chunks_per_query = 2048
max_input_tokens = 1024
max_output_tokens = 1024
max_tokens = 1024
mode = "rerank"
output_cost_per_token = 1.8e-8

[models."jp.anthropic.claude-haiku-4-5-20251001-v1:0"]
cache_creation_input_token_cost = 0.000001375
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.0000055
source = "https://aws.amazon.com/about-aws/whats-new/2025/10/claude-4-5-haiku-anthropic-amazon-bedrock"
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 346

[models."jp.anthropic.claude-sonnet-4-5-20250929-v1:0"]
cache_creation_input_token_cost = 0.000004125
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost = 3.3e-7
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token = 0.0000033
input_cost_per_token_above_200k_tokens = 0.0000066
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.0000165
output_cost_per_token_above_200k_tokens = 0.00002475
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 346

[models."jp.anthropic.claude-sonnet-4-5-20250929-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."lambda_ai/deepseek-llama3.3-70b"]
input_cost_per_token = 2e-7
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_reasoning = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/deepseek-r1-0528"]
input_cost_per_token = 2e-7
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_reasoning = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/deepseek-r1-671b"]
input_cost_per_token = 8e-7
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 8e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_reasoning = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/deepseek-v3-0324"]
input_cost_per_token = 2e-7
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/hermes3-405b"]
input_cost_per_token = 8e-7
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 8e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/hermes3-70b"]
input_cost_per_token = 1.2e-7
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/hermes3-8b"]
input_cost_per_token = 2.5e-8
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 4e-8
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/lfm-40b"]
input_cost_per_token = 1e-7
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/lfm-7b"]
input_cost_per_token = 2.5e-8
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 4e-8
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/llama-4-maverick-17b-128e-instruct-fp8"]
input_cost_per_token = 5e-8
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 8192
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/llama-4-scout-17b-16e-instruct"]
input_cost_per_token = 5e-8
litellm_provider = "lambda_ai"
max_input_tokens = 16384
max_output_tokens = 8192
max_tokens = 16384
mode = "chat"
output_cost_per_token = 1e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/llama3.1-405b-instruct-fp8"]
input_cost_per_token = 8e-7
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 8e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/llama3.1-70b-instruct-fp8"]
input_cost_per_token = 1.2e-7
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/llama3.1-8b-instruct"]
input_cost_per_token = 2.5e-8
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 4e-8
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/llama3.1-nemotron-70b-instruct-fp8"]
input_cost_per_token = 1.2e-7
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/llama3.2-11b-vision-instruct"]
input_cost_per_token = 1.5e-8
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2.5e-8
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."lambda_ai/llama3.2-3b-instruct"]
input_cost_per_token = 1.5e-8
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2.5e-8
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/llama3.3-70b-instruct-fp8"]
input_cost_per_token = 1.2e-7
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/qwen25-coder-32b-instruct"]
input_cost_per_token = 5e-8
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lambda_ai/qwen3-32b-fp8"]
input_cost_per_token = 5e-8
litellm_provider = "lambda_ai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_reasoning = true
supports_system_messages = true
supports_tool_choice = true

[models."lemonade/Gemma-3-4b-it-GGUF"]
input_cost_per_token = 0
litellm_provider = "lemonade"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."lemonade/Qwen3-4B-Instruct-2507-GGUF"]
input_cost_per_token = 0
litellm_provider = "lemonade"
max_input_tokens = 262144
max_output_tokens = 32768
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."lemonade/Qwen3-Coder-30B-A3B-Instruct-GGUF"]
input_cost_per_token = 0
litellm_provider = "lemonade"
max_input_tokens = 262144
max_output_tokens = 32768
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."lemonade/gpt-oss-120b-mxfp-GGUF"]
input_cost_per_token = 0
litellm_provider = "lemonade"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."lemonade/gpt-oss-20b-mxfp4-GGUF"]
input_cost_per_token = 0
litellm_provider = "lemonade"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."linkup/search"]
input_cost_per_query = 0.00587
litellm_provider = "linkup"
mode = "search"

[models."linkup/search-deep"]
input_cost_per_query = 0.05867
litellm_provider = "linkup"
mode = "search"

[models."low/1024-x-1024/gpt-image-1"]
input_cost_per_image = 0.011
input_cost_per_pixel = 1.0490417e-8
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."low/1024-x-1024/gpt-image-1-mini"]
input_cost_per_image = 0.005
litellm_provider = "openai"
mode = "image_generation"
supported_endpoints = ["/v1/images/generations"]

[models."low/1024-x-1536/gpt-image-1"]
input_cost_per_image = 0.016
input_cost_per_pixel = 1.0172526e-8
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."low/1024-x-1536/gpt-image-1-mini"]
input_cost_per_image = 0.006
litellm_provider = "openai"
mode = "image_generation"
supported_endpoints = ["/v1/images/generations"]

[models."low/1536-x-1024/gpt-image-1"]
input_cost_per_image = 0.016
input_cost_per_pixel = 1.0172526e-8
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."low/1536-x-1024/gpt-image-1-mini"]
input_cost_per_image = 0.006
litellm_provider = "openai"
mode = "image_generation"
supported_endpoints = ["/v1/images/generations"]

[models."luminous-base"]
input_cost_per_token = 0.00003
litellm_provider = "aleph_alpha"
max_tokens = 2048
mode = "completion"
output_cost_per_token = 0.000033

[models."luminous-base-control"]
input_cost_per_token = 0.0000375
litellm_provider = "aleph_alpha"
max_tokens = 2048
mode = "chat"
output_cost_per_token = 0.00004125

[models."luminous-extended"]
input_cost_per_token = 0.000045
litellm_provider = "aleph_alpha"
max_tokens = 2048
mode = "completion"
output_cost_per_token = 0.0000495

[models."luminous-extended-control"]
input_cost_per_token = 0.00005625
litellm_provider = "aleph_alpha"
max_tokens = 2048
mode = "chat"
output_cost_per_token = 0.000061875

[models."luminous-supreme"]
input_cost_per_token = 0.000175
litellm_provider = "aleph_alpha"
max_tokens = 2048
mode = "completion"
output_cost_per_token = 0.0001925

[models."luminous-supreme-control"]
input_cost_per_token = 0.00021875
litellm_provider = "aleph_alpha"
max_tokens = 2048
mode = "chat"
output_cost_per_token = 0.000240625

[models."max-x-max/50-steps/stability.stable-diffusion-xl-v0"]
litellm_provider = "bedrock"
max_input_tokens = 77
max_tokens = 77
mode = "image_generation"
output_cost_per_image = 0.036

[models."max-x-max/max-steps/stability.stable-diffusion-xl-v0"]
litellm_provider = "bedrock"
max_input_tokens = 77
max_tokens = 77
mode = "image_generation"
output_cost_per_image = 0.072

[models."medium/1024-x-1024/gpt-image-1"]
input_cost_per_image = 0.042
input_cost_per_pixel = 4.0054321e-8
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."medium/1024-x-1024/gpt-image-1-mini"]
input_cost_per_image = 0.011
litellm_provider = "openai"
mode = "image_generation"
supported_endpoints = ["/v1/images/generations"]

[models."medium/1024-x-1536/gpt-image-1"]
input_cost_per_image = 0.063
input_cost_per_pixel = 4.0054321e-8
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."medium/1024-x-1536/gpt-image-1-mini"]
input_cost_per_image = 0.015
litellm_provider = "openai"
mode = "image_generation"
supported_endpoints = ["/v1/images/generations"]

[models."medium/1536-x-1024/gpt-image-1"]
input_cost_per_image = 0.063
input_cost_per_pixel = 4.0054321e-8
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."medium/1536-x-1024/gpt-image-1-mini"]
input_cost_per_image = 0.015
litellm_provider = "openai"
mode = "image_generation"
supported_endpoints = ["/v1/images/generations"]

[models."medlm-large"]
input_cost_per_character = 0.000005
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
mode = "chat"
output_cost_per_character = 0.000015
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_tool_choice = true

[models."medlm-medium"]
input_cost_per_character = 5e-7
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 32768
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_character = 0.000001
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
supports_tool_choice = true

[models."meta.llama2-13b-chat-v1"]
input_cost_per_token = 7.5e-7
litellm_provider = "bedrock"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000001

[models."meta.llama2-70b-chat-v1"]
input_cost_per_token = 0.00000195
litellm_provider = "bedrock"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00000256

[models."meta.llama3-1-405b-instruct-v1:0"]
input_cost_per_token = 0.00000532
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000016
supports_function_calling = true
supports_tool_choice = false

[models."meta.llama3-1-70b-instruct-v1:0"]
input_cost_per_token = 9.9e-7
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 128000
mode = "chat"
output_cost_per_token = 9.9e-7
supports_function_calling = true
supports_tool_choice = false

[models."meta.llama3-1-8b-instruct-v1:0"]
input_cost_per_token = 2.2e-7
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 128000
mode = "chat"
output_cost_per_token = 2.2e-7
supports_function_calling = true
supports_tool_choice = false

[models."meta.llama3-2-11b-instruct-v1:0"]
input_cost_per_token = 3.5e-7
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 3.5e-7
supports_function_calling = true
supports_tool_choice = false
supports_vision = true

[models."meta.llama3-2-1b-instruct-v1:0"]
input_cost_per_token = 1e-7
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1e-7
supports_function_calling = true
supports_tool_choice = false

[models."meta.llama3-2-3b-instruct-v1:0"]
input_cost_per_token = 1.5e-7
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1.5e-7
supports_function_calling = true
supports_tool_choice = false

[models."meta.llama3-2-90b-instruct-v1:0"]
input_cost_per_token = 0.000002
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000002
supports_function_calling = true
supports_tool_choice = false
supports_vision = true

[models."meta.llama3-3-70b-instruct-v1:0"]
input_cost_per_token = 7.2e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 7.2e-7
supports_function_calling = true
supports_tool_choice = false

[models."meta.llama3-70b-instruct-v1:0"]
input_cost_per_token = 0.00000265
litellm_provider = "bedrock"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000035

[models."meta.llama3-8b-instruct-v1:0"]
input_cost_per_token = 3e-7
litellm_provider = "bedrock"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 6e-7

[models."meta.llama4-maverick-17b-instruct-v1:0"]
input_cost_per_token = 2.4e-7
input_cost_per_token_batches = 1.2e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9.7e-7
output_cost_per_token_batches = 4.85e-7
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "code"]
supports_function_calling = true
supports_tool_choice = false

[models."meta.llama4-scout-17b-instruct-v1:0"]
input_cost_per_token = 1.7e-7
input_cost_per_token_batches = 8.5e-8
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 6.6e-7
output_cost_per_token_batches = 3.3e-7
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "code"]
supports_function_calling = true
supports_tool_choice = false

[models."meta_llama/Llama-3.3-70B-Instruct"]
litellm_provider = "meta_llama"
max_input_tokens = 128000
max_output_tokens = 4028
max_tokens = 128000
mode = "chat"
source = "https://llama.developer.meta.com/docs/models"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_tool_choice = true

[models."meta_llama/Llama-3.3-8B-Instruct"]
litellm_provider = "meta_llama"
max_input_tokens = 128000
max_output_tokens = 4028
max_tokens = 128000
mode = "chat"
source = "https://llama.developer.meta.com/docs/models"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_tool_choice = true

[models."meta_llama/Llama-4-Maverick-17B-128E-Instruct-FP8"]
litellm_provider = "meta_llama"
max_input_tokens = 1000000
max_output_tokens = 4028
max_tokens = 128000
mode = "chat"
source = "https://llama.developer.meta.com/docs/models"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_tool_choice = true

[models."meta_llama/Llama-4-Scout-17B-16E-Instruct-FP8"]
litellm_provider = "meta_llama"
max_input_tokens = 10000000
max_output_tokens = 4028
max_tokens = 128000
mode = "chat"
source = "https://llama.developer.meta.com/docs/models"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_tool_choice = true

[models."minimax.minimax-m2"]
input_cost_per_token = 3e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000012
supports_system_messages = true

[models."minimax/MiniMax-M2"]
cache_creation_input_token_cost = 3.75e-7
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
litellm_provider = "minimax"
max_input_tokens = 200000
max_output_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000012
supports_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."minimax/MiniMax-M2.1"]
cache_creation_input_token_cost = 3.75e-7
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
litellm_provider = "minimax"
max_input_tokens = 1000000
max_output_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000012
supports_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."minimax/MiniMax-M2.1-lightning"]
cache_creation_input_token_cost = 3.75e-7
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
litellm_provider = "minimax"
max_input_tokens = 1000000
max_output_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000024
supports_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."minimax/speech-02-hd"]
input_cost_per_character = 0.0001
litellm_provider = "minimax"
mode = "audio_speech"
supported_endpoints = ["/v1/audio/speech"]

[models."minimax/speech-02-turbo"]
input_cost_per_character = 0.00006
litellm_provider = "minimax"
mode = "audio_speech"
supported_endpoints = ["/v1/audio/speech"]

[models."minimax/speech-2.6-hd"]
input_cost_per_character = 0.0001
litellm_provider = "minimax"
mode = "audio_speech"
supported_endpoints = ["/v1/audio/speech"]

[models."minimax/speech-2.6-turbo"]
input_cost_per_character = 0.00006
litellm_provider = "minimax"
mode = "audio_speech"
supported_endpoints = ["/v1/audio/speech"]

[models."mistral.magistral-small-2509"]
input_cost_per_token = 5e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000015
supports_function_calling = true
supports_reasoning = true
supports_system_messages = true

[models."mistral.ministral-3-14b-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7
supports_function_calling = true
supports_system_messages = true

[models."mistral.ministral-3-3b-instruct"]
input_cost_per_token = 1e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 1e-7
supports_function_calling = true
supports_system_messages = true

[models."mistral.ministral-3-8b-instruct"]
input_cost_per_token = 1.5e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 1.5e-7
supports_function_calling = true
supports_system_messages = true

[models."mistral.mistral-7b-instruct-v0:2"]
input_cost_per_token = 1.5e-7
litellm_provider = "bedrock"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 2e-7
supports_tool_choice = true

[models."mistral.mistral-large-2402-v1:0"]
input_cost_per_token = 0.000008
litellm_provider = "bedrock"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000024
supports_function_calling = true

[models."mistral.mistral-large-2407-v1:0"]
input_cost_per_token = 0.000003
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000009
supports_function_calling = true
supports_tool_choice = true

[models."mistral.mistral-large-3-675b-instruct"]
input_cost_per_token = 5e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000015
supports_function_calling = true
supports_system_messages = true

[models."mistral.mistral-small-2402-v1:0"]
input_cost_per_token = 0.000001
litellm_provider = "bedrock"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000003
supports_function_calling = true

[models."mistral.mixtral-8x7b-instruct-v0:1"]
input_cost_per_token = 4.5e-7
litellm_provider = "bedrock"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 7e-7
supports_tool_choice = true

[models."mistral.voxtral-mini-3b-2507"]
input_cost_per_token = 4e-8
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 4e-8
supports_audio_input = true
supports_system_messages = true

[models."mistral.voxtral-small-24b-2507"]
input_cost_per_token = 1e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 3e-7
supports_audio_input = true
supports_system_messages = true

[models."mistral/codestral-2405"]
input_cost_per_token = 0.000001
litellm_provider = "mistral"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000003
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/codestral-2508"]
input_cost_per_token = 3e-7
litellm_provider = "mistral"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 9e-7
source = "https://mistral.ai/news/codestral-25-08"
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/codestral-embed"]
input_cost_per_token = 1.5e-7
litellm_provider = "mistral"
max_input_tokens = 8192
max_tokens = 8192
mode = "embedding"

[models."mistral/codestral-embed-2505"]
input_cost_per_token = 1.5e-7
litellm_provider = "mistral"
max_input_tokens = 8192
max_tokens = 8192
mode = "embedding"

[models."mistral/codestral-latest"]
input_cost_per_token = 0.000001
litellm_provider = "mistral"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000003
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/codestral-mamba-latest"]
input_cost_per_token = 2.5e-7
litellm_provider = "mistral"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 2.5e-7
source = "https://mistral.ai/technology/"
supports_assistant_prefill = true
supports_tool_choice = true

[models."mistral/devstral-2512"]
input_cost_per_token = 4e-7
litellm_provider = "mistral"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.000002
source = "https://mistral.ai/news/devstral-2-vibe-cli"
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/devstral-medium-2507"]
input_cost_per_token = 4e-7
litellm_provider = "mistral"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000002
source = "https://mistral.ai/news/devstral"
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/devstral-small-2505"]
input_cost_per_token = 1e-7
litellm_provider = "mistral"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 3e-7
source = "https://mistral.ai/news/devstral"
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/devstral-small-2507"]
input_cost_per_token = 1e-7
litellm_provider = "mistral"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 3e-7
source = "https://mistral.ai/news/devstral"
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/labs-devstral-small-2512"]
input_cost_per_token = 1e-7
litellm_provider = "mistral"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 3e-7
source = "https://docs.mistral.ai/models/devstral-small-2-25-12"
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/magistral-medium-2506"]
input_cost_per_token = 0.000002
litellm_provider = "mistral"
max_input_tokens = 40000
max_output_tokens = 40000
max_tokens = 40000
mode = "chat"
output_cost_per_token = 0.000005
source = "https://mistral.ai/news/magistral"
supports_assistant_prefill = true
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/magistral-medium-2509"]
input_cost_per_token = 0.000002
litellm_provider = "mistral"
max_input_tokens = 40000
max_output_tokens = 40000
max_tokens = 40000
mode = "chat"
output_cost_per_token = 0.000005
source = "https://mistral.ai/news/magistral"
supports_assistant_prefill = true
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/magistral-medium-latest"]
input_cost_per_token = 0.000002
litellm_provider = "mistral"
max_input_tokens = 40000
max_output_tokens = 40000
max_tokens = 40000
mode = "chat"
output_cost_per_token = 0.000005
source = "https://mistral.ai/news/magistral"
supports_assistant_prefill = true
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/magistral-small-2506"]
input_cost_per_token = 5e-7
litellm_provider = "mistral"
max_input_tokens = 40000
max_output_tokens = 40000
max_tokens = 40000
mode = "chat"
output_cost_per_token = 0.0000015
source = "https://mistral.ai/pricing#api-pricing"
supports_assistant_prefill = true
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/magistral-small-latest"]
input_cost_per_token = 5e-7
litellm_provider = "mistral"
max_input_tokens = 40000
max_output_tokens = 40000
max_tokens = 40000
mode = "chat"
output_cost_per_token = 0.0000015
source = "https://mistral.ai/pricing#api-pricing"
supports_assistant_prefill = true
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/mistral-embed"]
input_cost_per_token = 1e-7
litellm_provider = "mistral"
max_input_tokens = 8192
max_tokens = 8192
mode = "embedding"

[models."mistral/mistral-large-2402"]
input_cost_per_token = 0.000004
litellm_provider = "mistral"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000012
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/mistral-large-2407"]
input_cost_per_token = 0.000003
litellm_provider = "mistral"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000009
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/mistral-large-2411"]
input_cost_per_token = 0.000002
litellm_provider = "mistral"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000006
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/mistral-large-3"]
input_cost_per_token = 5e-7
litellm_provider = "mistral"
max_input_tokens = 256000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.0000015
source = "https://docs.mistral.ai/models/mistral-large-3-25-12"
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."mistral/mistral-large-latest"]
input_cost_per_token = 0.000002
litellm_provider = "mistral"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000006
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/mistral-medium"]
input_cost_per_token = 0.0000027
litellm_provider = "mistral"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.0000081
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/mistral-medium-2312"]
input_cost_per_token = 0.0000027
litellm_provider = "mistral"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.0000081
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/mistral-medium-2505"]
input_cost_per_token = 4e-7
litellm_provider = "mistral"
max_input_tokens = 131072
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000002
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/mistral-medium-latest"]
input_cost_per_token = 4e-7
litellm_provider = "mistral"
max_input_tokens = 131072
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000002
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/mistral-ocr-2505-completion"]
annotation_cost_per_page = 0.003
litellm_provider = "mistral"
mode = "ocr"
ocr_cost_per_page = 0.001
source = "https://mistral.ai/pricing#api-pricing"
supported_endpoints = ["/v1/ocr"]

[models."mistral/mistral-ocr-latest"]
annotation_cost_per_page = 0.003
litellm_provider = "mistral"
mode = "ocr"
ocr_cost_per_page = 0.001
source = "https://mistral.ai/pricing#api-pricing"
supported_endpoints = ["/v1/ocr"]

[models."mistral/mistral-small"]
input_cost_per_token = 1e-7
litellm_provider = "mistral"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 3e-7
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/mistral-small-latest"]
input_cost_per_token = 1e-7
litellm_provider = "mistral"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 3e-7
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/mistral-tiny"]
input_cost_per_token = 2.5e-7
litellm_provider = "mistral"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 2.5e-7
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/open-codestral-mamba"]
input_cost_per_token = 2.5e-7
litellm_provider = "mistral"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 2.5e-7
source = "https://mistral.ai/technology/"
supports_assistant_prefill = true
supports_tool_choice = true

[models."mistral/open-mistral-7b"]
input_cost_per_token = 2.5e-7
litellm_provider = "mistral"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 2.5e-7
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/open-mistral-nemo"]
input_cost_per_token = 3e-7
litellm_provider = "mistral"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 3e-7
source = "https://mistral.ai/technology/"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/open-mistral-nemo-2407"]
input_cost_per_token = 3e-7
litellm_provider = "mistral"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 3e-7
source = "https://mistral.ai/technology/"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/open-mixtral-8x22b"]
input_cost_per_token = 0.000002
litellm_provider = "mistral"
max_input_tokens = 65336
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000006
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/open-mixtral-8x7b"]
input_cost_per_token = 7e-7
litellm_provider = "mistral"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 7e-7
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral/pixtral-12b-2409"]
input_cost_per_token = 1.5e-7
litellm_provider = "mistral"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1.5e-7
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."mistral/pixtral-large-2411"]
input_cost_per_token = 0.000002
litellm_provider = "mistral"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000006
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."mistral/pixtral-large-latest"]
input_cost_per_token = 0.000002
litellm_provider = "mistral"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000006
supports_assistant_prefill = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."moonshot.kimi-k2-thinking"]
input_cost_per_token = 6e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000025
supports_reasoning = true
supports_system_messages = true

[models."moonshot/kimi-k2-0711-preview"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
litellm_provider = "moonshot"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000025
source = "https://platform.moonshot.ai/docs/pricing/chat#generation-model-kimi-k2"
supports_function_calling = true
supports_tool_choice = true
supports_web_search = true

[models."moonshot/kimi-k2-0905-preview"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
litellm_provider = "moonshot"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000025
source = "https://platform.moonshot.ai/docs/pricing/chat#generation-model-kimi-k2"
supports_function_calling = true
supports_tool_choice = true
supports_web_search = true

[models."moonshot/kimi-k2-thinking"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
litellm_provider = "moonshot"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000025
source = "https://platform.moonshot.ai/docs/pricing/chat#generation-model-kimi-k2"
supports_function_calling = true
supports_tool_choice = true
supports_web_search = true

[models."moonshot/kimi-k2-thinking-turbo"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 0.00000115
litellm_provider = "moonshot"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.000008
source = "https://platform.moonshot.ai/docs/pricing/chat#generation-model-kimi-k2"
supports_function_calling = true
supports_tool_choice = true
supports_web_search = true

[models."moonshot/kimi-k2-turbo-preview"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 0.00000115
litellm_provider = "moonshot"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.000008
source = "https://platform.moonshot.ai/docs/pricing/chat#generation-model-kimi-k2"
supports_function_calling = true
supports_tool_choice = true
supports_web_search = true

[models."moonshot/kimi-latest"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 0.000002
litellm_provider = "moonshot"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000005
source = "https://platform.moonshot.ai/docs/pricing"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."moonshot/kimi-latest-128k"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 0.000002
litellm_provider = "moonshot"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000005
source = "https://platform.moonshot.ai/docs/pricing"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."moonshot/kimi-latest-32k"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 0.000001
litellm_provider = "moonshot"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000003
source = "https://platform.moonshot.ai/docs/pricing"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."moonshot/kimi-latest-8k"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 2e-7
litellm_provider = "moonshot"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000002
source = "https://platform.moonshot.ai/docs/pricing"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."moonshot/kimi-thinking-preview"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
litellm_provider = "moonshot"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000025
source = "https://platform.moonshot.ai/docs/pricing/chat#generation-model-kimi-k2"
supports_vision = true

[models."moonshot/moonshot-v1-128k"]
input_cost_per_token = 0.000002
litellm_provider = "moonshot"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000005
source = "https://platform.moonshot.ai/docs/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."moonshot/moonshot-v1-128k-0430"]
input_cost_per_token = 0.000002
litellm_provider = "moonshot"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000005
source = "https://platform.moonshot.ai/docs/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."moonshot/moonshot-v1-128k-vision-preview"]
input_cost_per_token = 0.000002
litellm_provider = "moonshot"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000005
source = "https://platform.moonshot.ai/docs/pricing"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."moonshot/moonshot-v1-32k"]
input_cost_per_token = 0.000001
litellm_provider = "moonshot"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000003
source = "https://platform.moonshot.ai/docs/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."moonshot/moonshot-v1-32k-0430"]
input_cost_per_token = 0.000001
litellm_provider = "moonshot"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000003
source = "https://platform.moonshot.ai/docs/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."moonshot/moonshot-v1-32k-vision-preview"]
input_cost_per_token = 0.000001
litellm_provider = "moonshot"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000003
source = "https://platform.moonshot.ai/docs/pricing"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."moonshot/moonshot-v1-8k"]
input_cost_per_token = 2e-7
litellm_provider = "moonshot"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000002
source = "https://platform.moonshot.ai/docs/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."moonshot/moonshot-v1-8k-0430"]
input_cost_per_token = 2e-7
litellm_provider = "moonshot"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000002
source = "https://platform.moonshot.ai/docs/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."moonshot/moonshot-v1-8k-vision-preview"]
input_cost_per_token = 2e-7
litellm_provider = "moonshot"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000002
source = "https://platform.moonshot.ai/docs/pricing"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."moonshot/moonshot-v1-auto"]
input_cost_per_token = 0.000002
litellm_provider = "moonshot"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000005
source = "https://platform.moonshot.ai/docs/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."morph/morph-v3-fast"]
input_cost_per_token = 8e-7
litellm_provider = "morph"
max_input_tokens = 16000
max_output_tokens = 16000
max_tokens = 16000
mode = "chat"
output_cost_per_token = 0.0000012
supports_function_calling = false
supports_parallel_function_calling = false
supports_system_messages = true
supports_tool_choice = false
supports_vision = false

[models."morph/morph-v3-large"]
input_cost_per_token = 9e-7
litellm_provider = "morph"
max_input_tokens = 16000
max_output_tokens = 16000
max_tokens = 16000
mode = "chat"
output_cost_per_token = 0.0000019
supports_function_calling = false
supports_parallel_function_calling = false
supports_system_messages = true
supports_tool_choice = false
supports_vision = false

[models."multimodalembedding"]
input_cost_per_character = 2e-7
input_cost_per_image = 0.0001
input_cost_per_token = 8e-7
input_cost_per_video_per_second = 0.0005
input_cost_per_video_per_second_above_15s_interval = 0.002
input_cost_per_video_per_second_above_8s_interval = 0.001
litellm_provider = "vertex_ai-embedding-models"
max_input_tokens = 2048
max_tokens = 2048
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 768
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"
supported_endpoints = ["/v1/embeddings"]
supported_modalities = ["text", "image", "video"]

[models."multimodalembedding@001"]
input_cost_per_character = 2e-7
input_cost_per_image = 0.0001
input_cost_per_token = 8e-7
input_cost_per_video_per_second = 0.0005
input_cost_per_video_per_second_above_15s_interval = 0.002
input_cost_per_video_per_second_above_8s_interval = 0.001
litellm_provider = "vertex_ai-embedding-models"
max_input_tokens = 2048
max_tokens = 2048
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 768
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"
supported_endpoints = ["/v1/embeddings"]
supported_modalities = ["text", "image", "video"]

[models."nscale/Qwen/QwQ-32B"]
input_cost_per_token = 1.8e-7
litellm_provider = "nscale"
mode = "chat"
output_cost_per_token = 2e-7
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."nscale/Qwen/Qwen2.5-Coder-32B-Instruct"]
input_cost_per_token = 6e-8
litellm_provider = "nscale"
mode = "chat"
output_cost_per_token = 2e-7
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."nscale/Qwen/Qwen2.5-Coder-3B-Instruct"]
input_cost_per_token = 1e-8
litellm_provider = "nscale"
mode = "chat"
output_cost_per_token = 3e-8
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."nscale/Qwen/Qwen2.5-Coder-7B-Instruct"]
input_cost_per_token = 1e-8
litellm_provider = "nscale"
mode = "chat"
output_cost_per_token = 3e-8
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."nscale/black-forest-labs/FLUX.1-schnell"]
input_cost_per_pixel = 1.3e-9
litellm_provider = "nscale"
mode = "image_generation"
output_cost_per_pixel = 0
source = "https://docs.nscale.com/docs/inference/serverless-models/current#image-models"
supported_endpoints = ["/v1/images/generations"]

[models."nscale/deepseek-ai/DeepSeek-R1-Distill-Llama-70B"]
input_cost_per_token = 3.75e-7
litellm_provider = "nscale"
mode = "chat"
output_cost_per_token = 3.75e-7
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."nscale/deepseek-ai/DeepSeek-R1-Distill-Llama-70B".metadata]
notes = "Pricing listed as $0.75/1M tokens total. Assumed 50/50 split for input/output."

[models."nscale/deepseek-ai/DeepSeek-R1-Distill-Llama-8B"]
input_cost_per_token = 2.5e-8
litellm_provider = "nscale"
mode = "chat"
output_cost_per_token = 2.5e-8
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."nscale/deepseek-ai/DeepSeek-R1-Distill-Llama-8B".metadata]
notes = "Pricing listed as $0.05/1M tokens total. Assumed 50/50 split for input/output."

[models."nscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"]
input_cost_per_token = 9e-8
litellm_provider = "nscale"
mode = "chat"
output_cost_per_token = 9e-8
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."nscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B".metadata]
notes = "Pricing listed as $0.18/1M tokens total. Assumed 50/50 split for input/output."

[models."nscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"]
input_cost_per_token = 7e-8
litellm_provider = "nscale"
mode = "chat"
output_cost_per_token = 7e-8
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."nscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B".metadata]
notes = "Pricing listed as $0.14/1M tokens total. Assumed 50/50 split for input/output."

[models."nscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"]
input_cost_per_token = 1.5e-7
litellm_provider = "nscale"
mode = "chat"
output_cost_per_token = 1.5e-7
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."nscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B".metadata]
notes = "Pricing listed as $0.30/1M tokens total. Assumed 50/50 split for input/output."

[models."nscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"]
input_cost_per_token = 2e-7
litellm_provider = "nscale"
mode = "chat"
output_cost_per_token = 2e-7
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."nscale/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B".metadata]
notes = "Pricing listed as $0.40/1M tokens total. Assumed 50/50 split for input/output."

[models."nscale/meta-llama/Llama-3.1-8B-Instruct"]
input_cost_per_token = 3e-8
litellm_provider = "nscale"
mode = "chat"
output_cost_per_token = 3e-8
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."nscale/meta-llama/Llama-3.1-8B-Instruct".metadata]
notes = "Pricing listed as $0.06/1M tokens total. Assumed 50/50 split for input/output."

[models."nscale/meta-llama/Llama-3.3-70B-Instruct"]
input_cost_per_token = 2e-7
litellm_provider = "nscale"
mode = "chat"
output_cost_per_token = 2e-7
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."nscale/meta-llama/Llama-3.3-70B-Instruct".metadata]
notes = "Pricing listed as $0.40/1M tokens total. Assumed 50/50 split for input/output."

[models."nscale/meta-llama/Llama-4-Scout-17B-16E-Instruct"]
input_cost_per_token = 9e-8
litellm_provider = "nscale"
mode = "chat"
output_cost_per_token = 2.9e-7
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."nscale/mistralai/mixtral-8x22b-instruct-v0.1"]
input_cost_per_token = 6e-7
litellm_provider = "nscale"
mode = "chat"
output_cost_per_token = 6e-7
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."nscale/mistralai/mixtral-8x22b-instruct-v0.1".metadata]
notes = "Pricing listed as $1.20/1M tokens total. Assumed 50/50 split for input/output."

[models."nscale/stabilityai/stable-diffusion-xl-base-1.0"]
input_cost_per_pixel = 3e-9
litellm_provider = "nscale"
mode = "image_generation"
output_cost_per_pixel = 0
source = "https://docs.nscale.com/docs/inference/serverless-models/current#image-models"
supported_endpoints = ["/v1/images/generations"]

[models."nvidia.nemotron-nano-12b-v2"]
input_cost_per_token = 2e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 6e-7
supports_system_messages = true
supports_vision = true

[models."nvidia.nemotron-nano-9b-v2"]
input_cost_per_token = 6e-8
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2.3e-7
supports_system_messages = true

[models."nvidia_nim/nvidia/llama-3_2-nv-rerankqa-1b-v2"]
input_cost_per_query = 0
input_cost_per_token = 0
litellm_provider = "nvidia_nim"
mode = "rerank"
output_cost_per_token = 0

[models."nvidia_nim/nvidia/nv-rerankqa-mistral-4b-v3"]
input_cost_per_query = 0
input_cost_per_token = 0
litellm_provider = "nvidia_nim"
mode = "rerank"
output_cost_per_token = 0

[models."nvidia_nim/ranking/nvidia/llama-3.2-nv-rerankqa-1b-v2"]
input_cost_per_query = 0
input_cost_per_token = 0
litellm_provider = "nvidia_nim"
mode = "rerank"
output_cost_per_token = 0

[models."o1"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."o1-2024-12-17"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."o1-mini"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.0000044
supports_pdf_input = true
supports_prompt_caching = true
supports_vision = true

[models."o1-mini-2024-09-12"]
cache_read_input_token_cost = 0.0000015
deprecation_date = "2025-10-27"
input_cost_per_token = 0.000003
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.000012
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_vision = true

[models."o1-preview"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.00006
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_vision = true

[models."o1-preview-2024-09-12"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
litellm_provider = "openai"
max_input_tokens = 128000
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.00006
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_vision = true

[models."o1-pro"]
input_cost_per_token = 0.00015
input_cost_per_token_batches = 0.000075
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "responses"
output_cost_per_token = 0.0006
output_cost_per_token_batches = 0.0003
supported_endpoints = ["/v1/responses", "/v1/batch"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = false
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."o1-pro-2025-03-19"]
input_cost_per_token = 0.00015
input_cost_per_token_batches = 0.000075
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "responses"
output_cost_per_token = 0.0006
output_cost_per_token_batches = 0.0003
supported_endpoints = ["/v1/responses", "/v1/batch"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = false
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."o3"]
cache_read_input_token_cost = 5e-7
cache_read_input_token_cost_flex = 2.5e-7
cache_read_input_token_cost_priority = 8.75e-7
input_cost_per_token = 0.000002
input_cost_per_token_flex = 0.000001
input_cost_per_token_priority = 0.0000035
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.000008
output_cost_per_token_flex = 0.000004
output_cost_per_token_priority = 0.000014
supported_endpoints = ["/v1/responses", "/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = false
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_tool_choice = true
supports_vision = true

[models."o3-2025-04-16"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.000008
supported_endpoints = ["/v1/responses", "/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = false
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_tool_choice = true
supports_vision = true

[models."o3-deep-research"]
cache_read_input_token_cost = 0.0000025
input_cost_per_token = 0.00001
input_cost_per_token_batches = 0.000005
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "responses"
output_cost_per_token = 0.00004
output_cost_per_token_batches = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."o3-deep-research-2025-06-26"]
cache_read_input_token_cost = 0.0000025
input_cost_per_token = 0.00001
input_cost_per_token_batches = 0.000005
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "responses"
output_cost_per_token = 0.00004
output_cost_per_token_batches = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."o3-mini"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.0000044
supports_function_calling = true
supports_parallel_function_calling = false
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = false

[models."o3-mini-2025-01-31"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.0000044
supports_function_calling = true
supports_parallel_function_calling = false
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = false

[models."o3-pro"]
input_cost_per_token = 0.00002
input_cost_per_token_batches = 0.00001
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "responses"
output_cost_per_token = 0.00008
output_cost_per_token_batches = 0.00004
supported_endpoints = ["/v1/responses", "/v1/batch"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = false
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."o3-pro-2025-06-10"]
input_cost_per_token = 0.00002
input_cost_per_token_batches = 0.00001
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "responses"
output_cost_per_token = 0.00008
output_cost_per_token_batches = 0.00004
supported_endpoints = ["/v1/responses", "/v1/batch"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_parallel_function_calling = false
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."o4-mini"]
cache_read_input_token_cost = 2.75e-7
cache_read_input_token_cost_flex = 1.375e-7
cache_read_input_token_cost_priority = 5e-7
input_cost_per_token = 0.0000011
input_cost_per_token_flex = 5.5e-7
input_cost_per_token_priority = 0.000002
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.0000044
output_cost_per_token_flex = 0.0000022
output_cost_per_token_priority = 0.000008
supports_function_calling = true
supports_parallel_function_calling = false
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_tool_choice = true
supports_vision = true

[models."o4-mini-2025-04-16"]
cache_read_input_token_cost = 2.75e-7
input_cost_per_token = 0.0000011
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.0000044
supports_function_calling = true
supports_parallel_function_calling = false
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_service_tier = true
supports_tool_choice = true
supports_vision = true

[models."o4-mini-deep-research"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "responses"
output_cost_per_token = 0.000008
output_cost_per_token_batches = 0.000004
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."o4-mini-deep-research-2025-06-26"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
litellm_provider = "openai"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "responses"
output_cost_per_token = 0.000008
output_cost_per_token_batches = 0.000004
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_function_calling = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."oci/cohere.command-a-03-2025"]
input_cost_per_token = 0.00000156
litellm_provider = "oci"
max_input_tokens = 256000
max_output_tokens = 4000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.00000156
source = "https://www.oracle.com/cloud/ai/generative-ai/pricing/"
supports_function_calling = true
supports_response_schema = false

[models."oci/cohere.command-latest"]
input_cost_per_token = 0.00000156
litellm_provider = "oci"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00000156
source = "https://www.oracle.com/cloud/ai/generative-ai/pricing/"
supports_function_calling = true
supports_response_schema = false

[models."oci/cohere.command-plus-latest"]
input_cost_per_token = 0.00000156
litellm_provider = "oci"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00000156
source = "https://www.oracle.com/cloud/ai/generative-ai/pricing/"
supports_function_calling = true
supports_response_schema = false

[models."oci/meta.llama-3.1-405b-instruct"]
input_cost_per_token = 0.00001068
litellm_provider = "oci"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001068
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_function_calling = true
supports_response_schema = false

[models."oci/meta.llama-3.2-90b-vision-instruct"]
input_cost_per_token = 0.000002
litellm_provider = "oci"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000002
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_function_calling = true
supports_response_schema = false

[models."oci/meta.llama-3.3-70b-instruct"]
input_cost_per_token = 7.2e-7
litellm_provider = "oci"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 7.2e-7
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_function_calling = true
supports_response_schema = false

[models."oci/meta.llama-4-maverick-17b-128e-instruct-fp8"]
input_cost_per_token = 7.2e-7
litellm_provider = "oci"
max_input_tokens = 512000
max_output_tokens = 4000
max_tokens = 512000
mode = "chat"
output_cost_per_token = 7.2e-7
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_function_calling = true
supports_response_schema = false

[models."oci/meta.llama-4-scout-17b-16e-instruct"]
input_cost_per_token = 7.2e-7
litellm_provider = "oci"
max_input_tokens = 192000
max_output_tokens = 4000
max_tokens = 192000
mode = "chat"
output_cost_per_token = 7.2e-7
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_function_calling = true
supports_response_schema = false

[models."oci/xai.grok-3"]
input_cost_per_token = 0.000003
litellm_provider = "oci"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1.5e-7
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_function_calling = true
supports_response_schema = false

[models."oci/xai.grok-3-fast"]
input_cost_per_token = 0.000005
litellm_provider = "oci"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000025
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_function_calling = true
supports_response_schema = false

[models."oci/xai.grok-3-mini"]
input_cost_per_token = 3e-7
litellm_provider = "oci"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 5e-7
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_function_calling = true
supports_response_schema = false

[models."oci/xai.grok-3-mini-fast"]
input_cost_per_token = 6e-7
litellm_provider = "oci"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000004
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_function_calling = true
supports_response_schema = false

[models."oci/xai.grok-4"]
input_cost_per_token = 0.000003
litellm_provider = "oci"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1.5e-7
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_function_calling = true
supports_response_schema = false

[models."ollama/codegeex4"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 32768
max_output_tokens = 8192
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0
supports_function_calling = false

[models."ollama/codegemma"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "completion"
output_cost_per_token = 0

[models."ollama/codellama"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "completion"
output_cost_per_token = 0

[models."ollama/deepseek-coder-v2-base"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "completion"
output_cost_per_token = 0
supports_function_calling = true

[models."ollama/deepseek-coder-v2-instruct"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 32768
max_output_tokens = 8192
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true

[models."ollama/deepseek-coder-v2-lite-base"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "completion"
output_cost_per_token = 0
supports_function_calling = true

[models."ollama/deepseek-coder-v2-lite-instruct"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 32768
max_output_tokens = 8192
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true

[models."ollama/deepseek-v3.1:671b-cloud"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true

[models."ollama/gpt-oss:120b-cloud"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true

[models."ollama/gpt-oss:20b-cloud"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true

[models."ollama/internlm2_5-20b-chat"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 32768
max_output_tokens = 8192
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true

[models."ollama/llama2"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0

[models."ollama/llama2-uncensored"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "completion"
output_cost_per_token = 0

[models."ollama/llama2:13b"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0

[models."ollama/llama2:70b"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0

[models."ollama/llama2:7b"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0

[models."ollama/llama3"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0

[models."ollama/llama3.1"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true

[models."ollama/llama3:70b"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0

[models."ollama/llama3:8b"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0

[models."ollama/mistral"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "completion"
output_cost_per_token = 0
supports_function_calling = true

[models."ollama/mistral-7B-Instruct-v0.1"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true

[models."ollama/mistral-7B-Instruct-v0.2"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true

[models."ollama/mistral-large-instruct-2407"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 65536
max_output_tokens = 8192
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true

[models."ollama/mixtral-8x22B-Instruct-v0.1"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true

[models."ollama/mixtral-8x7B-Instruct-v0.1"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true

[models."ollama/orca-mini"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "completion"
output_cost_per_token = 0

[models."ollama/qwen3-coder:480b-cloud"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true

[models."ollama/vicuna"]
input_cost_per_token = 0
litellm_provider = "ollama"
max_input_tokens = 2048
max_output_tokens = 2048
max_tokens = 2048
mode = "completion"
output_cost_per_token = 0

[models."omni-moderation-2024-09-26"]
input_cost_per_token = 0
litellm_provider = "openai"
max_input_tokens = 32768
max_output_tokens = 0
max_tokens = 32768
mode = "moderation"
output_cost_per_token = 0

[models."omni-moderation-latest"]
input_cost_per_token = 0
litellm_provider = "openai"
max_input_tokens = 32768
max_output_tokens = 0
max_tokens = 32768
mode = "moderation"
output_cost_per_token = 0

[models."omni-moderation-latest-intents"]
input_cost_per_token = 0
litellm_provider = "openai"
max_input_tokens = 32768
max_output_tokens = 0
max_tokens = 32768
mode = "moderation"
output_cost_per_token = 0

[models."openai.gpt-oss-120b-1:0"]
input_cost_per_token = 1.5e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."openai.gpt-oss-20b-1:0"]
input_cost_per_token = 7e-8
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 3e-7
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."openai.gpt-oss-safeguard-120b"]
input_cost_per_token = 1.5e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 6e-7
supports_system_messages = true

[models."openai.gpt-oss-safeguard-20b"]
input_cost_per_token = 7e-8
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7
supports_system_messages = true

[models."openai/container"]
code_interpreter_cost_per_session = 0.03
litellm_provider = "openai"
mode = "chat"

[models."openai/sora-2"]
litellm_provider = "openai"
mode = "video_generation"
output_cost_per_video_per_second = 0.1
source = "https://platform.openai.com/docs/api-reference/videos"
supported_modalities = ["text", "image"]
supported_output_modalities = ["video"]
supported_resolutions = ["720x1280", "1280x720"]

[models."openai/sora-2-pro"]
litellm_provider = "openai"
mode = "video_generation"
output_cost_per_video_per_second = 0.3
source = "https://platform.openai.com/docs/api-reference/videos"
supported_modalities = ["text", "image"]
supported_output_modalities = ["video"]
supported_resolutions = ["720x1280", "1280x720"]

[models."openrouter/anthropic/claude-2"]
input_cost_per_token = 0.00001102
litellm_provider = "openrouter"
max_output_tokens = 8191
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.00003268
supports_tool_choice = true

[models."openrouter/anthropic/claude-3-5-haiku"]
input_cost_per_token = 0.000001
litellm_provider = "openrouter"
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.000005
supports_function_calling = true
supports_tool_choice = true

[models."openrouter/anthropic/claude-3-5-haiku-20241022"]
input_cost_per_token = 0.000001
litellm_provider = "openrouter"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000005
supports_function_calling = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 264

[models."openrouter/anthropic/claude-3-haiku"]
input_cost_per_image = 0.0004
input_cost_per_token = 2.5e-7
litellm_provider = "openrouter"
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.00000125
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/anthropic/claude-3-haiku-20240307"]
input_cost_per_token = 2.5e-7
litellm_provider = "openrouter"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00000125
supports_function_calling = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 264

[models."openrouter/anthropic/claude-3-opus"]
input_cost_per_token = 0.000015
litellm_provider = "openrouter"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000075
supports_function_calling = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 395

[models."openrouter/anthropic/claude-3-sonnet"]
input_cost_per_image = 0.0048
input_cost_per_token = 0.000003
litellm_provider = "openrouter"
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/anthropic/claude-3.5-sonnet"]
input_cost_per_token = 0.000003
litellm_provider = "openrouter"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."openrouter/anthropic/claude-3.5-sonnet:beta"]
input_cost_per_token = 0.000003
litellm_provider = "openrouter"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_computer_use = true
supports_function_calling = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."openrouter/anthropic/claude-3.7-sonnet"]
input_cost_per_image = 0.0048
input_cost_per_token = 0.000003
litellm_provider = "openrouter"
max_input_tokens = 200000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."openrouter/anthropic/claude-3.7-sonnet:beta"]
input_cost_per_image = 0.0048
input_cost_per_token = 0.000003
litellm_provider = "openrouter"
max_input_tokens = 200000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000015
supports_computer_use = true
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."openrouter/anthropic/claude-haiku-4.5"]
cache_creation_input_token_cost = 0.00000125
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
litellm_provider = "openrouter"
max_input_tokens = 200000
max_output_tokens = 200000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.000005
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 346

[models."openrouter/anthropic/claude-instant-v1"]
input_cost_per_token = 0.00000163
litellm_provider = "openrouter"
max_output_tokens = 8191
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.00000551
supports_tool_choice = true

[models."openrouter/anthropic/claude-opus-4"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_image = 0.0048
input_cost_per_token = 0.000015
litellm_provider = "openrouter"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."openrouter/anthropic/claude-opus-4.1"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.00003
cache_read_input_token_cost = 0.0000015
input_cost_per_image = 0.0048
input_cost_per_token = 0.000015
litellm_provider = "openrouter"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."openrouter/anthropic/claude-opus-4.5"]
cache_creation_input_token_cost = 0.00000625
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
litellm_provider = "openrouter"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000025
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."openrouter/anthropic/claude-sonnet-4"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_image = 0.0048
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "openrouter"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."openrouter/anthropic/claude-sonnet-4.5"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_image = 0.0048
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "openrouter"
max_input_tokens = 1000000
max_output_tokens = 1000000
max_tokens = 1000000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."openrouter/bytedance/ui-tars-1.5-7b"]
input_cost_per_token = 1e-7
litellm_provider = "openrouter"
max_input_tokens = 131072
max_output_tokens = 2048
max_tokens = 2048
mode = "chat"
output_cost_per_token = 2e-7
source = "https://openrouter.ai/api/v1/models/bytedance/ui-tars-1.5-7b"
supports_tool_choice = true

[models."openrouter/cognitivecomputations/dolphin-mixtral-8x7b"]
input_cost_per_token = 5e-7
litellm_provider = "openrouter"
max_tokens = 32769
mode = "chat"
output_cost_per_token = 5e-7
supports_tool_choice = true

[models."openrouter/cohere/command-r-plus"]
input_cost_per_token = 0.000003
litellm_provider = "openrouter"
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000015
supports_tool_choice = true

[models."openrouter/databricks/dbrx-instruct"]
input_cost_per_token = 6e-7
litellm_provider = "openrouter"
max_tokens = 32768
mode = "chat"
output_cost_per_token = 6e-7
supports_tool_choice = true

[models."openrouter/deepseek/deepseek-chat"]
input_cost_per_token = 1.4e-7
litellm_provider = "openrouter"
max_input_tokens = 65536
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2.8e-7
supports_prompt_caching = true
supports_tool_choice = true

[models."openrouter/deepseek/deepseek-chat-v3-0324"]
input_cost_per_token = 1.4e-7
litellm_provider = "openrouter"
max_input_tokens = 65536
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2.8e-7
supports_prompt_caching = true
supports_tool_choice = true

[models."openrouter/deepseek/deepseek-chat-v3.1"]
input_cost_per_token = 2e-7
input_cost_per_token_cache_hit = 2e-8
litellm_provider = "openrouter"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 8192
mode = "chat"
output_cost_per_token = 8e-7
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true

[models."openrouter/deepseek/deepseek-coder"]
input_cost_per_token = 1.4e-7
litellm_provider = "openrouter"
max_input_tokens = 66000
max_output_tokens = 4096
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2.8e-7
supports_prompt_caching = true
supports_tool_choice = true

[models."openrouter/deepseek/deepseek-r1"]
input_cost_per_token = 5.5e-7
input_cost_per_token_cache_hit = 1.4e-7
litellm_provider = "openrouter"
max_input_tokens = 65336
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000219
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true

[models."openrouter/deepseek/deepseek-r1-0528"]
input_cost_per_token = 5e-7
input_cost_per_token_cache_hit = 1.4e-7
litellm_provider = "openrouter"
max_input_tokens = 65336
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000215
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true

[models."openrouter/deepseek/deepseek-v3.2"]
input_cost_per_token = 2.8e-7
input_cost_per_token_cache_hit = 2.8e-8
litellm_provider = "openrouter"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 8192
mode = "chat"
output_cost_per_token = 4e-7
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true

[models."openrouter/deepseek/deepseek-v3.2-exp"]
input_cost_per_token = 2e-7
input_cost_per_token_cache_hit = 2e-8
litellm_provider = "openrouter"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 8192
mode = "chat"
output_cost_per_token = 4e-7
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = false
supports_tool_choice = true

[models."openrouter/fireworks/firellava-13b"]
input_cost_per_token = 2e-7
litellm_provider = "openrouter"
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7
supports_tool_choice = true

[models."openrouter/google/gemini-2.0-flash-001"]
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
litellm_provider = "openrouter"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 4e-7
supports_audio_output = true
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/google/gemini-2.5-flash"]
input_cost_per_audio_token = 7e-7
input_cost_per_token = 3e-7
litellm_provider = "openrouter"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.0000025
supports_audio_output = true
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/google/gemini-2.5-pro"]
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
litellm_provider = "openrouter"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 8192
max_pdf_size_mb = 30
max_tokens = 8192
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.00001
supports_audio_output = true
supports_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/google/gemini-3-pro-preview"]
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost = 2e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
input_cost_per_token = 0.000002
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_batches = 0.000001
litellm_provider = "openrouter"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.000012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_batches = 0.000006
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true

[models."openrouter/google/gemini-pro-1.5"]
input_cost_per_image = 0.00265
input_cost_per_token = 0.0000025
litellm_provider = "openrouter"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000075
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/google/gemini-pro-vision"]
input_cost_per_image = 0.0025
input_cost_per_token = 1.25e-7
litellm_provider = "openrouter"
max_tokens = 45875
mode = "chat"
output_cost_per_token = 3.75e-7
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/google/palm-2-chat-bison"]
input_cost_per_token = 5e-7
litellm_provider = "openrouter"
max_tokens = 25804
mode = "chat"
output_cost_per_token = 5e-7
supports_tool_choice = true

[models."openrouter/google/palm-2-codechat-bison"]
input_cost_per_token = 5e-7
litellm_provider = "openrouter"
max_tokens = 20070
mode = "chat"
output_cost_per_token = 5e-7
supports_tool_choice = true

[models."openrouter/gryphe/mythomax-l2-13b"]
input_cost_per_token = 0.000001875
litellm_provider = "openrouter"
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000001875
supports_tool_choice = true

[models."openrouter/jondurbin/airoboros-l2-70b-2.1"]
input_cost_per_token = 0.000013875
litellm_provider = "openrouter"
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000013875
supports_tool_choice = true

[models."openrouter/mancer/weaver"]
input_cost_per_token = 0.000005625
litellm_provider = "openrouter"
max_tokens = 8000
mode = "chat"
output_cost_per_token = 0.000005625
supports_tool_choice = true

[models."openrouter/meta-llama/codellama-34b-instruct"]
input_cost_per_token = 5e-7
litellm_provider = "openrouter"
max_tokens = 8192
mode = "chat"
output_cost_per_token = 5e-7
supports_tool_choice = true

[models."openrouter/meta-llama/llama-2-13b-chat"]
input_cost_per_token = 2e-7
litellm_provider = "openrouter"
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7
supports_tool_choice = true

[models."openrouter/meta-llama/llama-2-70b-chat"]
input_cost_per_token = 0.0000015
litellm_provider = "openrouter"
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0000015
supports_tool_choice = true

[models."openrouter/meta-llama/llama-3-70b-instruct"]
input_cost_per_token = 5.9e-7
litellm_provider = "openrouter"
max_tokens = 8192
mode = "chat"
output_cost_per_token = 7.9e-7
supports_tool_choice = true

[models."openrouter/meta-llama/llama-3-70b-instruct:nitro"]
input_cost_per_token = 9e-7
litellm_provider = "openrouter"
max_tokens = 8192
mode = "chat"
output_cost_per_token = 9e-7
supports_tool_choice = true

[models."openrouter/meta-llama/llama-3-8b-instruct:extended"]
input_cost_per_token = 2.25e-7
litellm_provider = "openrouter"
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00000225
supports_tool_choice = true

[models."openrouter/meta-llama/llama-3-8b-instruct:free"]
input_cost_per_token = 0
litellm_provider = "openrouter"
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0
supports_tool_choice = true

[models."openrouter/microsoft/wizardlm-2-8x22b:nitro"]
input_cost_per_token = 0.000001
litellm_provider = "openrouter"
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.000001
supports_tool_choice = true

[models."openrouter/minimax/minimax-m2"]
input_cost_per_token = 2.55e-7
litellm_provider = "openrouter"
max_input_tokens = 204800
max_output_tokens = 204800
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.00000102
supports_function_calling = true
supports_prompt_caching = false
supports_reasoning = true
supports_tool_choice = true

[models."openrouter/mistralai/devstral-2512"]
input_cost_per_image = 0
input_cost_per_token = 1.5e-7
litellm_provider = "openrouter"
max_input_tokens = 262144
max_output_tokens = 65536
max_tokens = 262144
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_prompt_caching = false
supports_tool_choice = true
supports_vision = false

[models."openrouter/mistralai/devstral-2512:free"]
input_cost_per_image = 0
input_cost_per_token = 0
litellm_provider = "openrouter"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true
supports_prompt_caching = false
supports_tool_choice = true
supports_vision = false

[models."openrouter/mistralai/ministral-14b-2512"]
input_cost_per_image = 0
input_cost_per_token = 2e-7
litellm_provider = "openrouter"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 2e-7
supports_function_calling = true
supports_prompt_caching = false
supports_tool_choice = true
supports_vision = true

[models."openrouter/mistralai/ministral-3b-2512"]
input_cost_per_image = 0
input_cost_per_token = 1e-7
litellm_provider = "openrouter"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 1e-7
supports_function_calling = true
supports_prompt_caching = false
supports_tool_choice = true
supports_vision = true

[models."openrouter/mistralai/ministral-8b-2512"]
input_cost_per_image = 0
input_cost_per_token = 1.5e-7
litellm_provider = "openrouter"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 1.5e-7
supports_function_calling = true
supports_prompt_caching = false
supports_tool_choice = true
supports_vision = true

[models."openrouter/mistralai/mistral-7b-instruct"]
input_cost_per_token = 1.3e-7
litellm_provider = "openrouter"
max_tokens = 8192
mode = "chat"
output_cost_per_token = 1.3e-7
supports_tool_choice = true

[models."openrouter/mistralai/mistral-7b-instruct:free"]
input_cost_per_token = 0
litellm_provider = "openrouter"
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0
supports_tool_choice = true

[models."openrouter/mistralai/mistral-large"]
input_cost_per_token = 0.000008
litellm_provider = "openrouter"
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000024
supports_tool_choice = true

[models."openrouter/mistralai/mistral-large-2512"]
input_cost_per_image = 0
input_cost_per_token = 5e-7
litellm_provider = "openrouter"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000015
supports_function_calling = true
supports_prompt_caching = false
supports_tool_choice = true
supports_vision = true

[models."openrouter/mistralai/mistral-small-3.1-24b-instruct"]
input_cost_per_token = 1e-7
litellm_provider = "openrouter"
max_tokens = 32000
mode = "chat"
output_cost_per_token = 3e-7
supports_tool_choice = true

[models."openrouter/mistralai/mistral-small-3.2-24b-instruct"]
input_cost_per_token = 1e-7
litellm_provider = "openrouter"
max_tokens = 32000
mode = "chat"
output_cost_per_token = 3e-7
supports_tool_choice = true

[models."openrouter/mistralai/mixtral-8x22b-instruct"]
input_cost_per_token = 6.5e-7
litellm_provider = "openrouter"
max_tokens = 65536
mode = "chat"
output_cost_per_token = 6.5e-7
supports_tool_choice = true

[models."openrouter/nousresearch/nous-hermes-llama2-13b"]
input_cost_per_token = 2e-7
litellm_provider = "openrouter"
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2e-7
supports_tool_choice = true

[models."openrouter/openai/gpt-3.5-turbo"]
input_cost_per_token = 0.0000015
litellm_provider = "openrouter"
max_tokens = 4095
mode = "chat"
output_cost_per_token = 0.000002
supports_tool_choice = true

[models."openrouter/openai/gpt-3.5-turbo-16k"]
input_cost_per_token = 0.000003
litellm_provider = "openrouter"
max_tokens = 16383
mode = "chat"
output_cost_per_token = 0.000004
supports_tool_choice = true

[models."openrouter/openai/gpt-4"]
input_cost_per_token = 0.00003
litellm_provider = "openrouter"
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00006
supports_tool_choice = true

[models."openrouter/openai/gpt-4-vision-preview"]
input_cost_per_image = 0.01445
input_cost_per_token = 0.00001
litellm_provider = "openrouter"
max_tokens = 130000
mode = "chat"
output_cost_per_token = 0.00003
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/openai/gpt-4.1"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
litellm_provider = "openrouter"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000008
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/openai/gpt-4.1-2025-04-14"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
litellm_provider = "openrouter"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000008
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/openai/gpt-4.1-mini"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 4e-7
litellm_provider = "openrouter"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000016
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/openai/gpt-4.1-mini-2025-04-14"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 4e-7
litellm_provider = "openrouter"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000016
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/openai/gpt-4.1-nano"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 1e-7
litellm_provider = "openrouter"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 4e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/openai/gpt-4.1-nano-2025-04-14"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 1e-7
litellm_provider = "openrouter"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 4e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/openai/gpt-4o"]
input_cost_per_token = 0.0000025
litellm_provider = "openrouter"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/openai/gpt-4o-2024-05-13"]
input_cost_per_token = 0.000005
litellm_provider = "openrouter"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/openai/gpt-5"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "openrouter"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_reasoning = true
supports_tool_choice = true

[models."openrouter/openai/gpt-5-chat"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "openrouter"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_reasoning = true
supports_tool_choice = true

[models."openrouter/openai/gpt-5-codex"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
litellm_provider = "openrouter"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_reasoning = true
supports_tool_choice = true

[models."openrouter/openai/gpt-5-mini"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 2.5e-7
litellm_provider = "openrouter"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000002
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_reasoning = true
supports_tool_choice = true

[models."openrouter/openai/gpt-5-nano"]
cache_read_input_token_cost = 5e-9
input_cost_per_token = 5e-8
litellm_provider = "openrouter"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 4e-7
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_reasoning = true
supports_tool_choice = true

[models."openrouter/openai/gpt-5.2"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_image = 0
input_cost_per_token = 0.00000175
litellm_provider = "openrouter"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 400000
mode = "chat"
output_cost_per_token = 0.000014
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/openai/gpt-5.2-chat"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_image = 0
input_cost_per_token = 0.00000175
litellm_provider = "openrouter"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000014
supports_function_calling = true
supports_prompt_caching = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/openai/gpt-5.2-pro"]
input_cost_per_image = 0
input_cost_per_token = 0.000021
litellm_provider = "openrouter"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 400000
mode = "chat"
output_cost_per_token = 0.000168
supports_function_calling = true
supports_prompt_caching = false
supports_reasoning = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/openai/gpt-oss-120b"]
input_cost_per_token = 1.8e-7
litellm_provider = "openrouter"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 8e-7
source = "https://openrouter.ai/openai/gpt-oss-120b"
supports_function_calling = true
supports_parallel_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."openrouter/openai/gpt-oss-20b"]
input_cost_per_token = 1.8e-7
litellm_provider = "openrouter"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 8e-7
source = "https://openrouter.ai/openai/gpt-oss-20b"
supports_function_calling = true
supports_parallel_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."openrouter/openai/o1"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
litellm_provider = "openrouter"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."openrouter/openai/o1-mini"]
input_cost_per_token = 0.000003
litellm_provider = "openrouter"
max_input_tokens = 128000
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.000012
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true
supports_vision = false

[models."openrouter/openai/o1-mini-2024-09-12"]
input_cost_per_token = 0.000003
litellm_provider = "openrouter"
max_input_tokens = 128000
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.000012
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true
supports_vision = false

[models."openrouter/openai/o1-preview"]
input_cost_per_token = 0.000015
litellm_provider = "openrouter"
max_input_tokens = 128000
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true
supports_vision = false

[models."openrouter/openai/o1-preview-2024-09-12"]
input_cost_per_token = 0.000015
litellm_provider = "openrouter"
max_input_tokens = 128000
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.00006
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true
supports_vision = false

[models."openrouter/openai/o3-mini"]
input_cost_per_token = 0.0000011
litellm_provider = "openrouter"
max_input_tokens = 128000
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.0000044
supports_function_calling = true
supports_parallel_function_calling = true
supports_reasoning = true
supports_tool_choice = true
supports_vision = false

[models."openrouter/openai/o3-mini-high"]
input_cost_per_token = 0.0000011
litellm_provider = "openrouter"
max_input_tokens = 128000
max_output_tokens = 65536
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.0000044
supports_function_calling = true
supports_parallel_function_calling = true
supports_reasoning = true
supports_tool_choice = true
supports_vision = false

[models."openrouter/pygmalionai/mythalion-13b"]
input_cost_per_token = 0.000001875
litellm_provider = "openrouter"
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000001875
supports_tool_choice = true

[models."openrouter/qwen/qwen-2.5-coder-32b-instruct"]
input_cost_per_token = 1.8e-7
litellm_provider = "openrouter"
max_input_tokens = 33792
max_output_tokens = 33792
max_tokens = 33792
mode = "chat"
output_cost_per_token = 1.8e-7
supports_tool_choice = true

[models."openrouter/qwen/qwen-vl-plus"]
input_cost_per_token = 2.1e-7
litellm_provider = "openrouter"
max_input_tokens = 8192
max_output_tokens = 2048
max_tokens = 8192
mode = "chat"
output_cost_per_token = 6.3e-7
supports_tool_choice = true
supports_vision = true

[models."openrouter/qwen/qwen3-coder"]
input_cost_per_token = 2.2e-7
litellm_provider = "openrouter"
max_input_tokens = 262100
max_output_tokens = 262100
max_tokens = 262100
mode = "chat"
output_cost_per_token = 9.5e-7
source = "https://openrouter.ai/qwen/qwen3-coder"
supports_function_calling = true
supports_tool_choice = true

[models."openrouter/switchpoint/router"]
input_cost_per_token = 8.5e-7
litellm_provider = "openrouter"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000034
source = "https://openrouter.ai/switchpoint/router"
supports_tool_choice = true

[models."openrouter/undi95/remm-slerp-l2-13b"]
input_cost_per_token = 0.000001875
litellm_provider = "openrouter"
max_tokens = 6144
mode = "chat"
output_cost_per_token = 0.000001875
supports_tool_choice = true

[models."openrouter/x-ai/grok-4"]
input_cost_per_token = 0.000003
litellm_provider = "openrouter"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.000015
source = "https://openrouter.ai/x-ai/grok-4"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true
supports_web_search = true

[models."openrouter/x-ai/grok-4-fast:free"]
input_cost_per_token = 0
litellm_provider = "openrouter"
max_input_tokens = 2000000
max_output_tokens = 30000
max_tokens = 2000000
mode = "chat"
output_cost_per_token = 0
source = "https://openrouter.ai/x-ai/grok-4-fast:free"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true
supports_web_search = false

[models."openrouter/z-ai/glm-4.6"]
input_cost_per_token = 4e-7
litellm_provider = "openrouter"
max_input_tokens = 202800
max_output_tokens = 131000
max_tokens = 202800
mode = "chat"
output_cost_per_token = 0.00000175
source = "https://openrouter.ai/z-ai/glm-4.6"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."openrouter/z-ai/glm-4.6:exacto"]
input_cost_per_token = 4.5e-7
litellm_provider = "openrouter"
max_input_tokens = 202800
max_output_tokens = 131000
max_tokens = 202800
mode = "chat"
output_cost_per_token = 0.0000019
source = "https://openrouter.ai/z-ai/glm-4.6:exacto"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."ovhcloud/DeepSeek-R1-Distill-Llama-70B"]
input_cost_per_token = 6.7e-7
litellm_provider = "ovhcloud"
max_input_tokens = 131000
max_output_tokens = 131000
max_tokens = 131000
mode = "chat"
output_cost_per_token = 6.7e-7
source = "https://endpoints.ai.cloud.ovh.net/models/deepseek-r1-distill-llama-70b"
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."ovhcloud/Llama-3.1-8B-Instruct"]
input_cost_per_token = 1e-7
litellm_provider = "ovhcloud"
max_input_tokens = 131000
max_output_tokens = 131000
max_tokens = 131000
mode = "chat"
output_cost_per_token = 1e-7
source = "https://endpoints.ai.cloud.ovh.net/models/llama-3-1-8b-instruct"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."ovhcloud/Meta-Llama-3_1-70B-Instruct"]
input_cost_per_token = 6.7e-7
litellm_provider = "ovhcloud"
max_input_tokens = 131000
max_output_tokens = 131000
max_tokens = 131000
mode = "chat"
output_cost_per_token = 6.7e-7
source = "https://endpoints.ai.cloud.ovh.net/models/meta-llama-3-1-70b-instruct"
supports_function_calling = false
supports_response_schema = false
supports_tool_choice = false

[models."ovhcloud/Meta-Llama-3_3-70B-Instruct"]
input_cost_per_token = 6.7e-7
litellm_provider = "ovhcloud"
max_input_tokens = 131000
max_output_tokens = 131000
max_tokens = 131000
mode = "chat"
output_cost_per_token = 6.7e-7
source = "https://endpoints.ai.cloud.ovh.net/models/meta-llama-3-3-70b-instruct"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."ovhcloud/Mistral-7B-Instruct-v0.3"]
input_cost_per_token = 1e-7
litellm_provider = "ovhcloud"
max_input_tokens = 127000
max_output_tokens = 127000
max_tokens = 127000
mode = "chat"
output_cost_per_token = 1e-7
source = "https://endpoints.ai.cloud.ovh.net/models/mistral-7b-instruct-v0-3"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."ovhcloud/Mistral-Nemo-Instruct-2407"]
input_cost_per_token = 1.3e-7
litellm_provider = "ovhcloud"
max_input_tokens = 118000
max_output_tokens = 118000
max_tokens = 118000
mode = "chat"
output_cost_per_token = 1.3e-7
source = "https://endpoints.ai.cloud.ovh.net/models/mistral-nemo-instruct-2407"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."ovhcloud/Mistral-Small-3.2-24B-Instruct-2506"]
input_cost_per_token = 9e-8
litellm_provider = "ovhcloud"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 2.8e-7
source = "https://endpoints.ai.cloud.ovh.net/models/mistral-small-3-2-24b-instruct-2506"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."ovhcloud/Mixtral-8x7B-Instruct-v0.1"]
input_cost_per_token = 6.3e-7
litellm_provider = "ovhcloud"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 6.3e-7
source = "https://endpoints.ai.cloud.ovh.net/models/mixtral-8x7b-instruct-v0-1"
supports_function_calling = false
supports_response_schema = true
supports_tool_choice = false

[models."ovhcloud/Qwen2.5-Coder-32B-Instruct"]
input_cost_per_token = 8.7e-7
litellm_provider = "ovhcloud"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 8.7e-7
source = "https://endpoints.ai.cloud.ovh.net/models/qwen2-5-coder-32b-instruct"
supports_function_calling = false
supports_response_schema = true
supports_tool_choice = false

[models."ovhcloud/Qwen2.5-VL-72B-Instruct"]
input_cost_per_token = 9.1e-7
litellm_provider = "ovhcloud"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 9.1e-7
source = "https://endpoints.ai.cloud.ovh.net/models/qwen2-5-vl-72b-instruct"
supports_function_calling = false
supports_response_schema = true
supports_tool_choice = false
supports_vision = true

[models."ovhcloud/Qwen3-32B"]
input_cost_per_token = 8e-8
litellm_provider = "ovhcloud"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 2.3e-7
source = "https://endpoints.ai.cloud.ovh.net/models/qwen3-32b"
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."ovhcloud/gpt-oss-120b"]
input_cost_per_token = 8e-8
litellm_provider = "ovhcloud"
max_input_tokens = 131000
max_output_tokens = 131000
max_tokens = 131000
mode = "chat"
output_cost_per_token = 4e-7
source = "https://endpoints.ai.cloud.ovh.net/models/gpt-oss-120b"
supports_function_calling = false
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = false

[models."ovhcloud/gpt-oss-20b"]
input_cost_per_token = 4e-8
litellm_provider = "ovhcloud"
max_input_tokens = 131000
max_output_tokens = 131000
max_tokens = 131000
mode = "chat"
output_cost_per_token = 1.5e-7
source = "https://endpoints.ai.cloud.ovh.net/models/gpt-oss-20b"
supports_function_calling = false
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = false

[models."ovhcloud/llava-v1.6-mistral-7b-hf"]
input_cost_per_token = 2.9e-7
litellm_provider = "ovhcloud"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 2.9e-7
source = "https://endpoints.ai.cloud.ovh.net/models/llava-next-mistral-7b"
supports_function_calling = false
supports_response_schema = true
supports_tool_choice = false
supports_vision = true

[models."ovhcloud/mamba-codestral-7B-v0.1"]
input_cost_per_token = 1.9e-7
litellm_provider = "ovhcloud"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 1.9e-7
source = "https://endpoints.ai.cloud.ovh.net/models/mamba-codestral-7b-v0-1"
supports_function_calling = false
supports_response_schema = true
supports_tool_choice = false

[models."palm/chat-bison"]
input_cost_per_token = 1.25e-7
litellm_provider = "palm"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."palm/chat-bison-001"]
input_cost_per_token = 1.25e-7
litellm_provider = "palm"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."palm/text-bison"]
input_cost_per_token = 1.25e-7
litellm_provider = "palm"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
mode = "completion"
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."palm/text-bison-001"]
input_cost_per_token = 1.25e-7
litellm_provider = "palm"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
mode = "completion"
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."palm/text-bison-safety-off"]
input_cost_per_token = 1.25e-7
litellm_provider = "palm"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
mode = "completion"
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."palm/text-bison-safety-recitation-off"]
input_cost_per_token = 1.25e-7
litellm_provider = "palm"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
mode = "completion"
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."parallel_ai/search"]
input_cost_per_query = 0.004
litellm_provider = "parallel_ai"
mode = "search"

[models."parallel_ai/search-pro"]
input_cost_per_query = 0.009
litellm_provider = "parallel_ai"
mode = "search"

[models."perplexity/codellama-34b-instruct"]
input_cost_per_token = 3.5e-7
litellm_provider = "perplexity"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.0000014

[models."perplexity/codellama-70b-instruct"]
input_cost_per_token = 7e-7
litellm_provider = "perplexity"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.0000028

[models."perplexity/llama-2-70b-chat"]
input_cost_per_token = 7e-7
litellm_provider = "perplexity"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0000028

[models."perplexity/llama-3.1-70b-instruct"]
input_cost_per_token = 0.000001
litellm_provider = "perplexity"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000001

[models."perplexity/llama-3.1-8b-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "perplexity"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7

[models."perplexity/llama-3.1-sonar-huge-128k-online"]
deprecation_date = "2025-02-22"
input_cost_per_token = 0.000005
litellm_provider = "perplexity"
max_input_tokens = 127072
max_output_tokens = 127072
max_tokens = 127072
mode = "chat"
output_cost_per_token = 0.000005

[models."perplexity/llama-3.1-sonar-large-128k-chat"]
deprecation_date = "2025-02-22"
input_cost_per_token = 0.000001
litellm_provider = "perplexity"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000001

[models."perplexity/llama-3.1-sonar-large-128k-online"]
deprecation_date = "2025-02-22"
input_cost_per_token = 0.000001
litellm_provider = "perplexity"
max_input_tokens = 127072
max_output_tokens = 127072
max_tokens = 127072
mode = "chat"
output_cost_per_token = 0.000001

[models."perplexity/llama-3.1-sonar-small-128k-chat"]
deprecation_date = "2025-02-22"
input_cost_per_token = 2e-7
litellm_provider = "perplexity"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 2e-7

[models."perplexity/llama-3.1-sonar-small-128k-online"]
deprecation_date = "2025-02-22"
input_cost_per_token = 2e-7
litellm_provider = "perplexity"
max_input_tokens = 127072
max_output_tokens = 127072
max_tokens = 127072
mode = "chat"
output_cost_per_token = 2e-7

[models."perplexity/mistral-7b-instruct"]
input_cost_per_token = 7e-8
litellm_provider = "perplexity"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2.8e-7

[models."perplexity/mixtral-8x7b-instruct"]
input_cost_per_token = 7e-8
litellm_provider = "perplexity"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2.8e-7

[models."perplexity/pplx-70b-chat"]
input_cost_per_token = 7e-7
litellm_provider = "perplexity"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0000028

[models."perplexity/pplx-70b-online"]
input_cost_per_request = 0.005
input_cost_per_token = 0
litellm_provider = "perplexity"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0000028

[models."perplexity/pplx-7b-chat"]
input_cost_per_token = 7e-8
litellm_provider = "perplexity"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2.8e-7

[models."perplexity/pplx-7b-online"]
input_cost_per_request = 0.005
input_cost_per_token = 0
litellm_provider = "perplexity"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2.8e-7

[models."perplexity/search"]
input_cost_per_query = 0.005
litellm_provider = "perplexity"
mode = "search"

[models."perplexity/sonar"]
input_cost_per_token = 0.000001
litellm_provider = "perplexity"
max_input_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000001
supports_web_search = true

[models."perplexity/sonar".search_context_cost_per_query]
search_context_size_high = 0.012
search_context_size_low = 0.005
search_context_size_medium = 0.008

[models."perplexity/sonar-deep-research"]
citation_cost_per_token = 0.000002
input_cost_per_token = 0.000002
litellm_provider = "perplexity"
max_input_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_reasoning_token = 0.000003
output_cost_per_token = 0.000008
supports_reasoning = true
supports_web_search = true

[models."perplexity/sonar-deep-research".search_context_cost_per_query]
search_context_size_high = 0.005
search_context_size_low = 0.005
search_context_size_medium = 0.005

[models."perplexity/sonar-medium-chat"]
input_cost_per_token = 6e-7
litellm_provider = "perplexity"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.0000018

[models."perplexity/sonar-medium-online"]
input_cost_per_request = 0.005
input_cost_per_token = 0
litellm_provider = "perplexity"
max_input_tokens = 12000
max_output_tokens = 12000
max_tokens = 12000
mode = "chat"
output_cost_per_token = 0.0000018

[models."perplexity/sonar-pro"]
input_cost_per_token = 0.000003
litellm_provider = "perplexity"
max_input_tokens = 200000
max_output_tokens = 8000
max_tokens = 8000
mode = "chat"
output_cost_per_token = 0.000015
supports_web_search = true

[models."perplexity/sonar-pro".search_context_cost_per_query]
search_context_size_high = 0.014
search_context_size_low = 0.006
search_context_size_medium = 0.01

[models."perplexity/sonar-reasoning"]
input_cost_per_token = 0.000001
litellm_provider = "perplexity"
max_input_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000005
supports_reasoning = true
supports_web_search = true

[models."perplexity/sonar-reasoning".search_context_cost_per_query]
search_context_size_high = 0.014
search_context_size_low = 0.005
search_context_size_medium = 0.008

[models."perplexity/sonar-reasoning-pro"]
input_cost_per_token = 0.000002
litellm_provider = "perplexity"
max_input_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000008
supports_reasoning = true
supports_web_search = true

[models."perplexity/sonar-reasoning-pro".search_context_cost_per_query]
search_context_size_high = 0.014
search_context_size_low = 0.006
search_context_size_medium = 0.01

[models."perplexity/sonar-small-chat"]
input_cost_per_token = 7e-8
litellm_provider = "perplexity"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 2.8e-7

[models."perplexity/sonar-small-online"]
input_cost_per_request = 0.005
input_cost_per_token = 0
litellm_provider = "perplexity"
max_input_tokens = 12000
max_output_tokens = 12000
max_tokens = 12000
mode = "chat"
output_cost_per_token = 2.8e-7

[models."publicai/BSC-LT/ALIA-40b-instruct_Q8_0"]
input_cost_per_token = 0
litellm_provider = "publicai"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0
source = "https://platform.publicai.co/docs"
supports_function_calling = true
supports_tool_choice = true

[models."publicai/BSC-LT/salamandra-7b-instruct-tools-16k"]
input_cost_per_token = 0
litellm_provider = "publicai"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0
source = "https://platform.publicai.co/docs"
supports_function_calling = true
supports_tool_choice = true

[models."publicai/aisingapore/Gemma-SEA-LION-v4-27B-IT"]
input_cost_per_token = 0
litellm_provider = "publicai"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0
source = "https://platform.publicai.co/docs"
supports_function_calling = true
supports_tool_choice = true

[models."publicai/aisingapore/Qwen-SEA-LION-v4-32B-IT"]
input_cost_per_token = 0
litellm_provider = "publicai"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0
source = "https://platform.publicai.co/docs"
supports_function_calling = true
supports_tool_choice = true

[models."publicai/allenai/Olmo-3-32B-Think"]
input_cost_per_token = 0
litellm_provider = "publicai"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0
source = "https://platform.publicai.co/docs"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."publicai/allenai/Olmo-3-7B-Instruct"]
input_cost_per_token = 0
litellm_provider = "publicai"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0
source = "https://platform.publicai.co/docs"
supports_function_calling = true
supports_tool_choice = true

[models."publicai/allenai/Olmo-3-7B-Think"]
input_cost_per_token = 0
litellm_provider = "publicai"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0
source = "https://platform.publicai.co/docs"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."publicai/swiss-ai/apertus-70b-instruct"]
input_cost_per_token = 0
litellm_provider = "publicai"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0
source = "https://platform.publicai.co/docs"
supports_function_calling = true
supports_tool_choice = true

[models."publicai/swiss-ai/apertus-8b-instruct"]
input_cost_per_token = 0
litellm_provider = "publicai"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0
source = "https://platform.publicai.co/docs"
supports_function_calling = true
supports_tool_choice = true

[models."qwen.qwen3-235b-a22b-2507-v1:0"]
input_cost_per_token = 2.2e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 262144
max_output_tokens = 131072
max_tokens = 262144
mode = "chat"
output_cost_per_token = 8.8e-7
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."qwen.qwen3-32b-v1:0"]
input_cost_per_token = 1.5e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 131072
max_output_tokens = 16384
max_tokens = 131072
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."qwen.qwen3-coder-30b-a3b-v1:0"]
input_cost_per_token = 1.5e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 262144
max_output_tokens = 131072
max_tokens = 262144
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."qwen.qwen3-coder-480b-a35b-v1:0"]
input_cost_per_token = 2.2e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 262000
max_output_tokens = 65536
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000018
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."qwen.qwen3-next-80b-a3b"]
input_cost_per_token = 1.5e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000012
supports_function_calling = true
supports_system_messages = true

[models."qwen.qwen3-vl-235b-a22b"]
input_cost_per_token = 5.3e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000266
supports_function_calling = true
supports_system_messages = true
supports_vision = true

[models."recraft/recraftv2"]
litellm_provider = "recraft"
mode = "image_generation"
output_cost_per_image = 0.022
source = "https://www.recraft.ai/docs#pricing"
supported_endpoints = ["/v1/images/generations"]

[models."recraft/recraftv3"]
litellm_provider = "recraft"
mode = "image_generation"
output_cost_per_image = 0.04
source = "https://www.recraft.ai/docs#pricing"
supported_endpoints = ["/v1/images/generations"]

[models."replicate/meta/llama-2-13b"]
input_cost_per_token = 1e-7
litellm_provider = "replicate"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 5e-7
supports_tool_choice = true

[models."replicate/meta/llama-2-13b-chat"]
input_cost_per_token = 1e-7
litellm_provider = "replicate"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 5e-7
supports_tool_choice = true

[models."replicate/meta/llama-2-70b"]
input_cost_per_token = 6.5e-7
litellm_provider = "replicate"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00000275
supports_tool_choice = true

[models."replicate/meta/llama-2-70b-chat"]
input_cost_per_token = 6.5e-7
litellm_provider = "replicate"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00000275
supports_tool_choice = true

[models."replicate/meta/llama-2-7b"]
input_cost_per_token = 5e-8
litellm_provider = "replicate"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2.5e-7
supports_tool_choice = true

[models."replicate/meta/llama-2-7b-chat"]
input_cost_per_token = 5e-8
litellm_provider = "replicate"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2.5e-7
supports_tool_choice = true

[models."replicate/meta/llama-3-70b"]
input_cost_per_token = 6.5e-7
litellm_provider = "replicate"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000275
supports_tool_choice = true

[models."replicate/meta/llama-3-70b-instruct"]
input_cost_per_token = 6.5e-7
litellm_provider = "replicate"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000275
supports_tool_choice = true

[models."replicate/meta/llama-3-8b"]
input_cost_per_token = 5e-8
litellm_provider = "replicate"
max_input_tokens = 8086
max_output_tokens = 8086
max_tokens = 8086
mode = "chat"
output_cost_per_token = 2.5e-7
supports_tool_choice = true

[models."replicate/meta/llama-3-8b-instruct"]
input_cost_per_token = 5e-8
litellm_provider = "replicate"
max_input_tokens = 8086
max_output_tokens = 8086
max_tokens = 8086
mode = "chat"
output_cost_per_token = 2.5e-7
supports_tool_choice = true

[models."replicate/mistralai/mistral-7b-instruct-v0.2"]
input_cost_per_token = 5e-8
litellm_provider = "replicate"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2.5e-7
supports_tool_choice = true

[models."replicate/mistralai/mistral-7b-v0.1"]
input_cost_per_token = 5e-8
litellm_provider = "replicate"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 2.5e-7
supports_tool_choice = true

[models."replicate/mistralai/mixtral-8x7b-instruct-v0.1"]
input_cost_per_token = 3e-7
litellm_provider = "replicate"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000001
supports_tool_choice = true

[models."rerank-english-v2.0"]
input_cost_per_query = 0.002
input_cost_per_token = 0
litellm_provider = "cohere"
max_input_tokens = 4096
max_output_tokens = 4096
max_query_tokens = 2048
max_tokens = 4096
mode = "rerank"
output_cost_per_token = 0

[models."rerank-english-v3.0"]
input_cost_per_query = 0.002
input_cost_per_token = 0
litellm_provider = "cohere"
max_input_tokens = 4096
max_output_tokens = 4096
max_query_tokens = 2048
max_tokens = 4096
mode = "rerank"
output_cost_per_token = 0

[models."rerank-multilingual-v2.0"]
input_cost_per_query = 0.002
input_cost_per_token = 0
litellm_provider = "cohere"
max_input_tokens = 4096
max_output_tokens = 4096
max_query_tokens = 2048
max_tokens = 4096
mode = "rerank"
output_cost_per_token = 0

[models."rerank-multilingual-v3.0"]
input_cost_per_query = 0.002
input_cost_per_token = 0
litellm_provider = "cohere"
max_input_tokens = 4096
max_output_tokens = 4096
max_query_tokens = 2048
max_tokens = 4096
mode = "rerank"
output_cost_per_token = 0

[models."rerank-v3.5"]
input_cost_per_query = 0.002
input_cost_per_token = 0
litellm_provider = "cohere"
max_input_tokens = 4096
max_output_tokens = 4096
max_query_tokens = 2048
max_tokens = 4096
mode = "rerank"
output_cost_per_token = 0

[models."runwayml/eleven_multilingual_v2"]
input_cost_per_character = 3e-7
litellm_provider = "runwayml"
mode = "audio_speech"
source = "https://docs.dev.runwayml.com/guides/pricing/"

[models."runwayml/eleven_multilingual_v2".metadata]
comment = "Estimated cost based on standard TTS pricing. RunwayML uses ElevenLabs models."

[models."runwayml/gen3a_turbo"]
litellm_provider = "runwayml"
mode = "video_generation"
output_cost_per_video_per_second = 0.05
source = "https://docs.dev.runwayml.com/guides/pricing/"
supported_modalities = ["text", "image"]
supported_output_modalities = ["video"]
supported_resolutions = ["1280x720", "720x1280"]

[models."runwayml/gen3a_turbo".metadata]
comment = "5 credits per second @ $0.01 per credit = $0.05 per second"

[models."runwayml/gen4_aleph"]
litellm_provider = "runwayml"
mode = "video_generation"
output_cost_per_video_per_second = 0.15
source = "https://docs.dev.runwayml.com/guides/pricing/"
supported_modalities = ["text", "image"]
supported_output_modalities = ["video"]
supported_resolutions = ["1280x720", "720x1280"]

[models."runwayml/gen4_aleph".metadata]
comment = "15 credits per second @ $0.01 per credit = $0.15 per second"

[models."runwayml/gen4_image"]
input_cost_per_image = 0.05
litellm_provider = "runwayml"
mode = "image_generation"
output_cost_per_image = 0.05
source = "https://docs.dev.runwayml.com/guides/pricing/"
supported_modalities = ["text", "image"]
supported_output_modalities = ["image"]
supported_resolutions = ["1280x720", "1920x1080"]

[models."runwayml/gen4_image".metadata]
comment = "5 credits per 720p image or 8 credits per 1080p image @ $0.01 per credit. Using 5 credits ($0.05) as base cost"

[models."runwayml/gen4_image_turbo"]
input_cost_per_image = 0.02
litellm_provider = "runwayml"
mode = "image_generation"
output_cost_per_image = 0.02
source = "https://docs.dev.runwayml.com/guides/pricing/"
supported_modalities = ["text", "image"]
supported_output_modalities = ["image"]
supported_resolutions = ["1280x720", "1920x1080"]

[models."runwayml/gen4_image_turbo".metadata]
comment = "2 credits per image (any resolution) @ $0.01 per credit = $0.02 per image"

[models."runwayml/gen4_turbo"]
litellm_provider = "runwayml"
mode = "video_generation"
output_cost_per_video_per_second = 0.05
source = "https://docs.dev.runwayml.com/guides/pricing/"
supported_modalities = ["text", "image"]
supported_output_modalities = ["video"]
supported_resolutions = ["1280x720", "720x1280"]

[models."runwayml/gen4_turbo".metadata]
comment = "5 credits per second @ $0.01 per credit = $0.05 per second"

[models."sagemaker/meta-textgeneration-llama-2-13b"]
input_cost_per_token = 0
litellm_provider = "sagemaker"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "completion"
output_cost_per_token = 0

[models."sagemaker/meta-textgeneration-llama-2-13b-f"]
input_cost_per_token = 0
litellm_provider = "sagemaker"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0

[models."sagemaker/meta-textgeneration-llama-2-70b"]
input_cost_per_token = 0
litellm_provider = "sagemaker"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "completion"
output_cost_per_token = 0

[models."sagemaker/meta-textgeneration-llama-2-70b-b-f"]
input_cost_per_token = 0
litellm_provider = "sagemaker"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0

[models."sagemaker/meta-textgeneration-llama-2-7b"]
input_cost_per_token = 0
litellm_provider = "sagemaker"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "completion"
output_cost_per_token = 0

[models."sagemaker/meta-textgeneration-llama-2-7b-f"]
input_cost_per_token = 0
litellm_provider = "sagemaker"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0

[models."sambanova/DeepSeek-R1"]
input_cost_per_token = 0.000005
litellm_provider = "sambanova"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000007
source = "https://cloud.sambanova.ai/plans/pricing"

[models."sambanova/DeepSeek-R1-Distill-Llama-70B"]
input_cost_per_token = 7e-7
litellm_provider = "sambanova"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000014
source = "https://cloud.sambanova.ai/plans/pricing"

[models."sambanova/DeepSeek-V3-0324"]
input_cost_per_token = 0.000003
litellm_provider = "sambanova"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000045
source = "https://cloud.sambanova.ai/plans/pricing"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."sambanova/DeepSeek-V3.1"]
input_cost_per_token = 0.000003
litellm_provider = "sambanova"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000045
source = "https://cloud.sambanova.ai/plans/pricing"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."sambanova/Llama-4-Maverick-17B-128E-Instruct"]
input_cost_per_token = 6.3e-7
litellm_provider = "sambanova"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000018
source = "https://cloud.sambanova.ai/plans/pricing"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."sambanova/Llama-4-Maverick-17B-128E-Instruct".metadata]
notes = "For vision models, images are converted to 6432 input tokens and are billed at that amount"

[models."sambanova/Llama-4-Scout-17B-16E-Instruct"]
input_cost_per_token = 4e-7
litellm_provider = "sambanova"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 7e-7
source = "https://cloud.sambanova.ai/plans/pricing"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."sambanova/Llama-4-Scout-17B-16E-Instruct".metadata]
notes = "For vision models, images are converted to 6432 input tokens and are billed at that amount"

[models."sambanova/Meta-Llama-3.1-405B-Instruct"]
input_cost_per_token = 0.000005
litellm_provider = "sambanova"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.00001
source = "https://cloud.sambanova.ai/plans/pricing"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."sambanova/Meta-Llama-3.1-8B-Instruct"]
input_cost_per_token = 1e-7
litellm_provider = "sambanova"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 2e-7
source = "https://cloud.sambanova.ai/plans/pricing"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."sambanova/Meta-Llama-3.2-1B-Instruct"]
input_cost_per_token = 4e-8
litellm_provider = "sambanova"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 8e-8
source = "https://cloud.sambanova.ai/plans/pricing"

[models."sambanova/Meta-Llama-3.2-3B-Instruct"]
input_cost_per_token = 8e-8
litellm_provider = "sambanova"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 1.6e-7
source = "https://cloud.sambanova.ai/plans/pricing"

[models."sambanova/Meta-Llama-3.3-70B-Instruct"]
input_cost_per_token = 6e-7
litellm_provider = "sambanova"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000012
source = "https://cloud.sambanova.ai/plans/pricing"
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."sambanova/Meta-Llama-Guard-3-8B"]
input_cost_per_token = 3e-7
litellm_provider = "sambanova"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 3e-7
source = "https://cloud.sambanova.ai/plans/pricing"

[models."sambanova/QwQ-32B"]
input_cost_per_token = 5e-7
litellm_provider = "sambanova"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.000001
source = "https://cloud.sambanova.ai/plans/pricing"

[models."sambanova/Qwen2-Audio-7B-Instruct"]
input_cost_per_token = 5e-7
litellm_provider = "sambanova"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0001
source = "https://cloud.sambanova.ai/plans/pricing"
supports_audio_input = true

[models."sambanova/Qwen3-32B"]
input_cost_per_token = 4e-7
litellm_provider = "sambanova"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 8e-7
source = "https://cloud.sambanova.ai/plans/pricing"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."sambanova/gpt-oss-120b"]
input_cost_per_token = 0.000003
litellm_provider = "sambanova"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000045
source = "https://cloud.sambanova.ai/plans/pricing"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."sample_spec"]
code_interpreter_cost_per_session = 0
computer_use_input_cost_per_1k_tokens = 0
computer_use_output_cost_per_1k_tokens = 0
deprecation_date = "date when the model becomes deprecated in the format YYYY-MM-DD"
file_search_cost_per_1k_calls = 0
file_search_cost_per_gb_per_day = 0
input_cost_per_audio_token = 0
input_cost_per_token = 0
litellm_provider = "one of https://docs.litellm.ai/docs/providers"
max_input_tokens = "max input tokens, if the provider specifies it. if not default to max_tokens"
max_output_tokens = "max output tokens, if the provider specifies it. if not default to max_tokens"
max_tokens = "LEGACY parameter. set to max_output_tokens if provider specifies it. IF not set to max_input_tokens, if provider specifies it."
mode = "one of: chat, embedding, completion, image_generation, audio_transcription, audio_speech, image_generation, moderation, rerank, search"
output_cost_per_reasoning_token = 0
output_cost_per_token = 0
supported_regions = ["global", "us-west-2", "eu-west-1", "ap-southeast-1", "ap-northeast-1"]
supports_audio_input = true
supports_audio_output = true
supports_function_calling = true
supports_parallel_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_vision = true
supports_web_search = true
vector_store_cost_per_gb_per_day = 0

[models."sample_spec".search_context_cost_per_query]
search_context_size_high = 0
search_context_size_low = 0
search_context_size_medium = 0

[models."searxng/search"]
input_cost_per_query = 0
litellm_provider = "searxng"
mode = "search"

[models."searxng/search".metadata]
notes = "SearXNG is an open-source metasearch engine. Free to use when self-hosted or using public instances."

[models."snowflake/claude-3-5-sonnet"]
litellm_provider = "snowflake"
max_input_tokens = 18000
max_output_tokens = 8192
max_tokens = 18000
mode = "chat"
supports_computer_use = true

[models."snowflake/deepseek-r1"]
litellm_provider = "snowflake"
max_input_tokens = 32768
max_output_tokens = 8192
max_tokens = 32768
mode = "chat"
supports_reasoning = true

[models."snowflake/gemma-7b"]
litellm_provider = "snowflake"
max_input_tokens = 8000
max_output_tokens = 8192
max_tokens = 8000
mode = "chat"

[models."snowflake/jamba-1.5-large"]
litellm_provider = "snowflake"
max_input_tokens = 256000
max_output_tokens = 8192
max_tokens = 256000
mode = "chat"

[models."snowflake/jamba-1.5-mini"]
litellm_provider = "snowflake"
max_input_tokens = 256000
max_output_tokens = 8192
max_tokens = 256000
mode = "chat"

[models."snowflake/jamba-instruct"]
litellm_provider = "snowflake"
max_input_tokens = 256000
max_output_tokens = 8192
max_tokens = 256000
mode = "chat"

[models."snowflake/llama2-70b-chat"]
litellm_provider = "snowflake"
max_input_tokens = 4096
max_output_tokens = 8192
max_tokens = 4096
mode = "chat"

[models."snowflake/llama3-70b"]
litellm_provider = "snowflake"
max_input_tokens = 8000
max_output_tokens = 8192
max_tokens = 8000
mode = "chat"

[models."snowflake/llama3-8b"]
litellm_provider = "snowflake"
max_input_tokens = 8000
max_output_tokens = 8192
max_tokens = 8000
mode = "chat"

[models."snowflake/llama3.1-405b"]
litellm_provider = "snowflake"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"

[models."snowflake/llama3.1-70b"]
litellm_provider = "snowflake"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"

[models."snowflake/llama3.1-8b"]
litellm_provider = "snowflake"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"

[models."snowflake/llama3.2-1b"]
litellm_provider = "snowflake"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"

[models."snowflake/llama3.2-3b"]
litellm_provider = "snowflake"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"

[models."snowflake/llama3.3-70b"]
litellm_provider = "snowflake"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"

[models."snowflake/mistral-7b"]
litellm_provider = "snowflake"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 32000
mode = "chat"

[models."snowflake/mistral-large"]
litellm_provider = "snowflake"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 32000
mode = "chat"

[models."snowflake/mistral-large2"]
litellm_provider = "snowflake"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"

[models."snowflake/mixtral-8x7b"]
litellm_provider = "snowflake"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 32000
mode = "chat"

[models."snowflake/reka-core"]
litellm_provider = "snowflake"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 32000
mode = "chat"

[models."snowflake/reka-flash"]
litellm_provider = "snowflake"
max_input_tokens = 100000
max_output_tokens = 8192
max_tokens = 100000
mode = "chat"

[models."snowflake/snowflake-arctic"]
litellm_provider = "snowflake"
max_input_tokens = 4096
max_output_tokens = 8192
max_tokens = 4096
mode = "chat"

[models."snowflake/snowflake-llama-3.1-405b"]
litellm_provider = "snowflake"
max_input_tokens = 8000
max_output_tokens = 8192
max_tokens = 8000
mode = "chat"

[models."snowflake/snowflake-llama-3.3-70b"]
litellm_provider = "snowflake"
max_input_tokens = 8000
max_output_tokens = 8192
max_tokens = 8000
mode = "chat"

[models."stability.sd3-5-large-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
max_tokens = 77
mode = "image_generation"
output_cost_per_image = 0.08

[models."stability.sd3-large-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
max_tokens = 77
mode = "image_generation"
output_cost_per_image = 0.08

[models."stability.stable-conservative-upscale-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
mode = "image_edit"
output_cost_per_image = 0.4

[models."stability.stable-creative-upscale-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
mode = "image_edit"
output_cost_per_image = 0.6

[models."stability.stable-fast-upscale-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
mode = "image_edit"
output_cost_per_image = 0.03

[models."stability.stable-image-control-sketch-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
mode = "image_edit"
output_cost_per_image = 0.07

[models."stability.stable-image-control-structure-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
mode = "image_edit"
output_cost_per_image = 0.07

[models."stability.stable-image-core-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
max_tokens = 77
mode = "image_generation"
output_cost_per_image = 0.04

[models."stability.stable-image-core-v1:1"]
litellm_provider = "bedrock"
max_input_tokens = 77
max_tokens = 77
mode = "image_generation"
output_cost_per_image = 0.04

[models."stability.stable-image-erase-object-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
mode = "image_edit"
output_cost_per_image = 0.07

[models."stability.stable-image-inpaint-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
mode = "image_edit"
output_cost_per_image = 0.07

[models."stability.stable-image-remove-background-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
mode = "image_edit"
output_cost_per_image = 0.07

[models."stability.stable-image-search-recolor-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
mode = "image_edit"
output_cost_per_image = 0.07

[models."stability.stable-image-search-replace-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
mode = "image_edit"
output_cost_per_image = 0.07

[models."stability.stable-image-style-guide-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
mode = "image_edit"
output_cost_per_image = 0.07

[models."stability.stable-image-ultra-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
max_tokens = 77
mode = "image_generation"
output_cost_per_image = 0.14

[models."stability.stable-image-ultra-v1:1"]
litellm_provider = "bedrock"
max_input_tokens = 77
max_tokens = 77
mode = "image_generation"
output_cost_per_image = 0.14

[models."stability.stable-outpaint-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
mode = "image_edit"
output_cost_per_image = 0.06

[models."stability.stable-style-transfer-v1:0"]
litellm_provider = "bedrock"
max_input_tokens = 77
mode = "image_edit"
output_cost_per_image = 0.08

[models."stability/conservative"]
litellm_provider = "stability"
mode = "image_edit"
output_cost_per_image = 0.04
supported_endpoints = ["/v1/images/edits"]

[models."stability/creative"]
litellm_provider = "stability"
mode = "image_edit"
output_cost_per_image = 0.06
supported_endpoints = ["/v1/images/edits"]

[models."stability/erase"]
litellm_provider = "stability"
mode = "image_edit"
output_cost_per_image = 0.005
supported_endpoints = ["/v1/images/edits"]

[models."stability/fast"]
litellm_provider = "stability"
mode = "image_edit"
output_cost_per_image = 0.002
supported_endpoints = ["/v1/images/edits"]

[models."stability/inpaint"]
litellm_provider = "stability"
mode = "image_edit"
output_cost_per_image = 0.005
supported_endpoints = ["/v1/images/edits"]

[models."stability/outpaint"]
litellm_provider = "stability"
mode = "image_edit"
output_cost_per_image = 0.004
supported_endpoints = ["/v1/images/edits"]

[models."stability/remove-background"]
litellm_provider = "stability"
mode = "image_edit"
output_cost_per_image = 0.005
supported_endpoints = ["/v1/images/edits"]

[models."stability/replace-background-and-relight"]
litellm_provider = "stability"
mode = "image_edit"
output_cost_per_image = 0.008
supported_endpoints = ["/v1/images/edits"]

[models."stability/sd3"]
litellm_provider = "stability"
mode = "image_generation"
output_cost_per_image = 0.065
supported_endpoints = ["/v1/images/generations"]

[models."stability/sd3-large"]
litellm_provider = "stability"
mode = "image_generation"
output_cost_per_image = 0.065
supported_endpoints = ["/v1/images/generations"]

[models."stability/sd3-large-turbo"]
litellm_provider = "stability"
mode = "image_generation"
output_cost_per_image = 0.04
supported_endpoints = ["/v1/images/generations"]

[models."stability/sd3-medium"]
litellm_provider = "stability"
mode = "image_generation"
output_cost_per_image = 0.035
supported_endpoints = ["/v1/images/generations"]

[models."stability/sd3.5-large"]
litellm_provider = "stability"
mode = "image_generation"
output_cost_per_image = 0.065
supported_endpoints = ["/v1/images/generations"]

[models."stability/sd3.5-large-turbo"]
litellm_provider = "stability"
mode = "image_generation"
output_cost_per_image = 0.04
supported_endpoints = ["/v1/images/generations"]

[models."stability/sd3.5-medium"]
litellm_provider = "stability"
mode = "image_generation"
output_cost_per_image = 0.035
supported_endpoints = ["/v1/images/generations"]

[models."stability/search-and-recolor"]
litellm_provider = "stability"
mode = "image_edit"
output_cost_per_image = 0.005
supported_endpoints = ["/v1/images/edits"]

[models."stability/search-and-replace"]
litellm_provider = "stability"
mode = "image_edit"
output_cost_per_image = 0.005
supported_endpoints = ["/v1/images/edits"]

[models."stability/sketch"]
litellm_provider = "stability"
mode = "image_edit"
output_cost_per_image = 0.005
supported_endpoints = ["/v1/images/edits"]

[models."stability/stable-image-core"]
litellm_provider = "stability"
mode = "image_generation"
output_cost_per_image = 0.03
supported_endpoints = ["/v1/images/generations"]

[models."stability/stable-image-ultra"]
litellm_provider = "stability"
mode = "image_generation"
output_cost_per_image = 0.08
supported_endpoints = ["/v1/images/generations"]

[models."stability/structure"]
litellm_provider = "stability"
mode = "image_edit"
output_cost_per_image = 0.005
supported_endpoints = ["/v1/images/edits"]

[models."stability/style"]
litellm_provider = "stability"
mode = "image_edit"
output_cost_per_image = 0.005
supported_endpoints = ["/v1/images/edits"]

[models."stability/style-transfer"]
litellm_provider = "stability"
mode = "image_edit"
output_cost_per_image = 0.008
supported_endpoints = ["/v1/images/edits"]

[models."standard/1024-x-1024/dall-e-3"]
input_cost_per_pixel = 3.81469e-8
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0

[models."standard/1024-x-1792/dall-e-3"]
input_cost_per_pixel = 4.359e-8
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0

[models."standard/1792-x-1024/dall-e-3"]
input_cost_per_pixel = 4.359e-8
litellm_provider = "openai"
mode = "image_generation"
output_cost_per_pixel = 0

[models."tavily/search"]
input_cost_per_query = 0.008
litellm_provider = "tavily"
mode = "search"

[models."tavily/search-advanced"]
input_cost_per_query = 0.016
litellm_provider = "tavily"
mode = "search"

[models."text-bison"]
input_cost_per_character = 2.5e-7
litellm_provider = "vertex_ai-text-models"
max_input_tokens = 8192
max_output_tokens = 2048
max_tokens = 2048
mode = "completion"
output_cost_per_character = 5e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."text-bison32k"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-text-models"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
mode = "completion"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."text-bison32k@002"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-text-models"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
mode = "completion"
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."text-bison@001"]
input_cost_per_character = 2.5e-7
litellm_provider = "vertex_ai-text-models"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
mode = "completion"
output_cost_per_character = 5e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."text-bison@002"]
input_cost_per_character = 2.5e-7
litellm_provider = "vertex_ai-text-models"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
mode = "completion"
output_cost_per_character = 5e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."text-completion-codestral/codestral-2405"]
input_cost_per_token = 0
litellm_provider = "text-completion-codestral"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "completion"
output_cost_per_token = 0
source = "https://docs.mistral.ai/capabilities/code_generation/"

[models."text-completion-codestral/codestral-latest"]
input_cost_per_token = 0
litellm_provider = "text-completion-codestral"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "completion"
output_cost_per_token = 0
source = "https://docs.mistral.ai/capabilities/code_generation/"

[models."text-embedding-004"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-embedding-models"
max_input_tokens = 2048
max_tokens = 2048
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 768
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"

[models."text-embedding-005"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-embedding-models"
max_input_tokens = 2048
max_tokens = 2048
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 768
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"

[models."text-embedding-3-large"]
input_cost_per_token = 1.3e-7
input_cost_per_token_batches = 6.5e-8
litellm_provider = "openai"
max_input_tokens = 8191
max_tokens = 8191
mode = "embedding"
output_cost_per_token = 0
output_cost_per_token_batches = 0
output_vector_size = 3072

[models."text-embedding-3-small"]
input_cost_per_token = 2e-8
input_cost_per_token_batches = 1e-8
litellm_provider = "openai"
max_input_tokens = 8191
max_tokens = 8191
mode = "embedding"
output_cost_per_token = 0
output_cost_per_token_batches = 0
output_vector_size = 1536

[models."text-embedding-ada-002"]
input_cost_per_token = 1e-7
litellm_provider = "openai"
max_input_tokens = 8191
max_tokens = 8191
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 1536

[models."text-embedding-ada-002-v2"]
input_cost_per_token = 1e-7
input_cost_per_token_batches = 5e-8
litellm_provider = "openai"
max_input_tokens = 8191
max_tokens = 8191
mode = "embedding"
output_cost_per_token = 0
output_cost_per_token_batches = 0

[models."text-embedding-large-exp-03-07"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-embedding-models"
max_input_tokens = 8192
max_tokens = 8192
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 3072
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"

[models."text-embedding-preview-0409"]
input_cost_per_token = 6.25e-9
input_cost_per_token_batch_requests = 5e-9
litellm_provider = "vertex_ai-embedding-models"
max_input_tokens = 3072
max_tokens = 3072
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 768
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"

[models."text-moderation-007"]
input_cost_per_token = 0
litellm_provider = "openai"
max_input_tokens = 32768
max_output_tokens = 0
max_tokens = 32768
mode = "moderation"
output_cost_per_token = 0

[models."text-moderation-latest"]
input_cost_per_token = 0
litellm_provider = "openai"
max_input_tokens = 32768
max_output_tokens = 0
max_tokens = 32768
mode = "moderation"
output_cost_per_token = 0

[models."text-moderation-stable"]
input_cost_per_token = 0
litellm_provider = "openai"
max_input_tokens = 32768
max_output_tokens = 0
max_tokens = 32768
mode = "moderation"
output_cost_per_token = 0

[models."text-multilingual-embedding-002"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-embedding-models"
max_input_tokens = 2048
max_tokens = 2048
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 768
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"

[models."text-multilingual-embedding-preview-0409"]
input_cost_per_token = 6.25e-9
litellm_provider = "vertex_ai-embedding-models"
max_input_tokens = 3072
max_tokens = 3072
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 768
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."text-unicorn"]
input_cost_per_token = 0.00001
litellm_provider = "vertex_ai-text-models"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
mode = "completion"
output_cost_per_token = 0.000028
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."text-unicorn@001"]
input_cost_per_token = 0.00001
litellm_provider = "vertex_ai-text-models"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
mode = "completion"
output_cost_per_token = 0.000028
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."textembedding-gecko"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-embedding-models"
max_input_tokens = 3072
max_tokens = 3072
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 768
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."textembedding-gecko-multilingual"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-embedding-models"
max_input_tokens = 3072
max_tokens = 3072
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 768
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."textembedding-gecko-multilingual@001"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-embedding-models"
max_input_tokens = 3072
max_tokens = 3072
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 768
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."textembedding-gecko@001"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-embedding-models"
max_input_tokens = 3072
max_tokens = 3072
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 768
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."textembedding-gecko@003"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
litellm_provider = "vertex_ai-embedding-models"
max_input_tokens = 3072
max_tokens = 3072
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 768
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."together-ai-21.1b-41b"]
input_cost_per_token = 8e-7
litellm_provider = "together_ai"
mode = "chat"
output_cost_per_token = 8e-7

[models."together-ai-4.1b-8b"]
input_cost_per_token = 2e-7
litellm_provider = "together_ai"
mode = "chat"
output_cost_per_token = 2e-7

[models."together-ai-41.1b-80b"]
input_cost_per_token = 9e-7
litellm_provider = "together_ai"
mode = "chat"
output_cost_per_token = 9e-7

[models."together-ai-8.1b-21b"]
input_cost_per_token = 3e-7
litellm_provider = "together_ai"
max_tokens = 1000
mode = "chat"
output_cost_per_token = 3e-7

[models."together-ai-81.1b-110b"]
input_cost_per_token = 0.0000018
litellm_provider = "together_ai"
mode = "chat"
output_cost_per_token = 0.0000018

[models."together-ai-embedding-151m-to-350m"]
input_cost_per_token = 1.6e-8
litellm_provider = "together_ai"
mode = "embedding"
output_cost_per_token = 0

[models."together-ai-embedding-up-to-150m"]
input_cost_per_token = 8e-9
litellm_provider = "together_ai"
mode = "embedding"
output_cost_per_token = 0

[models."together-ai-up-to-4b"]
input_cost_per_token = 1e-7
litellm_provider = "together_ai"
mode = "chat"
output_cost_per_token = 1e-7

[models."together_ai/BAAI/bge-base-en-v1.5"]
input_cost_per_token = 8e-9
litellm_provider = "together_ai"
max_input_tokens = 512
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 768

[models."together_ai/Qwen/Qwen2.5-72B-Instruct-Turbo"]
litellm_provider = "together_ai"
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/Qwen/Qwen2.5-7B-Instruct-Turbo"]
litellm_provider = "together_ai"
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/Qwen/Qwen3-235B-A22B-Instruct-2507-tput"]
input_cost_per_token = 2e-7
litellm_provider = "together_ai"
max_input_tokens = 262000
mode = "chat"
output_cost_per_token = 0.000006
source = "https://www.together.ai/models/qwen3-235b-a22b-instruct-2507-fp8"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/Qwen/Qwen3-235B-A22B-Thinking-2507"]
input_cost_per_token = 6.5e-7
litellm_provider = "together_ai"
max_input_tokens = 256000
mode = "chat"
output_cost_per_token = 0.000003
source = "https://www.together.ai/models/qwen3-235b-a22b-thinking-2507"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/Qwen/Qwen3-235B-A22B-fp8-tput"]
input_cost_per_token = 2e-7
litellm_provider = "together_ai"
max_input_tokens = 40000
mode = "chat"
output_cost_per_token = 6e-7
source = "https://www.together.ai/models/qwen3-235b-a22b-fp8-tput"
supports_function_calling = false
supports_parallel_function_calling = false
supports_tool_choice = false

[models."together_ai/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"]
input_cost_per_token = 0.000002
litellm_provider = "together_ai"
max_input_tokens = 256000
mode = "chat"
output_cost_per_token = 0.000002
source = "https://www.together.ai/models/qwen3-coder-480b-a35b-instruct"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/Qwen/Qwen3-Next-80B-A3B-Instruct"]
input_cost_per_token = 1.5e-7
litellm_provider = "together_ai"
max_input_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000015
source = "https://www.together.ai/models/qwen3-next-80b-a3b-instruct"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/Qwen/Qwen3-Next-80B-A3B-Thinking"]
input_cost_per_token = 1.5e-7
litellm_provider = "together_ai"
max_input_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000015
source = "https://www.together.ai/models/qwen3-next-80b-a3b-thinking"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/baai/bge-base-en-v1.5"]
input_cost_per_token = 8e-9
litellm_provider = "together_ai"
max_input_tokens = 512
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 768

[models."together_ai/deepseek-ai/DeepSeek-R1"]
input_cost_per_token = 0.000003
litellm_provider = "together_ai"
max_input_tokens = 128000
max_output_tokens = 20480
max_tokens = 20480
mode = "chat"
output_cost_per_token = 0.000007
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/deepseek-ai/DeepSeek-R1-0528-tput"]
input_cost_per_token = 5.5e-7
litellm_provider = "together_ai"
max_input_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00000219
source = "https://www.together.ai/models/deepseek-r1-0528-throughput"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/deepseek-ai/DeepSeek-V3"]
input_cost_per_token = 0.00000125
litellm_provider = "together_ai"
max_input_tokens = 65536
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00000125
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/deepseek-ai/DeepSeek-V3.1"]
input_cost_per_token = 6e-7
litellm_provider = "together_ai"
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.0000017
source = "https://www.together.ai/models/deepseek-v3-1"
supports_function_calling = true
supports_parallel_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."together_ai/meta-llama/Llama-3.2-3B-Instruct-Turbo"]
litellm_provider = "together_ai"
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo"]
input_cost_per_token = 8.8e-7
litellm_provider = "together_ai"
mode = "chat"
output_cost_per_token = 8.8e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"]
input_cost_per_token = 0
litellm_provider = "together_ai"
mode = "chat"
output_cost_per_token = 0
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"]
input_cost_per_token = 2.7e-7
litellm_provider = "together_ai"
mode = "chat"
output_cost_per_token = 8.5e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/meta-llama/Llama-4-Scout-17B-16E-Instruct"]
input_cost_per_token = 1.8e-7
litellm_provider = "together_ai"
mode = "chat"
output_cost_per_token = 5.9e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo"]
input_cost_per_token = 0.0000035
litellm_provider = "together_ai"
mode = "chat"
output_cost_per_token = 0.0000035
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"]
input_cost_per_token = 8.8e-7
litellm_provider = "together_ai"
mode = "chat"
output_cost_per_token = 8.8e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"]
input_cost_per_token = 1.8e-7
litellm_provider = "together_ai"
mode = "chat"
output_cost_per_token = 1.8e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/mistralai/Mistral-7B-Instruct-v0.1"]
litellm_provider = "together_ai"
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/mistralai/Mistral-Small-24B-Instruct-2501"]
litellm_provider = "together_ai"
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1"]
input_cost_per_token = 6e-7
litellm_provider = "together_ai"
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/moonshotai/Kimi-K2-Instruct"]
input_cost_per_token = 0.000001
litellm_provider = "together_ai"
mode = "chat"
output_cost_per_token = 0.000003
source = "https://www.together.ai/models/kimi-k2-instruct"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/moonshotai/Kimi-K2-Instruct-0905"]
input_cost_per_token = 0.000001
litellm_provider = "together_ai"
max_input_tokens = 262144
mode = "chat"
output_cost_per_token = 0.000003
source = "https://www.together.ai/models/kimi-k2-0905"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."together_ai/openai/gpt-oss-120b"]
input_cost_per_token = 1.5e-7
litellm_provider = "together_ai"
max_input_tokens = 128000
mode = "chat"
output_cost_per_token = 6e-7
source = "https://www.together.ai/models/gpt-oss-120b"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/openai/gpt-oss-20b"]
input_cost_per_token = 5e-8
litellm_provider = "together_ai"
max_input_tokens = 128000
mode = "chat"
output_cost_per_token = 2e-7
source = "https://www.together.ai/models/gpt-oss-20b"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/togethercomputer/CodeLlama-34b-Instruct"]
litellm_provider = "together_ai"
mode = "chat"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."together_ai/zai-org/GLM-4.5-Air-FP8"]
input_cost_per_token = 2e-7
litellm_provider = "together_ai"
max_input_tokens = 128000
mode = "chat"
output_cost_per_token = 0.0000011
source = "https://www.together.ai/models/glm-4-5-air"
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."together_ai/zai-org/GLM-4.6"]
input_cost_per_token = 6e-7
litellm_provider = "together_ai"
max_input_tokens = 200000
max_output_tokens = 200000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.0000022
source = "https://www.together.ai/models/glm-4-6"
supports_function_calling = true
supports_parallel_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."tts-1"]
input_cost_per_character = 0.000015
litellm_provider = "openai"
mode = "audio_speech"
supported_endpoints = ["/v1/audio/speech"]

[models."tts-1-hd"]
input_cost_per_character = 0.00003
litellm_provider = "openai"
mode = "audio_speech"
supported_endpoints = ["/v1/audio/speech"]

[models."twelvelabs.marengo-embed-2-7-v1:0"]
input_cost_per_token = 0.00007
litellm_provider = "bedrock"
max_input_tokens = 77
max_tokens = 77
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 1024
supports_embedding_image_input = true
supports_image_input = true

[models."twelvelabs.pegasus-1-2-v1:0"]
input_cost_per_video_per_second = 0.00049
litellm_provider = "bedrock"
mode = "chat"
output_cost_per_token = 0.0000075
supports_video_input = true

[models."us.amazon.nova-2-lite-v1:0"]
cache_read_input_token_cost = 8.25e-8
input_cost_per_token = 3.3e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.00000275
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_video_input = true
supports_vision = true

[models."us.amazon.nova-lite-v1:0"]
input_cost_per_token = 6e-8
litellm_provider = "bedrock_converse"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 2.4e-7
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_vision = true

[models."us.amazon.nova-micro-v1:0"]
input_cost_per_token = 3.5e-8
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 1.4e-7
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true

[models."us.amazon.nova-premier-v1:0"]
input_cost_per_token = 0.0000025
litellm_provider = "bedrock_converse"
max_input_tokens = 1000000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 0.0000125
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = false
supports_response_schema = true
supports_vision = true

[models."us.amazon.nova-pro-v1:0"]
input_cost_per_token = 8e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
mode = "chat"
output_cost_per_token = 0.0000032
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_vision = true

[models."us.anthropic.claude-3-5-haiku-20241022-v1:0"]
cache_creation_input_token_cost = 0.000001
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8e-7
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000004
supports_assistant_prefill = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true

[models."us.anthropic.claude-3-5-sonnet-20240620-v1:0"]
input_cost_per_token = 0.000003
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."us.anthropic.claude-3-5-sonnet-20241022-v2:0"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."us.anthropic.claude-3-7-sonnet-20250219-v1:0"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."us.anthropic.claude-3-haiku-20240307-v1:0"]
input_cost_per_token = 2.5e-7
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00000125
supports_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."us.anthropic.claude-3-opus-20240229-v1:0"]
input_cost_per_token = 0.000015
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000075
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."us.anthropic.claude-3-sonnet-20240229-v1:0"]
input_cost_per_token = 0.000003
litellm_provider = "bedrock"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."us.anthropic.claude-haiku-4-5-20251001-v1:0"]
cache_creation_input_token_cost = 0.000001375
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.0000055
source = "https://aws.amazon.com/about-aws/whats-new/2025/10/claude-4-5-haiku-anthropic-amazon-bedrock"
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 346

[models."us.anthropic.claude-opus-4-1-20250805-v1:0"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."us.anthropic.claude-opus-4-1-20250805-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."us.anthropic.claude-opus-4-20250514-v1:0"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."us.anthropic.claude-opus-4-20250514-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."us.anthropic.claude-opus-4-5-20251101-v1:0"]
cache_creation_input_token_cost = 0.00000625
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000025
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."us.anthropic.claude-opus-4-5-20251101-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."us.anthropic.claude-sonnet-4-20250514-v1:0"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "bedrock_converse"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."us.anthropic.claude-sonnet-4-20250514-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."us.anthropic.claude-sonnet-4-5-20250929-v1:0"]
cache_creation_input_token_cost = 0.000004125
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost = 3.3e-7
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token = 0.0000033
input_cost_per_token_above_200k_tokens = 0.0000066
litellm_provider = "bedrock_converse"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.0000165
output_cost_per_token_above_200k_tokens = 0.00002475
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 346

[models."us.anthropic.claude-sonnet-4-5-20250929-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."us.deepseek.r1-v1:0"]
input_cost_per_token = 0.00000135
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.0000054
supports_function_calling = false
supports_reasoning = true
supports_tool_choice = false

[models."us.meta.llama3-1-405b-instruct-v1:0"]
input_cost_per_token = 0.00000532
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000016
supports_function_calling = true
supports_tool_choice = false

[models."us.meta.llama3-1-70b-instruct-v1:0"]
input_cost_per_token = 9.9e-7
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 128000
mode = "chat"
output_cost_per_token = 9.9e-7
supports_function_calling = true
supports_tool_choice = false

[models."us.meta.llama3-1-8b-instruct-v1:0"]
input_cost_per_token = 2.2e-7
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 128000
mode = "chat"
output_cost_per_token = 2.2e-7
supports_function_calling = true
supports_tool_choice = false

[models."us.meta.llama3-2-11b-instruct-v1:0"]
input_cost_per_token = 3.5e-7
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 3.5e-7
supports_function_calling = true
supports_tool_choice = false
supports_vision = true

[models."us.meta.llama3-2-1b-instruct-v1:0"]
input_cost_per_token = 1e-7
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1e-7
supports_function_calling = true
supports_tool_choice = false

[models."us.meta.llama3-2-3b-instruct-v1:0"]
input_cost_per_token = 1.5e-7
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1.5e-7
supports_function_calling = true
supports_tool_choice = false

[models."us.meta.llama3-2-90b-instruct-v1:0"]
input_cost_per_token = 0.000002
litellm_provider = "bedrock"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000002
supports_function_calling = true
supports_tool_choice = false
supports_vision = true

[models."us.meta.llama3-3-70b-instruct-v1:0"]
input_cost_per_token = 7.2e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 7.2e-7
supports_function_calling = true
supports_tool_choice = false

[models."us.meta.llama4-maverick-17b-instruct-v1:0"]
input_cost_per_token = 2.4e-7
input_cost_per_token_batches = 1.2e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 9.7e-7
output_cost_per_token_batches = 4.85e-7
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "code"]
supports_function_calling = true
supports_tool_choice = false

[models."us.meta.llama4-scout-17b-instruct-v1:0"]
input_cost_per_token = 1.7e-7
input_cost_per_token_batches = 8.5e-8
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 6.6e-7
output_cost_per_token_batches = 3.3e-7
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "code"]
supports_function_calling = true
supports_tool_choice = false

[models."us.mistral.pixtral-large-2502-v1:0"]
input_cost_per_token = 0.000002
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000006
supports_function_calling = true
supports_tool_choice = false

[models."us.twelvelabs.marengo-embed-2-7-v1:0"]
input_cost_per_audio_per_second = 0.00014
input_cost_per_image = 0.0001
input_cost_per_token = 0.00007
input_cost_per_video_per_second = 0.0007
litellm_provider = "bedrock"
max_input_tokens = 77
max_tokens = 77
mode = "embedding"
output_cost_per_token = 0
output_vector_size = 1024
supports_embedding_image_input = true
supports_image_input = true

[models."us.twelvelabs.pegasus-1-2-v1:0"]
input_cost_per_video_per_second = 0.00049
litellm_provider = "bedrock"
mode = "chat"
output_cost_per_token = 0.0000075
supports_video_input = true

[models."us.writer.palmyra-x4-v1:0"]
input_cost_per_token = 0.0000025
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_pdf_input = true

[models."us.writer.palmyra-x5-v1:0"]
input_cost_per_token = 6e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000006
supports_function_calling = true
supports_pdf_input = true

[models."v0/v0-1.0-md"]
input_cost_per_token = 0.000003
litellm_provider = "v0"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."v0/v0-1.5-lg"]
input_cost_per_token = 0.000015
litellm_provider = "v0"
max_input_tokens = 512000
max_output_tokens = 512000
max_tokens = 512000
mode = "chat"
output_cost_per_token = 0.000075
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."v0/v0-1.5-md"]
input_cost_per_token = 0.000003
litellm_provider = "v0"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."vercel_ai_gateway/alibaba/qwen-3-14b"]
input_cost_per_token = 8e-8
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 40960
max_output_tokens = 16384
max_tokens = 40960
mode = "chat"
output_cost_per_token = 2.4e-7

[models."vercel_ai_gateway/alibaba/qwen-3-235b"]
input_cost_per_token = 2e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 40960
max_output_tokens = 16384
max_tokens = 40960
mode = "chat"
output_cost_per_token = 6e-7

[models."vercel_ai_gateway/alibaba/qwen-3-30b"]
input_cost_per_token = 1e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 40960
max_output_tokens = 16384
max_tokens = 40960
mode = "chat"
output_cost_per_token = 3e-7

[models."vercel_ai_gateway/alibaba/qwen-3-32b"]
input_cost_per_token = 1e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 40960
max_output_tokens = 16384
max_tokens = 40960
mode = "chat"
output_cost_per_token = 3e-7

[models."vercel_ai_gateway/alibaba/qwen3-coder"]
input_cost_per_token = 4e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 262144
max_output_tokens = 66536
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000016

[models."vercel_ai_gateway/amazon/nova-lite"]
input_cost_per_token = 6e-8
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 300000
max_output_tokens = 8192
max_tokens = 300000
mode = "chat"
output_cost_per_token = 2.4e-7

[models."vercel_ai_gateway/amazon/nova-micro"]
input_cost_per_token = 3.5e-8
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1.4e-7

[models."vercel_ai_gateway/amazon/nova-pro"]
input_cost_per_token = 8e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 300000
max_output_tokens = 8192
max_tokens = 300000
mode = "chat"
output_cost_per_token = 0.0000032

[models."vercel_ai_gateway/amazon/titan-embed-text-v2"]
input_cost_per_token = 2e-8
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 0
max_output_tokens = 0
max_tokens = 0
mode = "chat"
output_cost_per_token = 0

[models."vercel_ai_gateway/anthropic/claude-3-haiku"]
cache_creation_input_token_cost = 3e-7
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.00000125

[models."vercel_ai_gateway/anthropic/claude-3-opus"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.000075

[models."vercel_ai_gateway/anthropic/claude-3.5-haiku"]
cache_creation_input_token_cost = 0.000001
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.000004

[models."vercel_ai_gateway/anthropic/claude-3.5-sonnet"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.000015

[models."vercel_ai_gateway/anthropic/claude-3.7-sonnet"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.000015

[models."vercel_ai_gateway/anthropic/claude-4-opus"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.000075

[models."vercel_ai_gateway/anthropic/claude-4-sonnet"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.000015

[models."vercel_ai_gateway/cohere/command-a"]
input_cost_per_token = 0.0000025
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 256000
max_output_tokens = 8000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.00001

[models."vercel_ai_gateway/cohere/command-r"]
input_cost_per_token = 1.5e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 6e-7

[models."vercel_ai_gateway/cohere/command-r-plus"]
input_cost_per_token = 0.0000025
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001

[models."vercel_ai_gateway/cohere/embed-v4.0"]
input_cost_per_token = 1.2e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 0
max_output_tokens = 0
max_tokens = 0
mode = "chat"
output_cost_per_token = 0

[models."vercel_ai_gateway/deepseek/deepseek-r1"]
input_cost_per_token = 5.5e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00000219

[models."vercel_ai_gateway/deepseek/deepseek-r1-distill-llama-70b"]
input_cost_per_token = 7.5e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 9.9e-7

[models."vercel_ai_gateway/deepseek/deepseek-v3"]
input_cost_per_token = 9e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"
output_cost_per_token = 9e-7

[models."vercel_ai_gateway/google/gemini-2.0-flash"]
input_cost_per_token = 1.5e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 1048576
mode = "chat"
output_cost_per_token = 6e-7

[models."vercel_ai_gateway/google/gemini-2.0-flash-lite"]
input_cost_per_token = 7.5e-8
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 1048576
mode = "chat"
output_cost_per_token = 3e-7

[models."vercel_ai_gateway/google/gemini-2.5-flash"]
input_cost_per_token = 3e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 1000000
max_output_tokens = 65536
max_tokens = 1000000
mode = "chat"
output_cost_per_token = 0.0000025

[models."vercel_ai_gateway/google/gemini-2.5-pro"]
input_cost_per_token = 0.0000025
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 1048576
max_output_tokens = 65536
max_tokens = 1048576
mode = "chat"
output_cost_per_token = 0.00001

[models."vercel_ai_gateway/google/gemini-embedding-001"]
input_cost_per_token = 1.5e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 0
max_output_tokens = 0
max_tokens = 0
mode = "embedding"
output_cost_per_token = 0

[models."vercel_ai_gateway/google/gemma-2-9b"]
input_cost_per_token = 2e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7

[models."vercel_ai_gateway/google/text-embedding-005"]
input_cost_per_token = 2.5e-8
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 0
max_output_tokens = 0
max_tokens = 0
mode = "embedding"
output_cost_per_token = 0

[models."vercel_ai_gateway/google/text-multilingual-embedding-002"]
input_cost_per_token = 2.5e-8
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 0
max_output_tokens = 0
max_tokens = 0
mode = "embedding"
output_cost_per_token = 0

[models."vercel_ai_gateway/inception/mercury-coder-small"]
input_cost_per_token = 2.5e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 32000
max_output_tokens = 16384
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000001

[models."vercel_ai_gateway/meta/llama-3-70b"]
input_cost_per_token = 5.9e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 7.9e-7

[models."vercel_ai_gateway/meta/llama-3-8b"]
input_cost_per_token = 5e-8
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 8e-8

[models."vercel_ai_gateway/meta/llama-3.1-70b"]
input_cost_per_token = 7.2e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"
output_cost_per_token = 7.2e-7

[models."vercel_ai_gateway/meta/llama-3.1-8b"]
input_cost_per_token = 5e-8
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 131000
max_output_tokens = 131072
max_tokens = 131000
mode = "chat"
output_cost_per_token = 8e-8

[models."vercel_ai_gateway/meta/llama-3.2-11b"]
input_cost_per_token = 1.6e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1.6e-7

[models."vercel_ai_gateway/meta/llama-3.2-1b"]
input_cost_per_token = 1e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1e-7

[models."vercel_ai_gateway/meta/llama-3.2-3b"]
input_cost_per_token = 1.5e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1.5e-7

[models."vercel_ai_gateway/meta/llama-3.2-90b"]
input_cost_per_token = 7.2e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"
output_cost_per_token = 7.2e-7

[models."vercel_ai_gateway/meta/llama-3.3-70b"]
input_cost_per_token = 7.2e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 128000
mode = "chat"
output_cost_per_token = 7.2e-7

[models."vercel_ai_gateway/meta/llama-4-maverick"]
input_cost_per_token = 2e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 131072
max_output_tokens = 8192
max_tokens = 131072
mode = "chat"
output_cost_per_token = 6e-7

[models."vercel_ai_gateway/meta/llama-4-scout"]
input_cost_per_token = 1e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 131072
max_output_tokens = 8192
max_tokens = 131072
mode = "chat"
output_cost_per_token = 3e-7

[models."vercel_ai_gateway/mistral/codestral"]
input_cost_per_token = 3e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 256000
max_output_tokens = 4000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 9e-7

[models."vercel_ai_gateway/mistral/codestral-embed"]
input_cost_per_token = 1.5e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 0
max_output_tokens = 0
max_tokens = 0
mode = "chat"
output_cost_per_token = 0

[models."vercel_ai_gateway/mistral/devstral-small"]
input_cost_per_token = 7e-8
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 2.8e-7

[models."vercel_ai_gateway/mistral/magistral-medium"]
input_cost_per_token = 0.000002
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 64000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000005

[models."vercel_ai_gateway/mistral/magistral-small"]
input_cost_per_token = 5e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 64000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.0000015

[models."vercel_ai_gateway/mistral/ministral-3b"]
input_cost_per_token = 4e-8
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 4e-8

[models."vercel_ai_gateway/mistral/ministral-8b"]
input_cost_per_token = 1e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1e-7

[models."vercel_ai_gateway/mistral/mistral-embed"]
input_cost_per_token = 1e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 0
max_output_tokens = 0
max_tokens = 0
mode = "chat"
output_cost_per_token = 0

[models."vercel_ai_gateway/mistral/mistral-large"]
input_cost_per_token = 0.000002
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 32000
max_output_tokens = 4000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000006

[models."vercel_ai_gateway/mistral/mistral-saba-24b"]
input_cost_per_token = 7.9e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 7.9e-7

[models."vercel_ai_gateway/mistral/mistral-small"]
input_cost_per_token = 1e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 32000
max_output_tokens = 4000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 3e-7

[models."vercel_ai_gateway/mistral/mixtral-8x22b-instruct"]
input_cost_per_token = 0.0000012
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 65536
max_output_tokens = 2048
max_tokens = 65536
mode = "chat"
output_cost_per_token = 0.0000012

[models."vercel_ai_gateway/mistral/pixtral-12b"]
input_cost_per_token = 1.5e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1.5e-7

[models."vercel_ai_gateway/mistral/pixtral-large"]
input_cost_per_token = 0.000002
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000006

[models."vercel_ai_gateway/moonshotai/kimi-k2"]
input_cost_per_token = 5.5e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 131072
max_output_tokens = 16384
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000022

[models."vercel_ai_gateway/morph/morph-v3-fast"]
input_cost_per_token = 8e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 32768
max_output_tokens = 16384
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000012

[models."vercel_ai_gateway/morph/morph-v3-large"]
input_cost_per_token = 9e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 32768
max_output_tokens = 16384
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.0000019

[models."vercel_ai_gateway/openai/gpt-3.5-turbo"]
input_cost_per_token = 5e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 16385
mode = "chat"
output_cost_per_token = 0.0000015

[models."vercel_ai_gateway/openai/gpt-3.5-turbo-instruct"]
input_cost_per_token = 0.0000015
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000002

[models."vercel_ai_gateway/openai/gpt-4-turbo"]
input_cost_per_token = 0.00001
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00003

[models."vercel_ai_gateway/openai/gpt-4.1"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 1047576
mode = "chat"
output_cost_per_token = 0.000008

[models."vercel_ai_gateway/openai/gpt-4.1-mini"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 1e-7
input_cost_per_token = 4e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 1047576
mode = "chat"
output_cost_per_token = 0.0000016

[models."vercel_ai_gateway/openai/gpt-4.1-nano"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 1e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 1047576
mode = "chat"
output_cost_per_token = 4e-7

[models."vercel_ai_gateway/openai/gpt-4o"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001

[models."vercel_ai_gateway/openai/gpt-4o-mini"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.5e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 128000
mode = "chat"
output_cost_per_token = 6e-7

[models."vercel_ai_gateway/openai/o1"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.00006

[models."vercel_ai_gateway/openai/o3"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.000008

[models."vercel_ai_gateway/openai/o3-mini"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.0000044

[models."vercel_ai_gateway/openai/o4-mini"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 2.75e-7
input_cost_per_token = 0.0000011
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.0000044

[models."vercel_ai_gateway/openai/text-embedding-3-large"]
input_cost_per_token = 1.3e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 0
max_output_tokens = 0
max_tokens = 0
mode = "embedding"
output_cost_per_token = 0

[models."vercel_ai_gateway/openai/text-embedding-3-small"]
input_cost_per_token = 2e-8
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 0
max_output_tokens = 0
max_tokens = 0
mode = "embedding"
output_cost_per_token = 0

[models."vercel_ai_gateway/openai/text-embedding-ada-002"]
input_cost_per_token = 1e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 0
max_output_tokens = 0
max_tokens = 0
mode = "embedding"
output_cost_per_token = 0

[models."vercel_ai_gateway/perplexity/sonar"]
input_cost_per_token = 0.000001
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 127000
max_output_tokens = 8000
max_tokens = 127000
mode = "chat"
output_cost_per_token = 0.000001

[models."vercel_ai_gateway/perplexity/sonar-pro"]
input_cost_per_token = 0.000003
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 200000
max_output_tokens = 8000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.000015

[models."vercel_ai_gateway/perplexity/sonar-reasoning"]
input_cost_per_token = 0.000001
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 127000
max_output_tokens = 8000
max_tokens = 127000
mode = "chat"
output_cost_per_token = 0.000005

[models."vercel_ai_gateway/perplexity/sonar-reasoning-pro"]
input_cost_per_token = 0.000002
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 127000
max_output_tokens = 8000
max_tokens = 127000
mode = "chat"
output_cost_per_token = 0.000008

[models."vercel_ai_gateway/vercel/v0-1.0-md"]
input_cost_per_token = 0.000003
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 32000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000015

[models."vercel_ai_gateway/vercel/v0-1.5-md"]
input_cost_per_token = 0.000003
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 32768
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000015

[models."vercel_ai_gateway/xai/grok-2"]
input_cost_per_token = 0.000002
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 131072
max_output_tokens = 4000
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.00001

[models."vercel_ai_gateway/xai/grok-2-vision"]
input_cost_per_token = 0.000002
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.00001

[models."vercel_ai_gateway/xai/grok-3"]
input_cost_per_token = 0.000003
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000015

[models."vercel_ai_gateway/xai/grok-3-fast"]
input_cost_per_token = 0.000005
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000025

[models."vercel_ai_gateway/xai/grok-3-mini"]
input_cost_per_token = 3e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 5e-7

[models."vercel_ai_gateway/xai/grok-3-mini-fast"]
input_cost_per_token = 6e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000004

[models."vercel_ai_gateway/xai/grok-4"]
input_cost_per_token = 0.000003
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.000015

[models."vercel_ai_gateway/zai/glm-4.5"]
input_cost_per_token = 6e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.0000022

[models."vercel_ai_gateway/zai/glm-4.5-air"]
input_cost_per_token = 2e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 128000
max_output_tokens = 96000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.0000011

[models."vercel_ai_gateway/zai/glm-4.6"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 4.5e-7
litellm_provider = "vercel_ai_gateway"
max_input_tokens = 200000
max_output_tokens = 200000
max_tokens = 200000
mode = "chat"
output_cost_per_token = 0.0000018
source = "https://vercel.com/ai-gateway/models/glm-4.6"
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."vertex_ai/chirp"]
input_cost_per_character = 0.00003
litellm_provider = "vertex_ai"
mode = "audio_speech"
source = "https://cloud.google.com/text-to-speech/pricing"
supported_endpoints = ["/v1/audio/speech"]

[models."vertex_ai/claude-3-5-haiku"]
input_cost_per_token = 0.000001
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000005
supports_assistant_prefill = true
supports_function_calling = true
supports_pdf_input = true
supports_tool_choice = true

[models."vertex_ai/claude-3-5-haiku@20241022"]
input_cost_per_token = 0.000001
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000005
supports_assistant_prefill = true
supports_function_calling = true
supports_pdf_input = true
supports_tool_choice = true

[models."vertex_ai/claude-3-5-sonnet"]
input_cost_per_token = 0.000003
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/claude-3-5-sonnet-v2"]
input_cost_per_token = 0.000003
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/claude-3-5-sonnet-v2@20241022"]
input_cost_per_token = 0.000003
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/claude-3-5-sonnet@20240620"]
input_cost_per_token = 0.000003
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_function_calling = true
supports_pdf_input = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/claude-3-7-sonnet@20250219"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
deprecation_date = "2025-06-01"
input_cost_per_token = 0.000003
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."vertex_ai/claude-3-haiku"]
input_cost_per_token = 2.5e-7
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00000125
supports_assistant_prefill = true
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/claude-3-haiku@20240307"]
input_cost_per_token = 2.5e-7
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.00000125
supports_assistant_prefill = true
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/claude-3-opus"]
input_cost_per_token = 0.000015
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/claude-3-opus@20240229"]
input_cost_per_token = 0.000015
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/claude-3-sonnet"]
input_cost_per_token = 0.000003
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/claude-3-sonnet@20240229"]
input_cost_per_token = 0.000003
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
mode = "chat"
output_cost_per_token = 0.000015
supports_assistant_prefill = true
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/claude-haiku-4-5@20251001"]
cache_creation_input_token_cost = 0.00000125
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000005
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/haiku-4-5"
supports_assistant_prefill = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true

[models."vertex_ai/claude-opus-4"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."vertex_ai/claude-opus-4".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."vertex_ai/claude-opus-4-1"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
input_cost_per_token_batches = 0.0000075
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
output_cost_per_token_batches = 0.0000375
supports_assistant_prefill = true
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/claude-opus-4-1@20250805"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
input_cost_per_token_batches = 0.0000075
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
output_cost_per_token_batches = 0.0000375
supports_assistant_prefill = true
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/claude-opus-4-5"]
cache_creation_input_token_cost = 0.00000625
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000025
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."vertex_ai/claude-opus-4-5".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."vertex_ai/claude-opus-4-5@20251101"]
cache_creation_input_token_cost = 0.00000625
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000025
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."vertex_ai/claude-opus-4-5@20251101".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."vertex_ai/claude-opus-4@20250514"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0.000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."vertex_ai/claude-opus-4@20250514".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."vertex_ai/claude-sonnet-4"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."vertex_ai/claude-sonnet-4".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."vertex_ai/claude-sonnet-4-5"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
input_cost_per_token_batches = 0.0000015
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
output_cost_per_token_batches = 0.0000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/claude-sonnet-4-5@20250929"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
input_cost_per_token_batches = 0.0000015
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
output_cost_per_token_batches = 0.0000075
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/claude-sonnet-4@20250514"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
litellm_provider = "vertex_ai-anthropic_models"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
tool_use_system_prompt_tokens = 159

[models."vertex_ai/claude-sonnet-4@20250514".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."vertex_ai/codestral-2"]
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 9e-7
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/codestral-2501"]
input_cost_per_token = 2e-7
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/codestral-2@001"]
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 9e-7
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/codestral@2405"]
input_cost_per_token = 2e-7
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/codestral@latest"]
input_cost_per_token = 2e-7
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/deepseek-ai/deepseek-ocr-maas"]
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai"
mode = "ocr"
ocr_cost_per_page = 0.0003
output_cost_per_token = 0.0000012
source = "https://cloud.google.com/vertex-ai/pricing"

[models."vertex_ai/deepseek-ai/deepseek-r1-0528-maas"]
input_cost_per_token = 0.00000135
litellm_provider = "vertex_ai-deepseek_models"
max_input_tokens = 65336
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000054
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true

[models."vertex_ai/deepseek-ai/deepseek-v3.1-maas"]
input_cost_per_token = 0.00000135
litellm_provider = "vertex_ai-deepseek_models"
max_input_tokens = 163840
max_output_tokens = 32768
max_tokens = 163840
mode = "chat"
output_cost_per_token = 0.0000054
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supported_regions = ["us-west2"]
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true

[models."vertex_ai/deepseek-ai/deepseek-v3.2-maas"]
input_cost_per_token = 5.6e-7
input_cost_per_token_batches = 2.8e-7
litellm_provider = "vertex_ai-deepseek_models"
max_input_tokens = 163840
max_output_tokens = 32768
max_tokens = 163840
mode = "chat"
output_cost_per_token = 0.00000168
output_cost_per_token_batches = 8.4e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supported_regions = ["us-west2"]
supports_assistant_prefill = true
supports_function_calling = true
supports_prompt_caching = true
supports_reasoning = true
supports_tool_choice = true

[models."vertex_ai/gemini-2.5-flash-image"]
cache_read_input_token_cost = 3e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-language-models"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 32768
max_output_tokens = 32768
max_pdf_size_mb = 30
max_tokens = 32768
max_video_length = 1
max_videos_per_prompt = 10
mode = "image_generation"
output_cost_per_image = 0.039
output_cost_per_image_token = 0.00003
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
rpm = 100000
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/image-generation#edit-an-image"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_vision = true
supports_web_search = false
tpm = 8000000

[models."vertex_ai/gemini-3-flash-preview"]
cache_read_input_token_cost = 5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 5e-7
litellm_provider = "vertex_ai"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.000003
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true

[models."vertex_ai/gemini-3-pro-image-preview"]
input_cost_per_image = 0.0011
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
litellm_provider = "vertex_ai-language-models"
max_input_tokens = 65536
max_output_tokens = 32768
max_tokens = 65536
mode = "image_generation"
output_cost_per_image = 0.134
output_cost_per_image_token = 0.00012
output_cost_per_token = 0.000012
output_cost_per_token_batches = 0.000006
source = "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-pro-image"

[models."vertex_ai/gemini-3-pro-preview"]
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost = 2e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
input_cost_per_token = 0.000002
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_batches = 0.000001
litellm_provider = "vertex_ai"
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_input_tokens = 1048576
max_output_tokens = 65535
max_pdf_size_mb = 30
max_tokens = 65535
max_video_length = 1
max_videos_per_prompt = 10
mode = "chat"
output_cost_per_token = 0.000012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_batches = 0.000006
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_function_calling = true
supports_pdf_input = true
supports_prompt_caching = true
supports_reasoning = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_vision = true
supports_web_search = true

[models."vertex_ai/imagegeneration@006"]
litellm_provider = "vertex_ai-image-models"
mode = "image_generation"
output_cost_per_image = 0.02
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"

[models."vertex_ai/imagen-3.0-capability-001"]
litellm_provider = "vertex_ai-image-models"
mode = "image_generation"
output_cost_per_image = 0.04
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/image/edit-insert-objects"

[models."vertex_ai/imagen-3.0-fast-generate-001"]
litellm_provider = "vertex_ai-image-models"
mode = "image_generation"
output_cost_per_image = 0.02
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"

[models."vertex_ai/imagen-3.0-generate-001"]
litellm_provider = "vertex_ai-image-models"
mode = "image_generation"
output_cost_per_image = 0.04
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"

[models."vertex_ai/imagen-3.0-generate-002"]
litellm_provider = "vertex_ai-image-models"
mode = "image_generation"
output_cost_per_image = 0.04
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"

[models."vertex_ai/imagen-4.0-fast-generate-001"]
litellm_provider = "vertex_ai-image-models"
mode = "image_generation"
output_cost_per_image = 0.02
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"

[models."vertex_ai/imagen-4.0-generate-001"]
litellm_provider = "vertex_ai-image-models"
mode = "image_generation"
output_cost_per_image = 0.04
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"

[models."vertex_ai/imagen-4.0-ultra-generate-001"]
litellm_provider = "vertex_ai-image-models"
mode = "image_generation"
output_cost_per_image = 0.06
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"

[models."vertex_ai/jamba-1.5"]
input_cost_per_token = 2e-7
litellm_provider = "vertex_ai-ai21_models"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 4e-7
supports_tool_choice = true

[models."vertex_ai/jamba-1.5-large"]
input_cost_per_token = 0.000002
litellm_provider = "vertex_ai-ai21_models"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.000008
supports_tool_choice = true

[models."vertex_ai/jamba-1.5-large@001"]
input_cost_per_token = 0.000002
litellm_provider = "vertex_ai-ai21_models"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.000008
supports_tool_choice = true

[models."vertex_ai/jamba-1.5-mini"]
input_cost_per_token = 2e-7
litellm_provider = "vertex_ai-ai21_models"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 4e-7
supports_tool_choice = true

[models."vertex_ai/jamba-1.5-mini@001"]
input_cost_per_token = 2e-7
litellm_provider = "vertex_ai-ai21_models"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 4e-7
supports_tool_choice = true

[models."vertex_ai/meta/llama-3.1-405b-instruct-maas"]
input_cost_per_token = 0.000005
litellm_provider = "vertex_ai-llama_models"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000016
source = "https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3.2-90b-vision-instruct-maas"
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/meta/llama-3.1-70b-instruct-maas"]
input_cost_per_token = 0
litellm_provider = "vertex_ai-llama_models"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0
source = "https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3.2-90b-vision-instruct-maas"
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/meta/llama-3.1-8b-instruct-maas"]
input_cost_per_token = 0
litellm_provider = "vertex_ai-llama_models"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0
source = "https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3.2-90b-vision-instruct-maas"
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/meta/llama-3.1-8b-instruct-maas".metadata]
notes = "VertexAI states that The Llama 3.1 API service for llama-3.1-70b-instruct-maas and llama-3.1-8b-instruct-maas are in public preview and at no cost."

[models."vertex_ai/meta/llama-3.2-90b-vision-instruct-maas"]
input_cost_per_token = 0
litellm_provider = "vertex_ai-llama_models"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0
source = "https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3.2-90b-vision-instruct-maas"
supports_system_messages = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/meta/llama-3.2-90b-vision-instruct-maas".metadata]
notes = "VertexAI states that The Llama 3.2 API service is at no cost during public preview, and will be priced as per dollar-per-1M-tokens at GA."

[models."vertex_ai/meta/llama-4-maverick-17b-128e-instruct-maas"]
input_cost_per_token = 3.5e-7
litellm_provider = "vertex_ai-llama_models"
max_input_tokens = 1000000
max_output_tokens = 1000000
max_tokens = 1000000
mode = "chat"
output_cost_per_token = 0.00000115
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "code"]
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/meta/llama-4-maverick-17b-16e-instruct-maas"]
input_cost_per_token = 3.5e-7
litellm_provider = "vertex_ai-llama_models"
max_input_tokens = 1000000
max_output_tokens = 1000000
max_tokens = 1000000
mode = "chat"
output_cost_per_token = 0.00000115
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "code"]
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/meta/llama-4-scout-17b-128e-instruct-maas"]
input_cost_per_token = 2.5e-7
litellm_provider = "vertex_ai-llama_models"
max_input_tokens = 10000000
max_output_tokens = 10000000
max_tokens = 10000000
mode = "chat"
output_cost_per_token = 7e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "code"]
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/meta/llama-4-scout-17b-16e-instruct-maas"]
input_cost_per_token = 2.5e-7
litellm_provider = "vertex_ai-llama_models"
max_input_tokens = 10000000
max_output_tokens = 10000000
max_tokens = 10000000
mode = "chat"
output_cost_per_token = 7e-7
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "code"]
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/meta/llama3-405b-instruct-maas"]
input_cost_per_token = 0
litellm_provider = "vertex_ai-llama_models"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_tool_choice = true

[models."vertex_ai/meta/llama3-70b-instruct-maas"]
input_cost_per_token = 0
litellm_provider = "vertex_ai-llama_models"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_tool_choice = true

[models."vertex_ai/meta/llama3-8b-instruct-maas"]
input_cost_per_token = 0
litellm_provider = "vertex_ai-llama_models"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 0
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_tool_choice = true

[models."vertex_ai/minimaxai/minimax-m2-maas"]
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-minimax_models"
max_input_tokens = 196608
max_output_tokens = 196608
max_tokens = 196608
mode = "chat"
output_cost_per_token = 0.0000012
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/mistral-large-2411"]
input_cost_per_token = 0.000002
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000006
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/mistral-large@2407"]
input_cost_per_token = 0.000002
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000006
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/mistral-large@2411-001"]
input_cost_per_token = 0.000002
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000006
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/mistral-large@latest"]
input_cost_per_token = 0.000002
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000006
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/mistral-medium-3"]
input_cost_per_token = 4e-7
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000002
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/mistral-medium-3@001"]
input_cost_per_token = 4e-7
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000002
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/mistral-nemo@2407"]
input_cost_per_token = 0.000003
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000003
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/mistral-nemo@latest"]
input_cost_per_token = 1.5e-7
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1.5e-7
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/mistral-ocr-2505"]
litellm_provider = "vertex_ai"
mode = "ocr"
ocr_cost_per_page = 0.0005
source = "https://cloud.google.com/generative-ai-app-builder/pricing"
supported_endpoints = ["/v1/ocr"]

[models."vertex_ai/mistral-small-2503"]
input_cost_per_token = 0.000001
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000003
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."vertex_ai/mistral-small-2503@001"]
input_cost_per_token = 0.000001
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000003
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/mistralai/codestral-2"]
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 9e-7
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/mistralai/codestral-2@001"]
input_cost_per_token = 3e-7
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 9e-7
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/mistralai/mistral-medium-3"]
input_cost_per_token = 4e-7
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000002
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/mistralai/mistral-medium-3@001"]
input_cost_per_token = 4e-7
litellm_provider = "vertex_ai-mistral_models"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
mode = "chat"
output_cost_per_token = 0.000002
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/moonshotai/kimi-k2-thinking-maas"]
input_cost_per_token = 6e-7
litellm_provider = "vertex_ai-moonshot_models"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.0000025
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_function_calling = true
supports_tool_choice = true
supports_web_search = true

[models."vertex_ai/openai/gpt-oss-120b-maas"]
input_cost_per_token = 1.5e-7
litellm_provider = "vertex_ai-openai_models"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 6e-7
source = "https://console.cloud.google.com/vertex-ai/publishers/openai/model-garden/gpt-oss-120b-maas"
supports_reasoning = true

[models."vertex_ai/openai/gpt-oss-20b-maas"]
input_cost_per_token = 7.5e-8
litellm_provider = "vertex_ai-openai_models"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 3e-7
source = "https://console.cloud.google.com/vertex-ai/publishers/openai/model-garden/gpt-oss-120b-maas"
supports_reasoning = true

[models."vertex_ai/qwen/qwen3-235b-a22b-instruct-2507-maas"]
input_cost_per_token = 2.5e-7
litellm_provider = "vertex_ai-qwen_models"
max_input_tokens = 262144
max_output_tokens = 16384
max_tokens = 16384
mode = "chat"
output_cost_per_token = 0.000001
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/qwen/qwen3-coder-480b-a35b-instruct-maas"]
input_cost_per_token = 0.000001
litellm_provider = "vertex_ai-qwen_models"
max_input_tokens = 262144
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.000004
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/qwen/qwen3-next-80b-a3b-instruct-maas"]
input_cost_per_token = 1.5e-7
litellm_provider = "vertex_ai-qwen_models"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000012
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/qwen/qwen3-next-80b-a3b-thinking-maas"]
input_cost_per_token = 1.5e-7
litellm_provider = "vertex_ai-qwen_models"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.0000012
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."vertex_ai/search_api"]
input_cost_per_query = 0.0015
litellm_provider = "vertex_ai"
mode = "vector_store"

[models."vertex_ai/veo-2.0-generate-001"]
litellm_provider = "vertex_ai-video-models"
max_input_tokens = 1024
max_tokens = 1024
mode = "video_generation"
output_cost_per_second = 0.35
source = "https://ai.google.dev/gemini-api/docs/video"
supported_modalities = ["text"]
supported_output_modalities = ["video"]

[models."vertex_ai/veo-3.0-fast-generate-001"]
litellm_provider = "vertex_ai-video-models"
max_input_tokens = 1024
max_tokens = 1024
mode = "video_generation"
output_cost_per_second = 0.15
source = "https://ai.google.dev/gemini-api/docs/video"
supported_modalities = ["text"]
supported_output_modalities = ["video"]

[models."vertex_ai/veo-3.0-fast-generate-preview"]
litellm_provider = "vertex_ai-video-models"
max_input_tokens = 1024
max_tokens = 1024
mode = "video_generation"
output_cost_per_second = 0.15
source = "https://ai.google.dev/gemini-api/docs/video"
supported_modalities = ["text"]
supported_output_modalities = ["video"]

[models."vertex_ai/veo-3.0-generate-001"]
litellm_provider = "vertex_ai-video-models"
max_input_tokens = 1024
max_tokens = 1024
mode = "video_generation"
output_cost_per_second = 0.4
source = "https://ai.google.dev/gemini-api/docs/video"
supported_modalities = ["text"]
supported_output_modalities = ["video"]

[models."vertex_ai/veo-3.0-generate-preview"]
litellm_provider = "vertex_ai-video-models"
max_input_tokens = 1024
max_tokens = 1024
mode = "video_generation"
output_cost_per_second = 0.4
source = "https://ai.google.dev/gemini-api/docs/video"
supported_modalities = ["text"]
supported_output_modalities = ["video"]

[models."vertex_ai/veo-3.1-fast-generate-001"]
litellm_provider = "vertex_ai-video-models"
max_input_tokens = 1024
max_tokens = 1024
mode = "video_generation"
output_cost_per_second = 0.15
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/veo"
supported_modalities = ["text"]
supported_output_modalities = ["video"]

[models."vertex_ai/veo-3.1-fast-generate-preview"]
litellm_provider = "vertex_ai-video-models"
max_input_tokens = 1024
max_tokens = 1024
mode = "video_generation"
output_cost_per_second = 0.15
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/veo"
supported_modalities = ["text"]
supported_output_modalities = ["video"]

[models."vertex_ai/veo-3.1-generate-001"]
litellm_provider = "vertex_ai-video-models"
max_input_tokens = 1024
max_tokens = 1024
mode = "video_generation"
output_cost_per_second = 0.4
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/veo"
supported_modalities = ["text"]
supported_output_modalities = ["video"]

[models."vertex_ai/veo-3.1-generate-preview"]
litellm_provider = "vertex_ai-video-models"
max_input_tokens = 1024
max_tokens = 1024
mode = "video_generation"
output_cost_per_second = 0.4
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/veo"
supported_modalities = ["text"]
supported_output_modalities = ["video"]

[models."voyage/rerank-2"]
input_cost_per_token = 5e-8
litellm_provider = "voyage"
max_input_tokens = 16000
max_output_tokens = 16000
max_query_tokens = 16000
max_tokens = 16000
mode = "rerank"
output_cost_per_token = 0

[models."voyage/rerank-2-lite"]
input_cost_per_token = 2e-8
litellm_provider = "voyage"
max_input_tokens = 8000
max_output_tokens = 8000
max_query_tokens = 8000
max_tokens = 8000
mode = "rerank"
output_cost_per_token = 0

[models."voyage/rerank-2.5"]
input_cost_per_token = 5e-8
litellm_provider = "voyage"
max_input_tokens = 32000
max_output_tokens = 32000
max_query_tokens = 32000
max_tokens = 32000
mode = "rerank"
output_cost_per_token = 0

[models."voyage/rerank-2.5-lite"]
input_cost_per_token = 2e-8
litellm_provider = "voyage"
max_input_tokens = 32000
max_output_tokens = 32000
max_query_tokens = 32000
max_tokens = 32000
mode = "rerank"
output_cost_per_token = 0

[models."voyage/voyage-2"]
input_cost_per_token = 1e-7
litellm_provider = "voyage"
max_input_tokens = 4000
max_tokens = 4000
mode = "embedding"
output_cost_per_token = 0

[models."voyage/voyage-3"]
input_cost_per_token = 6e-8
litellm_provider = "voyage"
max_input_tokens = 32000
max_tokens = 32000
mode = "embedding"
output_cost_per_token = 0

[models."voyage/voyage-3-large"]
input_cost_per_token = 1.8e-7
litellm_provider = "voyage"
max_input_tokens = 32000
max_tokens = 32000
mode = "embedding"
output_cost_per_token = 0

[models."voyage/voyage-3-lite"]
input_cost_per_token = 2e-8
litellm_provider = "voyage"
max_input_tokens = 32000
max_tokens = 32000
mode = "embedding"
output_cost_per_token = 0

[models."voyage/voyage-3.5"]
input_cost_per_token = 6e-8
litellm_provider = "voyage"
max_input_tokens = 32000
max_tokens = 32000
mode = "embedding"
output_cost_per_token = 0

[models."voyage/voyage-3.5-lite"]
input_cost_per_token = 2e-8
litellm_provider = "voyage"
max_input_tokens = 32000
max_tokens = 32000
mode = "embedding"
output_cost_per_token = 0

[models."voyage/voyage-code-2"]
input_cost_per_token = 1.2e-7
litellm_provider = "voyage"
max_input_tokens = 16000
max_tokens = 16000
mode = "embedding"
output_cost_per_token = 0

[models."voyage/voyage-code-3"]
input_cost_per_token = 1.8e-7
litellm_provider = "voyage"
max_input_tokens = 32000
max_tokens = 32000
mode = "embedding"
output_cost_per_token = 0

[models."voyage/voyage-context-3"]
input_cost_per_token = 1.8e-7
litellm_provider = "voyage"
max_input_tokens = 120000
max_tokens = 120000
mode = "embedding"
output_cost_per_token = 0

[models."voyage/voyage-finance-2"]
input_cost_per_token = 1.2e-7
litellm_provider = "voyage"
max_input_tokens = 32000
max_tokens = 32000
mode = "embedding"
output_cost_per_token = 0

[models."voyage/voyage-large-2"]
input_cost_per_token = 1.2e-7
litellm_provider = "voyage"
max_input_tokens = 16000
max_tokens = 16000
mode = "embedding"
output_cost_per_token = 0

[models."voyage/voyage-law-2"]
input_cost_per_token = 1.2e-7
litellm_provider = "voyage"
max_input_tokens = 16000
max_tokens = 16000
mode = "embedding"
output_cost_per_token = 0

[models."voyage/voyage-lite-01"]
input_cost_per_token = 1e-7
litellm_provider = "voyage"
max_input_tokens = 4096
max_tokens = 4096
mode = "embedding"
output_cost_per_token = 0

[models."voyage/voyage-lite-02-instruct"]
input_cost_per_token = 1e-7
litellm_provider = "voyage"
max_input_tokens = 4000
max_tokens = 4000
mode = "embedding"
output_cost_per_token = 0

[models."voyage/voyage-multimodal-3"]
input_cost_per_token = 1.2e-7
litellm_provider = "voyage"
max_input_tokens = 32000
max_tokens = 32000
mode = "embedding"
output_cost_per_token = 0

[models."wandb/Qwen/Qwen3-235B-A22B-Instruct-2507"]
input_cost_per_token = 0.01
litellm_provider = "wandb"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.01

[models."wandb/Qwen/Qwen3-235B-A22B-Thinking-2507"]
input_cost_per_token = 0.01
litellm_provider = "wandb"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.01

[models."wandb/Qwen/Qwen3-Coder-480B-A35B-Instruct"]
input_cost_per_token = 0.1
litellm_provider = "wandb"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
mode = "chat"
output_cost_per_token = 0.15

[models."wandb/deepseek-ai/DeepSeek-R1-0528"]
input_cost_per_token = 0.135
litellm_provider = "wandb"
max_input_tokens = 161000
max_output_tokens = 161000
max_tokens = 161000
mode = "chat"
output_cost_per_token = 0.54

[models."wandb/deepseek-ai/DeepSeek-V3-0324"]
input_cost_per_token = 0.114
litellm_provider = "wandb"
max_input_tokens = 161000
max_output_tokens = 161000
max_tokens = 161000
mode = "chat"
output_cost_per_token = 0.275

[models."wandb/deepseek-ai/DeepSeek-V3.1"]
input_cost_per_token = 0.055
litellm_provider = "wandb"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.165

[models."wandb/meta-llama/Llama-3.1-8B-Instruct"]
input_cost_per_token = 0.022
litellm_provider = "wandb"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.022

[models."wandb/meta-llama/Llama-3.3-70B-Instruct"]
input_cost_per_token = 0.071
litellm_provider = "wandb"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.071

[models."wandb/meta-llama/Llama-4-Scout-17B-16E-Instruct"]
input_cost_per_token = 0.017
litellm_provider = "wandb"
max_input_tokens = 64000
max_output_tokens = 64000
max_tokens = 64000
mode = "chat"
output_cost_per_token = 0.066

[models."wandb/microsoft/Phi-4-mini-instruct"]
input_cost_per_token = 0.008
litellm_provider = "wandb"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.035

[models."wandb/moonshotai/Kimi-K2-Instruct"]
input_cost_per_token = 6e-7
litellm_provider = "wandb"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.0000025

[models."wandb/openai/gpt-oss-120b"]
input_cost_per_token = 0.015
litellm_provider = "wandb"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.06

[models."wandb/openai/gpt-oss-20b"]
input_cost_per_token = 0.005
litellm_provider = "wandb"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.02

[models."wandb/zai-org/GLM-4.5"]
input_cost_per_token = 0.055
litellm_provider = "wandb"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.2

[models."watsonx/bigscience/mt0-xxl-13b"]
input_cost_per_token = 0.0005
litellm_provider = "watsonx"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.002
supports_function_calling = false
supports_parallel_function_calling = false
supports_vision = false

[models."watsonx/core42/jais-13b-chat"]
input_cost_per_token = 0.0005
litellm_provider = "watsonx"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.002
supports_function_calling = false
supports_parallel_function_calling = false
supports_vision = false

[models."watsonx/google/flan-t5-xl-3b"]
input_cost_per_token = 6e-7
litellm_provider = "watsonx"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = false
supports_parallel_function_calling = false
supports_vision = false

[models."watsonx/ibm/granite-13b-chat-v2"]
input_cost_per_token = 6e-7
litellm_provider = "watsonx"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = false
supports_parallel_function_calling = false
supports_vision = false

[models."watsonx/ibm/granite-13b-instruct-v2"]
input_cost_per_token = 6e-7
litellm_provider = "watsonx"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = false
supports_parallel_function_calling = false
supports_vision = false

[models."watsonx/ibm/granite-3-3-8b-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "watsonx"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = false

[models."watsonx/ibm/granite-3-8b-instruct"]
input_cost_per_token = 2e-7
litellm_provider = "watsonx"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7
supports_audio_input = false
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = false
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = false

[models."watsonx/ibm/granite-4-h-small"]
input_cost_per_token = 6e-8
litellm_provider = "watsonx"
max_input_tokens = 20480
max_output_tokens = 20480
max_tokens = 20480
mode = "chat"
output_cost_per_token = 2.5e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = false

[models."watsonx/ibm/granite-guardian-3-2-2b"]
input_cost_per_token = 1e-7
litellm_provider = "watsonx"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 1e-7
supports_function_calling = false
supports_parallel_function_calling = false
supports_vision = false

[models."watsonx/ibm/granite-guardian-3-3-8b"]
input_cost_per_token = 2e-7
litellm_provider = "watsonx"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 2e-7
supports_function_calling = false
supports_parallel_function_calling = false
supports_vision = false

[models."watsonx/ibm/granite-ttm-1024-96-r2"]
input_cost_per_token = 3.8e-7
litellm_provider = "watsonx"
max_input_tokens = 512
max_output_tokens = 512
max_tokens = 512
mode = "chat"
output_cost_per_token = 3.8e-7
supports_function_calling = false
supports_parallel_function_calling = false
supports_vision = false

[models."watsonx/ibm/granite-ttm-1536-96-r2"]
input_cost_per_token = 3.8e-7
litellm_provider = "watsonx"
max_input_tokens = 512
max_output_tokens = 512
max_tokens = 512
mode = "chat"
output_cost_per_token = 3.8e-7
supports_function_calling = false
supports_parallel_function_calling = false
supports_vision = false

[models."watsonx/ibm/granite-ttm-512-96-r2"]
input_cost_per_token = 3.8e-7
litellm_provider = "watsonx"
max_input_tokens = 512
max_output_tokens = 512
max_tokens = 512
mode = "chat"
output_cost_per_token = 3.8e-7
supports_function_calling = false
supports_parallel_function_calling = false
supports_vision = false

[models."watsonx/ibm/granite-vision-3-2-2b"]
input_cost_per_token = 1e-7
litellm_provider = "watsonx"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 1e-7
supports_function_calling = false
supports_parallel_function_calling = false
supports_vision = true

[models."watsonx/meta-llama/llama-3-2-11b-vision-instruct"]
input_cost_per_token = 3.5e-7
litellm_provider = "watsonx"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 3.5e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = true

[models."watsonx/meta-llama/llama-3-2-1b-instruct"]
input_cost_per_token = 1e-7
litellm_provider = "watsonx"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = false

[models."watsonx/meta-llama/llama-3-2-3b-instruct"]
input_cost_per_token = 1.5e-7
litellm_provider = "watsonx"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 1.5e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = false

[models."watsonx/meta-llama/llama-3-2-90b-vision-instruct"]
input_cost_per_token = 0.000002
litellm_provider = "watsonx"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.000002
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = true

[models."watsonx/meta-llama/llama-3-3-70b-instruct"]
input_cost_per_token = 7.1e-7
litellm_provider = "watsonx"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 7.1e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = false

[models."watsonx/meta-llama/llama-4-maverick-17b"]
input_cost_per_token = 3.5e-7
litellm_provider = "watsonx"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.0000014
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = false

[models."watsonx/meta-llama/llama-guard-3-11b-vision"]
input_cost_per_token = 3.5e-7
litellm_provider = "watsonx"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 3.5e-7
supports_function_calling = false
supports_parallel_function_calling = false
supports_vision = true

[models."watsonx/mistralai/mistral-large"]
input_cost_per_token = 0.000003
litellm_provider = "watsonx"
max_input_tokens = 131072
max_output_tokens = 16384
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.00001
supports_audio_input = false
supports_audio_output = false
supports_function_calling = true
supports_parallel_function_calling = false
supports_prompt_caching = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_vision = false

[models."watsonx/mistralai/mistral-medium-2505"]
input_cost_per_token = 0.000003
litellm_provider = "watsonx"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = false

[models."watsonx/mistralai/mistral-small-2503"]
input_cost_per_token = 1e-7
litellm_provider = "watsonx"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = false

[models."watsonx/mistralai/mistral-small-3-1-24b-instruct-2503"]
input_cost_per_token = 1e-7
litellm_provider = "watsonx"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
mode = "chat"
output_cost_per_token = 3e-7
supports_function_calling = true
supports_parallel_function_calling = true
supports_vision = false

[models."watsonx/mistralai/pixtral-12b-2409"]
input_cost_per_token = 3.5e-7
litellm_provider = "watsonx"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
mode = "chat"
output_cost_per_token = 3.5e-7
supports_function_calling = false
supports_parallel_function_calling = false
supports_vision = true

[models."watsonx/openai/gpt-oss-120b"]
input_cost_per_token = 1.5e-7
litellm_provider = "watsonx"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 6e-7
supports_function_calling = false
supports_parallel_function_calling = false
supports_vision = false

[models."watsonx/sdaia/allam-1-13b-instruct"]
input_cost_per_token = 0.0000018
litellm_provider = "watsonx"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.0000018
supports_function_calling = false
supports_parallel_function_calling = false
supports_vision = false

[models."watsonx/whisper-large-v3-turbo"]
input_cost_per_second = 0.0001
litellm_provider = "watsonx"
mode = "audio_transcription"
output_cost_per_second = 0.0001
supported_endpoints = ["/v1/audio/transcriptions"]

[models."whisper-1"]
input_cost_per_second = 0.0001
litellm_provider = "openai"
mode = "audio_transcription"
output_cost_per_second = 0.0001
supported_endpoints = ["/v1/audio/transcriptions"]

[models."writer.palmyra-x4-v1:0"]
input_cost_per_token = 0.0000025
litellm_provider = "bedrock_converse"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_pdf_input = true

[models."writer.palmyra-x5-v1:0"]
input_cost_per_token = 6e-7
litellm_provider = "bedrock_converse"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000006
supports_function_calling = true
supports_pdf_input = true

[models."xai/grok-2"]
input_cost_per_token = 0.000002
litellm_provider = "xai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-2-1212"]
input_cost_per_token = 0.000002
litellm_provider = "xai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-2-latest"]
input_cost_per_token = 0.000002
litellm_provider = "xai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-2-vision"]
input_cost_per_image = 0.000002
input_cost_per_token = 0.000002
litellm_provider = "xai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."xai/grok-2-vision-1212"]
input_cost_per_image = 0.000002
input_cost_per_token = 0.000002
litellm_provider = "xai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."xai/grok-2-vision-latest"]
input_cost_per_image = 0.000002
input_cost_per_token = 0.000002
litellm_provider = "xai"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
mode = "chat"
output_cost_per_token = 0.00001
supports_function_calling = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."xai/grok-3"]
input_cost_per_token = 0.000003
litellm_provider = "xai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000015
source = "https://x.ai/api#pricing"
supports_function_calling = true
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-3-beta"]
input_cost_per_token = 0.000003
litellm_provider = "xai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000015
source = "https://x.ai/api#pricing"
supports_function_calling = true
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-3-fast-beta"]
input_cost_per_token = 0.000005
litellm_provider = "xai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000025
source = "https://x.ai/api#pricing"
supports_function_calling = true
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-3-fast-latest"]
input_cost_per_token = 0.000005
litellm_provider = "xai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000025
source = "https://x.ai/api#pricing"
supports_function_calling = true
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-3-latest"]
input_cost_per_token = 0.000003
litellm_provider = "xai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000015
source = "https://x.ai/api#pricing"
supports_function_calling = true
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-3-mini"]
input_cost_per_token = 3e-7
litellm_provider = "xai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 5e-7
source = "https://x.ai/api#pricing"
supports_function_calling = true
supports_reasoning = true
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-3-mini-beta"]
input_cost_per_token = 3e-7
litellm_provider = "xai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 5e-7
source = "https://x.ai/api#pricing"
supports_function_calling = true
supports_reasoning = true
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-3-mini-fast"]
input_cost_per_token = 6e-7
litellm_provider = "xai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000004
source = "https://x.ai/api#pricing"
supports_function_calling = true
supports_reasoning = true
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-3-mini-fast-beta"]
input_cost_per_token = 6e-7
litellm_provider = "xai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000004
source = "https://x.ai/api#pricing"
supports_function_calling = true
supports_reasoning = true
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-3-mini-fast-latest"]
input_cost_per_token = 6e-7
litellm_provider = "xai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000004
source = "https://x.ai/api#pricing"
supports_function_calling = true
supports_reasoning = true
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-3-mini-latest"]
input_cost_per_token = 3e-7
litellm_provider = "xai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 5e-7
source = "https://x.ai/api#pricing"
supports_function_calling = true
supports_reasoning = true
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-4"]
input_cost_per_token = 0.000003
litellm_provider = "xai"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.000015
source = "https://docs.x.ai/docs/models"
supports_function_calling = true
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-4-0709"]
input_cost_per_token = 0.000003
input_cost_per_token_above_128k_tokens = 0.000006
litellm_provider = "xai"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_128k_tokens = 0.00003
source = "https://docs.x.ai/docs/models"
supports_function_calling = true
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-4-1-fast"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
input_cost_per_token_above_128k_tokens = 4e-7
litellm_provider = "xai"
max_input_tokens = 2000000
max_output_tokens = 2000000
max_tokens = 2000000
mode = "chat"
output_cost_per_token = 5e-7
output_cost_per_token_above_128k_tokens = 0.000001
source = "https://docs.x.ai/docs/models/grok-4-1-fast-reasoning"
supports_audio_input = true
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."xai/grok-4-1-fast-non-reasoning"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
input_cost_per_token_above_128k_tokens = 4e-7
litellm_provider = "xai"
max_input_tokens = 2000000
max_output_tokens = 2000000
max_tokens = 2000000
mode = "chat"
output_cost_per_token = 5e-7
output_cost_per_token_above_128k_tokens = 0.000001
source = "https://docs.x.ai/docs/models/grok-4-1-fast-non-reasoning"
supports_audio_input = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."xai/grok-4-1-fast-non-reasoning-latest"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
input_cost_per_token_above_128k_tokens = 4e-7
litellm_provider = "xai"
max_input_tokens = 2000000
max_output_tokens = 2000000
max_tokens = 2000000
mode = "chat"
output_cost_per_token = 5e-7
output_cost_per_token_above_128k_tokens = 0.000001
source = "https://docs.x.ai/docs/models/grok-4-1-fast-non-reasoning"
supports_audio_input = true
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."xai/grok-4-1-fast-reasoning"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
input_cost_per_token_above_128k_tokens = 4e-7
litellm_provider = "xai"
max_input_tokens = 2000000
max_output_tokens = 2000000
max_tokens = 2000000
mode = "chat"
output_cost_per_token = 5e-7
output_cost_per_token_above_128k_tokens = 0.000001
source = "https://docs.x.ai/docs/models/grok-4-1-fast-reasoning"
supports_audio_input = true
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."xai/grok-4-1-fast-reasoning-latest"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
input_cost_per_token_above_128k_tokens = 4e-7
litellm_provider = "xai"
max_input_tokens = 2000000
max_output_tokens = 2000000
max_tokens = 2000000
mode = "chat"
output_cost_per_token = 5e-7
output_cost_per_token_above_128k_tokens = 0.000001
source = "https://docs.x.ai/docs/models/grok-4-1-fast-reasoning"
supports_audio_input = true
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."xai/grok-4-fast-non-reasoning"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
input_cost_per_token_above_128k_tokens = 4e-7
litellm_provider = "xai"
max_input_tokens = 2000000
max_output_tokens = 2000000
max_tokens = 2000000
mode = "chat"
output_cost_per_token = 5e-7
output_cost_per_token_above_128k_tokens = 0.000001
source = "https://docs.x.ai/docs/models"
supports_function_calling = true
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-4-fast-reasoning"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
input_cost_per_token_above_128k_tokens = 4e-7
litellm_provider = "xai"
max_input_tokens = 2000000
max_output_tokens = 2000000
max_tokens = 2000000
mode = "chat"
output_cost_per_token = 5e-7
output_cost_per_token_above_128k_tokens = 0.000001
source = "https://docs.x.ai/docs/models"
supports_function_calling = true
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-4-latest"]
input_cost_per_token = 0.000003
input_cost_per_token_above_128k_tokens = 0.000006
litellm_provider = "xai"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.000015
output_cost_per_token_above_128k_tokens = 0.00003
source = "https://docs.x.ai/docs/models"
supports_function_calling = true
supports_tool_choice = true
supports_web_search = true

[models."xai/grok-beta"]
input_cost_per_token = 0.000005
litellm_provider = "xai"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."xai/grok-code-fast"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2e-7
litellm_provider = "xai"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.0000015
source = "https://docs.x.ai/docs/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."xai/grok-code-fast-1"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2e-7
litellm_provider = "xai"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.0000015
source = "https://docs.x.ai/docs/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."xai/grok-code-fast-1-0825"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2e-7
litellm_provider = "xai"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
mode = "chat"
output_cost_per_token = 0.0000015
source = "https://docs.x.ai/docs/models"
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."xai/grok-vision-beta"]
input_cost_per_image = 0.000005
input_cost_per_token = 0.000005
litellm_provider = "xai"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
mode = "chat"
output_cost_per_token = 0.000015
supports_function_calling = true
supports_tool_choice = true
supports_vision = true
supports_web_search = true

[models."zai/glm-4-32b-0414-128k"]
input_cost_per_token = 1e-7
litellm_provider = "zai"
max_input_tokens = 128000
max_output_tokens = 32000
mode = "chat"
output_cost_per_token = 1e-7
source = "https://docs.z.ai/guides/overview/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."zai/glm-4.5"]
input_cost_per_token = 6e-7
litellm_provider = "zai"
max_input_tokens = 128000
max_output_tokens = 32000
mode = "chat"
output_cost_per_token = 0.0000022
source = "https://docs.z.ai/guides/overview/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."zai/glm-4.5-air"]
input_cost_per_token = 2e-7
litellm_provider = "zai"
max_input_tokens = 128000
max_output_tokens = 32000
mode = "chat"
output_cost_per_token = 0.0000011
source = "https://docs.z.ai/guides/overview/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."zai/glm-4.5-airx"]
input_cost_per_token = 0.0000011
litellm_provider = "zai"
max_input_tokens = 128000
max_output_tokens = 32000
mode = "chat"
output_cost_per_token = 0.0000045
source = "https://docs.z.ai/guides/overview/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."zai/glm-4.5-flash"]
input_cost_per_token = 0
litellm_provider = "zai"
max_input_tokens = 128000
max_output_tokens = 32000
mode = "chat"
output_cost_per_token = 0
source = "https://docs.z.ai/guides/overview/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."zai/glm-4.5-x"]
input_cost_per_token = 0.0000022
litellm_provider = "zai"
max_input_tokens = 128000
max_output_tokens = 32000
mode = "chat"
output_cost_per_token = 0.0000089
source = "https://docs.z.ai/guides/overview/pricing"
supports_function_calling = true
supports_tool_choice = true

[models."zai/glm-4.5v"]
input_cost_per_token = 6e-7
litellm_provider = "zai"
max_input_tokens = 128000
max_output_tokens = 32000
mode = "chat"
output_cost_per_token = 0.0000018
source = "https://docs.z.ai/guides/overview/pricing"
supports_function_calling = true
supports_tool_choice = true
supports_vision = true

[models."zai/glm-4.6"]
input_cost_per_token = 6e-7
litellm_provider = "zai"
max_input_tokens = 200000
max_output_tokens = 128000
mode = "chat"
output_cost_per_token = 0.0000022
source = "https://docs.z.ai/guides/overview/pricing"
supports_function_calling = true
supports_tool_choice = true
