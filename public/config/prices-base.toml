# Generated by scripts/convert-litellm-to-toml.ts
# Sources: LiteLLM + models.dev

[metadata]
version = "2026.02.25"
checksum = "0fd41f61d490bf8ec3d45ffc35994f9053d448339b5c7e238c9a10752292a67a"
sources = ["litellm", "modelsdev"]
total_models = 2784
litellm_raw_models = 2599
modelsdev_raw_models = 3036
custom_models = 4

[models."*/1-month-commitment/cohere.command-light-text-v14"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.001902
output_cost_per_second = 0.001902
supports_tool_choice = true

[models."*/1-month-commitment/cohere.command-light-text-v14".pricing."bedrock"]
input_cost_per_second = 0.001902
output_cost_per_second = 0.001902

[models."*/1-month-commitment/cohere.command-text-v14"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.011
output_cost_per_second = 0.011
supports_tool_choice = true

[models."*/1-month-commitment/cohere.command-text-v14".pricing."bedrock"]
input_cost_per_second = 0.011
output_cost_per_second = 0.011

[models."*/6-month-commitment/cohere.command-light-text-v14"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.0011416
output_cost_per_second = 0.0011416
supports_tool_choice = true

[models."*/6-month-commitment/cohere.command-light-text-v14".pricing."bedrock"]
input_cost_per_second = 0.0011416
output_cost_per_second = 0.0011416

[models."*/6-month-commitment/cohere.command-text-v14"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.0066027
output_cost_per_second = 0.0066027
supports_tool_choice = true

[models."*/6-month-commitment/cohere.command-text-v14".pricing."bedrock"]
input_cost_per_second = 0.0066027
output_cost_per_second = 0.0066027

[models."1-month-commitment/anthropic.claude-instant-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.011
output_cost_per_second = 0.011
supports_tool_choice = true

[models."1-month-commitment/anthropic.claude-instant-v1".pricing."bedrock/us-east-1"]
input_cost_per_second = 0.011
output_cost_per_second = 0.011
[models."1-month-commitment/anthropic.claude-instant-v1".pricing."bedrock/us-west-2"]
input_cost_per_second = 0.011
output_cost_per_second = 0.011

[models."1-month-commitment/anthropic.claude-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.0175
output_cost_per_second = 0.0175

[models."1-month-commitment/anthropic.claude-v1".pricing."bedrock/us-east-1"]
input_cost_per_second = 0.0175
output_cost_per_second = 0.0175
[models."1-month-commitment/anthropic.claude-v1".pricing."bedrock/us-west-2"]
input_cost_per_second = 0.0175
output_cost_per_second = 0.0175

[models."1-month-commitment/anthropic.claude-v2:1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.0175
output_cost_per_second = 0.0175
supports_tool_choice = true

[models."1-month-commitment/anthropic.claude-v2:1".pricing."bedrock/us-east-1"]
input_cost_per_second = 0.0175
output_cost_per_second = 0.0175
[models."1-month-commitment/anthropic.claude-v2:1".pricing."bedrock/us-west-2"]
input_cost_per_second = 0.0175
output_cost_per_second = 0.0175

[models."1024-x-1024/50-steps/bedrock/amazon.nova-canvas-v1:0"]
mode = "image_generation"
max_input_tokens = 2600
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.06

[models."1024-x-1024/50-steps/bedrock/amazon.nova-canvas-v1:0".pricing."bedrock"]
output_cost_per_image = 0.06

[models."1024-x-1024/50-steps/stability.stable-diffusion-xl-v1"]
mode = "image_generation"
max_input_tokens = 77
max_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.04

[models."1024-x-1024/50-steps/stability.stable-diffusion-xl-v1".pricing."bedrock"]
output_cost_per_image = 0.04

[models."1024-x-1024/dall-e-2"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
input_cost_per_pixel = 1.9e-8
output_cost_per_pixel = 0

[models."1024-x-1024/dall-e-2".pricing."openai"]
input_cost_per_pixel = 1.9e-8
output_cost_per_pixel = 0

[models."1024-x-1024/gpt-image-1.5"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.009
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."1024-x-1024/gpt-image-1.5".pricing."openai"]
input_cost_per_image = 0.009

[models."1024-x-1024/gpt-image-1.5-2025-12-16"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.009
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."1024-x-1024/gpt-image-1.5-2025-12-16".pricing."openai"]
input_cost_per_image = 0.009

[models."1024-x-1024/max-steps/stability.stable-diffusion-xl-v1"]
mode = "image_generation"
max_input_tokens = 77
max_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.08

[models."1024-x-1024/max-steps/stability.stable-diffusion-xl-v1".pricing."bedrock"]
output_cost_per_image = 0.08

[models."1024-x-1536/gpt-image-1.5"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.013
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."1024-x-1536/gpt-image-1.5".pricing."openai"]
input_cost_per_image = 0.013

[models."1024-x-1536/gpt-image-1.5-2025-12-16"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.013
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."1024-x-1536/gpt-image-1.5-2025-12-16".pricing."openai"]
input_cost_per_image = 0.013

[models."1536-x-1024/gpt-image-1.5"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.013
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."1536-x-1024/gpt-image-1.5".pricing."openai"]
input_cost_per_image = 0.013

[models."1536-x-1024/gpt-image-1.5-2025-12-16"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.013
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."1536-x-1024/gpt-image-1.5-2025-12-16".pricing."openai"]
input_cost_per_image = 0.013

[models."256-x-256/dall-e-2"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
input_cost_per_pixel = 2.4414e-7
output_cost_per_pixel = 0

[models."256-x-256/dall-e-2".pricing."openai"]
input_cost_per_pixel = 2.4414e-7
output_cost_per_pixel = 0

[models."512-x-512/50-steps/stability.stable-diffusion-xl-v0"]
mode = "image_generation"
max_input_tokens = 77
max_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.018

[models."512-x-512/50-steps/stability.stable-diffusion-xl-v0".pricing."bedrock"]
output_cost_per_image = 0.018

[models."512-x-512/dall-e-2"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
input_cost_per_pixel = 6.86e-8
output_cost_per_pixel = 0

[models."512-x-512/dall-e-2".pricing."openai"]
input_cost_per_pixel = 6.86e-8
output_cost_per_pixel = 0

[models."512-x-512/max-steps/stability.stable-diffusion-xl-v0"]
mode = "image_generation"
max_input_tokens = 77
max_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.036

[models."512-x-512/max-steps/stability.stable-diffusion-xl-v0".pricing."bedrock"]
output_cost_per_image = 0.036

[models."6-month-commitment/anthropic.claude-instant-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.00611
output_cost_per_second = 0.00611
supports_tool_choice = true

[models."6-month-commitment/anthropic.claude-instant-v1".pricing."bedrock/us-east-1"]
input_cost_per_second = 0.00611
output_cost_per_second = 0.00611
[models."6-month-commitment/anthropic.claude-instant-v1".pricing."bedrock/us-west-2"]
input_cost_per_second = 0.00611
output_cost_per_second = 0.00611

[models."6-month-commitment/anthropic.claude-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.00972
output_cost_per_second = 0.00972

[models."6-month-commitment/anthropic.claude-v1".pricing."bedrock/us-east-1"]
input_cost_per_second = 0.00972
output_cost_per_second = 0.00972
[models."6-month-commitment/anthropic.claude-v1".pricing."bedrock/us-west-2"]
input_cost_per_second = 0.00972
output_cost_per_second = 0.00972

[models."6-month-commitment/anthropic.claude-v2:1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.00972
output_cost_per_second = 0.00972
supports_tool_choice = true

[models."6-month-commitment/anthropic.claude-v2:1".pricing."bedrock/us-east-1"]
input_cost_per_second = 0.00972
output_cost_per_second = 0.00972
[models."6-month-commitment/anthropic.claude-v2:1".pricing."bedrock/us-west-2"]
input_cost_per_second = 0.00972
output_cost_per_second = 0.00972

[models."@cf/ai4bharat/indictrans2-en-indic-1B"]
display_name = "IndicTrans2 EN-Indic 1B"
model_family = "indictrans"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 3.4000000000000003e-7
output_cost_per_token = 3.4000000000000003e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-09-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/ai4bharat/indictrans2-en-indic-1B".pricing."cloudflare-workers-ai"]
input_cost_per_token = 3.4000000000000003e-7
output_cost_per_token = 3.4000000000000003e-7

[models."@cf/aisingapore/gemma-sea-lion-v4-27b-it"]
display_name = "Gemma SEA-LION v4 27B IT"
model_family = "gemma"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 3.5e-7
output_cost_per_token = 5.6e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-09-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/aisingapore/gemma-sea-lion-v4-27b-it".pricing."cloudflare-workers-ai"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 5.6e-7

[models."@cf/baai/bge-base-en-v1-5"]
display_name = "BGE Base EN v1.5"
model_family = "bge"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 6.7e-8
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/baai/bge-base-en-v1-5".pricing."cloudflare-workers-ai"]
input_cost_per_token = 6.7e-8

[models."@cf/baai/bge-large-en-v1-5"]
display_name = "BGE Large EN v1.5"
model_family = "bge"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.0000000000000002e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/baai/bge-large-en-v1-5".pricing."cloudflare-workers-ai"]
input_cost_per_token = 2.0000000000000002e-7

[models."@cf/baai/bge-m3"]
display_name = "BGE M3"
model_family = "bge"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.2e-8
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/baai/bge-m3".pricing."cloudflare-workers-ai"]
input_cost_per_token = 1.2e-8

[models."@cf/baai/bge-reranker-base"]
display_name = "BGE Reranker Base"
model_family = "bge"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 3.1e-9
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/baai/bge-reranker-base".pricing."cloudflare-workers-ai"]
input_cost_per_token = 3.1e-9

[models."@cf/baai/bge-small-en-v1-5"]
display_name = "BGE Small EN v1.5"
model_family = "bge"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2e-8
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/baai/bge-small-en-v1-5".pricing."cloudflare-workers-ai"]
input_cost_per_token = 2e-8

[models."@cf/deepgram/aura-2-en"]
display_name = "Deepgram Aura 2 (EN)"
model_family = "aura"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-11-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/deepgram/aura-2-es"]
display_name = "Deepgram Aura 2 (ES)"
model_family = "aura"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-11-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/deepgram/nova-3"]
display_name = "Deepgram Nova 3"
model_family = "nova"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-11-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/deepseek-ai/deepseek-r1-distill-qwen-32b"]
display_name = "DeepSeek R1 Distill Qwen 32B"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 5e-7
output_cost_per_token = 0.00000488
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/deepseek-ai/deepseek-r1-distill-qwen-32b".pricing."cloudflare-workers-ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.00000488

[models."@cf/facebook/bart-large-cnn"]
display_name = "BART Large CNN"
model_family = "bart"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/google/gemma-3-12b-it"]
display_name = "Gemma 3 12B IT"
model_family = "gemma"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 3.5e-7
output_cost_per_token = 5.6e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/google/gemma-3-12b-it".pricing."cloudflare-workers-ai"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 5.6e-7

[models."@cf/huggingface/distilbert-sst-2-int8"]
display_name = "DistilBERT SST-2 INT8"
model_family = "distilbert"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.5999999999999998e-8
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/huggingface/distilbert-sst-2-int8".pricing."cloudflare-workers-ai"]
input_cost_per_token = 2.5999999999999998e-8

[models."@cf/ibm-granite/granite-4-0-h-micro"]
display_name = "IBM Granite 4.0 H Micro"
model_family = "granite"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.7e-8
output_cost_per_token = 1.1e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-10-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/ibm-granite/granite-4-0-h-micro".pricing."cloudflare-workers-ai"]
input_cost_per_token = 1.7e-8
output_cost_per_token = 1.1e-7

[models."@cf/meta/llama-2-7b-chat-fp16"]
display_name = "Llama 2 7B Chat FP16"
model_family = "llama"
mode = "chat"
max_input_tokens = 3072
max_output_tokens = 3072
max_tokens = 3072
input_cost_per_token = 0.000001923
output_cost_per_token = 0.000001923
litellm_provider = "cloudflare"
providers = ["cloudflare", "cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."@cf/meta/llama-2-7b-chat-fp16".pricing."cloudflare"]
input_cost_per_token = 0.000001923
output_cost_per_token = 0.000001923
[models."@cf/meta/llama-2-7b-chat-fp16".pricing."cloudflare-workers-ai"]
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000667

[models."@cf/meta/llama-2-7b-chat-int8"]
mode = "chat"
max_input_tokens = 2048
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 0.000001923
output_cost_per_token = 0.000001923
litellm_provider = "cloudflare"
providers = ["cloudflare"]

[models."@cf/meta/llama-2-7b-chat-int8".pricing."cloudflare"]
input_cost_per_token = 0.000001923
output_cost_per_token = 0.000001923

[models."@cf/meta/llama-3-1-8b-instruct"]
display_name = "Llama 3.1 8B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.8e-7
output_cost_per_token = 8.299999999999999e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/meta/llama-3-1-8b-instruct".pricing."cloudflare-workers-ai"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 8.299999999999999e-7

[models."@cf/meta/llama-3-1-8b-instruct-awq"]
display_name = "Llama 3.1 8B Instruct AWQ"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.2e-7
output_cost_per_token = 2.7e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/meta/llama-3-1-8b-instruct-awq".pricing."cloudflare-workers-ai"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 2.7e-7

[models."@cf/meta/llama-3-1-8b-instruct-fp8"]
display_name = "Llama 3.1 8B Instruct FP8"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.5e-7
output_cost_per_token = 2.9e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/meta/llama-3-1-8b-instruct-fp8".pricing."cloudflare-workers-ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 2.9e-7

[models."@cf/meta/llama-3-2-11b-vision-instruct"]
display_name = "Llama 3.2 11B Vision Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 4.9e-8
output_cost_per_token = 6.800000000000001e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/meta/llama-3-2-11b-vision-instruct".pricing."cloudflare-workers-ai"]
input_cost_per_token = 4.9e-8
output_cost_per_token = 6.800000000000001e-7

[models."@cf/meta/llama-3-2-1b-instruct"]
display_name = "Llama 3.2 1B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.7e-8
output_cost_per_token = 2.0000000000000002e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/meta/llama-3-2-1b-instruct".pricing."cloudflare-workers-ai"]
input_cost_per_token = 2.7e-8
output_cost_per_token = 2.0000000000000002e-7

[models."@cf/meta/llama-3-2-3b-instruct"]
display_name = "Llama 3.2 3B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 5.0999999999999993e-8
output_cost_per_token = 3.4000000000000003e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/meta/llama-3-2-3b-instruct".pricing."cloudflare-workers-ai"]
input_cost_per_token = 5.0999999999999993e-8
output_cost_per_token = 3.4000000000000003e-7

[models."@cf/meta/llama-3-3-70b-instruct-fp8-fast"]
display_name = "Llama 3.3 70B Instruct FP8 Fast"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.00000225
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/meta/llama-3-3-70b-instruct-fp8-fast".pricing."cloudflare-workers-ai"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.00000225

[models."@cf/meta/llama-3-8b-instruct"]
display_name = "Llama 3 8B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.8e-7
output_cost_per_token = 8.3e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/meta/llama-3-8b-instruct".pricing."cloudflare-workers-ai"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 8.3e-7

[models."@cf/meta/llama-3-8b-instruct-awq"]
display_name = "Llama 3 8B Instruct AWQ"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.2e-7
output_cost_per_token = 2.7e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/meta/llama-3-8b-instruct-awq".pricing."cloudflare-workers-ai"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 2.7e-7

[models."@cf/meta/llama-4-scout-17b-16e-instruct"]
display_name = "Llama 4 Scout 17B 16E Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.7e-7
output_cost_per_token = 8.5e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/meta/llama-4-scout-17b-16e-instruct".pricing."cloudflare-workers-ai"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 8.5e-7

[models."@cf/meta/llama-guard-3-8b"]
display_name = "Llama Guard 3 8B"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 4.8e-7
output_cost_per_token = 3e-8
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/meta/llama-guard-3-8b".pricing."cloudflare-workers-ai"]
input_cost_per_token = 4.8e-7
output_cost_per_token = 3e-8

[models."@cf/meta/m2m100-1-2b"]
display_name = "M2M100 1.2B"
model_family = "m2m"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 3.4000000000000003e-7
output_cost_per_token = 3.4000000000000003e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/meta/m2m100-1-2b".pricing."cloudflare-workers-ai"]
input_cost_per_token = 3.4000000000000003e-7
output_cost_per_token = 3.4000000000000003e-7

[models."@cf/mistral/mistral-7b-instruct-v0.1"]
display_name = "Mistral 7B Instruct v0.1"
model_family = "mistral"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000001923
output_cost_per_token = 0.000001923
litellm_provider = "cloudflare"
providers = ["cloudflare", "cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."@cf/mistral/mistral-7b-instruct-v0.1".pricing."cloudflare"]
input_cost_per_token = 0.000001923
output_cost_per_token = 0.000001923
[models."@cf/mistral/mistral-7b-instruct-v0.1".pricing."cloudflare-workers-ai"]
input_cost_per_token = 1.1e-7
output_cost_per_token = 1.9e-7

[models."@cf/mistralai/mistral-small-3-1-24b-instruct"]
display_name = "Mistral Small 3.1 24B Instruct"
model_family = "mistral-small"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 3.5e-7
output_cost_per_token = 5.6e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/mistralai/mistral-small-3-1-24b-instruct".pricing."cloudflare-workers-ai"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 5.6e-7

[models."@cf/myshell-ai/melotts"]
display_name = "MyShell MeloTTS"
model_family = "melotts"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-11-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/openai/gpt-oss-120b"]
display_name = "GPT OSS 120B"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 3.5e-7
output_cost_per_token = 7.5e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-08-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/openai/gpt-oss-120b".pricing."cloudflare-workers-ai"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 7.5e-7

[models."@cf/openai/gpt-oss-20b"]
display_name = "GPT OSS 20B"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 3e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-08-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/openai/gpt-oss-20b".pricing."cloudflare-workers-ai"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 3e-7

[models."@cf/pfnet/plamo-embedding-1b"]
display_name = "PLaMo Embedding 1B"
model_family = "plamo"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.8999999999999998e-8
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-09-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/pfnet/plamo-embedding-1b".pricing."cloudflare-workers-ai"]
input_cost_per_token = 1.8999999999999998e-8

[models."@cf/pipecat-ai/smart-turn-v2"]
display_name = "Pipecat Smart Turn v2"
model_family = "smart-turn"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-11-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/qwen/qwen2-5-coder-32b-instruct"]
display_name = "Qwen 2.5 Coder 32B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 6.6e-7
output_cost_per_token = 0.000001
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/qwen/qwen2-5-coder-32b-instruct".pricing."cloudflare-workers-ai"]
input_cost_per_token = 6.6e-7
output_cost_per_token = 0.000001

[models."@cf/qwen/qwen3-30b-a3b-fp8"]
display_name = "Qwen3 30B A3B FP8"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 5.0999999999999993e-8
output_cost_per_token = 3.4000000000000003e-7
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-11-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/qwen/qwen3-30b-a3b-fp8".pricing."cloudflare-workers-ai"]
input_cost_per_token = 5.0999999999999993e-8
output_cost_per_token = 3.4000000000000003e-7

[models."@cf/qwen/qwen3-embedding-0-6b"]
display_name = "Qwen3 Embedding 0.6B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.2e-8
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-11-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/qwen/qwen3-embedding-0-6b".pricing."cloudflare-workers-ai"]
input_cost_per_token = 1.2e-8

[models."@cf/qwen/qwq-32b"]
display_name = "QwQ 32B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 6.6e-7
output_cost_per_token = 0.000001
litellm_provider = "cloudflare-workers-ai"
providers = ["cloudflare-workers-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."@cf/qwen/qwq-32b".pricing."cloudflare-workers-ai"]
input_cost_per_token = 6.6e-7
output_cost_per_token = 0.000001

[models."@hf/thebloke/codellama-7b-instruct-awq"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000001923
output_cost_per_token = 0.000001923
litellm_provider = "cloudflare"
providers = ["cloudflare"]

[models."@hf/thebloke/codellama-7b-instruct-awq".pricing."cloudflare"]
input_cost_per_token = 0.000001923
output_cost_per_token = 0.000001923

[models."BAAI/bge-base-en-v1.5"]
mode = "embedding"
max_input_tokens = 512
input_cost_per_token = 8e-9
output_cost_per_token = 0
litellm_provider = "together_ai"
providers = ["together_ai"]
output_vector_size = 768

[models."BAAI/bge-base-en-v1.5".pricing."together_ai"]
input_cost_per_token = 8e-9
output_cost_per_token = 0

[models."BAAI/bge-en-icl"]
display_name = "BGE-ICL"
model_family = "text-embedding"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 0
input_cost_per_token = 1e-8
litellm_provider = "nebius"
providers = ["nebius"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-06"
release_date = "2024-07-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."BAAI/bge-en-icl".pricing."nebius"]
input_cost_per_token = 1e-8

[models."BAAI/bge-multilingual-gemma2"]
display_name = "bge-multilingual-gemma2"
model_family = "text-embedding"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 0
input_cost_per_token = 1e-8
litellm_provider = "nebius"
providers = ["nebius"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-06"
release_date = "2024-07-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."BAAI/bge-multilingual-gemma2".pricing."nebius"]
input_cost_per_token = 1e-8

[models."BSC-LT/ALIA-40b-instruct_Q8_0"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "publicai"
providers = ["publicai"]
supports_function_calling = true
source = "https://platform.publicai.co/docs"
supports_tool_choice = true

[models."BSC-LT/ALIA-40b-instruct_Q8_0".pricing."publicai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."BSC-LT/salamandra-7b-instruct-tools-16k"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "publicai"
providers = ["publicai"]
supports_function_calling = true
source = "https://platform.publicai.co/docs"
supports_tool_choice = true

[models."BSC-LT/salamandra-7b-instruct-tools-16k".pricing."publicai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."ByteDance-Seed/Seed-OSS-36B-Instruct"]
display_name = "ByteDance-Seed/Seed-OSS-36B-Instruct"
model_family = "seed"
mode = "chat"
max_input_tokens = 262000
max_output_tokens = 262000
input_cost_per_token = 2.1e-7
output_cost_per_token = 5.699999999999999e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "siliconflow"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-09-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."ByteDance-Seed/Seed-OSS-36B-Instruct".pricing."siliconflow"]
input_cost_per_token = 2.1e-7
output_cost_per_token = 5.699999999999999e-7
[models."ByteDance-Seed/Seed-OSS-36B-Instruct".pricing."siliconflow-cn"]
input_cost_per_token = 2.1e-7
output_cost_per_token = 5.699999999999999e-7

[models."Cohere-embed-v3-english"]
display_name = "Embed v3 English"
model_family = "cohere-embed"
mode = "embedding"
max_input_tokens = 512
max_output_tokens = 1024
max_tokens = 512
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2023-11-07"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/cohere.cohere-embed-v3-english-offer?tab=PlansAndPrice"
output_vector_size = 1024
supports_embedding_image_input = true

[models."Cohere-embed-v3-english".pricing."azure"]
input_cost_per_token = 1.0000000000000001e-7
[models."Cohere-embed-v3-english".pricing."azure-cognitive-services"]
input_cost_per_token = 1.0000000000000001e-7
[models."Cohere-embed-v3-english".pricing."azure_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."Cohere-embed-v3-multilingual"]
display_name = "Embed v3 Multilingual"
model_family = "cohere-embed"
mode = "embedding"
max_input_tokens = 512
max_output_tokens = 1024
max_tokens = 512
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2023-11-07"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/cohere.cohere-embed-v3-english-offer?tab=PlansAndPrice"
output_vector_size = 1024
supports_embedding_image_input = true

[models."Cohere-embed-v3-multilingual".pricing."azure"]
input_cost_per_token = 1.0000000000000001e-7
[models."Cohere-embed-v3-multilingual".pricing."azure-cognitive-services"]
input_cost_per_token = 1.0000000000000001e-7
[models."Cohere-embed-v3-multilingual".pricing."azure_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."DeepSeek-R1"]
display_name = "DeepSeek R1"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000005
output_cost_per_token = 0.000007
litellm_provider = "sambanova"
providers = ["sambanova", "alibaba-cn", "azure", "azure-cognitive-services", "iflowcn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://cloud.sambanova.ai/plans/pricing"

[models."DeepSeek-R1".pricing."alibaba-cn"]
input_cost_per_token = 5.739999999999999e-7
output_cost_per_token = 0.000002294
[models."DeepSeek-R1".pricing."azure"]
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054
[models."DeepSeek-R1".pricing."azure-cognitive-services"]
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054
[models."DeepSeek-R1".pricing."sambanova"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000007

[models."DeepSeek-R1-Distill-Llama-70B"]
display_name = "DeepSeek R1 Distill Llama 70B"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 6.7e-7
output_cost_per_token = 6.7e-7
litellm_provider = "ovhcloud"
providers = ["ovhcloud", "alibaba-cn", "groq", "helicone", "sambanova", "scaleway", "vultr"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-01-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://endpoints.ai.cloud.ovh.net/models/deepseek-r1-distill-llama-70b"
supports_response_schema = true
supports_tool_choice = true

[models."DeepSeek-R1-Distill-Llama-70B".pricing."alibaba-cn"]
input_cost_per_token = 2.8699999999999996e-7
output_cost_per_token = 8.61e-7
[models."DeepSeek-R1-Distill-Llama-70B".pricing."groq"]
input_cost_per_token = 7.5e-7
output_cost_per_token = 9.9e-7
[models."DeepSeek-R1-Distill-Llama-70B".pricing."helicone"]
input_cost_per_token = 3e-8
output_cost_per_token = 1.3e-7
[models."DeepSeek-R1-Distill-Llama-70B".pricing."ovhcloud"]
input_cost_per_token = 6.7e-7
output_cost_per_token = 6.7e-7
[models."DeepSeek-R1-Distill-Llama-70B".pricing."sambanova"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000014
[models."DeepSeek-R1-Distill-Llama-70B".pricing."scaleway"]
input_cost_per_token = 9.000000000000001e-7
output_cost_per_token = 9.000000000000001e-7
[models."DeepSeek-R1-Distill-Llama-70B".pricing."vultr"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7

[models."DeepSeek-V3-0324"]
display_name = "DeepSeek V3 0324"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000003
output_cost_per_token = 0.0000045
litellm_provider = "sambanova"
providers = ["sambanova", "azure", "azure-cognitive-services", "cortecs"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2025-03-24"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://cloud.sambanova.ai/plans/pricing"
supports_tool_choice = true

[models."DeepSeek-V3-0324".pricing."azure"]
input_cost_per_token = 0.0000011399999999999999
output_cost_per_token = 0.0000045599999999999995
[models."DeepSeek-V3-0324".pricing."azure-cognitive-services"]
input_cost_per_token = 0.0000011399999999999999
output_cost_per_token = 0.0000045599999999999995
[models."DeepSeek-V3-0324".pricing."cortecs"]
input_cost_per_token = 5.51e-7
output_cost_per_token = 0.000001654
[models."DeepSeek-V3-0324".pricing."sambanova"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.0000045

[models."DeepSeek-V3.1"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000003
output_cost_per_token = 0.0000045
litellm_provider = "sambanova"
providers = ["sambanova"]
supports_function_calling = true
supports_reasoning = true
source = "https://cloud.sambanova.ai/plans/pricing"
supports_tool_choice = true

[models."DeepSeek-V3.1".pricing."sambanova"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.0000045

[models."Embeddings"]
mode = "embedding"
max_input_tokens = 512
max_tokens = 512
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "gigachat"
providers = ["gigachat"]
output_vector_size = 1024

[models."Embeddings".pricing."gigachat"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."Embeddings-2"]
mode = "embedding"
max_input_tokens = 512
max_tokens = 512
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "gigachat"
providers = ["gigachat"]
output_vector_size = 1024

[models."Embeddings-2".pricing."gigachat"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."EmbeddingsGigaR"]
mode = "embedding"
max_input_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "gigachat"
providers = ["gigachat"]
output_vector_size = 2560

[models."EmbeddingsGigaR".pricing."gigachat"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."FLUX-1.1-pro"]
mode = "image_generation"
litellm_provider = "azure_ai"
providers = ["azure_ai"]
source = "https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/black-forest-labs-flux-1-kontext-pro-and-flux1-1-pro-now-available-in-azure-ai-f/4434659"
output_cost_per_image = 0.04
supported_endpoints = ["/v1/images/generations"]

[models."FLUX-1.1-pro".pricing."azure_ai"]
output_cost_per_image = 0.04

[models."FLUX.1-Kontext-pro"]
mode = "image_generation"
litellm_provider = "azure_ai"
providers = ["azure_ai"]
source = "https://azuremarketplace.microsoft.com/pt-br/marketplace/apps/cohere.cohere-embed-4-offer?tab=PlansAndPrice"
output_cost_per_image = 0.04
supported_endpoints = ["/v1/images/generations"]

[models."FLUX.1-Kontext-pro".pricing."azure_ai"]
output_cost_per_image = 0.04

[models."Gemma-3-4b-it-GGUF"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "lemonade"
providers = ["lemonade"]
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."Gemma-3-4b-it-GGUF".pricing."lemonade"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."GigaChat-2-Lite"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "gigachat"
providers = ["gigachat"]
supports_function_calling = true
supports_system_messages = true

[models."GigaChat-2-Lite".pricing."gigachat"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."GigaChat-2-Max"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "gigachat"
providers = ["gigachat"]
supports_function_calling = true
supports_vision = true
supports_system_messages = true

[models."GigaChat-2-Max".pricing."gigachat"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."GigaChat-2-Pro"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "gigachat"
providers = ["gigachat"]
supports_function_calling = true
supports_vision = true
supports_system_messages = true

[models."GigaChat-2-Pro".pricing."gigachat"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."Gryphe/MythoMax-L2-13b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 8e-8
output_cost_per_token = 9e-8
litellm_provider = "deepinfra"
providers = ["deepinfra"]
supports_tool_choice = true

[models."Gryphe/MythoMax-L2-13b".pricing."deepinfra"]
input_cost_per_token = 8e-8
output_cost_per_token = 9e-8

[models."HuggingFaceH4/zephyr-7b-beta"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "anyscale"
providers = ["anyscale"]

[models."HuggingFaceH4/zephyr-7b-beta".pricing."anyscale"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7

[models."Intel/Qwen3-Coder-480B-A35B-Instruct-int4-mixed-ar"]
display_name = "Qwen 3 Coder 480B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 106000
max_output_tokens = 4096
input_cost_per_token = 2.2e-7
output_cost_per_token = 9.499999999999999e-7
cache_read_input_token_cost = 1.1e-7
litellm_provider = "io-net"
providers = ["io-net"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2025-01-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Intel/Qwen3-Coder-480B-A35B-Instruct-int4-mixed-ar".pricing."io-net"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 2.2e-7
output_cost_per_token = 9.499999999999999e-7

[models."KBLab/kb-whisper-large"]
display_name = "KB Whisper"
model_family = "whisper"
mode = "chat"
max_input_tokens = 448
max_output_tokens = 448
input_cost_per_token = 2.36e-9
output_cost_per_token = 2.36e-9
litellm_provider = "evroc"
providers = ["evroc", "berget"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2024-10-01"
supported_modalities = ["audio"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."KBLab/kb-whisper-large".pricing."berget"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000003
[models."KBLab/kb-whisper-large".pricing."evroc"]
input_cost_per_token = 2.36e-9
output_cost_per_token = 2.36e-9

[models."Kwaipilot/KAT-Dev"]
display_name = "Kwaipilot/KAT-Dev"
model_family = "kat-coder"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-09-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Kwaipilot/KAT-Dev".pricing."siliconflow-cn"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7

[models."LGAI-EXAONE/EXAONE-4-0.1-32B"]
display_name = "EXAONE 4.0.1 32B"
model_family = "exaone"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 6e-7
output_cost_per_token = 0.000001
litellm_provider = "friendli"
providers = ["friendli"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-07-31"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."LGAI-EXAONE/EXAONE-4-0.1-32B".pricing."friendli"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000001

[models."LGAI-EXAONE/K-EXAONE-236B-A23B"]
display_name = "K EXAONE 236B A23B"
model_family = "exaone"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
litellm_provider = "friendli"
providers = ["friendli"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-31"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Ling-1T"]
display_name = "Ling-1T"
model_family = "ling"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
input_cost_per_token = 5.699999999999999e-7
output_cost_per_token = 0.00000229
litellm_provider = "bailing"
providers = ["bailing"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-06"
release_date = "2025-10"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Ling-1T".pricing."bailing"]
input_cost_per_token = 5.699999999999999e-7
output_cost_per_token = 0.00000229

[models."Llama-3.1-8B-Instruct"]
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 131000
max_tokens = 131000
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "ovhcloud"
providers = ["ovhcloud"]
supports_function_calling = true
source = "https://endpoints.ai.cloud.ovh.net/models/llama-3-1-8b-instruct"
supports_response_schema = true
supports_tool_choice = true

[models."Llama-3.1-8B-Instruct".pricing."ovhcloud"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."Llama-3.2-11B-Vision-Instruct"]
display_name = "Llama-3.2-11B-Vision-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 3.7e-7
output_cost_per_token = 3.7e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-09-25"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://azuremarketplace.microsoft.com/en/marketplace/apps/metagenai.meta-llama-3-2-11b-vision-instruct-offer?tab=Overview"
supports_tool_choice = true

[models."Llama-3.2-11B-Vision-Instruct".pricing."azure"]
input_cost_per_token = 3.7e-7
output_cost_per_token = 3.7e-7
[models."Llama-3.2-11B-Vision-Instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 3.7e-7
output_cost_per_token = 3.7e-7
[models."Llama-3.2-11B-Vision-Instruct".pricing."azure_ai"]
input_cost_per_token = 3.7e-7
output_cost_per_token = 3.7e-7

[models."Llama-3.2-90B-Vision-Instruct"]
display_name = "Llama-3.2-90B-Vision-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 0.00000204
output_cost_per_token = 0.00000204
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-09-25"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://azuremarketplace.microsoft.com/en/marketplace/apps/metagenai.meta-llama-3-2-90b-vision-instruct-offer?tab=Overview"
supports_tool_choice = true

[models."Llama-3.2-90B-Vision-Instruct".pricing."azure"]
input_cost_per_token = 0.00000204
output_cost_per_token = 0.00000204
[models."Llama-3.2-90B-Vision-Instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 0.00000204
output_cost_per_token = 0.00000204
[models."Llama-3.2-90B-Vision-Instruct".pricing."azure_ai"]
input_cost_per_token = 0.00000204
output_cost_per_token = 0.00000204

[models."Llama-3.3-70B-Instruct"]
display_name = "Llama-3.3-70B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4028
max_tokens = 4028
input_cost_per_token = 7.1e-7
output_cost_per_token = 7.1e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services", "helicone", "llama", "meta_llama", "scaleway"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-12-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azuremarketplace.microsoft.com/en/marketplace/apps/metagenai.llama-3-3-70b-instruct-offer?tab=Overview"
supports_tool_choice = true

[models."Llama-3.3-70B-Instruct".pricing."azure"]
input_cost_per_token = 7.1e-7
output_cost_per_token = 7.1e-7
[models."Llama-3.3-70B-Instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 7.1e-7
output_cost_per_token = 7.1e-7
[models."Llama-3.3-70B-Instruct".pricing."azure_ai"]
input_cost_per_token = 7.1e-7
output_cost_per_token = 7.1e-7
[models."Llama-3.3-70B-Instruct".pricing."helicone"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 3.9e-7
[models."Llama-3.3-70B-Instruct".pricing."scaleway"]
input_cost_per_token = 9.000000000000001e-7
output_cost_per_token = 9.000000000000001e-7

[models."Llama-3.3-8B-Instruct"]
display_name = "Llama-3.3-8B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4028
max_tokens = 4028
litellm_provider = "meta_llama"
providers = ["meta_llama", "llama"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-12-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://llama.developer.meta.com/docs/models"
supports_tool_choice = true

[models."Llama-4-Maverick-17B-128E-Instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 6.3e-7
output_cost_per_token = 0.0000018
litellm_provider = "sambanova"
providers = ["sambanova"]
supports_function_calling = true
supports_vision = true
source = "https://cloud.sambanova.ai/plans/pricing"
supports_response_schema = true
supports_tool_choice = true

[models."Llama-4-Maverick-17B-128E-Instruct".metadata]
notes = "For vision models, images are converted to 6432 input tokens and are billed at that amount"

[models."Llama-4-Maverick-17B-128E-Instruct".pricing."sambanova"]
input_cost_per_token = 6.3e-7
output_cost_per_token = 0.0000018

[models."Llama-4-Maverick-17B-128E-Instruct-FP8"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.00000141
output_cost_per_token = 3.5e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "meta_llama"]
supports_function_calling = true
supports_vision = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/blog/introducing-the-llama-4-herd-in-azure-ai-foundry-and-azure-databricks/"
supports_tool_choice = true

[models."Llama-4-Maverick-17B-128E-Instruct-FP8".pricing."azure_ai"]
input_cost_per_token = 0.00000141
output_cost_per_token = 3.5e-7

[models."Llama-4-Scout-17B-16E-Instruct"]
mode = "chat"
max_input_tokens = 10000000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2e-7
output_cost_per_token = 7.8e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "sambanova"]
supports_function_calling = true
supports_vision = true
source = "https://azure.microsoft.com/en-us/blog/introducing-the-llama-4-herd-in-azure-ai-foundry-and-azure-databricks/"
supports_response_schema = true
supports_tool_choice = true

[models."Llama-4-Scout-17B-16E-Instruct".pricing."azure_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 7.8e-7
[models."Llama-4-Scout-17B-16E-Instruct".pricing."sambanova"]
input_cost_per_token = 4e-7
output_cost_per_token = 7e-7

[models."Llama-4-Scout-17B-16E-Instruct-FP8"]
display_name = "Llama-4-Scout-17B-16E-Instruct-FP8"
model_family = "llama"
mode = "chat"
max_input_tokens = 10000000
max_output_tokens = 4028
max_tokens = 4028
litellm_provider = "meta_llama"
providers = ["meta_llama", "llama"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-04-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://llama.developer.meta.com/docs/models"
supports_tool_choice = true

[models."MAI-DS-R1"]
display_name = "MAI-DS-R1"
model_family = "mai"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = false
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-06"
release_date = "2025-01-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/pricing/details/ai-foundry-models/microsoft/"
supports_tool_choice = true

[models."MAI-DS-R1".pricing."azure"]
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054
[models."MAI-DS-R1".pricing."azure-cognitive-services"]
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054
[models."MAI-DS-R1".pricing."azure_ai"]
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054

[models."Meta-Llama-3-70B-Instruct"]
display_name = "Meta-Llama-3-70B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 0.0000011
output_cost_per_token = 3.7e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-04-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."Meta-Llama-3-70B-Instruct".pricing."azure"]
input_cost_per_token = 0.00000268
output_cost_per_token = 0.00000354
[models."Meta-Llama-3-70B-Instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 0.00000268
output_cost_per_token = 0.00000354
[models."Meta-Llama-3-70B-Instruct".pricing."azure_ai"]
input_cost_per_token = 0.0000011
output_cost_per_token = 3.7e-7

[models."Meta-Llama-3.1-405B-Instruct"]
display_name = "Meta-Llama-3.1-405B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.00000533
output_cost_per_token = 0.000016
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services", "sambanova"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/metagenai.meta-llama-3-1-405b-instruct-offer?tab=PlansAndPrice"
supports_response_schema = true
supports_tool_choice = true

[models."Meta-Llama-3.1-405B-Instruct".pricing."azure"]
input_cost_per_token = 0.00000533
output_cost_per_token = 0.000016
[models."Meta-Llama-3.1-405B-Instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 0.00000533
output_cost_per_token = 0.000016
[models."Meta-Llama-3.1-405B-Instruct".pricing."azure_ai"]
input_cost_per_token = 0.00000533
output_cost_per_token = 0.000016
[models."Meta-Llama-3.1-405B-Instruct".pricing."sambanova"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.00001

[models."Meta-Llama-3.1-70B-Instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 0.00000268
output_cost_per_token = 0.00000354
litellm_provider = "azure_ai"
providers = ["azure_ai"]
source = "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/metagenai.meta-llama-3-1-70b-instruct-offer?tab=PlansAndPrice"
supports_tool_choice = true

[models."Meta-Llama-3.1-70B-Instruct".pricing."azure_ai"]
input_cost_per_token = 0.00000268
output_cost_per_token = 0.00000354

[models."Meta-Llama-3.1-8B-Instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 3e-7
output_cost_per_token = 6.1e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "sambanova"]
supports_function_calling = true
source = "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/metagenai.meta-llama-3-1-8b-instruct-offer?tab=PlansAndPrice"
supports_response_schema = true
supports_tool_choice = true

[models."Meta-Llama-3.1-8B-Instruct".pricing."azure_ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 6.1e-7
[models."Meta-Llama-3.1-8B-Instruct".pricing."sambanova"]
input_cost_per_token = 1e-7
output_cost_per_token = 2e-7

[models."Meta-Llama-3.2-1B-Instruct"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 4e-8
output_cost_per_token = 8e-8
litellm_provider = "sambanova"
providers = ["sambanova"]
source = "https://cloud.sambanova.ai/plans/pricing"

[models."Meta-Llama-3.2-1B-Instruct".pricing."sambanova"]
input_cost_per_token = 4e-8
output_cost_per_token = 8e-8

[models."Meta-Llama-3.2-3B-Instruct"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 8e-8
output_cost_per_token = 1.6e-7
litellm_provider = "sambanova"
providers = ["sambanova"]
source = "https://cloud.sambanova.ai/plans/pricing"

[models."Meta-Llama-3.2-3B-Instruct".pricing."sambanova"]
input_cost_per_token = 8e-8
output_cost_per_token = 1.6e-7

[models."Meta-Llama-3.3-70B-Instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000012
litellm_provider = "sambanova"
providers = ["sambanova"]
supports_function_calling = true
source = "https://cloud.sambanova.ai/plans/pricing"
supports_response_schema = true
supports_tool_choice = true

[models."Meta-Llama-3.3-70B-Instruct".pricing."sambanova"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000012

[models."Meta-Llama-3_1-70B-Instruct"]
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 131000
max_tokens = 131000
input_cost_per_token = 6.7e-7
output_cost_per_token = 6.7e-7
litellm_provider = "ovhcloud"
providers = ["ovhcloud"]
supports_function_calling = false
source = "https://endpoints.ai.cloud.ovh.net/models/meta-llama-3-1-70b-instruct"
supports_response_schema = false
supports_tool_choice = false

[models."Meta-Llama-3_1-70B-Instruct".pricing."ovhcloud"]
input_cost_per_token = 6.7e-7
output_cost_per_token = 6.7e-7

[models."Meta-Llama-3_3-70B-Instruct"]
display_name = "Meta-Llama-3_3-70B-Instruct"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 131000
max_tokens = 131000
input_cost_per_token = 6.7e-7
output_cost_per_token = 6.7e-7
litellm_provider = "ovhcloud"
providers = ["ovhcloud"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-04-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://endpoints.ai.cloud.ovh.net/models/meta-llama-3-3-70b-instruct"
supports_response_schema = true
supports_tool_choice = true

[models."Meta-Llama-3_3-70B-Instruct".pricing."ovhcloud"]
input_cost_per_token = 6.7e-7
output_cost_per_token = 6.7e-7

[models."Meta-Llama-Guard-3-8B"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 3e-7
output_cost_per_token = 3e-7
litellm_provider = "sambanova"
providers = ["sambanova"]
source = "https://cloud.sambanova.ai/plans/pricing"

[models."Meta-Llama-Guard-3-8B".pricing."sambanova"]
input_cost_per_token = 3e-7
output_cost_per_token = 3e-7

[models."MiniMax-M1"]
display_name = "MiniMax-M1"
model_family = "minimax"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 128000
input_cost_per_token = 1.3200000000000002e-7
output_cost_per_token = 0.000001254
litellm_provider = "302ai"
providers = ["302ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-06-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."MiniMax-M1".pricing."302ai"]
input_cost_per_token = 1.3200000000000002e-7
output_cost_per_token = 0.000001254

[models."MiniMax-M2"]
display_name = "MiniMax-M2"
model_family = "minimax"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
cache_read_input_token_cost = 3e-8
cache_creation_input_token_cost = 3.75e-7
litellm_provider = "minimax"
providers = ["minimax", "302ai", "cortecs", "minimax-cn", "minimax-cn-coding-plan", "minimax-coding-plan"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-10-26"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true
supports_tool_choice = true

[models."MiniMax-M2".pricing."302ai"]
input_cost_per_token = 3.3e-7
output_cost_per_token = 0.00000132
[models."MiniMax-M2".pricing."cortecs"]
input_cost_per_token = 3.9e-7
output_cost_per_token = 0.00000157
[models."MiniMax-M2".pricing."minimax"]
cache_creation_input_token_cost = 3.75e-7
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."MiniMax-M2".pricing."minimax-cn"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."MiniMax-M2-5-highspeed"]
display_name = "MiniMax-M2.5-highspeed"
model_family = "minimax"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000024
cache_read_input_token_cost = 6e-8
litellm_provider = "minimax"
providers = ["minimax", "minimax-cn", "minimax-cn-coding-plan", "minimax-coding-plan"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-02-13"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."MiniMax-M2-5-highspeed".pricing."minimax"]
cache_read_input_token_cost = 6e-8
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000024
[models."MiniMax-M2-5-highspeed".pricing."minimax-cn"]
cache_read_input_token_cost = 6e-8
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000024

[models."MiniMax-M2.1"]
display_name = "MiniMax-M2.1"
model_family = "minimax"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 8192
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
cache_read_input_token_cost = 3e-8
cache_creation_input_token_cost = 3.75e-7
litellm_provider = "minimax"
providers = ["minimax", "302ai", "aihubmix", "minimax-cn", "minimax-cn-coding-plan", "minimax-coding-plan", "moark", "opencode"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-12-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true
supports_tool_choice = true

[models."MiniMax-M2.1".pricing."302ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."MiniMax-M2.1".pricing."aihubmix"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.00000115
[models."MiniMax-M2.1".pricing."minimax"]
cache_creation_input_token_cost = 3.75e-7
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."MiniMax-M2.1".pricing."minimax-cn"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."MiniMax-M2.1".pricing."moark"]
input_cost_per_token = 0.0000021000000000000002
output_cost_per_token = 0.000008400000000000001
[models."MiniMax-M2.1".pricing."opencode"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."MiniMax-M2.1-lightning"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 8192
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000024
cache_read_input_token_cost = 3e-8
cache_creation_input_token_cost = 3.75e-7
litellm_provider = "minimax"
providers = ["minimax"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."MiniMax-M2.1-lightning".pricing."minimax"]
cache_creation_input_token_cost = 3.75e-7
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000024

[models."MiniMax-M2.5"]
display_name = "minimax-m2.5"
model_family = "minimax"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 8192
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
cache_read_input_token_cost = 3e-8
cache_creation_input_token_cost = 3.75e-7
litellm_provider = "minimax"
providers = ["minimax", "minimax-cn", "minimax-cn-coding-plan", "minimax-coding-plan", "opencode"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2026-02-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true
supports_tool_choice = true

[models."MiniMax-M2.5".pricing."minimax"]
cache_creation_input_token_cost = 3.75e-7
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."MiniMax-M2.5".pricing."minimax-cn"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."MiniMax-M2.5".pricing."opencode"]
cache_read_input_token_cost = 6e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."MiniMax-M2.5-lightning"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 8192
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000024
cache_read_input_token_cost = 3e-8
cache_creation_input_token_cost = 3.75e-7
litellm_provider = "minimax"
providers = ["minimax"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."MiniMax-M2.5-lightning".pricing."minimax"]
cache_creation_input_token_cost = 3.75e-7
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000024

[models."MiniMaxAI/MiniMax-M2-1-TEE"]
display_name = "MiniMax M2.1 TEE"
model_family = "minimax"
mode = "chat"
max_input_tokens = 196608
max_output_tokens = 65536
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.00000112
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."MiniMaxAI/MiniMax-M2-1-TEE".pricing."chutes"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.00000112

[models."MiniMaxAI/MiniMax-M2-5"]
display_name = "MiniMax-M2.5"
model_family = "minimax"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
litellm_provider = "togetherai"
providers = ["togetherai", "baseten", "huggingface", "meganova"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2026-01"
release_date = "2026-02-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."MiniMaxAI/MiniMax-M2-5".pricing."baseten"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."MiniMaxAI/MiniMax-M2-5".pricing."huggingface"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."MiniMaxAI/MiniMax-M2-5".pricing."meganova"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."MiniMaxAI/MiniMax-M2-5".pricing."togetherai"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."MiniMaxAI/MiniMax-M2-5-TEE"]
display_name = "MiniMax M2.5 TEE"
model_family = "minimax"
mode = "chat"
max_input_tokens = 196608
max_output_tokens = 65536
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-02-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."MiniMaxAI/MiniMax-M2-5-TEE".pricing."chutes"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."MiniMaxAI/MiniMax-M2.1"]
display_name = "MiniMax-M2.1"
model_family = "minimax"
mode = "chat"
max_input_tokens = 196608
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
litellm_provider = "gmi"
providers = ["gmi", "deepinfra", "friendli", "huggingface", "meganova", "nebius", "nvidia", "siliconflow"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-10"
release_date = "2025-12-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
reasoning_cost_per_token = 0.0000012

[models."MiniMaxAI/MiniMax-M2.1".pricing."deepinfra"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 0.0000012
[models."MiniMaxAI/MiniMax-M2.1".pricing."friendli"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."MiniMaxAI/MiniMax-M2.1".pricing."gmi"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."MiniMaxAI/MiniMax-M2.1".pricing."huggingface"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."MiniMaxAI/MiniMax-M2.1".pricing."meganova"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 0.0000012
[models."MiniMaxAI/MiniMax-M2.1".pricing."nebius"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
reasoning_cost_per_token = 0.0000012
[models."MiniMaxAI/MiniMax-M2.1".pricing."siliconflow"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."Mistral-7B-Instruct-v0.3"]
display_name = "Mistral-7B-Instruct-v0.3"
mode = "chat"
max_input_tokens = 127000
max_output_tokens = 127000
max_tokens = 127000
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "ovhcloud"
providers = ["ovhcloud"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-04-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://endpoints.ai.cloud.ovh.net/models/mistral-7b-instruct-v0-3"
supports_response_schema = true
supports_tool_choice = true

[models."Mistral-7B-Instruct-v0.3".pricing."ovhcloud"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."Mistral-Nemo-Instruct-2407"]
display_name = "Mistral-Nemo-Instruct-2407"
model_family = "mistral-nemo"
mode = "chat"
max_input_tokens = 118000
max_output_tokens = 118000
max_tokens = 118000
input_cost_per_token = 1.3e-7
output_cost_per_token = 1.3e-7
litellm_provider = "ovhcloud"
providers = ["ovhcloud", "scaleway"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-11-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://endpoints.ai.cloud.ovh.net/models/mistral-nemo-instruct-2407"
supports_response_schema = true
supports_tool_choice = true

[models."Mistral-Nemo-Instruct-2407".pricing."ovhcloud"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 1.3e-7
[models."Mistral-Nemo-Instruct-2407".pricing."scaleway"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7

[models."Mistral-Small-3.2-24B-Instruct-2506"]
display_name = "Mistral-Small-3.2-24B-Instruct-2506"
model_family = "mistral-small"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 9e-8
output_cost_per_token = 2.8e-7
litellm_provider = "ovhcloud"
providers = ["ovhcloud", "scaleway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
release_date = "2025-07-16"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://endpoints.ai.cloud.ovh.net/models/mistral-small-3-2-24b-instruct-2506"
supports_response_schema = true
supports_tool_choice = true

[models."Mistral-Small-3.2-24B-Instruct-2506".pricing."ovhcloud"]
input_cost_per_token = 9e-8
output_cost_per_token = 2.8e-7
[models."Mistral-Small-3.2-24B-Instruct-2506".pricing."scaleway"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 3.5e-7

[models."Mixtral-8x7B-Instruct-v0.1"]
display_name = "Mixtral-8x7B-Instruct-v0.1"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 6.3e-7
output_cost_per_token = 6.3e-7
litellm_provider = "ovhcloud"
providers = ["ovhcloud"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-04-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://endpoints.ai.cloud.ovh.net/models/mixtral-8x7b-instruct-v0-1"
supports_response_schema = true
supports_tool_choice = false

[models."Mixtral-8x7B-Instruct-v0.1".pricing."ovhcloud"]
input_cost_per_token = 6.3e-7
output_cost_per_token = 6.3e-7

[models."NousResearch/DeepHermes-3-Mistral-24B-Preview"]
display_name = "DeepHermes 3 Mistral 24B Preview"
model_family = "nousresearch"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
input_cost_per_token = 2e-8
output_cost_per_token = 1.0000000000000001e-7
litellm_provider = "chutes"
providers = ["chutes", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."NousResearch/DeepHermes-3-Mistral-24B-Preview".pricing."chutes"]
input_cost_per_token = 2e-8
output_cost_per_token = 1.0000000000000001e-7
[models."NousResearch/DeepHermes-3-Mistral-24B-Preview".pricing."kilo"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 2e-8
output_cost_per_token = 1.0000000000000001e-7

[models."NousResearch/Hermes-3-Llama-3.1-405B"]
display_name = "Nous: Hermes 3 405B Instruct"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001
litellm_provider = "deepinfra"
providers = ["deepinfra", "kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2024-08-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."NousResearch/Hermes-3-Llama-3.1-405B".pricing."deepinfra"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001
[models."NousResearch/Hermes-3-Llama-3.1-405B".pricing."kilo"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001

[models."NousResearch/Hermes-3-Llama-3.1-70B"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 3e-7
output_cost_per_token = 3e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "hyperbolic"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."NousResearch/Hermes-3-Llama-3.1-70B".pricing."deepinfra"]
input_cost_per_token = 3e-7
output_cost_per_token = 3e-7
[models."NousResearch/Hermes-3-Llama-3.1-70B".pricing."hyperbolic"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7

[models."NousResearch/Hermes-4-14B"]
display_name = "Hermes 4 14B"
model_family = "nousresearch"
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 40960
input_cost_per_token = 1e-8
output_cost_per_token = 5.0000000000000004e-8
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."NousResearch/Hermes-4-14B".pricing."chutes"]
input_cost_per_token = 1e-8
output_cost_per_token = 5.0000000000000004e-8

[models."NousResearch/Hermes-4-3-36B"]
display_name = "Hermes 4.3 36B"
model_family = "nousresearch"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3.9e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."NousResearch/Hermes-4-3-36B".pricing."chutes"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3.9e-7

[models."NousResearch/Hermes-4-405B-FP8-TEE"]
display_name = "Hermes 4 405B FP8 TEE"
model_family = "nousresearch"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 65536
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."NousResearch/Hermes-4-405B-FP8-TEE".pricing."chutes"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."NousResearch/hermes-4-405b"]
display_name = "Hermes-4-405B"
model_family = "hermes"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
cache_read_input_token_cost = 1.0000000000000001e-7
litellm_provider = "nebius"
providers = ["nebius", "kilo", "openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-11"
release_date = "2026-01-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"
reasoning_cost_per_token = 0.000003

[models."NousResearch/hermes-4-405b".pricing."kilo"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
[models."NousResearch/hermes-4-405b".pricing."nebius"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
reasoning_cost_per_token = 0.000003
[models."NousResearch/hermes-4-405b".pricing."openrouter"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003

[models."NousResearch/hermes-4-70b"]
display_name = "Hermes-4-70B"
model_family = "nousresearch"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 1.3e-7
output_cost_per_token = 4.0000000000000003e-7
cache_read_input_token_cost = 1.2999999999999999e-8
litellm_provider = "nebius"
providers = ["nebius", "chutes", "kilo", "openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-11"
release_date = "2026-01-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"
reasoning_cost_per_token = 4.0000000000000003e-7

[models."NousResearch/hermes-4-70b".pricing."chutes"]
input_cost_per_token = 1.1e-7
output_cost_per_token = 3.8e-7
[models."NousResearch/hermes-4-70b".pricing."kilo"]
cache_read_input_token_cost = 5.5e-8
input_cost_per_token = 1.1e-7
output_cost_per_token = 3.8e-7
[models."NousResearch/hermes-4-70b".pricing."nebius"]
cache_read_input_token_cost = 1.2999999999999999e-8
input_cost_per_token = 1.3e-7
output_cost_per_token = 4.0000000000000003e-7
reasoning_cost_per_token = 4.0000000000000003e-7
[models."NousResearch/hermes-4-70b".pricing."openrouter"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 4.0000000000000003e-7

[models."OpenGVLab/InternVL3-78B-TEE"]
display_name = "InternVL3 78B TEE"
model_family = "opengvlab"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3.9e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-01-06"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."OpenGVLab/InternVL3-78B-TEE".pricing."chutes"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3.9e-7

[models."PaddlePaddle/PaddleOCR-VL-1-5"]
display_name = "PaddlePaddle/PaddleOCR-VL-1.5"
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2026-01-29"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Phi-3-medium-128k-instruct"]
display_name = "Phi-3-medium-instruct (128k)"
model_family = "phi"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.7e-7
output_cost_per_token = 6.8e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = false
supports_vision = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-04-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true

[models."Phi-3-medium-128k-instruct".pricing."azure"]
input_cost_per_token = 1.7000000000000001e-7
output_cost_per_token = 6.800000000000001e-7
[models."Phi-3-medium-128k-instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 1.7000000000000001e-7
output_cost_per_token = 6.800000000000001e-7
[models."Phi-3-medium-128k-instruct".pricing."azure_ai"]
input_cost_per_token = 1.7e-7
output_cost_per_token = 6.8e-7

[models."Phi-3-medium-4k-instruct"]
display_name = "Phi-3-medium-instruct (4k)"
model_family = "phi"
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.7e-7
output_cost_per_token = 6.8e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = false
supports_vision = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-04-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true

[models."Phi-3-medium-4k-instruct".pricing."azure"]
input_cost_per_token = 1.7000000000000001e-7
output_cost_per_token = 6.800000000000001e-7
[models."Phi-3-medium-4k-instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 1.7000000000000001e-7
output_cost_per_token = 6.800000000000001e-7
[models."Phi-3-medium-4k-instruct".pricing."azure_ai"]
input_cost_per_token = 1.7e-7
output_cost_per_token = 6.8e-7

[models."Phi-3-mini-128k-instruct"]
display_name = "Phi-3-mini-instruct (128k)"
model_family = "phi"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.3e-7
output_cost_per_token = 5.2e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = false
supports_vision = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-04-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true

[models."Phi-3-mini-128k-instruct".pricing."azure"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 5.2e-7
[models."Phi-3-mini-128k-instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 5.2e-7
[models."Phi-3-mini-128k-instruct".pricing."azure_ai"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 5.2e-7

[models."Phi-3-mini-4k-instruct"]
display_name = "Phi-3-mini-instruct (4k)"
model_family = "phi"
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.3e-7
output_cost_per_token = 5.2e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = false
supports_vision = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-04-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true

[models."Phi-3-mini-4k-instruct".pricing."azure"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 5.2e-7
[models."Phi-3-mini-4k-instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 5.2e-7
[models."Phi-3-mini-4k-instruct".pricing."azure_ai"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 5.2e-7

[models."Phi-3-small-128k-instruct"]
display_name = "Phi-3-small-instruct (128k)"
model_family = "phi"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = false
supports_vision = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-04-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true

[models."Phi-3-small-128k-instruct".pricing."azure"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."Phi-3-small-128k-instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."Phi-3-small-128k-instruct".pricing."azure_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."Phi-3-small-8k-instruct"]
display_name = "Phi-3-small-instruct (8k)"
model_family = "phi"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = false
supports_vision = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-04-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true

[models."Phi-3-small-8k-instruct".pricing."azure"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."Phi-3-small-8k-instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."Phi-3-small-8k-instruct".pricing."azure_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."Phi-3.5-MoE-instruct"]
display_name = "Phi-3.5-MoE-instruct"
model_family = "phi"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.6e-7
output_cost_per_token = 6.4e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = false
supports_vision = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-08-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true

[models."Phi-3.5-MoE-instruct".pricing."azure"]
input_cost_per_token = 1.6e-7
output_cost_per_token = 6.4e-7
[models."Phi-3.5-MoE-instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 1.6e-7
output_cost_per_token = 6.4e-7
[models."Phi-3.5-MoE-instruct".pricing."azure_ai"]
input_cost_per_token = 1.6e-7
output_cost_per_token = 6.4e-7

[models."Phi-3.5-mini-instruct"]
display_name = "Phi-3.5-mini-instruct"
model_family = "phi"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.3e-7
output_cost_per_token = 5.2e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = false
supports_vision = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-08-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true

[models."Phi-3.5-mini-instruct".pricing."azure"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 5.2e-7
[models."Phi-3.5-mini-instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 5.2e-7
[models."Phi-3.5-mini-instruct".pricing."azure_ai"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 5.2e-7

[models."Phi-3.5-vision-instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.3e-7
output_cost_per_token = 5.2e-7
litellm_provider = "azure_ai"
providers = ["azure_ai"]
supports_vision = true
source = "https://azure.microsoft.com/en-us/pricing/details/phi-3/"
supports_tool_choice = true

[models."Phi-3.5-vision-instruct".pricing."azure_ai"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 5.2e-7

[models."Phi-4"]
display_name = "Phi-4"
model_family = "phi"
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1.25e-7
output_cost_per_token = 5e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = true
supports_vision = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-12-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://techcommunity.microsoft.com/blog/machinelearningblog/affordable-innovation-unveiling-the-pricing-of-phi-3-slms-on-models-as-a-service/4156495"
supports_tool_choice = true

[models."Phi-4".pricing."azure"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 5e-7
[models."Phi-4".pricing."azure-cognitive-services"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 5e-7
[models."Phi-4".pricing."azure_ai"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 5e-7

[models."Phi-4-mini-instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
litellm_provider = "azure_ai"
providers = ["azure_ai"]
supports_function_calling = true
source = "https://techcommunity.microsoft.com/blog/Azure-AI-Services-blog/announcing-new-phi-pricing-empowering-your-business-with-small-language-models/4395112"

[models."Phi-4-mini-instruct".pricing."azure_ai"]
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7

[models."Phi-4-mini-reasoning"]
display_name = "Phi-4-mini-reasoning"
model_family = "phi"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 8e-8
output_cost_per_token = 3.2e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-12-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/pricing/details/ai-foundry-models/microsoft/"

[models."Phi-4-mini-reasoning".pricing."azure"]
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."Phi-4-mini-reasoning".pricing."azure-cognitive-services"]
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."Phi-4-mini-reasoning".pricing."azure_ai"]
input_cost_per_token = 8e-8
output_cost_per_token = 3.2e-7

[models."Phi-4-multimodal-instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 8e-8
output_cost_per_token = 3.2e-7
litellm_provider = "azure_ai"
providers = ["azure_ai"]
supports_function_calling = true
supports_vision = true
source = "https://techcommunity.microsoft.com/blog/Azure-AI-Services-blog/announcing-new-phi-pricing-empowering-your-business-with-small-language-models/4395112"
input_cost_per_audio_token = 0.000004
supports_audio_input = true

[models."Phi-4-multimodal-instruct".pricing."azure_ai"]
input_cost_per_audio_token = 0.000004
input_cost_per_token = 8e-8
output_cost_per_token = 3.2e-7

[models."Phi-4-reasoning"]
display_name = "Phi-4-reasoning"
model_family = "phi"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.25e-7
output_cost_per_token = 5e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-12-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/pricing/details/ai-foundry-models/microsoft/"
supports_tool_choice = true

[models."Phi-4-reasoning".pricing."azure"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 5e-7
[models."Phi-4-reasoning".pricing."azure-cognitive-services"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 5e-7
[models."Phi-4-reasoning".pricing."azure_ai"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 5e-7

[models."PrimeIntellect/intellect-3"]
display_name = "INTELLECT-3"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000011
cache_read_input_token_cost = 2e-8
litellm_provider = "nebius"
providers = ["nebius"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-10"
release_date = "2026-01-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."PrimeIntellect/intellect-3".pricing."nebius"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000011

[models."Pro/MiniMaxAI/MiniMax-M2-1"]
display_name = "Pro/MiniMaxAI/MiniMax-M2.1"
model_family = "minimax"
mode = "chat"
max_input_tokens = 197000
max_output_tokens = 131000
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-12-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Pro/MiniMaxAI/MiniMax-M2-1".pricing."siliconflow-cn"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."Pro/MiniMaxAI/MiniMax-M2-5"]
display_name = "Pro/MiniMaxAI/MiniMax-M2.5"
model_family = "minimax"
mode = "chat"
max_input_tokens = 192000
max_output_tokens = 131000
input_cost_per_token = 3e-7
output_cost_per_token = 0.00000122
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2026-02-13"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Pro/MiniMaxAI/MiniMax-M2-5".pricing."siliconflow-cn"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.00000122

[models."Pro/deepseek-ai/DeepSeek-R1"]
display_name = "Pro/deepseek-ai/DeepSeek-R1"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 164000
max_output_tokens = 164000
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000021800000000000003
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-05-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Pro/deepseek-ai/DeepSeek-R1".pricing."siliconflow-cn"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000021800000000000003

[models."Pro/deepseek-ai/DeepSeek-V3"]
display_name = "Pro/deepseek-ai/DeepSeek-V3"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 164000
max_output_tokens = 164000
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-12-26"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Pro/deepseek-ai/DeepSeek-V3".pricing."siliconflow-cn"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001

[models."Pro/deepseek-ai/DeepSeek-V3-1-Terminus"]
display_name = "Pro/deepseek-ai/DeepSeek-V3.1-Terminus"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 164000
max_output_tokens = 164000
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-09-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Pro/deepseek-ai/DeepSeek-V3-1-Terminus".pricing."siliconflow-cn"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001

[models."Pro/deepseek-ai/DeepSeek-V3-2"]
display_name = "Pro/deepseek-ai/DeepSeek-V3.2"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 164000
max_output_tokens = 164000
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.2e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-12-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Pro/deepseek-ai/DeepSeek-V3-2".pricing."siliconflow-cn"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.2e-7

[models."Pro/moonshotai/Kimi-K2-5"]
display_name = "Pro/moonshotai/Kimi-K2.5"
model_family = "kimi"
mode = "chat"
max_input_tokens = 262000
max_output_tokens = 262000
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.000003
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2026-01-27"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Pro/moonshotai/Kimi-K2-5".pricing."siliconflow-cn"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.000003

[models."Pro/moonshotai/Kimi-K2-Instruct-0905"]
display_name = "Pro/moonshotai/Kimi-K2-Instruct-0905"
model_family = "kimi"
mode = "chat"
max_input_tokens = 262000
max_output_tokens = 262000
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-09-08"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Pro/moonshotai/Kimi-K2-Instruct-0905".pricing."siliconflow-cn"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002

[models."Pro/moonshotai/Kimi-K2-Thinking"]
display_name = "Pro/moonshotai/Kimi-K2-Thinking"
model_family = "kimi-thinking"
mode = "chat"
max_input_tokens = 262000
max_output_tokens = 262000
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000025
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-11-07"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Pro/moonshotai/Kimi-K2-Thinking".pricing."siliconflow-cn"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000025

[models."Pro/zai-org/GLM-4-7"]
display_name = "Pro/zai-org/GLM-4.7"
model_family = "glm"
mode = "chat"
max_input_tokens = 205000
max_output_tokens = 205000
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-12-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Pro/zai-org/GLM-4-7".pricing."siliconflow-cn"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022

[models."Pro/zai-org/GLM-5"]
display_name = "Pro/zai-org/GLM-5"
model_family = "glm"
mode = "chat"
max_input_tokens = 205000
max_output_tokens = 205000
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2026-02-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Pro/zai-org/GLM-5".pricing."siliconflow-cn"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003

[models."QwQ-32B"]
display_name = "QwQ 32B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 5e-7
output_cost_per_token = 0.000001
litellm_provider = "sambanova"
providers = ["sambanova", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://cloud.sambanova.ai/plans/pricing"

[models."QwQ-32B".pricing."alibaba-cn"]
input_cost_per_token = 2.8699999999999996e-7
output_cost_per_token = 8.61e-7
[models."QwQ-32B".pricing."sambanova"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.000001

[models."Qwen/QwQ-32B"]
display_name = "Qwq 32b"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.5e-7
output_cost_per_token = 4e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "abacus", "hyperbolic", "kilo", "nscale", "nvidia", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-03-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."Qwen/QwQ-32B".pricing."abacus"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 4.0000000000000003e-7
[models."Qwen/QwQ-32B".pricing."deepinfra"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 4e-7
[models."Qwen/QwQ-32B".pricing."hyperbolic"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
[models."Qwen/QwQ-32B".pricing."kilo"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 4.0000000000000003e-7
[models."Qwen/QwQ-32B".pricing."nscale"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 2e-7
[models."Qwen/QwQ-32B".pricing."siliconflow"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 5.8e-7
[models."Qwen/QwQ-32B".pricing."siliconflow-cn"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 5.8e-7

[models."Qwen/Qwen2-5-14B-Instruct"]
display_name = "Qwen/Qwen2.5-14B-Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 33000
max_output_tokens = 4000
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 1.0000000000000001e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "siliconflow"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-09-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen2-5-14B-Instruct".pricing."siliconflow"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 1.0000000000000001e-7
[models."Qwen/Qwen2-5-14B-Instruct".pricing."siliconflow-cn"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 1.0000000000000001e-7

[models."Qwen/Qwen2-5-32B-Instruct"]
display_name = "Qwen/Qwen2.5-32B-Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 33000
max_output_tokens = 4000
input_cost_per_token = 1.8e-7
output_cost_per_token = 1.8e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "siliconflow"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-09-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen2-5-32B-Instruct".pricing."siliconflow"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 1.8e-7
[models."Qwen/Qwen2-5-32B-Instruct".pricing."siliconflow-cn"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 1.8e-7

[models."Qwen/Qwen2-5-72B-Instruct-128K"]
display_name = "Qwen/Qwen2.5-72B-Instruct-128K"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 4000
input_cost_per_token = 5.9e-7
output_cost_per_token = 5.9e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "siliconflow"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-09-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen2-5-72B-Instruct-128K".pricing."siliconflow"]
input_cost_per_token = 5.9e-7
output_cost_per_token = 5.9e-7
[models."Qwen/Qwen2-5-72B-Instruct-128K".pricing."siliconflow-cn"]
input_cost_per_token = 5.9e-7
output_cost_per_token = 5.9e-7

[models."Qwen/Qwen2-5-VL-72B-Instruct-TEE"]
display_name = "Qwen2.5 VL 72B Instruct TEE"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen2-5-VL-72B-Instruct-TEE".pricing."chutes"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."Qwen/Qwen2-5-VL-7B-Instruct"]
display_name = "Qwen/Qwen2.5-VL-7B-Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 33000
max_output_tokens = 4000
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 5.0000000000000004e-8
litellm_provider = "siliconflow"
providers = ["siliconflow"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-01-28"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen2-5-VL-7B-Instruct".pricing."siliconflow"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 5.0000000000000004e-8

[models."Qwen/Qwen2.5-72B-Instruct"]
display_name = "Qwen 2.5 72B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.2e-7
output_cost_per_token = 3.9e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "abacus", "chutes", "hyperbolic", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-09-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."Qwen/Qwen2.5-72B-Instruct".pricing."abacus"]
input_cost_per_token = 1.1e-7
output_cost_per_token = 3.8e-7
[models."Qwen/Qwen2.5-72B-Instruct".pricing."chutes"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 5.2e-7
[models."Qwen/Qwen2.5-72B-Instruct".pricing."deepinfra"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 3.9e-7
[models."Qwen/Qwen2.5-72B-Instruct".pricing."hyperbolic"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7
[models."Qwen/Qwen2.5-72B-Instruct".pricing."siliconflow"]
input_cost_per_token = 5.9e-7
output_cost_per_token = 5.9e-7
[models."Qwen/Qwen2.5-72B-Instruct".pricing."siliconflow-cn"]
input_cost_per_token = 5.9e-7
output_cost_per_token = 5.9e-7

[models."Qwen/Qwen2.5-72B-Instruct-Turbo"]
mode = "chat"
litellm_provider = "together_ai"
providers = ["together_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."Qwen/Qwen2.5-7B-Instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 4e-8
output_cost_per_token = 1e-7
litellm_provider = "deepinfra"
providers = ["deepinfra"]
supports_tool_choice = false

[models."Qwen/Qwen2.5-7B-Instruct".pricing."deepinfra"]
input_cost_per_token = 4e-8
output_cost_per_token = 1e-7

[models."Qwen/Qwen2.5-7B-Instruct-Turbo"]
mode = "chat"
litellm_provider = "together_ai"
providers = ["together_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."Qwen/Qwen2.5-Coder-32B-Instruct"]
display_name = "Qwen2.5 Coder 32b Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7
litellm_provider = "hyperbolic"
providers = ["hyperbolic", "chutes", "nscale", "nvidia", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-11-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."Qwen/Qwen2.5-Coder-32B-Instruct".pricing."chutes"]
input_cost_per_token = 3e-8
output_cost_per_token = 1.1e-7
[models."Qwen/Qwen2.5-Coder-32B-Instruct".pricing."hyperbolic"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7
[models."Qwen/Qwen2.5-Coder-32B-Instruct".pricing."nscale"]
input_cost_per_token = 6e-8
output_cost_per_token = 2e-7
[models."Qwen/Qwen2.5-Coder-32B-Instruct".pricing."siliconflow"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 1.8e-7
[models."Qwen/Qwen2.5-Coder-32B-Instruct".pricing."siliconflow-cn"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 1.8e-7

[models."Qwen/Qwen2.5-Coder-3B-Instruct"]
mode = "chat"
input_cost_per_token = 1e-8
output_cost_per_token = 3e-8
litellm_provider = "nscale"
providers = ["nscale"]
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."Qwen/Qwen2.5-Coder-3B-Instruct".pricing."nscale"]
input_cost_per_token = 1e-8
output_cost_per_token = 3e-8

[models."Qwen/Qwen2.5-Coder-7B-Instruct"]
display_name = "Qwen2.5 Coder 7b Instruct"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
input_cost_per_token = 1e-8
output_cost_per_token = 3e-8
litellm_provider = "nscale"
providers = ["nscale", "kilo", "nvidia"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-09-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."Qwen/Qwen2.5-Coder-7B-Instruct".pricing."kilo"]
input_cost_per_token = 3e-8
output_cost_per_token = 9e-8
[models."Qwen/Qwen2.5-Coder-7B-Instruct".pricing."nscale"]
input_cost_per_token = 1e-8
output_cost_per_token = 3e-8

[models."Qwen/Qwen2.5-VL-32B-Instruct"]
display_name = "Qwen/Qwen2.5-VL-32B-Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "chutes", "io-net", "kilo", "meganova", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-09"
release_date = "2025-03-24"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."Qwen/Qwen2.5-VL-32B-Instruct".pricing."chutes"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.2e-7
[models."Qwen/Qwen2.5-VL-32B-Instruct".pricing."deepinfra"]
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7
[models."Qwen/Qwen2.5-VL-32B-Instruct".pricing."io-net"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.2e-7
[models."Qwen/Qwen2.5-VL-32B-Instruct".pricing."kilo"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.2e-7
[models."Qwen/Qwen2.5-VL-32B-Instruct".pricing."meganova"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7
[models."Qwen/Qwen2.5-VL-32B-Instruct".pricing."siliconflow"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 2.7e-7
[models."Qwen/Qwen2.5-VL-32B-Instruct".pricing."siliconflow-cn"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 2.7e-7

[models."Qwen/Qwen3-14B"]
display_name = "Qwen/Qwen3-14B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "chutes", "kilo", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-04-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."Qwen/Qwen3-14B".pricing."chutes"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.2e-7
[models."Qwen/Qwen3-14B".pricing."deepinfra"]
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7
[models."Qwen/Qwen3-14B".pricing."kilo"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.2e-7
[models."Qwen/Qwen3-14B".pricing."siliconflow"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
[models."Qwen/Qwen3-14B".pricing."siliconflow-cn"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7

[models."Qwen/Qwen3-235B-A22B"]
display_name = "Qwen3-235B-A22B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.8e-7
output_cost_per_token = 5.4e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "chutes", "hyperbolic", "kilo", "nvidia", "siliconflow"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-12"
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."Qwen/Qwen3-235B-A22B".pricing."chutes"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."Qwen/Qwen3-235B-A22B".pricing."deepinfra"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 5.4e-7
[models."Qwen/Qwen3-235B-A22B".pricing."hyperbolic"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002
[models."Qwen/Qwen3-235B-A22B".pricing."kilo"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."Qwen/Qwen3-235B-A22B".pricing."siliconflow"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000142

[models."Qwen/Qwen3-235B-A22B-Instruct-2507"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 9e-8
output_cost_per_token = 6e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "wandb"]
supports_tool_choice = true

[models."Qwen/Qwen3-235B-A22B-Instruct-2507".pricing."deepinfra"]
input_cost_per_token = 9e-8
output_cost_per_token = 6e-7
[models."Qwen/Qwen3-235B-A22B-Instruct-2507".pricing."wandb"]
input_cost_per_token = 0.01
output_cost_per_token = 0.01

[models."Qwen/Qwen3-235B-A22B-Instruct-2507-TEE"]
display_name = "Qwen3 235B A22B Instruct 2507 TEE"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65536
input_cost_per_token = 8e-8
output_cost_per_token = 5.5e-7
cache_read_input_token_cost = 4e-8
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen3-235B-A22B-Instruct-2507-TEE".pricing."chutes"]
cache_read_input_token_cost = 4e-8
input_cost_per_token = 8e-8
output_cost_per_token = 5.5e-7

[models."Qwen/Qwen3-235B-A22B-Instruct-2507-tput"]
display_name = "Qwen3 235B A22B Instruct 2507 FP8"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262000
max_output_tokens = 262144
input_cost_per_token = 2e-7
output_cost_per_token = 0.000006
litellm_provider = "together_ai"
providers = ["together_ai", "togetherai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2025-07-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://www.together.ai/models/qwen3-235b-a22b-instruct-2507-fp8"
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."Qwen/Qwen3-235B-A22B-Instruct-2507-tput".pricing."together_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 0.000006
[models."Qwen/Qwen3-235B-A22B-Instruct-2507-tput".pricing."togetherai"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7

[models."Qwen/Qwen3-235B-A22B-Thinking-2507"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000029
litellm_provider = "deepinfra"
providers = ["deepinfra", "together_ai", "wandb"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."Qwen/Qwen3-235B-A22B-Thinking-2507".pricing."deepinfra"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000029
[models."Qwen/Qwen3-235B-A22B-Thinking-2507".pricing."together_ai"]
input_cost_per_token = 6.5e-7
output_cost_per_token = 0.000003
[models."Qwen/Qwen3-235B-A22B-Thinking-2507".pricing."wandb"]
input_cost_per_token = 0.01
output_cost_per_token = 0.01

[models."Qwen/Qwen3-235B-A22B-fp8-tput"]
mode = "chat"
max_input_tokens = 40000
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7
litellm_provider = "together_ai"
providers = ["together_ai"]
supports_function_calling = false
source = "https://www.together.ai/models/qwen3-235b-a22b-fp8-tput"
supports_parallel_function_calling = false
supports_tool_choice = false

[models."Qwen/Qwen3-235B-A22B-fp8-tput".pricing."together_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7

[models."Qwen/Qwen3-30B-A3B"]
display_name = "Qwen3 30B A3B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
input_cost_per_token = 8e-8
output_cost_per_token = 2.9e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "chutes", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."Qwen/Qwen3-30B-A3B".pricing."chutes"]
input_cost_per_token = 6e-8
output_cost_per_token = 2.2e-7
[models."Qwen/Qwen3-30B-A3B".pricing."deepinfra"]
input_cost_per_token = 8e-8
output_cost_per_token = 2.9e-7
[models."Qwen/Qwen3-30B-A3B".pricing."kilo"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 6e-8
output_cost_per_token = 2.2e-7

[models."Qwen/Qwen3-30B-A3B-Instruct-2507-FP8"]
display_name = "Qwen3 30B 2507"
model_family = "qwen"
mode = "chat"
max_input_tokens = 64000
max_output_tokens = 64000
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000142
litellm_provider = "evroc"
providers = ["evroc"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-07-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen3-30B-A3B-Instruct-2507-FP8".pricing."evroc"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000142

[models."Qwen/Qwen3-32B"]
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
input_cost_per_token = 1e-7
output_cost_per_token = 2.8e-7
litellm_provider = "deepinfra"
providers = ["deepinfra"]
supports_tool_choice = true

[models."Qwen/Qwen3-32B".pricing."deepinfra"]
input_cost_per_token = 1e-7
output_cost_per_token = 2.8e-7

[models."Qwen/Qwen3-5-397B-A17B-TEE"]
display_name = "Qwen3.5 397B A17B TEE"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65536
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
cache_read_input_token_cost = 1.5e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-02-18"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen3-5-397B-A17B-TEE".pricing."chutes"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."Qwen/Qwen3-5-Plus"]
display_name = "Qwen3.5 Plus"
model_family = "qwen"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 65536
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000024
litellm_provider = "meganova"
providers = ["meganova", "nano-gpt"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2026-02"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"
reasoning_cost_per_token = 0.0000024

[models."Qwen/Qwen3-5-Plus".pricing."meganova"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000024
reasoning_cost_per_token = 0.0000024
[models."Qwen/Qwen3-5-Plus".pricing."nano-gpt"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000024

[models."Qwen/Qwen3-8B"]
display_name = "Qwen/Qwen3-8B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 131000
input_cost_per_token = 6e-8
output_cost_per_token = 6e-8
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "kilo", "siliconflow"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-04-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen3-8B".pricing."kilo"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 4.0000000000000003e-7
[models."Qwen/Qwen3-8B".pricing."siliconflow"]
input_cost_per_token = 6e-8
output_cost_per_token = 6e-8
[models."Qwen/Qwen3-8B".pricing."siliconflow-cn"]
input_cost_per_token = 6e-8
output_cost_per_token = 6e-8

[models."Qwen/Qwen3-Coder-480B-A35B-Instruct"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000016
litellm_provider = "deepinfra"
providers = ["deepinfra", "wandb"]
supports_tool_choice = true

[models."Qwen/Qwen3-Coder-480B-A35B-Instruct".pricing."deepinfra"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000016
[models."Qwen/Qwen3-Coder-480B-A35B-Instruct".pricing."wandb"]
input_cost_per_token = 0.1
output_cost_per_token = 0.15

[models."Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"]
display_name = "Qwen3 Coder 480B A35B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 262144
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002
litellm_provider = "together_ai"
providers = ["together_ai", "submodel", "togetherai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://www.together.ai/models/qwen3-coder-480b-a35b-instruct"
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8".pricing."submodel"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7
[models."Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8".pricing."together_ai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002
[models."Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8".pricing."togetherai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002

[models."Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8-TEE"]
display_name = "Qwen3 Coder 480B A35B Instruct FP8 TEE"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
input_cost_per_token = 2.2e-7
output_cost_per_token = 9.499999999999999e-7
cache_read_input_token_cost = 1.1e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8-TEE".pricing."chutes"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 2.2e-7
output_cost_per_token = 9.499999999999999e-7

[models."Qwen/Qwen3-Coder-480B-A35B-Instruct-Turbo"]
display_name = "Qwen3 Coder 480B A35B Instruct Turbo"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.0000012
litellm_provider = "deepinfra"
providers = ["deepinfra"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."Qwen/Qwen3-Coder-480B-A35B-Instruct-Turbo".pricing."deepinfra"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.0000012

[models."Qwen/Qwen3-Coder-Next-FP8"]
display_name = "Qwen3 Coder Next FP8"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000012
litellm_provider = "togetherai"
providers = ["togetherai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2026-02-03"
release_date = "2026-02-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen3-Coder-Next-FP8".pricing."togetherai"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000012

[models."Qwen/Qwen3-Embedding-4B"]
display_name = "Qwen 3 Embedding 4B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 2048
input_cost_per_token = 1e-8
litellm_provider = "huggingface"
providers = ["huggingface", "inference"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen3-Embedding-4B".pricing."huggingface"]
input_cost_per_token = 1e-8
[models."Qwen/Qwen3-Embedding-4B".pricing."inference"]
input_cost_per_token = 1e-8

[models."Qwen/Qwen3-Next-80B-A3B-Instruct"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 1.4e-7
output_cost_per_token = 0.0000014
litellm_provider = "deepinfra"
providers = ["deepinfra", "together_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."Qwen/Qwen3-Next-80B-A3B-Instruct".pricing."deepinfra"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 0.0000014
[models."Qwen/Qwen3-Next-80B-A3B-Instruct".pricing."together_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000015

[models."Qwen/Qwen3-Next-80B-A3B-Thinking"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 1.4e-7
output_cost_per_token = 0.0000014
litellm_provider = "deepinfra"
providers = ["deepinfra", "together_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."Qwen/Qwen3-Next-80B-A3B-Thinking".pricing."deepinfra"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 0.0000014
[models."Qwen/Qwen3-Next-80B-A3B-Thinking".pricing."together_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000015

[models."Qwen/Qwen3-Omni-30B-A3B-Captioner"]
display_name = "Qwen/Qwen3-Omni-30B-A3B-Captioner"
model_family = "qwen"
mode = "chat"
max_input_tokens = 66000
max_output_tokens = 66000
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "siliconflow"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-10-04"
supported_modalities = ["audio"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen3-Omni-30B-A3B-Captioner".pricing."siliconflow"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."Qwen/Qwen3-Omni-30B-A3B-Captioner".pricing."siliconflow-cn"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7

[models."Qwen/Qwen3-VL-235B-A22B-Instruct-FP8"]
display_name = "Qwen3-VL 235B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000014
litellm_provider = "gmi"
providers = ["gmi", "stackit"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
release_date = "2024-11-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]

[models."Qwen/Qwen3-VL-235B-A22B-Instruct-FP8".pricing."gmi"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000014
[models."Qwen/Qwen3-VL-235B-A22B-Instruct-FP8".pricing."stackit"]
input_cost_per_token = 0.00000164
output_cost_per_token = 0.00000191

[models."Qwen/Qwen3-VL-32B-Instruct"]
display_name = "Qwen/Qwen3-VL-32B-Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262000
max_output_tokens = 262000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "kilo", "siliconflow"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-10-21"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen3-VL-32B-Instruct".pricing."kilo"]
input_cost_per_token = 1.0399999999999999e-7
output_cost_per_token = 4.1599999999999997e-7
[models."Qwen/Qwen3-VL-32B-Instruct".pricing."siliconflow"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7
[models."Qwen/Qwen3-VL-32B-Instruct".pricing."siliconflow-cn"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7

[models."Qwen/Qwen3-VL-32B-Thinking"]
display_name = "Qwen/Qwen3-VL-32B-Thinking"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262000
max_output_tokens = 262000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "siliconflow"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-10-21"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen3-VL-32B-Thinking".pricing."siliconflow"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015
[models."Qwen/Qwen3-VL-32B-Thinking".pricing."siliconflow-cn"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015

[models."Qwen/Qwen3-VL-8B-Thinking"]
display_name = "Qwen/Qwen3-VL-8B-Thinking"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262000
max_output_tokens = 262000
input_cost_per_token = 1.8e-7
output_cost_per_token = 0.000002
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "kilo", "siliconflow"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-10-15"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen3-VL-8B-Thinking".pricing."kilo"]
input_cost_per_token = 1.17e-7
output_cost_per_token = 0.000001365
[models."Qwen/Qwen3-VL-8B-Thinking".pricing."siliconflow"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 0.000002
[models."Qwen/Qwen3-VL-8B-Thinking".pricing."siliconflow-cn"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 0.000002

[models."Qwen/Qwen3-VL-Embedding-8B"]
display_name = "Qwen3-VL Embedding 8B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 4096
input_cost_per_token = 9e-8
output_cost_per_token = 9e-8
litellm_provider = "stackit"
providers = ["stackit"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2026-02-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen3-VL-Embedding-8B".pricing."stackit"]
input_cost_per_token = 9e-8
output_cost_per_token = 9e-8

[models."Qwen/Qwen3Guard-Gen-0-6B"]
display_name = "Qwen3Guard Gen 0.6B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8
cache_read_input_token_cost = 5e-9
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Qwen/Qwen3Guard-Gen-0-6B".pricing."chutes"]
cache_read_input_token_cost = 5e-9
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8

[models."Qwen2-Audio-7B-Instruct"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 5e-7
output_cost_per_token = 0.0001
litellm_provider = "sambanova"
providers = ["sambanova"]
source = "https://cloud.sambanova.ai/plans/pricing"
supports_audio_input = true

[models."Qwen2-Audio-7B-Instruct".pricing."sambanova"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0001

[models."Qwen2.5-Coder-32B-Instruct"]
display_name = "Qwen2.5 Coder 32B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 8.7e-7
output_cost_per_token = 8.7e-7
litellm_provider = "ovhcloud"
providers = ["ovhcloud", "vultr"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2024-11-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://endpoints.ai.cloud.ovh.net/models/qwen2-5-coder-32b-instruct"
supports_response_schema = true
supports_tool_choice = false

[models."Qwen2.5-Coder-32B-Instruct".pricing."ovhcloud"]
input_cost_per_token = 8.7e-7
output_cost_per_token = 8.7e-7
[models."Qwen2.5-Coder-32B-Instruct".pricing."vultr"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7

[models."Qwen2.5-VL-72B-Instruct"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 9.1e-7
output_cost_per_token = 9.1e-7
litellm_provider = "ovhcloud"
providers = ["ovhcloud"]
supports_function_calling = false
supports_vision = true
source = "https://endpoints.ai.cloud.ovh.net/models/qwen2-5-vl-72b-instruct"
supports_response_schema = true
supports_tool_choice = false

[models."Qwen2.5-VL-72B-Instruct".pricing."ovhcloud"]
input_cost_per_token = 9.1e-7
output_cost_per_token = 9.1e-7

[models."Qwen3-32B"]
display_name = "Qwen3 32B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 8e-8
output_cost_per_token = 2.3e-7
litellm_provider = "ovhcloud"
providers = ["ovhcloud", "alibaba", "alibaba-cn", "cortecs", "helicone", "iflowcn", "sambanova"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://endpoints.ai.cloud.ovh.net/models/qwen3-32b"
reasoning_cost_per_token = 0.000008400000000000001
supports_response_schema = true
supports_tool_choice = true

[models."Qwen3-32B".pricing."alibaba"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000028
reasoning_cost_per_token = 0.000008400000000000001
[models."Qwen3-32B".pricing."alibaba-cn"]
input_cost_per_token = 2.8699999999999996e-7
output_cost_per_token = 0.000001147
reasoning_cost_per_token = 0.000002868
[models."Qwen3-32B".pricing."cortecs"]
input_cost_per_token = 9.9e-8
output_cost_per_token = 3.3e-7
[models."Qwen3-32B".pricing."helicone"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 5.9e-7
[models."Qwen3-32B".pricing."ovhcloud"]
input_cost_per_token = 8e-8
output_cost_per_token = 2.3e-7
[models."Qwen3-32B".pricing."sambanova"]
input_cost_per_token = 4e-7
output_cost_per_token = 8e-7

[models."Qwen3-4B-Instruct-2507-GGUF"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "lemonade"
providers = ["lemonade"]
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."Qwen3-4B-Instruct-2507-GGUF".pricing."lemonade"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."Qwen3-Coder-30B-A3B-Instruct-GGUF"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "lemonade"
providers = ["lemonade"]
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."Qwen3-Coder-30B-A3B-Instruct-GGUF".pricing."lemonade"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."Ring-1T"]
display_name = "Ring-1T"
model_family = "ring"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
input_cost_per_token = 5.699999999999999e-7
output_cost_per_token = 0.00000229
litellm_provider = "bailing"
providers = ["bailing"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-06"
release_date = "2025-10"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."Ring-1T".pricing."bailing"]
input_cost_per_token = 5.699999999999999e-7
output_cost_per_token = 0.00000229

[models."Sao10K/L3-8B-Lunaris-v1-Turbo"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 4e-8
output_cost_per_token = 5e-8
litellm_provider = "deepinfra"
providers = ["deepinfra"]
supports_tool_choice = false

[models."Sao10K/L3-8B-Lunaris-v1-Turbo".pricing."deepinfra"]
input_cost_per_token = 4e-8
output_cost_per_token = 5e-8

[models."Sao10K/L3-8B-Stheno-v3.2"]
display_name = "L3 8B Stheno V3.2"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 5e-8
output_cost_per_token = 5e-8
litellm_provider = "novita"
providers = ["novita", "novita-ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-11-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."Sao10K/L3-8B-Stheno-v3.2".pricing."novita"]
input_cost_per_token = 5e-8
output_cost_per_token = 5e-8
[models."Sao10K/L3-8B-Stheno-v3.2".pricing."novita-ai"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 5.0000000000000004e-8

[models."Sao10K/L3.1-70B-Euryale-v2.2"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 6.5e-7
output_cost_per_token = 7.5e-7
litellm_provider = "deepinfra"
providers = ["deepinfra"]
supports_tool_choice = false

[models."Sao10K/L3.1-70B-Euryale-v2.2".pricing."deepinfra"]
input_cost_per_token = 6.5e-7
output_cost_per_token = 7.5e-7

[models."Sao10K/L3.3-70B-Euryale-v2.3"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 6.5e-7
output_cost_per_token = 7.5e-7
litellm_provider = "deepinfra"
providers = ["deepinfra"]
supports_tool_choice = false

[models."Sao10K/L3.3-70B-Euryale-v2.3".pricing."deepinfra"]
input_cost_per_token = 6.5e-7
output_cost_per_token = 7.5e-7

[models."THUDM/GLM-4-32B-0414"]
display_name = "THUDM/GLM-4-32B-0414"
model_family = "glm"
mode = "chat"
max_input_tokens = 33000
max_output_tokens = 33000
input_cost_per_token = 2.7e-7
output_cost_per_token = 2.7e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "siliconflow"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-04-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."THUDM/GLM-4-32B-0414".pricing."siliconflow"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 2.7e-7
[models."THUDM/GLM-4-32B-0414".pricing."siliconflow-cn"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 2.7e-7

[models."THUDM/GLM-4-9B-0414"]
display_name = "THUDM/GLM-4-9B-0414"
model_family = "glm"
mode = "chat"
max_input_tokens = 33000
max_output_tokens = 33000
input_cost_per_token = 8.599999999999999e-8
output_cost_per_token = 8.599999999999999e-8
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "siliconflow"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-04-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."THUDM/GLM-4-9B-0414".pricing."siliconflow"]
input_cost_per_token = 8.599999999999999e-8
output_cost_per_token = 8.599999999999999e-8
[models."THUDM/GLM-4-9B-0414".pricing."siliconflow-cn"]
input_cost_per_token = 8.599999999999999e-8
output_cost_per_token = 8.599999999999999e-8

[models."THUDM/GLM-Z1-32B-0414"]
display_name = "THUDM/GLM-Z1-32B-0414"
model_family = "glm-z"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 131000
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "siliconflow"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-04-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."THUDM/GLM-Z1-32B-0414".pricing."siliconflow"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7
[models."THUDM/GLM-Z1-32B-0414".pricing."siliconflow-cn"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7

[models."THUDM/GLM-Z1-9B-0414"]
display_name = "THUDM/GLM-Z1-9B-0414"
model_family = "glm-z"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 131000
input_cost_per_token = 8.599999999999999e-8
output_cost_per_token = 8.599999999999999e-8
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "siliconflow"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-04-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."THUDM/GLM-Z1-9B-0414".pricing."siliconflow"]
input_cost_per_token = 8.599999999999999e-8
output_cost_per_token = 8.599999999999999e-8
[models."THUDM/GLM-Z1-9B-0414".pricing."siliconflow-cn"]
input_cost_per_token = 8.599999999999999e-8
output_cost_per_token = 8.599999999999999e-8

[models."WhereIsAI/UAE-Large-V1"]
mode = "embedding"
max_input_tokens = 512
max_tokens = 512
input_cost_per_token = 1.6e-8
output_cost_per_token = 0
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
source = "https://fireworks.ai/pricing"

[models."WhereIsAI/UAE-Large-V1".pricing."fireworks_ai"]
input_cost_per_token = 1.6e-8
output_cost_per_token = 0

[models."ZhipuAI/GLM-4-5"]
display_name = "GLM-4.5"
model_family = "glm"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 98304
litellm_provider = "modelscope"
providers = ["modelscope"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."ZhipuAI/GLM-4-6"]
display_name = "GLM-4.6"
model_family = "glm"
mode = "chat"
max_input_tokens = 202752
max_output_tokens = 98304
litellm_provider = "modelscope"
providers = ["modelscope"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2025-09-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."accounts/fireworks/models/"]
mode = "embedding"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."accounts/fireworks/models/SSD-1B"]
mode = "image_generation"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.3e-10
output_cost_per_token = 1.3e-10
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/SSD-1B".pricing."fireworks_ai"]
input_cost_per_token = 1.3e-10
output_cost_per_token = 1.3e-10

[models."accounts/fireworks/models/chronos-hermes-13b-v2"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/chronos-hermes-13b-v2".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/code-llama-13b"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/code-llama-13b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/code-llama-13b-instruct"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/code-llama-13b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/code-llama-13b-python"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/code-llama-13b-python".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/code-llama-34b"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/code-llama-34b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/code-llama-34b-instruct"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/code-llama-34b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/code-llama-34b-python"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/code-llama-34b-python".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/code-llama-70b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/code-llama-70b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/code-llama-70b-instruct"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/code-llama-70b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/code-llama-70b-python"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/code-llama-70b-python".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/code-llama-7b"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/code-llama-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/code-llama-7b-instruct"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/code-llama-7b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/code-llama-7b-python"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/code-llama-7b-python".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/code-qwen-1p5-7b"]
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/code-qwen-1p5-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/codegemma-2b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/codegemma-2b".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/codegemma-7b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/codegemma-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/cogito-671b-v2-p1"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/cogito-671b-v2-p1".pricing."fireworks_ai"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."accounts/fireworks/models/cogito-v1-preview-llama-3b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/cogito-v1-preview-llama-3b".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/cogito-v1-preview-llama-70b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/cogito-v1-preview-llama-70b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/cogito-v1-preview-llama-8b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/cogito-v1-preview-llama-8b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/cogito-v1-preview-qwen-14b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/cogito-v1-preview-qwen-14b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/cogito-v1-preview-qwen-32b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/cogito-v1-preview-qwen-32b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/dbrx-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/dbrx-instruct".pricing."fireworks_ai"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."accounts/fireworks/models/deepseek-coder-1b-base"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-coder-1b-base".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/deepseek-coder-33b-instruct"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-coder-33b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/deepseek-coder-7b-base"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-coder-7b-base".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/deepseek-coder-7b-base-v1p5"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-coder-7b-base-v1p5".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/deepseek-coder-7b-instruct-v1p5"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-coder-7b-instruct-v1p5".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/deepseek-coder-v2-instruct"]
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_function_calling = false
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."accounts/fireworks/models/deepseek-coder-v2-instruct".pricing."fireworks_ai"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."accounts/fireworks/models/deepseek-coder-v2-lite-base"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-coder-v2-lite-base".pricing."fireworks_ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7

[models."accounts/fireworks/models/deepseek-coder-v2-lite-instruct"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-coder-v2-lite-instruct".pricing."fireworks_ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7

[models."accounts/fireworks/models/deepseek-prover-v2"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-prover-v2".pricing."fireworks_ai"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."accounts/fireworks/models/deepseek-r1"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 20480
max_tokens = 20480
input_cost_per_token = 0.000003
output_cost_per_token = 0.000008
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."accounts/fireworks/models/deepseek-r1".pricing."fireworks_ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000008

[models."accounts/fireworks/models/deepseek-r1-0528"]
mode = "chat"
max_input_tokens = 160000
max_output_tokens = 160000
max_tokens = 160000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000008
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."accounts/fireworks/models/deepseek-r1-0528".pricing."fireworks_ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000008

[models."accounts/fireworks/models/deepseek-r1-0528-distill-qwen3-8b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-r1-0528-distill-qwen3-8b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/deepseek-r1-basic"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 20480
max_tokens = 20480
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.00000219
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."accounts/fireworks/models/deepseek-r1-basic".pricing."fireworks_ai"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.00000219

[models."accounts/fireworks/models/deepseek-r1-distill-llama-70b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-r1-distill-llama-70b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/deepseek-r1-distill-llama-8b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-r1-distill-llama-8b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/deepseek-r1-distill-qwen-14b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-r1-distill-qwen-14b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/deepseek-r1-distill-qwen-1p5b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-r1-distill-qwen-1p5b".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/deepseek-r1-distill-qwen-32b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-r1-distill-qwen-32b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/deepseek-r1-distill-qwen-7b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-r1-distill-qwen-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/deepseek-v2-lite-chat"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-v2-lite-chat".pricing."fireworks_ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7

[models."accounts/fireworks/models/deepseek-v2p5"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/deepseek-v2p5".pricing."fireworks_ai"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."accounts/fireworks/models/deepseek-v3"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."accounts/fireworks/models/deepseek-v3".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/deepseek-v3-0324"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
source = "https://fireworks.ai/models/fireworks/deepseek-v3-0324"
supports_response_schema = true
supports_tool_choice = false

[models."accounts/fireworks/models/deepseek-v3-0324".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/deepseek-v3p1"]
display_name = "DeepSeek V3.1"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000168
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai", "fireworks-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2025-08-21"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = true

[models."accounts/fireworks/models/deepseek-v3p1".pricing."fireworks-ai"]
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000168
[models."accounts/fireworks/models/deepseek-v3p1".pricing."fireworks_ai"]
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000168

[models."accounts/fireworks/models/deepseek-v3p1-terminus"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000168
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_reasoning = true
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = true

[models."accounts/fireworks/models/deepseek-v3p1-terminus".pricing."fireworks_ai"]
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000168

[models."accounts/fireworks/models/deepseek-v3p2"]
display_name = "DeepSeek V3.2"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000168
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai", "fireworks-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-09"
release_date = "2025-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://fireworks.ai/models/fireworks/deepseek-v3p2"
supports_response_schema = true
supports_tool_choice = true

[models."accounts/fireworks/models/deepseek-v3p2".pricing."fireworks-ai"]
cache_read_input_token_cost = 2.8e-7
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000168
[models."accounts/fireworks/models/deepseek-v3p2".pricing."fireworks_ai"]
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000168

[models."accounts/fireworks/models/devstral-small-2505"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/devstral-small-2505".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/dobby-mini-unhinged-plus-llama-3-1-8b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/dobby-mini-unhinged-plus-llama-3-1-8b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/dobby-unhinged-llama-3-3-70b-new"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/dobby-unhinged-llama-3-3-70b-new".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/dolphin-2-9-2-qwen2-72b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/dolphin-2-9-2-qwen2-72b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/dolphin-2p6-mixtral-8x7b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/dolphin-2p6-mixtral-8x7b".pricing."fireworks_ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7

[models."accounts/fireworks/models/ernie-4p5-21b-a3b-pt"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/ernie-4p5-21b-a3b-pt".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/ernie-4p5-300b-a47b-pt"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/ernie-4p5-300b-a47b-pt".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/fare-20b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/fare-20b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/firefunction-v1"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/firefunction-v1".pricing."fireworks_ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7

[models."accounts/fireworks/models/firefunction-v2"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_function_calling = true
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = true

[models."accounts/fireworks/models/firefunction-v2".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/firellava-13b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/firellava-13b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/firesearch-ocr-v6"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/firesearch-ocr-v6".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/fireworks-asr-large"]
mode = "audio_transcription"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/fireworks-asr-large".pricing."fireworks_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."accounts/fireworks/models/fireworks-asr-v2"]
mode = "audio_transcription"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/fireworks-asr-v2".pricing."fireworks_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."accounts/fireworks/models/flux-1-dev"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/flux-1-dev".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/flux-1-dev-controlnet-union"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-9
output_cost_per_token = 1e-9
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/flux-1-dev-controlnet-union".pricing."fireworks_ai"]
input_cost_per_token = 1e-9
output_cost_per_token = 1e-9

[models."accounts/fireworks/models/flux-1-dev-fp8"]
mode = "image_generation"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 5e-10
output_cost_per_token = 5e-10
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/flux-1-dev-fp8".pricing."fireworks_ai"]
input_cost_per_token = 5e-10
output_cost_per_token = 5e-10

[models."accounts/fireworks/models/flux-1-schnell"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/flux-1-schnell".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/flux-1-schnell-fp8"]
mode = "image_generation"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 3.5e-10
output_cost_per_token = 3.5e-10
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/flux-1-schnell-fp8".pricing."fireworks_ai"]
input_cost_per_token = 3.5e-10
output_cost_per_token = 3.5e-10

[models."accounts/fireworks/models/flux-kontext-max"]
mode = "image_generation"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 8e-8
output_cost_per_token = 8e-8
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/flux-kontext-max".pricing."fireworks_ai"]
input_cost_per_token = 8e-8
output_cost_per_token = 8e-8

[models."accounts/fireworks/models/flux-kontext-pro"]
mode = "image_generation"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 4e-8
output_cost_per_token = 4e-8
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/flux-kontext-pro".pricing."fireworks_ai"]
input_cost_per_token = 4e-8
output_cost_per_token = 4e-8

[models."accounts/fireworks/models/gemma-2b-it"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/gemma-2b-it".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/gemma-3-27b-it"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/gemma-3-27b-it".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/gemma-7b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/gemma-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/gemma-7b-it"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/gemma-7b-it".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/gemma2-9b-it"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/gemma2-9b-it".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/glm-4p5"]
display_name = "GLM 4.5"
model_family = "glm"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 96000
max_tokens = 96000
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.00000219
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai", "fireworks-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://fireworks.ai/models/fireworks/glm-4p5"
supports_response_schema = true
supports_tool_choice = true

[models."accounts/fireworks/models/glm-4p5".pricing."fireworks-ai"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998
[models."accounts/fireworks/models/glm-4p5".pricing."fireworks_ai"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.00000219

[models."accounts/fireworks/models/glm-4p5-air"]
display_name = "GLM 4.5 Air"
model_family = "glm-air"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 96000
max_tokens = 96000
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai", "fireworks-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-08-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://artificialanalysis.ai/models/glm-4-5-air"
supports_response_schema = true
supports_tool_choice = true

[models."accounts/fireworks/models/glm-4p5-air".pricing."fireworks-ai"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7
[models."accounts/fireworks/models/glm-4p5-air".pricing."fireworks_ai"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7

[models."accounts/fireworks/models/glm-4p5v"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_reasoning = true

[models."accounts/fireworks/models/glm-4p5v".pricing."fireworks_ai"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."accounts/fireworks/models/glm-4p6"]
mode = "chat"
max_input_tokens = 202800
max_output_tokens = 202800
max_tokens = 202800
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.00000219
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_function_calling = true
supports_reasoning = true
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = true

[models."accounts/fireworks/models/glm-4p6".pricing."fireworks_ai"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.00000219

[models."accounts/fireworks/models/glm-4p7"]
display_name = "GLM 4.7"
model_family = "glm"
mode = "chat"
max_input_tokens = 202800
max_output_tokens = 202800
max_tokens = 202800
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
cache_read_input_token_cost = 3e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai", "fireworks-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-12-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://fireworks.ai/models/fireworks/glm-4p7"
supports_response_schema = true
supports_tool_choice = true

[models."accounts/fireworks/models/glm-4p7".pricing."fireworks-ai"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."accounts/fireworks/models/glm-4p7".pricing."fireworks_ai"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022

[models."accounts/fireworks/models/glm-5"]
display_name = "GLM 5"
model_family = "glm"
mode = "chat"
max_input_tokens = 202752
max_output_tokens = 131072
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003
cache_read_input_token_cost = 5e-7
litellm_provider = "fireworks-ai"
providers = ["fireworks-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-02-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."accounts/fireworks/models/glm-5".pricing."fireworks-ai"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003

[models."accounts/fireworks/models/gpt-oss-120b"]
display_name = "GPT OSS 120B"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai", "fireworks-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-08-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = true

[models."accounts/fireworks/models/gpt-oss-120b".pricing."fireworks-ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."accounts/fireworks/models/gpt-oss-120b".pricing."fireworks_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."accounts/fireworks/models/gpt-oss-20b"]
display_name = "GPT OSS 20B"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 5e-8
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai", "fireworks-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-08-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = true

[models."accounts/fireworks/models/gpt-oss-20b".pricing."fireworks-ai"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.0000000000000002e-7
[models."accounts/fireworks/models/gpt-oss-20b".pricing."fireworks_ai"]
input_cost_per_token = 5e-8
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/gpt-oss-safeguard-120b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/gpt-oss-safeguard-120b".pricing."fireworks_ai"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."accounts/fireworks/models/gpt-oss-safeguard-20b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/gpt-oss-safeguard-20b".pricing."fireworks_ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7

[models."accounts/fireworks/models/hermes-2-pro-mistral-7b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/hermes-2-pro-mistral-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/internvl3-38b"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/internvl3-38b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/internvl3-78b"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/internvl3-78b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/internvl3-8b"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/internvl3-8b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/japanese-stable-diffusion-xl"]
mode = "image_generation"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.3e-10
output_cost_per_token = 1.3e-10
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/japanese-stable-diffusion-xl".pricing."fireworks_ai"]
input_cost_per_token = 1.3e-10
output_cost_per_token = 1.3e-10

[models."accounts/fireworks/models/kat-coder"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/kat-coder".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/kat-dev-32b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/kat-dev-32b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/kat-dev-72b-exp"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/kat-dev-72b-exp".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/kimi-k2-instruct"]
display_name = "Kimi K2 Instruct"
model_family = "kimi"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai", "fireworks-ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-07-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://fireworks.ai/models/fireworks/kimi-k2-instruct"
supports_response_schema = true
supports_tool_choice = true

[models."accounts/fireworks/models/kimi-k2-instruct".pricing."fireworks-ai"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
[models."accounts/fireworks/models/kimi-k2-instruct".pricing."fireworks_ai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025

[models."accounts/fireworks/models/kimi-k2-instruct-0905"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_function_calling = true
source = "https://app.fireworks.ai/models/fireworks/kimi-k2-instruct-0905"
supports_response_schema = true
supports_tool_choice = true

[models."accounts/fireworks/models/kimi-k2-instruct-0905".pricing."fireworks_ai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025

[models."accounts/fireworks/models/kimi-k2-thinking"]
display_name = "Kimi K2 Thinking"
model_family = "kimi-thinking"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai", "fireworks-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-11-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."accounts/fireworks/models/kimi-k2-thinking".pricing."fireworks-ai"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."accounts/fireworks/models/kimi-k2-thinking".pricing."fireworks_ai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025

[models."accounts/fireworks/models/kimi-k2p5"]
display_name = "Kimi K2.5"
model_family = "kimi-thinking"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
cache_read_input_token_cost = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai", "fireworks-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2026-01-27"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = true

[models."accounts/fireworks/models/kimi-k2p5".pricing."fireworks-ai"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
[models."accounts/fireworks/models/kimi-k2p5".pricing."fireworks_ai"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003

[models."accounts/fireworks/models/llama-guard-2-8b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-guard-2-8b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/llama-guard-3-1b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-guard-3-1b".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/llama-guard-3-8b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-guard-3-8b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/llama-v2-13b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v2-13b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/llama-v2-13b-chat"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v2-13b-chat".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/llama-v2-70b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v2-70b".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/llama-v2-70b-chat"]
mode = "chat"
max_input_tokens = 2048
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v2-70b-chat".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/llama-v2-7b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v2-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/llama-v2-7b-chat"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v2-7b-chat".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/llama-v3-70b-instruct"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v3-70b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/llama-v3-70b-instruct-hf"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v3-70b-instruct-hf".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/llama-v3-8b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v3-8b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/llama-v3-8b-instruct-hf"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v3-8b-instruct-hf".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/llama-v3p1-405b-instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.000003
output_cost_per_token = 0.000003
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_function_calling = true
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = true

[models."accounts/fireworks/models/llama-v3p1-405b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000003

[models."accounts/fireworks/models/llama-v3p1-405b-instruct-long"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v3p1-405b-instruct-long".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/llama-v3p1-70b-instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v3p1-70b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/llama-v3p1-70b-instruct-1b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v3p1-70b-instruct-1b".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/llama-v3p1-8b-instruct"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_function_calling = false
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."accounts/fireworks/models/llama-v3p1-8b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/llama-v3p1-nemotron-70b-instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v3p1-nemotron-70b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/llama-v3p2-11b-vision-instruct"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_function_calling = false
supports_vision = true
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."accounts/fireworks/models/llama-v3p2-11b-vision-instruct".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/llama-v3p2-1b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v3p2-1b".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/llama-v3p2-1b-instruct"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_function_calling = false
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."accounts/fireworks/models/llama-v3p2-1b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/llama-v3p2-3b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v3p2-3b".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/llama-v3p2-3b-instruct"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_function_calling = false
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."accounts/fireworks/models/llama-v3p2-3b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/llama-v3p2-90b-vision-instruct"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_vision = true
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."accounts/fireworks/models/llama-v3p2-90b-vision-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/llama-v3p3-70b-instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llama-v3p3-70b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/llama4-maverick-instruct-basic"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."accounts/fireworks/models/llama4-maverick-instruct-basic".pricing."fireworks_ai"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7

[models."accounts/fireworks/models/llama4-scout-instruct-basic"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."accounts/fireworks/models/llama4-scout-instruct-basic".pricing."fireworks_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."accounts/fireworks/models/llamaguard-7b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llamaguard-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/llava-yi-34b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/llava-yi-34b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/minimax-m1-80k"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/minimax-m1-80k".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/minimax-m2"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/minimax-m2".pricing."fireworks_ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."accounts/fireworks/models/minimax-m2p1"]
display_name = "MiniMax-M2.1"
model_family = "minimax"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 204800
max_tokens = 204800
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
cache_read_input_token_cost = 3e-8
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai", "fireworks-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://fireworks.ai/models/fireworks/minimax-m2p1"
supports_response_schema = true
supports_tool_choice = true

[models."accounts/fireworks/models/minimax-m2p1".pricing."fireworks-ai"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."accounts/fireworks/models/minimax-m2p1".pricing."fireworks_ai"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."accounts/fireworks/models/minimax-m2p5"]
display_name = "MiniMax-M2.5"
model_family = "minimax"
mode = "chat"
max_input_tokens = 196608
max_output_tokens = 196608
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
cache_read_input_token_cost = 3e-8
litellm_provider = "fireworks-ai"
providers = ["fireworks-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-02-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."accounts/fireworks/models/minimax-m2p5".pricing."fireworks-ai"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."accounts/fireworks/models/ministral-3-14b-instruct-2512"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/ministral-3-14b-instruct-2512".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/ministral-3-3b-instruct-2512"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/ministral-3-3b-instruct-2512".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/ministral-3-8b-instruct-2512"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/ministral-3-8b-instruct-2512".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/mistral-7b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/mistral-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/mistral-7b-instruct-4k"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/mistral-7b-instruct-4k".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/mistral-7b-instruct-v0p2"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/mistral-7b-instruct-v0p2".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/mistral-7b-instruct-v3"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/mistral-7b-instruct-v3".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/mistral-7b-v0p2"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/mistral-7b-v0p2".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/mistral-large-3-fp8"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/mistral-large-3-fp8".pricing."fireworks_ai"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."accounts/fireworks/models/mistral-nemo-base-2407"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/mistral-nemo-base-2407".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/mistral-nemo-instruct-2407"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/mistral-nemo-instruct-2407".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/mistral-small-24b-instruct-2501"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/mistral-small-24b-instruct-2501".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/mixtral-8x22b"]
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/mixtral-8x22b".pricing."fireworks_ai"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."accounts/fireworks/models/mixtral-8x22b-instruct"]
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/mixtral-8x22b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."accounts/fireworks/models/mixtral-8x22b-instruct-hf"]
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_function_calling = true
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = true

[models."accounts/fireworks/models/mixtral-8x22b-instruct-hf".pricing."fireworks_ai"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."accounts/fireworks/models/mixtral-8x7b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/mixtral-8x7b".pricing."fireworks_ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7

[models."accounts/fireworks/models/mixtral-8x7b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/mixtral-8x7b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7

[models."accounts/fireworks/models/mixtral-8x7b-instruct-hf"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/mixtral-8x7b-instruct-hf".pricing."fireworks_ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7

[models."accounts/fireworks/models/mythomax-l2-13b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/mythomax-l2-13b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/nemotron-nano-v2-12b-vl"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/nemotron-nano-v2-12b-vl".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/nous-capybara-7b-v1p9"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/nous-capybara-7b-v1p9".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/nous-hermes-2-mixtral-8x7b-dpo"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/nous-hermes-2-mixtral-8x7b-dpo".pricing."fireworks_ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7

[models."accounts/fireworks/models/nous-hermes-2-yi-34b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/nous-hermes-2-yi-34b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/nous-hermes-llama2-13b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/nous-hermes-llama2-13b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/nous-hermes-llama2-70b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/nous-hermes-llama2-70b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/nous-hermes-llama2-7b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/nous-hermes-llama2-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/nvidia-nemotron-nano-12b-v2"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/nvidia-nemotron-nano-12b-v2".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/nvidia-nemotron-nano-9b-v2"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/nvidia-nemotron-nano-9b-v2".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/openchat-3p5-0106-7b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/openchat-3p5-0106-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/openhermes-2-mistral-7b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/openhermes-2-mistral-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/openhermes-2p5-mistral-7b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/openhermes-2p5-mistral-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/openorca-7b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/openorca-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/phi-2-3b"]
mode = "chat"
max_input_tokens = 2048
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/phi-2-3b".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/phi-3-mini-128k-instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/phi-3-mini-128k-instruct".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/phi-3-vision-128k-instruct"]
mode = "chat"
max_input_tokens = 32064
max_output_tokens = 32064
max_tokens = 32064
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/phi-3-vision-128k-instruct".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/phind-code-llama-34b-python-v1"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/phind-code-llama-34b-python-v1".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/phind-code-llama-34b-v1"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/phind-code-llama-34b-v1".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/phind-code-llama-34b-v2"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/phind-code-llama-34b-v2".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/playground-v2-1024px-aesthetic"]
mode = "image_generation"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.3e-10
output_cost_per_token = 1.3e-10
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/playground-v2-1024px-aesthetic".pricing."fireworks_ai"]
input_cost_per_token = 1.3e-10
output_cost_per_token = 1.3e-10

[models."accounts/fireworks/models/playground-v2-5-1024px-aesthetic"]
mode = "image_generation"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.3e-10
output_cost_per_token = 1.3e-10
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/playground-v2-5-1024px-aesthetic".pricing."fireworks_ai"]
input_cost_per_token = 1.3e-10
output_cost_per_token = 1.3e-10

[models."accounts/fireworks/models/pythia-12b"]
mode = "chat"
max_input_tokens = 2048
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/pythia-12b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen-qwq-32b-preview"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen-qwq-32b-preview".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen-v2p5-14b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen-v2p5-14b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen-v2p5-7b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen-v2p5-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen1p5-72b-chat"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen1p5-72b-chat".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen2-72b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_function_calling = false
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."accounts/fireworks/models/qwen2-72b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen2-7b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2-7b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen2-vl-2b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2-vl-2b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/qwen2-vl-72b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2-vl-72b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen2-vl-7b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2-vl-7b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen2p5-0p5b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-0p5b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/qwen2p5-14b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-14b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen2p5-1p5b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-1p5b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/qwen2p5-32b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-32b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen2p5-32b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-32b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen2p5-72b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-72b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen2p5-72b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-72b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen2p5-7b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-7b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen2p5-coder-0p5b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-coder-0p5b".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/qwen2p5-coder-0p5b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-coder-0p5b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/qwen2p5-coder-14b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-coder-14b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen2p5-coder-14b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-coder-14b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen2p5-coder-1p5b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-coder-1p5b".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/qwen2p5-coder-1p5b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-coder-1p5b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/qwen2p5-coder-32b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-coder-32b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen2p5-coder-32b-instruct"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_function_calling = false
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."accounts/fireworks/models/qwen2p5-coder-32b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen2p5-coder-32b-instruct-128k"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-coder-32b-instruct-128k".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen2p5-coder-32b-instruct-32k-rope"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-coder-32b-instruct-32k-rope".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen2p5-coder-32b-instruct-64k"]
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-coder-32b-instruct-64k".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen2p5-coder-3b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-coder-3b".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/qwen2p5-coder-3b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-coder-3b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/qwen2p5-coder-7b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-coder-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen2p5-coder-7b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-coder-7b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen2p5-math-72b-instruct"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-math-72b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen2p5-vl-32b-instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-vl-32b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen2p5-vl-3b-instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-vl-3b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen2p5-vl-72b-instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-vl-72b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen2p5-vl-7b-instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen2p5-vl-7b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen3-0p6b"]
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-0p6b".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/qwen3-14b"]
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-14b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen3-1p7b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-1p7b".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/qwen3-1p7b-fp8-draft"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-1p7b-fp8-draft".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/qwen3-1p7b-fp8-draft-131072"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-1p7b-fp8-draft-131072".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/qwen3-1p7b-fp8-draft-40960"]
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-1p7b-fp8-draft-40960".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/qwen3-235b-a22b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-235b-a22b".pricing."fireworks_ai"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7

[models."accounts/fireworks/models/qwen3-235b-a22b-instruct-2507"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-235b-a22b-instruct-2507".pricing."fireworks_ai"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7

[models."accounts/fireworks/models/qwen3-235b-a22b-thinking-2507"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-235b-a22b-thinking-2507".pricing."fireworks_ai"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7

[models."accounts/fireworks/models/qwen3-30b-a3b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-30b-a3b".pricing."fireworks_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."accounts/fireworks/models/qwen3-30b-a3b-instruct-2507"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-30b-a3b-instruct-2507".pricing."fireworks_ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7

[models."accounts/fireworks/models/qwen3-30b-a3b-thinking-2507"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-30b-a3b-thinking-2507".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen3-32b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_reasoning = true

[models."accounts/fireworks/models/qwen3-32b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen3-4b"]
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-4b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen3-4b-instruct-2507"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-4b-instruct-2507".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen3-8b"]
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_reasoning = true

[models."accounts/fireworks/models/qwen3-8b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwen3-coder-30b-a3b-instruct"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-coder-30b-a3b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."accounts/fireworks/models/qwen3-coder-480b-a35b-instruct"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 4.5e-7
output_cost_per_token = 0.0000018
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_reasoning = true

[models."accounts/fireworks/models/qwen3-coder-480b-a35b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 4.5e-7
output_cost_per_token = 0.0000018

[models."accounts/fireworks/models/qwen3-coder-480b-instruct-bf16"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-coder-480b-instruct-bf16".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen3-embedding-0p6b"]
mode = "embedding"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-embedding-0p6b".pricing."fireworks_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."accounts/fireworks/models/qwen3-embedding-4b"]
mode = "embedding"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-embedding-4b".pricing."fireworks_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."accounts/fireworks/models/qwen3-next-80b-a3b-instruct"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-next-80b-a3b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen3-next-80b-a3b-thinking"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-next-80b-a3b-thinking".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen3-reranker-0p6b"]
mode = "rerank"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-reranker-0p6b".pricing."fireworks_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."accounts/fireworks/models/qwen3-reranker-4b"]
mode = "rerank"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-reranker-4b".pricing."fireworks_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."accounts/fireworks/models/qwen3-reranker-8b"]
mode = "rerank"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-reranker-8b".pricing."fireworks_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."accounts/fireworks/models/qwen3-vl-235b-a22b-instruct"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-vl-235b-a22b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7

[models."accounts/fireworks/models/qwen3-vl-235b-a22b-thinking"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-vl-235b-a22b-thinking".pricing."fireworks_ai"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7

[models."accounts/fireworks/models/qwen3-vl-30b-a3b-instruct"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-vl-30b-a3b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."accounts/fireworks/models/qwen3-vl-30b-a3b-thinking"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-vl-30b-a3b-thinking".pricing."fireworks_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."accounts/fireworks/models/qwen3-vl-32b-instruct"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-vl-32b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/qwen3-vl-8b-instruct"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwen3-vl-8b-instruct".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/qwq-32b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/qwq-32b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/rolm-ocr"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/rolm-ocr".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/snorkel-mistral-7b-pairrm-dpo"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/snorkel-mistral-7b-pairrm-dpo".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/stable-diffusion-xl-1024-v1-0"]
mode = "image_generation"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.3e-10
output_cost_per_token = 1.3e-10
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/stable-diffusion-xl-1024-v1-0".pricing."fireworks_ai"]
input_cost_per_token = 1.3e-10
output_cost_per_token = 1.3e-10

[models."accounts/fireworks/models/stablecode-3b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/stablecode-3b".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/starcoder-16b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/starcoder-16b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/starcoder-7b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/starcoder-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/starcoder2-15b"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/starcoder2-15b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/starcoder2-3b"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/starcoder2-3b".pricing."fireworks_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."accounts/fireworks/models/starcoder2-7b"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/starcoder2-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/toppy-m-7b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/toppy-m-7b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/whisper-v3"]
mode = "audio_transcription"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/whisper-v3".pricing."fireworks_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."accounts/fireworks/models/whisper-v3-turbo"]
mode = "audio_transcription"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/whisper-v3-turbo".pricing."fireworks_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."accounts/fireworks/models/yi-34b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/yi-34b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/yi-34b-200k-capybara"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 200000
max_tokens = 200000
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/yi-34b-200k-capybara".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/yi-34b-chat"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/yi-34b-chat".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."accounts/fireworks/models/yi-6b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/yi-6b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."accounts/fireworks/models/yi-large"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000003
output_cost_per_token = 0.000003
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_function_calling = false
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = false

[models."accounts/fireworks/models/yi-large".pricing."fireworks_ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000003

[models."accounts/fireworks/models/zephyr-7b-beta"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."accounts/fireworks/models/zephyr-7b-beta".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."ada"]
mode = "embedding"
max_input_tokens = 8191
max_tokens = 8191
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "azure"
providers = ["azure"]

[models."ada".pricing."azure"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."ai21-labs/ai21-jamba-1-5-large"]
display_name = "AI21 Jamba 1.5 Large"
model_family = "jamba"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 4096
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-03"
release_date = "2024-08-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."ai21-labs/ai21-jamba-1-5-mini"]
display_name = "AI21 Jamba 1.5 Mini"
model_family = "jamba"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 4096
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-03"
release_date = "2024-08-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."ai21.j2-mid-v1"]
mode = "chat"
max_input_tokens = 8191
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.0000125
output_cost_per_token = 0.0000125
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."ai21.j2-mid-v1".pricing."bedrock"]
input_cost_per_token = 0.0000125
output_cost_per_token = 0.0000125

[models."ai21.j2-ultra-v1"]
mode = "chat"
max_input_tokens = 8191
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.0000188
output_cost_per_token = 0.0000188
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."ai21.j2-ultra-v1".pricing."bedrock"]
input_cost_per_token = 0.0000188
output_cost_per_token = 0.0000188

[models."ai21.jamba-1-5-large-v1:0"]
display_name = "Jamba 1.5 Large"
model_family = "jamba"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2024-08-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."ai21.jamba-1-5-large-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."ai21.jamba-1-5-large-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008

[models."ai21.jamba-1-5-mini-v1:0"]
display_name = "Jamba 1.5 Mini"
model_family = "jamba"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 2e-7
output_cost_per_token = 4e-7
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2024-08-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."ai21.jamba-1-5-mini-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 4.0000000000000003e-7
[models."ai21.jamba-1-5-mini-v1:0".pricing."bedrock"]
input_cost_per_token = 2e-7
output_cost_per_token = 4e-7

[models."ai21.jamba-instruct-v1:0"]
mode = "chat"
max_input_tokens = 70000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 5e-7
output_cost_per_token = 7e-7
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_system_messages = true

[models."ai21.jamba-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 5e-7
output_cost_per_token = 7e-7

[models."aisingapore/Gemma-SEA-LION-v4-27B-IT"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "publicai"
providers = ["publicai"]
supports_function_calling = true
source = "https://platform.publicai.co/docs"
supports_tool_choice = true

[models."aisingapore/Gemma-SEA-LION-v4-27B-IT".pricing."publicai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."aisingapore/Qwen-SEA-LION-v4-32B-IT"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "publicai"
providers = ["publicai"]
supports_function_calling = true
source = "https://platform.publicai.co/docs"
supports_tool_choice = true

[models."aisingapore/Qwen-SEA-LION-v4-32B-IT".pricing."publicai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."alibaba-qwen3-32b"]
mode = "chat"
max_tokens = 2048
litellm_provider = "gradient_ai"
providers = ["gradient_ai"]
supported_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions"]
supports_tool_choice = false

[models."alibaba/qwen-3-14b"]
display_name = "Qwen3-14B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 8e-8
output_cost_per_token = 2.4e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."alibaba/qwen-3-14b".pricing."vercel"]
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7
[models."alibaba/qwen-3-14b".pricing."vercel_ai_gateway"]
input_cost_per_token = 8e-8
output_cost_per_token = 2.4e-7

[models."alibaba/qwen-3-235b"]
display_name = "Qwen3 235B A22B Instruct 2507"
model_family = "qwen"
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."alibaba/qwen-3-235b".pricing."vercel"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 6e-7
[models."alibaba/qwen-3-235b".pricing."vercel_ai_gateway"]
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7

[models."alibaba/qwen-3-30b"]
display_name = "Qwen3-30B-A3B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."alibaba/qwen-3-30b".pricing."vercel"]
input_cost_per_token = 8e-8
output_cost_per_token = 2.9e-7
[models."alibaba/qwen-3-30b".pricing."vercel_ai_gateway"]
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7

[models."alibaba/qwen-3-32b"]
display_name = "Qwen 3.32B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."alibaba/qwen-3-32b".pricing."vercel"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
[models."alibaba/qwen-3-32b".pricing."vercel_ai_gateway"]
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7

[models."alibaba/qwen3-235b-a22b-thinking"]
display_name = "Qwen3 235B A22B Thinking 2507"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262114
max_output_tokens = 262114
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000028999999999999998
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-04"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."alibaba/qwen3-235b-a22b-thinking".pricing."vercel"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000028999999999999998

[models."alibaba/qwen3-5-flash"]
display_name = "Qwen 3.5 Flash"
model_family = "qwen"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
cache_read_input_token_cost = 1e-9
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2026-02-24"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."alibaba/qwen3-5-flash".pricing."vercel"]
cache_read_input_token_cost = 1e-9
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7

[models."alibaba/qwen3-5-plus"]
display_name = "Qwen 3.5 Plus"
model_family = "qwen"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000024
cache_read_input_token_cost = 4e-8
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2026-02-16"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."alibaba/qwen3-5-plus".pricing."vercel"]
cache_read_input_token_cost = 4e-8
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000024

[models."alibaba/qwen3-coder"]
display_name = "Qwen3 Coder 480B A35B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 66536
max_tokens = 66536
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000016
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."alibaba/qwen3-coder".pricing."vercel"]
input_cost_per_token = 3.8e-7
output_cost_per_token = 0.00000153
[models."alibaba/qwen3-coder".pricing."vercel_ai_gateway"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000016

[models."alibaba/qwen3-coder-30b-a3b"]
display_name = "Qwen 3 Coder 30B A3B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 160000
max_output_tokens = 32768
input_cost_per_token = 7e-8
output_cost_per_token = 2.7e-7
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."alibaba/qwen3-coder-30b-a3b".pricing."vercel"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.7e-7

[models."alibaba/qwen3-coder-next"]
display_name = "Qwen3 Coder Next"
model_family = "qwen"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000012
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-07-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."alibaba/qwen3-coder-next".pricing."vercel"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000012

[models."alibaba/qwen3-coder-plus"]
display_name = "Qwen3 Coder Plus"
model_family = "qwen"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 1000000
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."alibaba/qwen3-coder-plus".pricing."vercel"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005

[models."alibaba/qwen3-embedding-0-6b"]
display_name = "Qwen3 Embedding 0.6B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
input_cost_per_token = 1e-8
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-11-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."alibaba/qwen3-embedding-0-6b".pricing."vercel"]
input_cost_per_token = 1e-8

[models."alibaba/qwen3-embedding-4b"]
display_name = "Qwen3 Embedding 4B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
input_cost_per_token = 2e-8
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-06-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."alibaba/qwen3-embedding-4b".pricing."vercel"]
input_cost_per_token = 2e-8

[models."alibaba/qwen3-embedding-8b"]
display_name = "Qwen3 Embedding 8B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
input_cost_per_token = 5.0000000000000004e-8
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-06-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."alibaba/qwen3-embedding-8b".pricing."vercel"]
input_cost_per_token = 5.0000000000000004e-8

[models."alibaba/qwen3-max"]
display_name = "Qwen3 Max"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 32768
input_cost_per_token = 0.0000012
output_cost_per_token = 0.000006
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-09-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."alibaba/qwen3-max".pricing."vercel"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.000006

[models."alibaba/qwen3-max-preview"]
display_name = "Qwen3 Max Preview"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 32768
input_cost_per_token = 0.0000012
output_cost_per_token = 0.000006
cache_read_input_token_cost = 2.4e-7
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-09-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."alibaba/qwen3-max-preview".pricing."vercel"]
cache_read_input_token_cost = 2.4e-7
input_cost_per_token = 0.0000012
output_cost_per_token = 0.000006

[models."alibaba/qwen3-max-thinking"]
display_name = "Qwen 3 Max Thinking"
model_family = "qwen"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 65536
input_cost_per_token = 0.0000012
output_cost_per_token = 0.000006
cache_read_input_token_cost = 2.4e-7
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2025-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."alibaba/qwen3-max-thinking".pricing."vercel"]
cache_read_input_token_cost = 2.4e-7
input_cost_per_token = 0.0000012
output_cost_per_token = 0.000006

[models."alibaba/qwen3-next-80b-a3b-instruct"]
display_name = "Qwen3 Next 80B A3B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 32768
input_cost_per_token = 9e-8
output_cost_per_token = 0.0000011
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-09-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."alibaba/qwen3-next-80b-a3b-instruct".pricing."vercel"]
input_cost_per_token = 9e-8
output_cost_per_token = 0.0000011

[models."alibaba/qwen3-next-80b-a3b-thinking"]
display_name = "Qwen3 Next 80B A3B Thinking"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 65536
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000015
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-09"
release_date = "2025-09-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."alibaba/qwen3-next-80b-a3b-thinking".pricing."vercel"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000015

[models."alibaba/qwen3-vl-instruct"]
display_name = "Qwen3 VL Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 129024
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000028
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-09-24"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."alibaba/qwen3-vl-instruct".pricing."vercel"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000028

[models."alibaba/qwen3-vl-thinking"]
display_name = "Qwen3 VL Thinking"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 129024
input_cost_per_token = 7e-7
output_cost_per_token = 0.000008400000000000001
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-09"
release_date = "2025-09-24"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."alibaba/qwen3-vl-thinking".pricing."vercel"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.000008400000000000001

[models."allenai/Olmo-3-32B-Think"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "publicai"
providers = ["publicai"]
supports_function_calling = true
supports_reasoning = true
source = "https://platform.publicai.co/docs"
supports_tool_choice = true

[models."allenai/Olmo-3-32B-Think".pricing."publicai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."allenai/Olmo-3-7B-Instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "publicai"
providers = ["publicai"]
supports_function_calling = true
source = "https://platform.publicai.co/docs"
supports_tool_choice = true

[models."allenai/Olmo-3-7B-Instruct".pricing."publicai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."allenai/Olmo-3-7B-Think"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "publicai"
providers = ["publicai"]
supports_function_calling = true
supports_reasoning = true
source = "https://platform.publicai.co/docs"
supports_tool_choice = true

[models."allenai/Olmo-3-7B-Think".pricing."publicai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."allenai/molmo-2-8b"]
display_name = "AllenAI: Molmo2 8B"
mode = "chat"
max_input_tokens = 36864
max_output_tokens = 36864
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2026-01-09"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."allenai/molmo-2-8b".pricing."kilo"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7

[models."allenai/molmo-2-8b:free"]
display_name = "Molmo2 8B (free)"
model_family = "allenai"
mode = "chat"
max_input_tokens = 36864
max_output_tokens = 36864
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2026-01-09"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."allenai/olmOCR-7B-0725-FP8"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.0000015
litellm_provider = "deepinfra"
providers = ["deepinfra"]
supports_tool_choice = false

[models."allenai/olmOCR-7B-0725-FP8".pricing."deepinfra"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.0000015

[models."amazon.nova-2-lite-v1:0"]
display_name = "Nova 2 Lite"
model_family = "nova"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
cache_read_input_token_cost = 7.5e-8
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
release_date = "2024-12-01"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_video_input = true

[models."amazon.nova-2-lite-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 3.3e-7
output_cost_per_token = 0.00000275
[models."amazon.nova-2-lite-v1:0".pricing."bedrock_converse"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025

[models."amazon.nova-2-multimodal-embeddings-v1:0"]
mode = "embedding"
max_input_tokens = 8172
max_tokens = 8172
input_cost_per_token = 1.35e-7
output_cost_per_token = 0
litellm_provider = "bedrock"
providers = ["bedrock"]
source = "https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/model-catalog/serverless/amazon.nova-2-multimodal-embeddings-v1:0"
input_cost_per_audio_per_second = 0.00014
input_cost_per_image = 0.00006
input_cost_per_video_per_second = 0.0007
output_vector_size = 3072
supports_audio_input = true
supports_embedding_image_input = true
supports_image_input = true
supports_video_input = true

[models."amazon.nova-2-multimodal-embeddings-v1:0".pricing."bedrock"]
input_cost_per_audio_per_second = 0.00014
input_cost_per_image = 0.00006
input_cost_per_token = 1.35e-7
input_cost_per_video_per_second = 0.0007
output_cost_per_token = 0

[models."amazon.nova-2-pro-preview-20251202-v1:0"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000021875
output_cost_per_token = 0.0000175
cache_read_input_token_cost = 5.46875e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
input_cost_per_audio_token = 0.0000021875
input_cost_per_image_token = 0.0000021875
supports_response_schema = true
supports_video_input = true

[models."amazon.nova-2-pro-preview-20251202-v1:0".pricing."bedrock_converse"]
cache_read_input_token_cost = 5.46875e-7
input_cost_per_audio_token = 0.0000021875
input_cost_per_image_token = 0.0000021875
input_cost_per_token = 0.0000021875
output_cost_per_token = 0.0000175

[models."amazon.nova-canvas-v1:0"]
mode = "image_generation"
max_input_tokens = 2600
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.06

[models."amazon.nova-canvas-v1:0".pricing."bedrock"]
output_cost_per_image = 0.06

[models."amazon.nova-lite-v1:0"]
display_name = "Nova Lite"
model_family = "nova-lite"
mode = "chat"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-12-03"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
supports_response_schema = true

[models."amazon.nova-lite-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 1.5e-8
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7
[models."amazon.nova-lite-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7

[models."amazon.nova-micro-v1:0"]
display_name = "Nova Micro"
model_family = "nova-micro"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 3.5e-8
output_cost_per_token = 1.4e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-12-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true

[models."amazon.nova-micro-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 8.75e-9
input_cost_per_token = 3.5e-8
output_cost_per_token = 1.4e-7
[models."amazon.nova-micro-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 3.5e-8
output_cost_per_token = 1.4e-7

[models."amazon.nova-premier-v1:0"]
display_name = "Nova Premier"
model_family = "nova"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 16384
input_cost_per_token = 0.0000025
output_cost_per_token = 0.0000125
litellm_provider = "amazon-bedrock"
providers = ["amazon-bedrock"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-12-03"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."amazon.nova-premier-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.0000125

[models."amazon.nova-pro-v1:0"]
display_name = "Nova Pro"
model_family = "nova-pro"
mode = "chat"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000032
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-12-03"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
supports_response_schema = true

[models."amazon.nova-pro-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.0000032000000000000003
[models."amazon.nova-pro-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000032

[models."amazon.rerank-v1:0"]
mode = "rerank"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_query = 0.001
max_document_chunks_per_query = 100
max_query_tokens = 32000
max_tokens_per_document_chunk = 512

[models."amazon.rerank-v1:0".pricing."bedrock"]
input_cost_per_query = 0.001
input_cost_per_token = 0
output_cost_per_token = 0

[models."amazon.titan-embed-image-v1"]
mode = "embedding"
max_input_tokens = 128
max_tokens = 128
input_cost_per_token = 8e-7
output_cost_per_token = 0
litellm_provider = "bedrock"
providers = ["bedrock"]
source = "https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/providers?model=amazon.titan-image-generator-v1"
input_cost_per_image = 0.00006
output_vector_size = 1024
supports_embedding_image_input = true
supports_image_input = true

[models."amazon.titan-embed-image-v1".metadata]
notes = "'supports_image_input' is a deprecated field. Use 'supports_embedding_image_input' instead."

[models."amazon.titan-embed-image-v1".pricing."bedrock"]
input_cost_per_image = 0.00006
input_cost_per_token = 8e-7
output_cost_per_token = 0

[models."amazon.titan-embed-text-v1"]
mode = "embedding"
max_input_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "bedrock"
providers = ["bedrock"]
output_vector_size = 1536

[models."amazon.titan-embed-text-v1".pricing."bedrock"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."amazon.titan-embed-text-v2:0"]
mode = "embedding"
max_input_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 0
litellm_provider = "bedrock"
providers = ["bedrock"]
output_vector_size = 1024

[models."amazon.titan-embed-text-v2:0".pricing."bedrock"]
input_cost_per_token = 2e-7
output_cost_per_token = 0

[models."amazon.titan-image-generator-v1"]
mode = "image_generation"
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_image = 0
output_cost_per_image = 0.008
output_cost_per_image_above_512_and_512_pixels = 0.01
output_cost_per_image_above_512_and_512_pixels_and_premium_image = 0.012
output_cost_per_image_premium_image = 0.01

[models."amazon.titan-image-generator-v1".pricing."bedrock"]
input_cost_per_image = 0
output_cost_per_image = 0.008
output_cost_per_image_above_512_and_512_pixels = 0.01
output_cost_per_image_above_512_and_512_pixels_and_premium_image = 0.012
output_cost_per_image_premium_image = 0.01

[models."amazon.titan-image-generator-v2"]
mode = "image_generation"
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_image = 0
output_cost_per_image = 0.008
output_cost_per_image_above_1024_and_1024_pixels = 0.01
output_cost_per_image_above_1024_and_1024_pixels_and_premium_image = 0.012
output_cost_per_image_premium_image = 0.01

[models."amazon.titan-image-generator-v2".pricing."bedrock"]
input_cost_per_image = 0
output_cost_per_image = 0.008
output_cost_per_image_above_1024_and_1024_pixels = 0.01
output_cost_per_image_above_1024_and_1024_pixels_and_premium_image = 0.012
output_cost_per_image_premium_image = 0.01

[models."amazon.titan-image-generator-v2:0"]
mode = "image_generation"
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_image = 0
output_cost_per_image = 0.008
output_cost_per_image_above_1024_and_1024_pixels = 0.01
output_cost_per_image_above_1024_and_1024_pixels_and_premium_image = 0.012
output_cost_per_image_premium_image = 0.01

[models."amazon.titan-image-generator-v2:0".pricing."bedrock"]
input_cost_per_image = 0
output_cost_per_image = 0.008
output_cost_per_image_above_1024_and_1024_pixels = 0.01
output_cost_per_image_above_1024_and_1024_pixels_and_premium_image = 0.012
output_cost_per_image_premium_image = 0.01

[models."amazon.titan-text-express-v1"]
display_name = "Titan Text G1 - Express"
model_family = "titan"
mode = "chat"
max_input_tokens = 42000
max_output_tokens = 8000
max_tokens = 8000
input_cost_per_token = 0.0000013
output_cost_per_token = 0.0000017
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."amazon.titan-text-express-v1".pricing."amazon-bedrock"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7
[models."amazon.titan-text-express-v1".pricing."bedrock"]
input_cost_per_token = 0.0000013
output_cost_per_token = 0.0000017

[models."amazon.titan-text-express-v1:0:8k"]
display_name = "Titan Text G1 - Express"
model_family = "titan"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7
litellm_provider = "amazon-bedrock"
providers = ["amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."amazon.titan-text-express-v1:0:8k".pricing."amazon-bedrock"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7

[models."amazon.titan-text-lite-v1"]
mode = "chat"
max_input_tokens = 42000
max_output_tokens = 4000
max_tokens = 4000
input_cost_per_token = 3e-7
output_cost_per_token = 4e-7
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."amazon.titan-text-lite-v1".pricing."bedrock"]
input_cost_per_token = 3e-7
output_cost_per_token = 4e-7

[models."amazon.titan-text-premier-v1:0"]
mode = "chat"
max_input_tokens = 42000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."amazon.titan-text-premier-v1:0".pricing."bedrock"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."anthropic--claude-3-5-sonnet"]
display_name = "anthropic--claude-3.5-sonnet"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
litellm_provider = "sap-ai-core"
providers = ["sap-ai-core"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04-30"
release_date = "2024-10-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."anthropic--claude-3-5-sonnet".pricing."sap-ai-core"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."anthropic--claude-3-7-sonnet"]
display_name = "anthropic--claude-3.7-sonnet"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
litellm_provider = "sap-ai-core"
providers = ["sap-ai-core"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10-31"
release_date = "2025-02-24"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."anthropic--claude-3-7-sonnet".pricing."sap-ai-core"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."anthropic--claude-3-haiku"]
display_name = "anthropic--claude-3-haiku"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125
cache_read_input_token_cost = 3e-8
litellm_provider = "sap-ai-core"
providers = ["sap-ai-core"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-08-31"
release_date = "2024-03-13"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."anthropic--claude-3-haiku".pricing."sap-ai-core"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125

[models."anthropic--claude-3-opus"]
display_name = "anthropic--claude-3-opus"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
litellm_provider = "sap-ai-core"
providers = ["sap-ai-core"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-08-31"
release_date = "2024-02-29"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."anthropic--claude-3-opus".pricing."sap-ai-core"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."anthropic--claude-3-sonnet"]
display_name = "anthropic--claude-3-sonnet"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
litellm_provider = "sap-ai-core"
providers = ["sap-ai-core"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-08-31"
release_date = "2024-03-04"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."anthropic--claude-3-sonnet".pricing."sap-ai-core"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."anthropic--claude-4-5-haiku"]
display_name = "anthropic--claude-4.5-haiku"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
cache_read_input_token_cost = 1.0000000000000001e-7
litellm_provider = "sap-ai-core"
providers = ["sap-ai-core"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-02-28"
release_date = "2025-10-01"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."anthropic--claude-4-5-haiku".pricing."sap-ai-core"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005

[models."anthropic--claude-4-5-opus"]
display_name = "anthropic--claude-4.5-opus"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 5e-7
litellm_provider = "sap-ai-core"
providers = ["sap-ai-core"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04-30"
release_date = "2025-11-24"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."anthropic--claude-4-5-opus".pricing."sap-ai-core"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025

[models."anthropic--claude-4-5-sonnet"]
display_name = "anthropic--claude-4.5-sonnet"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
litellm_provider = "sap-ai-core"
providers = ["sap-ai-core"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-31"
release_date = "2025-09-29"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."anthropic--claude-4-5-sonnet".pricing."sap-ai-core"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."anthropic--claude-4-opus"]
display_name = "anthropic--claude-4-opus"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
litellm_provider = "sap-ai-core"
providers = ["sap-ai-core"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-31"
release_date = "2025-05-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."anthropic--claude-4-opus".pricing."sap-ai-core"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."anthropic--claude-4-sonnet"]
display_name = "anthropic--claude-4-sonnet"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
litellm_provider = "sap-ai-core"
providers = ["sap-ai-core"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-31"
release_date = "2025-05-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."anthropic--claude-4-sonnet".pricing."sap-ai-core"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."anthropic-claude-3-opus"]
mode = "chat"
max_tokens = 1024
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
litellm_provider = "gradient_ai"
providers = ["gradient_ai"]
supported_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions"]
supports_tool_choice = false

[models."anthropic-claude-3-opus".pricing."gradient_ai"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."anthropic-claude-3.5-haiku"]
mode = "chat"
max_tokens = 1024
input_cost_per_token = 8e-7
output_cost_per_token = 0.000004
litellm_provider = "gradient_ai"
providers = ["gradient_ai"]
supported_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions"]
supports_tool_choice = false

[models."anthropic-claude-3.5-haiku".pricing."gradient_ai"]
input_cost_per_token = 8e-7
output_cost_per_token = 0.000004

[models."anthropic-claude-3.5-sonnet"]
mode = "chat"
max_tokens = 1024
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "gradient_ai"
providers = ["gradient_ai"]
supported_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions"]
supports_tool_choice = false

[models."anthropic-claude-3.5-sonnet".pricing."gradient_ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."anthropic-claude-3.7-sonnet"]
mode = "chat"
max_tokens = 1024
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "gradient_ai"
providers = ["gradient_ai"]
supported_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions"]
supports_tool_choice = false

[models."anthropic-claude-3.7-sonnet".pricing."gradient_ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."anthropic.claude-3-5-haiku-20241022-v1:0"]
display_name = "Claude Haiku 3.5"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 8e-7
output_cost_per_token = 0.000004
cache_read_input_token_cost = 8e-8
cache_creation_input_token_cost = 0.000001
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2024-10-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."anthropic.claude-3-5-haiku-20241022-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.000004
[models."anthropic.claude-3-5-haiku-20241022-v1:0".pricing."bedrock"]
cache_creation_input_token_cost = 0.000001
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8e-7
output_cost_per_token = 0.000004

[models."anthropic.claude-3-5-sonnet-20240620-v1:0"]
display_name = "Claude Sonnet 3.5"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-06-20"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.0000075
cache_creation_input_token_cost_above_1hr_above_200k_tokens = 0.000015
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.00003
supports_response_schema = true
supports_tool_choice = true

[models."anthropic.claude-3-5-sonnet-20240620-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."anthropic.claude-3-5-sonnet-20240620-v1:0".pricing."bedrock"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.0000075
cache_creation_input_token_cost_above_1hr_above_200k_tokens = 0.000015
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.00003

[models."anthropic.claude-3-5-sonnet-20241022-v2:0"]
display_name = "Claude Sonnet 3.5 v2"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-10-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.0000075
cache_creation_input_token_cost_above_1hr_above_200k_tokens = 0.000015
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.00003
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true

[models."anthropic.claude-3-5-sonnet-20241022-v2:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."anthropic.claude-3-5-sonnet-20241022-v2:0".pricing."bedrock"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.0000075
cache_creation_input_token_cost_above_1hr_above_200k_tokens = 0.000015
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.00003

[models."anthropic.claude-3-7-sonnet-20240620-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.0000036
output_cost_per_token = 0.000018
cache_read_input_token_cost = 3.6e-7
cache_creation_input_token_cost = 0.0000045
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true

[models."anthropic.claude-3-7-sonnet-20240620-v1:0".pricing."bedrock"]
cache_creation_input_token_cost = 0.0000045
cache_read_input_token_cost = 3.6e-7
input_cost_per_token = 0.0000036
output_cost_per_token = 0.000018

[models."anthropic.claude-3-7-sonnet-20250219-v1:0"]
display_name = "Claude Sonnet 3.7"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-02-19"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true

[models."anthropic.claude-3-7-sonnet-20250219-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."anthropic.claude-3-7-sonnet-20250219-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."anthropic.claude-3-haiku-20240307-v1:0"]
display_name = "Claude Haiku 3"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-02"
release_date = "2024-03-13"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_tool_choice = true

[models."anthropic.claude-3-haiku-20240307-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125
[models."anthropic.claude-3-haiku-20240307-v1:0".pricing."bedrock"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125

[models."anthropic.claude-3-opus-20240229-v1:0"]
display_name = "Claude Opus 3"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-08"
release_date = "2024-02-29"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_tool_choice = true

[models."anthropic.claude-3-opus-20240229-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."anthropic.claude-3-opus-20240229-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."anthropic.claude-3-sonnet-20240229-v1:0"]
display_name = "Claude Sonnet 3"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2023-08"
release_date = "2024-03-04"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_tool_choice = true

[models."anthropic.claude-3-sonnet-20240229-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."anthropic.claude-3-sonnet-20240229-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."anthropic.claude-haiku-4-5-20251001-v1:0"]
display_name = "Claude Haiku 4.5"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
cache_read_input_token_cost = 1e-7
cache_creation_input_token_cost = 0.00000125
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-02-28"
release_date = "2025-10-15"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "https://aws.amazon.com/about-aws/whats-new/2025/10/claude-4-5-haiku-anthropic-amazon-bedrock"
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."anthropic.claude-haiku-4-5-20251001-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."anthropic.claude-haiku-4-5-20251001-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000125
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005

[models."anthropic.claude-haiku-4-5@20251001"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
cache_read_input_token_cost = 1e-7
cache_creation_input_token_cost = 0.00000125
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
source = "https://aws.amazon.com/about-aws/whats-new/2025/10/claude-4-5-haiku-anthropic-amazon-bedrock"
supports_assistant_prefill = true
supports_computer_use = true
supports_native_streaming = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."anthropic.claude-haiku-4-5@20251001".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000125
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005

[models."anthropic.claude-instant-v1"]
display_name = "Claude Instant"
model_family = "claude"
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000024
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-08"
release_date = "2023-03-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."anthropic.claude-instant-v1".pricing."amazon-bedrock"]
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.0000024
[models."anthropic.claude-instant-v1".pricing."bedrock"]
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000024
[models."anthropic.claude-instant-v1".pricing."bedrock/us-east-1"]
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000024
[models."anthropic.claude-instant-v1".pricing."bedrock/us-west-2"]
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000024

[models."anthropic.claude-opus-4-1-20250805-v1:0"]
display_name = "Claude Opus 4.1"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
cache_creation_input_token_cost = 0.00001875
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-08-05"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."anthropic.claude-opus-4-1-20250805-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."anthropic.claude-opus-4-1-20250805-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."anthropic.claude-opus-4-1-20250805-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."anthropic.claude-opus-4-20250514-v1:0"]
display_name = "Claude Opus 4"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
cache_creation_input_token_cost = 0.00001875
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-05-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."anthropic.claude-opus-4-20250514-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."anthropic.claude-opus-4-20250514-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."anthropic.claude-opus-4-20250514-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."anthropic.claude-opus-4-5-20251101-v1:0"]
display_name = "Claude Opus 4.5"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 5e-7
cache_creation_input_token_cost = 0.00000625
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-11-24"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."anthropic.claude-opus-4-5-20251101-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."anthropic.claude-opus-4-5-20251101-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."anthropic.claude-opus-4-5-20251101-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000625
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025

[models."anthropic.claude-opus-4-6-v1"]
display_name = "Claude Opus 4.6"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 5e-7
cache_creation_input_token_cost = 0.00000625
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-05"
release_date = "2026-02-05"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.0000125
cache_read_input_token_cost_above_200k_tokens = 0.000001
input_cost_per_token_above_200k_tokens = 0.00001
output_cost_per_token_above_200k_tokens = 0.0000375
supports_assistant_prefill = false
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."anthropic.claude-opus-4-6-v1".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."anthropic.claude-opus-4-6-v1".pricing."amazon-bedrock"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."anthropic.claude-opus-4-6-v1".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000625
cache_creation_input_token_cost_above_200k_tokens = 0.0000125
cache_read_input_token_cost = 5e-7
cache_read_input_token_cost_above_200k_tokens = 0.000001
input_cost_per_token = 0.000005
input_cost_per_token_above_200k_tokens = 0.00001
output_cost_per_token = 0.000025
output_cost_per_token_above_200k_tokens = 0.0000375

[models."anthropic.claude-sonnet-4-20250514-v1:0"]
display_name = "Claude Sonnet 4"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-05-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."anthropic.claude-sonnet-4-20250514-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."anthropic.claude-sonnet-4-20250514-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."anthropic.claude-sonnet-4-20250514-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225

[models."anthropic.claude-sonnet-4-5-20250929-v1:0"]
display_name = "Claude Sonnet 4.5"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-07-31"
release_date = "2025-09-29"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."anthropic.claude-sonnet-4-5-20250929-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."anthropic.claude-sonnet-4-5-20250929-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."anthropic.claude-sonnet-4-5-20250929-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225

[models."anthropic.claude-sonnet-4-6"]
display_name = "Claude Sonnet 4.6"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-08"
release_date = "2026-02-17"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."anthropic.claude-sonnet-4-6".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."anthropic.claude-sonnet-4-6".pricing."amazon-bedrock"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."anthropic.claude-sonnet-4-6".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225

[models."anthropic.claude-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_tool_choice = true

[models."anthropic.claude-v1".pricing."bedrock"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
[models."anthropic.claude-v1".pricing."bedrock/us-east-1"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
[models."anthropic.claude-v1".pricing."bedrock/us-west-2"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024

[models."anthropic.claude-v2"]
display_name = "Claude 2"
model_family = "claude"
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 4096
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
litellm_provider = "amazon-bedrock"
providers = ["amazon-bedrock"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-08"
release_date = "2023-07-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."anthropic.claude-v2".pricing."amazon-bedrock"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024

[models."anthropic.claude-v2:1"]
display_name = "Claude 2.1"
model_family = "claude"
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-08"
release_date = "2023-11-21"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."anthropic.claude-v2:1".pricing."amazon-bedrock"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
[models."anthropic.claude-v2:1".pricing."bedrock"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
[models."anthropic.claude-v2:1".pricing."bedrock/us-east-1"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
[models."anthropic.claude-v2:1".pricing."bedrock/us-west-2"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024

[models."ap-northeast-1/1-month-commitment/anthropic.claude-instant-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.01475
output_cost_per_second = 0.01475
supports_tool_choice = true

[models."ap-northeast-1/1-month-commitment/anthropic.claude-instant-v1".pricing."bedrock"]
input_cost_per_second = 0.01475
output_cost_per_second = 0.01475

[models."ap-northeast-1/1-month-commitment/anthropic.claude-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.0455
output_cost_per_second = 0.0455

[models."ap-northeast-1/1-month-commitment/anthropic.claude-v1".pricing."bedrock"]
input_cost_per_second = 0.0455
output_cost_per_second = 0.0455

[models."ap-northeast-1/1-month-commitment/anthropic.claude-v2:1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.0455
output_cost_per_second = 0.0455
supports_tool_choice = true

[models."ap-northeast-1/1-month-commitment/anthropic.claude-v2:1".pricing."bedrock"]
input_cost_per_second = 0.0455
output_cost_per_second = 0.0455

[models."ap-northeast-1/6-month-commitment/anthropic.claude-instant-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.008194
output_cost_per_second = 0.008194
supports_tool_choice = true

[models."ap-northeast-1/6-month-commitment/anthropic.claude-instant-v1".pricing."bedrock"]
input_cost_per_second = 0.008194
output_cost_per_second = 0.008194

[models."ap-northeast-1/6-month-commitment/anthropic.claude-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.02527
output_cost_per_second = 0.02527

[models."ap-northeast-1/6-month-commitment/anthropic.claude-v1".pricing."bedrock"]
input_cost_per_second = 0.02527
output_cost_per_second = 0.02527

[models."ap-northeast-1/6-month-commitment/anthropic.claude-v2:1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.02527
output_cost_per_second = 0.02527
supports_tool_choice = true

[models."ap-northeast-1/6-month-commitment/anthropic.claude-v2:1".pricing."bedrock"]
input_cost_per_second = 0.02527
output_cost_per_second = 0.02527

[models."ap-northeast-1/anthropic.claude-instant-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.00000223
output_cost_per_token = 0.00000755
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_tool_choice = true

[models."ap-northeast-1/anthropic.claude-instant-v1".pricing."bedrock"]
input_cost_per_token = 0.00000223
output_cost_per_token = 0.00000755

[models."ap-northeast-1/anthropic.claude-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_tool_choice = true

[models."ap-northeast-1/anthropic.claude-v1".pricing."bedrock"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024

[models."ap-northeast-1/anthropic.claude-v2:1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_tool_choice = true

[models."ap-northeast-1/anthropic.claude-v2:1".pricing."bedrock"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024

[models."ap-northeast-1/deepseek.v3.2"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 7.4e-7
output_cost_per_token = 0.00000222
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_reasoning = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_tool_choice = true

[models."ap-northeast-1/deepseek.v3.2".pricing."bedrock"]
input_cost_per_token = 7.4e-7
output_cost_per_token = 0.00000222

[models."ap-northeast-1/minimax.minimax-m2.1"]
mode = "chat"
max_input_tokens = 196000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.00000144
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."ap-northeast-1/minimax.minimax-m2.1".pricing."bedrock"]
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.00000144

[models."ap-northeast-1/moonshotai.kimi-k2-thinking"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 7.3e-7
output_cost_per_token = 0.00000303
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_reasoning = true

[models."ap-northeast-1/moonshotai.kimi-k2-thinking".pricing."bedrock"]
input_cost_per_token = 7.3e-7
output_cost_per_token = 0.00000303

[models."ap-northeast-1/moonshotai.kimi-k2.5"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 7.2e-7
output_cost_per_token = 0.0000036
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."ap-northeast-1/moonshotai.kimi-k2.5".pricing."bedrock"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 0.0000036

[models."ap-northeast-1/qwen.qwen3-coder-next"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-7
output_cost_per_token = 0.00000144
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."ap-northeast-1/qwen.qwen3-coder-next".pricing."bedrock"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.00000144

[models."ap-south-1/deepseek.v3.2"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 7.4e-7
output_cost_per_token = 0.00000222
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_reasoning = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_tool_choice = true

[models."ap-south-1/deepseek.v3.2".pricing."bedrock"]
input_cost_per_token = 7.4e-7
output_cost_per_token = 0.00000222

[models."ap-south-1/meta.llama3-70b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000318
output_cost_per_token = 0.0000042
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."ap-south-1/meta.llama3-70b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 0.00000318
output_cost_per_token = 0.0000042

[models."ap-south-1/meta.llama3-8b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3.6e-7
output_cost_per_token = 7.2e-7
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."ap-south-1/meta.llama3-8b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 3.6e-7
output_cost_per_token = 7.2e-7

[models."ap-south-1/minimax.minimax-m2.1"]
mode = "chat"
max_input_tokens = 196000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.00000144
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."ap-south-1/minimax.minimax-m2.1".pricing."bedrock"]
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.00000144

[models."ap-south-1/moonshotai.kimi-k2-thinking"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 7.1e-7
output_cost_per_token = 0.00000294
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_reasoning = true

[models."ap-south-1/moonshotai.kimi-k2-thinking".pricing."bedrock"]
input_cost_per_token = 7.1e-7
output_cost_per_token = 0.00000294

[models."ap-south-1/moonshotai.kimi-k2.5"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 7.2e-7
output_cost_per_token = 0.0000036
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."ap-south-1/moonshotai.kimi-k2.5".pricing."bedrock"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 0.0000036

[models."ap-south-1/qwen.qwen3-coder-next"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-7
output_cost_per_token = 0.00000144
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."ap-south-1/qwen.qwen3-coder-next".pricing."bedrock"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.00000144

[models."ap-southeast-3/deepseek.v3.2"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 7.4e-7
output_cost_per_token = 0.00000222
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_reasoning = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_tool_choice = true

[models."ap-southeast-3/deepseek.v3.2".pricing."bedrock"]
input_cost_per_token = 7.4e-7
output_cost_per_token = 0.00000222

[models."ap-southeast-3/minimax.minimax-m2.1"]
mode = "chat"
max_input_tokens = 196000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.00000144
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."ap-southeast-3/minimax.minimax-m2.1".pricing."bedrock"]
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.00000144

[models."ap-southeast-3/moonshotai.kimi-k2.5"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 7.2e-7
output_cost_per_token = 0.0000036
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."ap-southeast-3/moonshotai.kimi-k2.5".pricing."bedrock"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 0.0000036

[models."ap-southeast-3/qwen.qwen3-coder-next"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-7
output_cost_per_token = 0.00000144
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."ap-southeast-3/qwen.qwen3-coder-next".pricing."bedrock"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.00000144

[models."apac.amazon.nova-2-lite-v1:0"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 3.3e-7
output_cost_per_token = 0.00000275
cache_read_input_token_cost = 8.25e-8
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_response_schema = true
supports_video_input = true

[models."apac.amazon.nova-2-lite-v1:0".pricing."bedrock_converse"]
cache_read_input_token_cost = 8.25e-8
input_cost_per_token = 3.3e-7
output_cost_per_token = 0.00000275

[models."apac.amazon.nova-2-pro-preview-20251202-v1:0"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000021875
output_cost_per_token = 0.0000175
cache_read_input_token_cost = 5.46875e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
input_cost_per_audio_token = 0.0000021875
input_cost_per_image_token = 0.0000021875
supports_response_schema = true
supports_video_input = true

[models."apac.amazon.nova-2-pro-preview-20251202-v1:0".pricing."bedrock_converse"]
cache_read_input_token_cost = 5.46875e-7
input_cost_per_audio_token = 0.0000021875
input_cost_per_image_token = 0.0000021875
input_cost_per_token = 0.0000021875
output_cost_per_token = 0.0000175

[models."apac.amazon.nova-lite-v1:0"]
mode = "chat"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 6.3e-8
output_cost_per_token = 2.52e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supports_response_schema = true

[models."apac.amazon.nova-lite-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 6.3e-8
output_cost_per_token = 2.52e-7

[models."apac.amazon.nova-micro-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 3.7e-8
output_cost_per_token = 1.48e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true

[models."apac.amazon.nova-micro-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 3.7e-8
output_cost_per_token = 1.48e-7

[models."apac.amazon.nova-pro-v1:0"]
mode = "chat"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 8.4e-7
output_cost_per_token = 0.00000336
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supports_response_schema = true

[models."apac.amazon.nova-pro-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 8.4e-7
output_cost_per_token = 0.00000336

[models."apac.anthropic.claude-3-5-sonnet-20240620-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true

[models."apac.anthropic.claude-3-5-sonnet-20240620-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."apac.anthropic.claude-3-5-sonnet-20241022-v2:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true

[models."apac.anthropic.claude-3-5-sonnet-20241022-v2:0".pricing."bedrock"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."apac.anthropic.claude-3-haiku-20240307-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true

[models."apac.anthropic.claude-3-haiku-20240307-v1:0".pricing."bedrock"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125

[models."apac.anthropic.claude-3-sonnet-20240229-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true

[models."apac.anthropic.claude-3-sonnet-20240229-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."apac.anthropic.claude-haiku-4-5-20251001-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000055
cache_read_input_token_cost = 1.1e-7
cache_creation_input_token_cost = 0.000001375
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
source = "https://aws.amazon.com/about-aws/whats-new/2025/10/claude-4-5-haiku-anthropic-amazon-bedrock"
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."apac.anthropic.claude-haiku-4-5-20251001-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.000001375
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000055

[models."apac.anthropic.claude-sonnet-4-20250514-v1:0"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."apac.anthropic.claude-sonnet-4-20250514-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."apac.anthropic.claude-sonnet-4-20250514-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225

[models."apac.anthropic.claude-sonnet-4-6"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000033
output_cost_per_token = 0.0000165
cache_read_input_token_cost = 3.3e-7
cache_creation_input_token_cost = 0.000004125
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token_above_200k_tokens = 0.0000066
output_cost_per_token_above_200k_tokens = 0.00002475
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."apac.anthropic.claude-sonnet-4-6".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."apac.anthropic.claude-sonnet-4-6".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.000004125
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost = 3.3e-7
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token = 0.0000033
input_cost_per_token_above_200k_tokens = 0.0000066
output_cost_per_token = 0.0000165
output_cost_per_token_above_200k_tokens = 0.00002475

[models."arcee-ai/trinity-large-preview"]
display_name = "Trinity Large Preview"
model_family = "trinity"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 131000
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."arcee-ai/trinity-large-preview".pricing."vercel"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001

[models."arcee-ai/trinity-large-preview:free"]
display_name = "Trinity Large Preview"
model_family = "trinity"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2026-01-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."arcee-ai/trinity-mini"]
display_name = "Trinity Mini"
model_family = "trinity"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 1.5e-7
litellm_provider = "vercel"
providers = ["vercel", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."arcee-ai/trinity-mini".pricing."kilo"]
input_cost_per_token = 4.5e-8
output_cost_per_token = 1.5e-7
[models."arcee-ai/trinity-mini".pricing."vercel"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 1.5e-7

[models."arcee-ai/trinity-mini:free"]
display_name = "Trinity Mini"
model_family = "trinity-mini"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2026-01-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."ascend-tribe/pangu-pro-moe"]
display_name = "ascend-tribe/pangu-pro-moe"
model_family = "pangu"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn"]
supports_function_calling = false
supports_reasoning = true
open_weights = false
release_date = "2025-07-02"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."ascend-tribe/pangu-pro-moe".pricing."siliconflow-cn"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7

[models."au.anthropic.claude-haiku-4-5-20251001-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000055
cache_read_input_token_cost = 1.1e-7
cache_creation_input_token_cost = 0.000001375
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."au.anthropic.claude-haiku-4-5-20251001-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.000001375
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000055

[models."au.anthropic.claude-opus-4-6-v1"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.0000055
output_cost_per_token = 0.0000275
cache_read_input_token_cost = 5.5e-7
cache_creation_input_token_cost = 0.000006875
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
cache_creation_input_token_cost_above_200k_tokens = 0.00001375
cache_read_input_token_cost_above_200k_tokens = 0.0000011
input_cost_per_token_above_200k_tokens = 0.000011
output_cost_per_token_above_200k_tokens = 0.00004125
supports_assistant_prefill = false
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."au.anthropic.claude-opus-4-6-v1".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."au.anthropic.claude-opus-4-6-v1".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.000006875
cache_creation_input_token_cost_above_200k_tokens = 0.00001375
cache_read_input_token_cost = 5.5e-7
cache_read_input_token_cost_above_200k_tokens = 0.0000011
input_cost_per_token = 0.0000055
input_cost_per_token_above_200k_tokens = 0.000011
output_cost_per_token = 0.0000275
output_cost_per_token_above_200k_tokens = 0.00004125

[models."au.anthropic.claude-sonnet-4-5-20250929-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000033
output_cost_per_token = 0.0000165
cache_read_input_token_cost = 3.3e-7
cache_creation_input_token_cost = 0.000004125
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token_above_200k_tokens = 0.0000066
output_cost_per_token_above_200k_tokens = 0.00002475
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."au.anthropic.claude-sonnet-4-5-20250929-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."au.anthropic.claude-sonnet-4-5-20250929-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.000004125
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost = 3.3e-7
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token = 0.0000033
input_cost_per_token_above_200k_tokens = 0.0000066
output_cost_per_token = 0.0000165
output_cost_per_token_above_200k_tokens = 0.00002475

[models."aurora-alpha"]
display_name = "Aurora Alpha"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 50000
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2026-02-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."auto"]
display_name = "Auto"
model_family = "auto"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 32000
input_cost_per_token = 8.5e-7
output_cost_per_token = 0.00000155
litellm_provider = "morph"
providers = ["morph"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2024-06-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."auto".pricing."morph"]
input_cost_per_token = 8.5e-7
output_cost_per_token = 0.00000155

[models."baai/bge-base-en-v1.5"]
mode = "embedding"
max_input_tokens = 512
input_cost_per_token = 8e-9
output_cost_per_token = 0
litellm_provider = "together_ai"
providers = ["together_ai"]
output_vector_size = 768

[models."baai/bge-base-en-v1.5".pricing."together_ai"]
input_cost_per_token = 8e-9
output_cost_per_token = 0

[models."baai/bge-m3"]
mode = "embedding"
max_input_tokens = 8192
max_output_tokens = 96000
max_tokens = 96000
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8
litellm_provider = "novita"
providers = ["novita"]

[models."baai/bge-m3".pricing."novita"]
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8

[models."baai/bge-reranker-v2-m3"]
display_name = "bge-reranker-v2-m3"
model_family = "bge"
mode = "rerank"
max_input_tokens = 8000
max_output_tokens = 8000
max_tokens = 8000
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8
litellm_provider = "novita"
providers = ["novita", "berget"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."baai/bge-reranker-v2-m3".pricing."berget"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 1.0000000000000001e-7
[models."baai/bge-reranker-v2-m3".pricing."novita"]
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8

[models."babbage-002"]
mode = "completion"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 4e-7
output_cost_per_token = 4e-7
litellm_provider = "text-completion-openai"
providers = ["text-completion-openai"]

[models."babbage-002".pricing."text-completion-openai"]
input_cost_per_token = 4e-7
output_cost_per_token = 4e-7

[models."baichuan/baichuan-m2-32b"]
display_name = "baichuan-m2-32b"
model_family = "baichuan"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 7e-8
output_cost_per_token = 7e-8
litellm_provider = "novita"
providers = ["novita", "novita-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2025-08-13"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."baichuan/baichuan-m2-32b".pricing."novita"]
input_cost_per_token = 7e-8
output_cost_per_token = 7e-8
[models."baichuan/baichuan-m2-32b".pricing."novita-ai"]
input_cost_per_token = 7e-8
output_cost_per_token = 7e-8

[models."baidu/ERNIE-4-5-300B-A47B"]
display_name = "baidu/ERNIE-4.5-300B-A47B"
model_family = "ernie"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 131000
input_cost_per_token = 2.8e-7
output_cost_per_token = 0.0000011
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "kilo", "siliconflow"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-07-02"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."baidu/ERNIE-4-5-300B-A47B".pricing."kilo"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 0.0000011
[models."baidu/ERNIE-4-5-300B-A47B".pricing."siliconflow"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 0.0000011
[models."baidu/ERNIE-4-5-300B-A47B".pricing."siliconflow-cn"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 0.0000011

[models."baidu/ernie-4.5-21B-a3b"]
display_name = "ERNIE 4.5 21B A3B"
model_family = "ernie"
mode = "chat"
max_input_tokens = 120000
max_output_tokens = 8000
max_tokens = 8000
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
litellm_provider = "novita"
providers = ["novita", "kilo", "novita-ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-03"
release_date = "2025-06-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."baidu/ernie-4.5-21B-a3b".pricing."kilo"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
[models."baidu/ernie-4.5-21B-a3b".pricing."novita"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
[models."baidu/ernie-4.5-21B-a3b".pricing."novita-ai"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7

[models."baidu/ernie-4.5-21B-a3b-thinking"]
display_name = "ERNIE-4.5-21B-A3B-Thinking"
model_family = "ernie"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
litellm_provider = "novita"
providers = ["novita", "kilo", "novita-ai"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-03"
release_date = "2025-09-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."baidu/ernie-4.5-21B-a3b-thinking".pricing."kilo"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
[models."baidu/ernie-4.5-21B-a3b-thinking".pricing."novita"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
[models."baidu/ernie-4.5-21B-a3b-thinking".pricing."novita-ai"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7

[models."baidu/ernie-4.5-300b-a47b-paddle"]
display_name = "ERNIE 4.5 300B A47B"
model_family = "ernie"
mode = "chat"
max_input_tokens = 123000
max_output_tokens = 12000
max_tokens = 12000
input_cost_per_token = 2.8e-7
output_cost_per_token = 0.0000011
litellm_provider = "novita"
providers = ["novita", "jiekou", "novita-ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."baidu/ernie-4.5-300b-a47b-paddle".pricing."jiekou"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 0.0000011
[models."baidu/ernie-4.5-300b-a47b-paddle".pricing."novita"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 0.0000011
[models."baidu/ernie-4.5-300b-a47b-paddle".pricing."novita-ai"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 0.0000011

[models."baidu/ernie-4.5-vl-28b-a3b"]
display_name = "ERNIE 4.5 VL 28B A3B"
mode = "chat"
max_input_tokens = 30000
max_output_tokens = 8000
max_tokens = 8000
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.6e-7
litellm_provider = "novita"
providers = ["novita", "kilo", "novita-ai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = true
release_date = "2025-06-30"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."baidu/ernie-4.5-vl-28b-a3b".pricing."kilo"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.6e-7
[models."baidu/ernie-4.5-vl-28b-a3b".pricing."novita"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.6e-7
[models."baidu/ernie-4.5-vl-28b-a3b".pricing."novita-ai"]
input_cost_per_token = 0.0000014
output_cost_per_token = 0.0000056

[models."baidu/ernie-4.5-vl-28b-a3b-thinking"]
display_name = "ERNIE-4.5-VL-28B-A3B-Thinking"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 3.9e-7
output_cost_per_token = 3.9e-7
litellm_provider = "novita"
providers = ["novita", "novita-ai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = true
release_date = "2025-11-26"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."baidu/ernie-4.5-vl-28b-a3b-thinking".pricing."novita"]
input_cost_per_token = 3.9e-7
output_cost_per_token = 3.9e-7
[models."baidu/ernie-4.5-vl-28b-a3b-thinking".pricing."novita-ai"]
input_cost_per_token = 3.9e-7
output_cost_per_token = 3.9e-7

[models."baidu/ernie-4.5-vl-424b-a47b"]
display_name = "ERNIE 4.5 VL 424B A47B"
model_family = "ernie"
mode = "chat"
max_input_tokens = 123000
max_output_tokens = 16000
max_tokens = 16000
input_cost_per_token = 4.2e-7
output_cost_per_token = 0.00000125
litellm_provider = "novita"
providers = ["novita", "jiekou", "kilo", "novita-ai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = true
release_date = "2026-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."baidu/ernie-4.5-vl-424b-a47b".pricing."jiekou"]
input_cost_per_token = 4.2e-7
output_cost_per_token = 0.00000125
[models."baidu/ernie-4.5-vl-424b-a47b".pricing."kilo"]
input_cost_per_token = 4.2e-7
output_cost_per_token = 0.00000125
[models."baidu/ernie-4.5-vl-424b-a47b".pricing."novita"]
input_cost_per_token = 4.2e-7
output_cost_per_token = 0.00000125
[models."baidu/ernie-4.5-vl-424b-a47b".pricing."novita-ai"]
input_cost_per_token = 4.2e-7
output_cost_per_token = 0.00000125

[models."baidu/ernie-5-0-thinking-preview"]
display_name = "ERNIE 5.0"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 64000
input_cost_per_token = 8.4e-7
output_cost_per_token = 0.00000337
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2026-01-22"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."baidu/ernie-5-0-thinking-preview".pricing."zenmux"]
input_cost_per_token = 8.4e-7
output_cost_per_token = 0.00000337

[models."base"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00020833
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."base".metadata]
calculation = "$0.0125/60 seconds = $0.00020833 per second"
original_pricing_per_minute = 0.0125

[models."base".pricing."deepgram"]
input_cost_per_second = 0.00020833
output_cost_per_second = 0

[models."base-conversationalai"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00020833
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."base-conversationalai".metadata]
calculation = "$0.0125/60 seconds = $0.00020833 per second"
original_pricing_per_minute = 0.0125

[models."base-conversationalai".pricing."deepgram"]
input_cost_per_second = 0.00020833
output_cost_per_second = 0

[models."base-finance"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00020833
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."base-finance".metadata]
calculation = "$0.0125/60 seconds = $0.00020833 per second"
original_pricing_per_minute = 0.0125

[models."base-finance".pricing."deepgram"]
input_cost_per_second = 0.00020833
output_cost_per_second = 0

[models."base-general"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00020833
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."base-general".metadata]
calculation = "$0.0125/60 seconds = $0.00020833 per second"
original_pricing_per_minute = 0.0125

[models."base-general".pricing."deepgram"]
input_cost_per_second = 0.00020833
output_cost_per_second = 0

[models."base-meeting"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00020833
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."base-meeting".metadata]
calculation = "$0.0125/60 seconds = $0.00020833 per second"
original_pricing_per_minute = 0.0125

[models."base-meeting".pricing."deepgram"]
input_cost_per_second = 0.00020833
output_cost_per_second = 0

[models."base-phonecall"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00020833
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."base-phonecall".metadata]
calculation = "$0.0125/60 seconds = $0.00020833 per second"
original_pricing_per_minute = 0.0125

[models."base-phonecall".pricing."deepgram"]
input_cost_per_second = 0.00020833
output_cost_per_second = 0

[models."base-video"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00020833
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."base-video".metadata]
calculation = "$0.0125/60 seconds = $0.00020833 per second"
original_pricing_per_minute = 0.0125

[models."base-video".pricing."deepgram"]
input_cost_per_second = 0.00020833
output_cost_per_second = 0

[models."base-voicemail"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00020833
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."base-voicemail".metadata]
calculation = "$0.0125/60 seconds = $0.00020833 per second"
original_pricing_per_minute = 0.0125

[models."base-voicemail".pricing."deepgram"]
input_cost_per_second = 0.00020833
output_cost_per_second = 0

[models."best"]
mode = "audio_transcription"
litellm_provider = "assemblyai"
providers = ["assemblyai"]
input_cost_per_second = 0.00003333
output_cost_per_second = 0

[models."best".pricing."assemblyai"]
input_cost_per_second = 0.00003333
output_cost_per_second = 0

[models."bge-multilingual-gemma2"]
display_name = "BGE Multilingual Gemma2"
model_family = "gemma"
mode = "chat"
max_input_tokens = 8191
max_output_tokens = 3072
input_cost_per_token = 1.3e-7
litellm_provider = "scaleway"
providers = ["scaleway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2024-07-26"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."bge-multilingual-gemma2".pricing."scaleway"]
input_cost_per_token = 1.3e-7

[models."big-pickle"]
display_name = "Big Pickle"
model_family = "big-pickle"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 128000
litellm_provider = "opencode"
providers = ["opencode"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-10-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."bigscience/mt0-xxl-13b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.0005
output_cost_per_token = 0.002
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = false
supports_vision = false
supports_parallel_function_calling = false

[models."bigscience/mt0-xxl-13b".pricing."watsonx"]
input_cost_per_token = 0.0005
output_cost_per_token = 0.002

[models."black-forest-labs/FLUX.1-schnell"]
mode = "image_generation"
litellm_provider = "nscale"
providers = ["nscale"]
source = "https://docs.nscale.com/docs/inference/serverless-models/current#image-models"
input_cost_per_pixel = 1.3e-9
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."black-forest-labs/FLUX.1-schnell".pricing."nscale"]
input_cost_per_pixel = 1.3e-9
output_cost_per_pixel = 0

[models."black-forest-labs/flux-dev"]
display_name = "FLUX.1-dev"
mode = "chat"
max_input_tokens = 77
max_output_tokens = 0
litellm_provider = "nebius"
providers = ["nebius"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2024-08-01"
supported_modalities = ["text"]
supported_output_modalities = ["image"]
source = "modelsdev"

[models."black-forest-labs/flux-schnell"]
display_name = "FLUX.1-schnell"
mode = "chat"
max_input_tokens = 77
max_output_tokens = 0
litellm_provider = "nebius"
providers = ["nebius"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2024-08-01"
supported_modalities = ["text"]
supported_output_modalities = ["image"]
source = "modelsdev"

[models."black-forest-labs/flux.1-dev"]
display_name = "FLUX.1-dev"
model_family = "flux"
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 0
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-08"
release_date = "2024-08-01"
supported_modalities = ["text"]
supported_output_modalities = ["image"]
source = "modelsdev"

[models."black-forest-labs/flux.2-flex"]
display_name = "FLUX.2 Flex"
model_family = "flux"
mode = "chat"
max_input_tokens = 67344
max_output_tokens = 67344
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-06"
release_date = "2025-11-25"
supported_modalities = ["image", "text"]
supported_output_modalities = ["image"]
source = "modelsdev"

[models."black-forest-labs/flux.2-klein-4b"]
display_name = "FLUX.2 Klein 4B"
model_family = "flux"
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 40960
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2026-01-14"
supported_modalities = ["image", "text"]
supported_output_modalities = ["image"]
source = "modelsdev"

[models."black-forest-labs/flux.2-max"]
display_name = "FLUX.2 Max"
model_family = "flux"
mode = "chat"
max_input_tokens = 46864
max_output_tokens = 46864
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-06"
release_date = "2025-12-16"
supported_modalities = ["image", "text"]
supported_output_modalities = ["image"]
source = "modelsdev"

[models."black-forest-labs/flux.2-pro"]
display_name = "FLUX.2 Pro"
model_family = "flux"
mode = "chat"
max_input_tokens = 46864
max_output_tokens = 46864
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-06"
release_date = "2025-11-25"
supported_modalities = ["image", "text"]
supported_output_modalities = ["image"]
source = "modelsdev"

[models."bria/text-to-image/3.2"]
mode = "image_generation"
litellm_provider = "fal_ai"
providers = ["fal_ai"]
output_cost_per_image = 0.0398
supported_endpoints = ["/v1/images/generations"]

[models."bria/text-to-image/3.2".pricing."fal_ai"]
output_cost_per_image = 0.0398

[models."bytedance-seed/seed-1-6"]
display_name = "ByteDance Seed: Seed 1.6"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 32768
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-09"
supported_modalities = ["image", "text", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."bytedance-seed/seed-1-6".pricing."kilo"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002

[models."bytedance-seed/seedream-4-5"]
display_name = "Seedream 4.5"
model_family = "seed"
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2025-12-23"
supported_modalities = ["image", "text"]
supported_output_modalities = ["image"]
source = "modelsdev"

[models."bytedance/seed-1-6"]
display_name = "Seed 1.6"
model_family = "seed"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 32000
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
cache_read_input_token_cost = 5.0000000000000004e-8
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."bytedance/seed-1-6".pricing."vercel"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002

[models."bytedance/seed-1-8"]
display_name = "Seed 1.8"
model_family = "seed"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 64000
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
cache_read_input_token_cost = 5.0000000000000004e-8
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-10"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."bytedance/seed-1-8".pricing."vercel"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002

[models."bytedance/ui-tars-1.5-7b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 1e-7
output_cost_per_token = 2e-7
litellm_provider = "openrouter"
providers = ["openrouter"]
source = "https://openrouter.ai/api/v1/models/bytedance/ui-tars-1.5-7b"
supports_tool_choice = true

[models."bytedance/ui-tars-1.5-7b".pricing."openrouter"]
input_cost_per_token = 1e-7
output_cost_per_token = 2e-7

[models."ca-central-1/meta.llama3-70b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000305
output_cost_per_token = 0.00000403
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."ca-central-1/meta.llama3-70b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 0.00000305
output_cost_per_token = 0.00000403

[models."ca-central-1/meta.llama3-8b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3.5e-7
output_cost_per_token = 6.9e-7
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."ca-central-1/meta.llama3-8b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 6.9e-7

[models."cerebras-llama-4-maverick-17b-128e-instruct"]
display_name = "Cerebras-Llama-4-Maverick-17B-128E-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "llama"
providers = ["llama"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2025-04-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."cerebras-llama-4-scout-17b-16e-instruct"]
display_name = "Cerebras-Llama-4-Scout-17B-16E-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "llama"
providers = ["llama"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2025-04-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."chat-bison"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-chat-models"
providers = ["vertex_ai-chat-models", "palm"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7
supports_tool_choice = true

[models."chat-bison".pricing."palm"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
[models."chat-bison".pricing."vertex_ai-chat-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."chat-bison-001"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "palm"
providers = ["palm"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."chat-bison-001".pricing."palm"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7

[models."chat-bison-32k"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-chat-models"
providers = ["vertex_ai-chat-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7
supports_tool_choice = true

[models."chat-bison-32k".pricing."vertex_ai-chat-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."chat-bison-32k@002"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-chat-models"
providers = ["vertex_ai-chat-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7
supports_tool_choice = true

[models."chat-bison-32k@002".pricing."vertex_ai-chat-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."chat-bison@001"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-chat-models"
providers = ["vertex_ai-chat-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7
supports_tool_choice = true

[models."chat-bison@001".pricing."vertex_ai-chat-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."chat-bison@002"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-chat-models"
providers = ["vertex_ai-chat-models"]
deprecation_date = "2025-04-09"
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7
supports_tool_choice = true

[models."chat-bison@002".pricing."vertex_ai-chat-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."chatdolphin"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7
litellm_provider = "nlp_cloud"
providers = ["nlp_cloud"]

[models."chatdolphin".pricing."nlp_cloud"]
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7

[models."chatgpt-4o-latest"]
display_name = "chatgpt-4o-latest"
model_family = "gpt"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000005
output_cost_per_token = 0.000015
litellm_provider = "openai"
providers = ["openai", "302ai", "helicone", "kilo", "poe"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2023-09"
release_date = "2024-08-08"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."chatgpt-4o-latest".pricing."302ai"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000015
[models."chatgpt-4o-latest".pricing."helicone"]
cache_read_input_token_cost = 0.0000025
input_cost_per_token = 0.000005
output_cost_per_token = 0.00002
[models."chatgpt-4o-latest".pricing."kilo"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000015
[models."chatgpt-4o-latest".pricing."openai"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000015
[models."chatgpt-4o-latest".pricing."poe"]
input_cost_per_token = 0.0000045
output_cost_per_token = 0.000014

[models."chatgpt-image-latest"]
mode = "image_generation"
input_cost_per_token = 0.000005
cache_read_input_token_cost = 0.00000125
litellm_provider = "openai"
providers = ["openai"]
cache_read_input_image_token_cost = 0.0000025
input_cost_per_image_token = 0.00001
output_cost_per_image_token = 0.00004
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."chatgpt-image-latest".pricing."openai"]
cache_read_input_image_token_cost = 0.0000025
cache_read_input_token_cost = 0.00000125
input_cost_per_image_token = 0.00001
input_cost_per_token = 0.000005
output_cost_per_image_token = 0.00004

[models."chirp"]
mode = "audio_speech"
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
source = "https://cloud.google.com/text-to-speech/pricing"
input_cost_per_character = 0.00003
supported_endpoints = ["/v1/audio/speech"]

[models."chirp".pricing."vertex_ai"]
input_cost_per_character = 0.00003

[models."chutesai/Mistral-Small-3-1-24B-Instruct-2503"]
display_name = "Mistral Small 3.1 24B Instruct 2503"
model_family = "chutesai"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 3e-8
output_cost_per_token = 1.1e-7
cache_read_input_token_cost = 1.5e-8
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."chutesai/Mistral-Small-3-1-24B-Instruct-2503".pricing."chutes"]
cache_read_input_token_cost = 1.5e-8
input_cost_per_token = 3e-8
output_cost_per_token = 1.1e-7

[models."chutesai/Mistral-Small-3-2-24B-Instruct-2506"]
display_name = "Mistral Small 3.2 24B Instruct 2506"
model_family = "chutesai"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 6e-8
output_cost_per_token = 1.8e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."chutesai/Mistral-Small-3-2-24B-Instruct-2506".pricing."chutes"]
input_cost_per_token = 6e-8
output_cost_per_token = 1.8e-7

[models."claude-3-5-haiku"]
display_name = "Claude Haiku 3.5"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "heroku"
providers = ["heroku", "cloudflare-ai-gateway", "helicone", "kilo", "opencode", "openrouter", "vercel", "vertex_ai", "zenmux"]
supports_function_calling = true
supports_reasoning = false
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-07-31"
release_date = "2024-10-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_system_messages = true
supports_tool_choice = true

[models."claude-3-5-haiku".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.000004
[models."claude-3-5-haiku".pricing."helicone"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8e-7
output_cost_per_token = 0.000004
[models."claude-3-5-haiku".pricing."kilo"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.000004
[models."claude-3-5-haiku".pricing."opencode"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.000004
[models."claude-3-5-haiku".pricing."openrouter"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.000004
[models."claude-3-5-haiku".pricing."vercel"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.000004
[models."claude-3-5-haiku".pricing."vertex_ai"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-3-5-haiku".pricing."zenmux"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.000004

[models."claude-3-5-haiku-20241022"]
display_name = "Claude Haiku 3.5"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 8e-7
output_cost_per_token = 0.000004
cache_read_input_token_cost = 8e-8
cache_creation_input_token_cost = 0.000001
litellm_provider = "anthropic"
providers = ["anthropic"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-07-31"
release_date = "2024-10-22"
deprecation_date = "2025-10-01"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.000006
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true
tool_use_system_prompt_tokens = 264

[models."claude-3-5-haiku-20241022".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-5-haiku-20241022".pricing."anthropic"]
cache_creation_input_token_cost = 0.000001
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8e-7
output_cost_per_token = 0.000004

[models."claude-3-5-haiku-latest"]
display_name = "Claude Haiku 3.5 (latest)"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
cache_read_input_token_cost = 1e-7
cache_creation_input_token_cost = 0.00000125
litellm_provider = "anthropic"
providers = ["anthropic"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-07-31"
release_date = "2024-10-22"
deprecation_date = "2025-10-01"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.000006
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true
tool_use_system_prompt_tokens = 264

[models."claude-3-5-haiku-latest".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-5-haiku-latest".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000125
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005

[models."claude-3-5-haiku@20241022"]
display_name = "Claude Haiku 3.5"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex-anthropic"]
supports_function_calling = true
supports_reasoning = false
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-07-31"
release_date = "2024-10-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_tool_choice = true

[models."claude-3-5-haiku@20241022".pricing."google-vertex-anthropic"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.000004
[models."claude-3-5-haiku@20241022".pricing."vertex_ai"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005

[models."claude-3-5-sonnet"]
display_name = "Claude Sonnet 3.5 v2"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake", "cloudflare-ai-gateway", "kilo", "vercel", "vercel_ai_gateway", "vertex_ai", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04-30"
release_date = "2024-10-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true

[models."claude-3-5-sonnet".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3-5-sonnet".pricing."kilo"]
input_cost_per_token = 0.000006
output_cost_per_token = 0.00003
[models."claude-3-5-sonnet".pricing."vercel"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3-5-sonnet".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3-5-sonnet".pricing."vertex_ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3-5-sonnet".pricing."zenmux"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-3-5-sonnet-20240620"]
display_name = "Claude 3.5 Sonnet (2024-06-20)"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "anthropic"
providers = ["anthropic", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-06-20"
deprecation_date = "2025-06-01"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.000006
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."claude-3-5-sonnet-20240620".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3-5-sonnet-20240620".pricing."vercel"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-3-5-sonnet-20241022"]
display_name = "Claude Sonnet 3.5 v2"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "anthropic"
providers = ["anthropic", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04-30"
release_date = "2024-10-22"
deprecation_date = "2025-10-01"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.000006
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true
tool_use_system_prompt_tokens = 159

[models."claude-3-5-sonnet-20241022".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-5-sonnet-20241022".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3-5-sonnet-20241022".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-3-5-sonnet-latest"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "anthropic"
providers = ["anthropic", "heroku"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
deprecation_date = "2025-06-01"
cache_creation_input_token_cost_above_1hr = 0.000006
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true
tool_use_system_prompt_tokens = 159

[models."claude-3-5-sonnet-latest".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-5-sonnet-latest".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-3-5-sonnet-v2"]
display_name = "Anthropic: Claude 3.5 Sonnet v2"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "helicone"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-10-22"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_computer_use = true
supports_tool_choice = true

[models."claude-3-5-sonnet-v2".pricing."helicone"]
cache_read_input_token_cost = 3.0000000000000004e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3-5-sonnet-v2".pricing."vertex_ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-3-5-sonnet-v2@20241022"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_vision = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_computer_use = true
supports_tool_choice = true

[models."claude-3-5-sonnet-v2@20241022".pricing."vertex_ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-3-5-sonnet@20240620"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_vision = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_tool_choice = true

[models."claude-3-5-sonnet@20240620".pricing."vertex_ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-3-5-sonnet@20241022"]
display_name = "Claude Sonnet 3.5 v2"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
litellm_provider = "google-vertex-anthropic"
providers = ["google-vertex-anthropic"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04-30"
release_date = "2024-10-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-3-5-sonnet@20241022".pricing."google-vertex-anthropic"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-3-7-sonnet"]
display_name = "Claude Sonnet 3.7"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
litellm_provider = "heroku"
providers = ["heroku", "helicone", "kilo", "openrouter", "requesty", "vercel", "vercel_ai_gateway", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2024-10-31"
release_date = "2025-02-19"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."claude-3-7-sonnet".pricing."helicone"]
cache_read_input_token_cost = 3.0000000000000004e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3-7-sonnet".pricing."kilo"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3-7-sonnet".pricing."openrouter"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-3-7-sonnet".pricing."requesty"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3-7-sonnet".pricing."vercel"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3-7-sonnet".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3-7-sonnet".pricing."zenmux"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-3-7-sonnet-20250219"]
display_name = "Claude Sonnet 3.7"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "anthropic"
providers = ["anthropic", "abacus"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-10-31"
release_date = "2025-02-19"
deprecation_date = "2026-02-19"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.000006
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true
tool_use_system_prompt_tokens = 159

[models."claude-3-7-sonnet-20250219".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-7-sonnet-20250219".pricing."abacus"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3-7-sonnet-20250219".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-3-7-sonnet-latest"]
display_name = "Claude Sonnet 3.7 (Latest)"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 200000
max_tokens = 200000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "anthropic"
providers = ["anthropic", "deepinfra"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-10-31"
release_date = "2025-03-13"
deprecation_date = "2025-06-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.000006
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."claude-3-7-sonnet-latest".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-3-7-sonnet-latest".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3-7-sonnet-latest".pricing."deepinfra"]
cache_read_input_token_cost = 3.3e-7
input_cost_per_token = 0.0000033
output_cost_per_token = 0.0000165

[models."claude-3-7-sonnet:thinking"]
display_name = "Anthropic: Claude 3.7 Sonnet (thinking)"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-02-19"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-3-7-sonnet:thinking".pricing."kilo"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-3-7-sonnet@20250219"]
display_name = "Claude Sonnet 3.7"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex-anthropic"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-10-31"
release_date = "2025-02-19"
deprecation_date = "2025-06-01"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."claude-3-7-sonnet@20250219".pricing."google-vertex-anthropic"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3-7-sonnet@20250219".pricing."vertex_ai"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-3-haiku"]
display_name = "Claude Haiku 3"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 200000
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125
litellm_provider = "openrouter"
providers = ["openrouter", "cloudflare-ai-gateway", "kilo", "vercel", "vercel_ai_gateway", "vertex_ai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-08-31"
release_date = "2024-03-13"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
input_cost_per_image = 0.0004
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."claude-3-haiku".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125
[models."claude-3-haiku".pricing."kilo"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125
[models."claude-3-haiku".pricing."openrouter"]
input_cost_per_image = 0.0004
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125
[models."claude-3-haiku".pricing."vercel"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125
[models."claude-3-haiku".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 3e-7
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125
[models."claude-3-haiku".pricing."vertex_ai"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125

[models."claude-3-haiku-20240307"]
display_name = "Anthropic: Claude 3 Haiku"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125
cache_read_input_token_cost = 3e-8
cache_creation_input_token_cost = 3e-7
litellm_provider = "anthropic"
providers = ["anthropic", "helicone"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2024-03"
release_date = "2024-03-07"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.000006
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 264

[models."claude-3-haiku-20240307".pricing."anthropic"]
cache_creation_input_token_cost = 3e-7
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125
[models."claude-3-haiku-20240307".pricing."helicone"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125

[models."claude-3-haiku@20240307"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_vision = true
supports_assistant_prefill = true
supports_tool_choice = true

[models."claude-3-haiku@20240307".pricing."vertex_ai"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125

[models."claude-3-opus"]
display_name = "Claude Opus 3"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
cache_creation_input_token_cost = 0.00001875
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "cloudflare-ai-gateway", "vercel", "vertex_ai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-08-31"
release_date = "2024-02-29"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."claude-3-opus".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-3-opus".pricing."vercel"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-3-opus".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-3-opus".pricing."vertex_ai"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."claude-3-opus-20240229"]
display_name = "Claude Opus 3"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
cache_creation_input_token_cost = 0.00001875
litellm_provider = "anthropic"
providers = ["anthropic"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2023-08-31"
release_date = "2024-02-29"
deprecation_date = "2026-05-01"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.000006
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 395

[models."claude-3-opus-20240229".pricing."anthropic"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."claude-3-opus-latest"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
cache_creation_input_token_cost = 0.00001875
litellm_provider = "anthropic"
providers = ["anthropic"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
deprecation_date = "2025-03-01"
cache_creation_input_token_cost_above_1hr = 0.000006
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 395

[models."claude-3-opus-latest".pricing."anthropic"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."claude-3-opus@20240229"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_vision = true
supports_assistant_prefill = true
supports_tool_choice = true

[models."claude-3-opus@20240229".pricing."vertex_ai"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."claude-3-sonnet"]
display_name = "Claude Sonnet 3"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "cloudflare-ai-gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-08-31"
release_date = "2024-03-04"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_tool_choice = true

[models."claude-3-sonnet".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3-sonnet".pricing."vertex_ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-3-sonnet-20240229"]
display_name = "Claude Sonnet 3"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
litellm_provider = "anthropic"
providers = ["anthropic"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-08-31"
release_date = "2024-03-04"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-3-sonnet-20240229".pricing."anthropic"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-3-sonnet@20240229"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_vision = true
supports_assistant_prefill = true
supports_tool_choice = true

[models."claude-3-sonnet@20240229".pricing."vertex_ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-3.5-haiku"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
litellm_provider = "replicate"
providers = ["replicate", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."claude-3.5-haiku".pricing."replicate"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-3.5-haiku".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0.000001
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8e-7
output_cost_per_token = 0.000004

[models."claude-3.5-sonnet"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "openrouter"
providers = ["openrouter", "replicate", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_assistant_prefill = true
supports_computer_use = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."claude-3.5-sonnet".pricing."openrouter"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3.5-sonnet".pricing."replicate"]
input_cost_per_token = 0.00000375
output_cost_per_token = 0.00001875
[models."claude-3.5-sonnet".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-3.7-sonnet"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "openrouter"
providers = ["openrouter", "replicate", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
input_cost_per_image = 0.0048
supports_assistant_prefill = true
supports_computer_use = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."claude-3.7-sonnet".pricing."openrouter"]
input_cost_per_image = 0.0048
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3.7-sonnet".pricing."replicate"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-3.7-sonnet".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-4-5-opus"]
display_name = "Anthropic: Claude Opus 4.5"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 5e-7
litellm_provider = "helicone"
providers = ["helicone"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-11"
release_date = "2025-11-24"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-4-5-opus".pricing."helicone"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025

[models."claude-4-5-sonnet"]
display_name = "Claude 4.5 Sonnet"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 200000
input_cost_per_token = 0.000003259
output_cost_per_token = 0.000016295999999999998
litellm_provider = "cortecs"
providers = ["cortecs", "helicone"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-07-31"
release_date = "2025-09-29"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-4-5-sonnet".pricing."cortecs"]
input_cost_per_token = 0.000003259
output_cost_per_token = 0.000016295999999999998
[models."claude-4-5-sonnet".pricing."helicone"]
cache_read_input_token_cost = 3.0000000000000004e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-4-opus"]
display_name = "Claude Opus 4"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 200000
max_tokens = 200000
input_cost_per_token = 0.0000165
output_cost_per_token = 0.0000825
litellm_provider = "deepinfra"
providers = ["deepinfra", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-06-12"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_tool_choice = true

[models."claude-4-opus".pricing."deepinfra"]
input_cost_per_token = 0.0000165
output_cost_per_token = 0.0000825
[models."claude-4-opus".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."claude-4-opus-20250514"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
cache_creation_input_token_cost = 0.00001875
litellm_provider = "anthropic"
providers = ["anthropic"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."claude-4-opus-20250514".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-4-opus-20250514".pricing."anthropic"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."claude-4-sonnet"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 200000
max_tokens = 200000
input_cost_per_token = 0.0000033
output_cost_per_token = 0.0000165
litellm_provider = "deepinfra"
providers = ["deepinfra", "heroku", "replicate", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."claude-4-sonnet".pricing."deepinfra"]
input_cost_per_token = 0.0000033
output_cost_per_token = 0.0000165
[models."claude-4-sonnet".pricing."replicate"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-4-sonnet".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-4-sonnet-20250514"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "anthropic"
providers = ["anthropic"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true
tool_use_system_prompt_tokens = 159

[models."claude-4-sonnet-20250514".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-4-sonnet-20250514".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225

[models."claude-4.5-haiku"]
display_name = "Anthropic: Claude 4.5 Haiku"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
litellm_provider = "replicate"
providers = ["replicate", "helicone"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2025-10"
release_date = "2025-10-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."claude-4.5-haiku".pricing."helicone"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-4.5-haiku".pricing."replicate"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005

[models."claude-4.5-sonnet"]
mode = "chat"
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "replicate"
providers = ["replicate"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."claude-4.5-sonnet".pricing."replicate"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-haiku-3"]
display_name = "Claude-Haiku-3"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 189096
max_output_tokens = 8192
input_cost_per_token = 2.1e-7
output_cost_per_token = 0.0000011
cache_read_input_token_cost = 2.1000000000000003e-8
litellm_provider = "poe"
providers = ["poe"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-03-09"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-haiku-3".pricing."poe"]
cache_read_input_token_cost = 2.1000000000000003e-8
input_cost_per_token = 2.1e-7
output_cost_per_token = 0.0000011

[models."claude-haiku-3-5"]
display_name = "Claude-Haiku-3.5"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 189096
max_output_tokens = 8192
input_cost_per_token = 6.800000000000001e-7
output_cost_per_token = 0.0000034
cache_read_input_token_cost = 6.8e-8
litellm_provider = "poe"
providers = ["poe"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-10-01"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-haiku-3-5".pricing."poe"]
cache_read_input_token_cost = 6.8e-8
input_cost_per_token = 6.800000000000001e-7
output_cost_per_token = 0.0000034

[models."claude-haiku-4-5"]
display_name = "Claude Haiku 4.5"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
cache_read_input_token_cost = 1e-7
cache_creation_input_token_cost = 0.00000125
litellm_provider = "azure_ai"
providers = ["azure_ai", "aihubmix", "anthropic", "azure", "azure-cognitive-services", "cloudflare-ai-gateway", "firmware", "github-copilot", "kilo", "opencode", "openrouter", "perplexity", "poe", "requesty", "vercel", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-02-28"
release_date = "2025-10-15"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.000002
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."claude-haiku-4-5".pricing."aihubmix"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000055
[models."claude-haiku-4-5".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000125
cache_creation_input_token_cost_above_1hr = 0.000002
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4-5".pricing."azure"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4-5".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4-5".pricing."azure_ai"]
cache_creation_input_token_cost = 0.00000125
cache_creation_input_token_cost_above_1hr = 0.000002
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4-5".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4-5".pricing."firmware"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4-5".pricing."kilo"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4-5".pricing."opencode"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4-5".pricing."openrouter"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4-5".pricing."poe"]
cache_read_input_token_cost = 8.500000000000001e-8
input_cost_per_token = 8.5e-7
output_cost_per_token = 0.0000042999999999999995
[models."claude-haiku-4-5".pricing."requesty"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4-5".pricing."vercel"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4-5".pricing."zenmux"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005

[models."claude-haiku-4-5-20251001"]
display_name = "claude-haiku-4-5-20251001"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
cache_read_input_token_cost = 1e-7
cache_creation_input_token_cost = 0.00000125
litellm_provider = "anthropic"
providers = ["anthropic", "302ai", "abacus", "helicone", "jiekou", "qihang-ai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03"
release_date = "2025-10-16"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.000002
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true

[models."claude-haiku-4-5-20251001".pricing."302ai"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4-5-20251001".pricing."abacus"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4-5-20251001".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000125
cache_creation_input_token_cost_above_1hr = 0.000002
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4-5-20251001".pricing."helicone"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4-5-20251001".pricing."jiekou"]
input_cost_per_token = 9.000000000000001e-7
output_cost_per_token = 0.0000045
[models."claude-haiku-4-5-20251001".pricing."qihang-ai"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 7.1e-7

[models."claude-haiku-4-5@20251001"]
display_name = "Claude Haiku 4.5"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
cache_read_input_token_cost = 1e-7
cache_creation_input_token_cost = 0.00000125
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex-anthropic"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-02-28"
release_date = "2025-10-15"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/haiku-4-5"
supports_assistant_prefill = true
supports_native_streaming = true
supports_response_schema = true
supports_tool_choice = true

[models."claude-haiku-4-5@20251001".pricing."google-vertex-anthropic"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4-5@20251001".pricing."vertex_ai"]
cache_creation_input_token_cost = 0.00000125
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005

[models."claude-haiku-4.5"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 200000
max_tokens = 200000
litellm_provider = "github_copilot"
providers = ["github_copilot", "openrouter", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supported_endpoints = ["/v1/chat/completions"]
supports_assistant_prefill = true
supports_computer_use = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."claude-haiku-4.5".pricing."openrouter"]
cache_creation_input_token_cost = 0.00000125
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."claude-haiku-4.5".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0.00000125
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005

[models."claude-opus-4"]
display_name = "Claude Opus 4"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 409600
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
litellm_provider = "gmi"
providers = ["gmi", "cloudflare-ai-gateway", "helicone", "kilo", "openrouter", "poe", "requesty", "vercel", "vercel_ai_gateway", "vertex_ai", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-05-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true

[models."claude-opus-4".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4".pricing."gmi"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4".pricing."helicone"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4".pricing."kilo"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4".pricing."openrouter"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_image = 0.0048
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4".pricing."poe"]
cache_read_input_token_cost = 0.0000013
input_cost_per_token = 0.000013
output_cost_per_token = 0.000064
[models."claude-opus-4".pricing."requesty"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4".pricing."vercel"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4".pricing."vertex_ai"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4".pricing."zenmux"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."claude-opus-4-0"]
display_name = "Claude Opus 4 (latest)"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
litellm_provider = "anthropic"
providers = ["anthropic"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-05-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-opus-4-0".pricing."anthropic"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."claude-opus-4-1"]
display_name = "Claude Opus 4"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
cache_creation_input_token_cost = 0.00001875
litellm_provider = "azure_ai"
providers = ["azure_ai", "aihubmix", "anthropic", "azure", "azure-cognitive-services", "cloudflare-ai-gateway", "fastrouter", "helicone", "kilo", "opencode", "openrouter", "poe", "requesty", "vercel", "vertex_ai", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-05-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.00003
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true

[models."claude-opus-4-1".pricing."aihubmix"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.0000165
output_cost_per_token = 0.0000825
[models."claude-opus-4-1".pricing."anthropic"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.00003
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1".pricing."azure"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1".pricing."azure_ai"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.00003
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1".pricing."fastrouter"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1".pricing."helicone"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1".pricing."kilo"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1".pricing."opencode"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1".pricing."openrouter"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1".pricing."poe"]
cache_read_input_token_cost = 0.0000013
input_cost_per_token = 0.000013
output_cost_per_token = 0.000064
[models."claude-opus-4-1".pricing."requesty"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1".pricing."vercel"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1".pricing."vertex_ai"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
input_cost_per_token_batches = 0.0000075
output_cost_per_token = 0.000075
output_cost_per_token_batches = 0.0000375
[models."claude-opus-4-1".pricing."zenmux"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."claude-opus-4-1-20250805"]
display_name = "claude-opus-4-1-20250805"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
cache_creation_input_token_cost = 0.00001875
litellm_provider = "anthropic"
providers = ["anthropic", "302ai", "abacus", "helicone", "jiekou"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03"
release_date = "2025-08-05"
deprecation_date = "2026-08-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.00003
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4-1-20250805".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-opus-4-1-20250805".pricing."302ai"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1-20250805".pricing."abacus"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1-20250805".pricing."anthropic"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.00003
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1-20250805".pricing."helicone"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1-20250805".pricing."jiekou"]
input_cost_per_token = 0.0000135
output_cost_per_token = 0.0000675

[models."claude-opus-4-1-20250805-thinking"]
display_name = "claude-opus-4-1-20250805-thinking"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
litellm_provider = "302ai"
providers = ["302ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-03"
release_date = "2025-05-27"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-opus-4-1-20250805-thinking".pricing."302ai"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."claude-opus-4-1@20250805"]
display_name = "Claude Opus 4.1"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
cache_creation_input_token_cost = 0.00001875
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex-anthropic"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-08-05"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
input_cost_per_token_batches = 0.0000075
output_cost_per_token_batches = 0.0000375
supports_assistant_prefill = true
supports_tool_choice = true

[models."claude-opus-4-1@20250805".pricing."google-vertex-anthropic"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-1@20250805".pricing."vertex_ai"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
input_cost_per_token_batches = 0.0000075
output_cost_per_token = 0.000075
output_cost_per_token_batches = 0.0000375

[models."claude-opus-4-20250514"]
display_name = "claude-opus-4-20250514"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
cache_creation_input_token_cost = 0.00001875
litellm_provider = "anthropic"
providers = ["anthropic", "abacus", "jiekou"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2026-01"
deprecation_date = "2026-05-14"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.00003
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4-20250514".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-opus-4-20250514".pricing."abacus"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-20250514".pricing."anthropic"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.00003
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4-20250514".pricing."jiekou"]
input_cost_per_token = 0.0000135
output_cost_per_token = 0.0000675

[models."claude-opus-4-5"]
display_name = "Claude Opus 4.5"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 5e-7
cache_creation_input_token_cost = 0.00000625
litellm_provider = "azure_ai"
providers = ["azure_ai", "aihubmix", "anthropic", "azure", "azure-cognitive-services", "cloudflare-ai-gateway", "firmware", "github-copilot", "kilo", "opencode", "openrouter", "perplexity", "poe", "requesty", "vercel", "vertex_ai", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-11-24"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.00001
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."claude-opus-4-5".pricing."aihubmix"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000625
cache_creation_input_token_cost_above_1hr = 0.00001
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5".pricing."azure"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5".pricing."azure_ai"]
cache_creation_input_token_cost = 0.00000625
cache_creation_input_token_cost_above_1hr = 0.00001
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5".pricing."firmware"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5".pricing."kilo"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5".pricing."opencode"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5".pricing."openrouter"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5".pricing."poe"]
cache_read_input_token_cost = 4.3e-7
input_cost_per_token = 0.0000042999999999999995
output_cost_per_token = 0.000021
[models."claude-opus-4-5".pricing."requesty"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5".pricing."vercel"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5".pricing."vertex_ai"]
cache_creation_input_token_cost = 0.00000625
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5".pricing."zenmux"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025

[models."claude-opus-4-5-20251101"]
display_name = "claude-opus-4-5-20251101"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 5e-7
cache_creation_input_token_cost = 0.00000625
litellm_provider = "anthropic"
providers = ["anthropic", "302ai", "abacus", "jiekou", "qihang-ai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03"
release_date = "2025-11-25"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.00001
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4-5-20251101".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-opus-4-5-20251101".pricing."302ai"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5-20251101".pricing."abacus"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5-20251101".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000625
cache_creation_input_token_cost_above_1hr = 0.00001
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5-20251101".pricing."jiekou"]
input_cost_per_token = 0.0000045
output_cost_per_token = 0.0000225
[models."claude-opus-4-5-20251101".pricing."qihang-ai"]
input_cost_per_token = 7.1e-7
output_cost_per_token = 0.0000035699999999999997

[models."claude-opus-4-5-20251101-thinking"]
display_name = "claude-opus-4-5-20251101-thinking"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
litellm_provider = "302ai"
providers = ["302ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-03"
release_date = "2025-11-25"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-opus-4-5-20251101-thinking".pricing."302ai"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025

[models."claude-opus-4-5@20251101"]
display_name = "Claude Opus 4.5"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 5e-7
cache_creation_input_token_cost = 0.00000625
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex-anthropic"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-11-24"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_computer_use = true
supports_native_streaming = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4-5@20251101".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-opus-4-5@20251101".pricing."google-vertex-anthropic"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-5@20251101".pricing."vertex_ai"]
cache_creation_input_token_cost = 0.00000625
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025

[models."claude-opus-4-6"]
display_name = "Claude Opus 4.6"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 5e-7
cache_creation_input_token_cost = 0.00000625
litellm_provider = "opencode"
providers = ["opencode", "aihubmix", "anthropic", "azure", "azure_ai", "azure-cognitive-services", "cloudflare-ai-gateway", "firmware", "github-copilot", "jiekou", "kilo", "openrouter", "perplexity", "poe", "venice", "vercel", "vertex_ai", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-08-31"
release_date = "2026-02-05"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "custom"
cache_creation_input_token_cost_above_1hr = 0.00001
cache_creation_input_token_cost_above_200k_tokens = 0.0000125
cache_read_input_token_cost_above_200k_tokens = 0.000001
input_cost_per_token_above_200k_tokens = 0.00001
output_cost_per_token_above_200k_tokens = 0.0000375
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4-6".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-opus-4-6".pricing."aihubmix"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-6".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000625
cache_creation_input_token_cost_above_1hr = 0.00001
cache_creation_input_token_cost_above_200k_tokens = 0.0000125
cache_read_input_token_cost = 5e-7
cache_read_input_token_cost_above_200k_tokens = 0.000001
input_cost_per_token = 0.000005
input_cost_per_token_above_200k_tokens = 0.00001
output_cost_per_token = 0.000025
output_cost_per_token_above_200k_tokens = 0.0000375
[models."claude-opus-4-6".pricing."azure"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-6".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-6".pricing."azure_ai"]
cache_creation_input_token_cost = 0.00000625
cache_creation_input_token_cost_above_1hr = 0.00001
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-6".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-6".pricing."firmware"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-6".pricing."jiekou"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-6".pricing."kilo"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-6".pricing."opencode"]
cache_creation_input_token_cost = 0.00000625
cache_creation_input_token_cost_above_200k_tokens = 0.0000125
cache_read_input_token_cost = 5e-7
cache_read_input_token_cost_above_200k_tokens = 0.000001
input_cost_per_token = 0.000005
input_cost_per_token_above_200k_tokens = 0.00001
output_cost_per_token = 0.000025
output_cost_per_token_above_200k_tokens = 0.0000375
[models."claude-opus-4-6".pricing."openrouter"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-6".pricing."poe"]
cache_read_input_token_cost = 4.3e-7
input_cost_per_token = 0.0000042999999999999995
output_cost_per_token = 0.000021
[models."claude-opus-4-6".pricing."venice"]
cache_read_input_token_cost = 6e-7
input_cost_per_token = 0.000006
output_cost_per_token = 0.00003
[models."claude-opus-4-6".pricing."vercel"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-6".pricing."vertex_ai"]
cache_creation_input_token_cost = 0.00000625
cache_creation_input_token_cost_above_200k_tokens = 0.0000125
cache_read_input_token_cost = 5e-7
cache_read_input_token_cost_above_200k_tokens = 0.000001
input_cost_per_token = 0.000005
input_cost_per_token_above_200k_tokens = 0.00001
output_cost_per_token = 0.000025
output_cost_per_token_above_200k_tokens = 0.0000375
[models."claude-opus-4-6".pricing."zenmux"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025

[models."claude-opus-4-6-20260205"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 5e-7
cache_creation_input_token_cost = 0.00000625
litellm_provider = "anthropic"
providers = ["anthropic"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
cache_creation_input_token_cost_above_1hr = 0.00001
cache_creation_input_token_cost_above_200k_tokens = 0.0000125
cache_read_input_token_cost_above_200k_tokens = 0.000001
input_cost_per_token_above_200k_tokens = 0.00001
output_cost_per_token_above_200k_tokens = 0.0000375
supports_assistant_prefill = false
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."claude-opus-4-6-20260205".provider_specific_entry]
fast = 6
us = 1.1

[models."claude-opus-4-6-20260205".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-opus-4-6-20260205".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000625
cache_creation_input_token_cost_above_1hr = 0.00001
cache_creation_input_token_cost_above_200k_tokens = 0.0000125
cache_read_input_token_cost = 5e-7
cache_read_input_token_cost_above_200k_tokens = 0.000001
input_cost_per_token = 0.000005
input_cost_per_token_above_200k_tokens = 0.00001
output_cost_per_token = 0.000025
output_cost_per_token_above_200k_tokens = 0.0000375

[models."claude-opus-4-6-think"]
display_name = "Claude Opus 4.6 Think"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 128000
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 3e-7
litellm_provider = "aihubmix"
providers = ["aihubmix"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-05"
release_date = "2026-02-05"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-opus-4-6-think".pricing."aihubmix"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025

[models."claude-opus-4-6@default"]
display_name = "Claude Opus 4.6"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 5e-7
cache_creation_input_token_cost = 0.00000625
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex-anthropic"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-05"
release_date = "2026-02-05"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.0000125
cache_read_input_token_cost_above_200k_tokens = 0.000001
input_cost_per_token_above_200k_tokens = 0.00001
output_cost_per_token_above_200k_tokens = 0.0000375
supports_assistant_prefill = false
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."claude-opus-4-6@default".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-opus-4-6@default".pricing."google-vertex-anthropic"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4-6@default".pricing."vertex_ai"]
cache_creation_input_token_cost = 0.00000625
cache_creation_input_token_cost_above_200k_tokens = 0.0000125
cache_read_input_token_cost = 5e-7
cache_read_input_token_cost_above_200k_tokens = 0.000001
input_cost_per_token = 0.000005
input_cost_per_token_above_200k_tokens = 0.00001
output_cost_per_token = 0.000025
output_cost_per_token_above_200k_tokens = 0.0000375

[models."claude-opus-4.1"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
cache_creation_input_token_cost = 0.00001875
litellm_provider = "openrouter"
providers = ["openrouter", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
cache_creation_input_token_cost_above_1hr = 0.00003
input_cost_per_image = 0.0048
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4.1".pricing."openrouter"]
cache_creation_input_token_cost = 0.00001875
cache_creation_input_token_cost_above_1hr = 0.00003
cache_read_input_token_cost = 0.0000015
input_cost_per_image = 0.0048
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4.1".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."claude-opus-4.5"]
mode = "chat"
max_input_tokens = 409600
max_output_tokens = 64000
max_tokens = 64000
litellm_provider = "github_copilot"
providers = ["github_copilot", "gmi", "openrouter", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supported_endpoints = ["/v1/chat/completions"]
supports_assistant_prefill = true
supports_computer_use = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."claude-opus-4.5".pricing."gmi"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4.5".pricing."openrouter"]
cache_creation_input_token_cost = 0.00000625
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."claude-opus-4.5".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0.00000625
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025

[models."claude-opus-4.6"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 5e-7
cache_creation_input_token_cost = 0.00000625
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true

[models."claude-opus-4.6".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0.00000625
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025

[models."claude-opus-4.6-fast"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16000
max_tokens = 16000
litellm_provider = "github_copilot"
providers = ["github_copilot"]
supports_function_calling = true
supports_vision = true
supported_endpoints = ["/v1/chat/completions"]
supports_parallel_function_calling = true

[models."claude-opus-41"]
display_name = "Claude Opus 4.1"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 80000
max_output_tokens = 16000
max_tokens = 16000
litellm_provider = "github_copilot"
providers = ["github_copilot", "github-copilot"]
supports_function_calling = false
supports_vision = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-08-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions"]

[models."claude-opus-45"]
display_name = "Claude Opus 4.5"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 198000
max_output_tokens = 49500
input_cost_per_token = 0.000006
output_cost_per_token = 0.00003
cache_read_input_token_cost = 6e-7
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-03"
release_date = "2025-12-06"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-opus-45".pricing."venice"]
cache_read_input_token_cost = 6e-7
input_cost_per_token = 0.000006
output_cost_per_token = 0.00003

[models."claude-opus-4@20250514"]
display_name = "Claude Opus 4"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
cache_creation_input_token_cost = 0.00001875
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex-anthropic"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-05-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."claude-opus-4@20250514".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-opus-4@20250514".pricing."google-vertex-anthropic"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."claude-opus-4@20250514".pricing."vertex_ai"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."claude-sonnet-3-5"]
display_name = "Claude-Sonnet-3.5"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 189096
max_output_tokens = 8192
input_cost_per_token = 0.0000026
output_cost_per_token = 0.000013
cache_read_input_token_cost = 2.6e-7
litellm_provider = "poe"
providers = ["poe"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-06-05"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-sonnet-3-5".pricing."poe"]
cache_read_input_token_cost = 2.6e-7
input_cost_per_token = 0.0000026
output_cost_per_token = 0.000013

[models."claude-sonnet-3-5-june"]
display_name = "Claude-Sonnet-3.5-June"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 189096
max_output_tokens = 8192
input_cost_per_token = 0.0000026
output_cost_per_token = 0.000013
cache_read_input_token_cost = 2.6e-7
litellm_provider = "poe"
providers = ["poe"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-11-18"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-sonnet-3-5-june".pricing."poe"]
cache_read_input_token_cost = 2.6e-7
input_cost_per_token = 0.0000026
output_cost_per_token = 0.000013

[models."claude-sonnet-3-7"]
display_name = "Claude-Sonnet-3.7"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 196608
max_output_tokens = 128000
input_cost_per_token = 0.0000026
output_cost_per_token = 0.000013
cache_read_input_token_cost = 2.6e-7
litellm_provider = "poe"
providers = ["poe"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-02-19"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-sonnet-3-7".pricing."poe"]
cache_read_input_token_cost = 2.6e-7
input_cost_per_token = 0.0000026
output_cost_per_token = 0.000013

[models."claude-sonnet-4"]
display_name = "Claude Sonnet 4"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
litellm_provider = "github_copilot"
providers = ["github_copilot", "cloudflare-ai-gateway", "cortecs", "fastrouter", "github-copilot", "gmi", "helicone", "kilo", "opencode", "openrouter", "poe", "requesty", "vercel", "vercel_ai_gateway", "vertex_ai", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-05-22"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions"]
supports_assistant_prefill = true
supports_computer_use = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."claude-sonnet-4".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4".pricing."cortecs"]
input_cost_per_token = 0.000003307
output_cost_per_token = 0.000016536000000000002
[models."claude-sonnet-4".pricing."fastrouter"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4".pricing."gmi"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4".pricing."helicone"]
cache_read_input_token_cost = 3.0000000000000004e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4".pricing."kilo"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4".pricing."opencode"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4".pricing."openrouter"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_image = 0.0048
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
[models."claude-sonnet-4".pricing."poe"]
cache_read_input_token_cost = 2.6e-7
input_cost_per_token = 0.0000026
output_cost_per_token = 0.000013
[models."claude-sonnet-4".pricing."requesty"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4".pricing."vercel"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4".pricing."vertex_ai"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
[models."claude-sonnet-4".pricing."zenmux"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-sonnet-4-0"]
display_name = "Claude Sonnet 4 (latest)"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
litellm_provider = "anthropic"
providers = ["anthropic"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-05-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-sonnet-4-0".pricing."anthropic"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-sonnet-4-20250514"]
display_name = "claude-sonnet-4-20250514"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "anthropic"
providers = ["anthropic", "abacus", "jiekou"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2026-01"
deprecation_date = "2026-05-14"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.000006
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."claude-sonnet-4-20250514".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-sonnet-4-20250514".pricing."abacus"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-20250514".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
[models."claude-sonnet-4-20250514".pricing."jiekou"]
input_cost_per_token = 0.0000027
output_cost_per_token = 0.0000135

[models."claude-sonnet-4-5"]
display_name = "Claude Sonnet 4.5"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "azure_ai"
providers = ["azure_ai", "aihubmix", "anthropic", "azure", "azure-cognitive-services", "cloudflare-ai-gateway", "firmware", "github-copilot", "kilo", "opencode", "openrouter", "perplexity", "poe", "requesty", "vercel", "vertex_ai", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-07-31"
release_date = "2025-09-29"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.000006
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."claude-sonnet-4-5".pricing."aihubmix"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.0000032999999999999997
output_cost_per_token = 0.0000165
[models."claude-sonnet-4-5".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
[models."claude-sonnet-4-5".pricing."azure"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-5".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-5".pricing."azure_ai"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-5".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-5".pricing."firmware"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-5".pricing."kilo"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-5".pricing."opencode"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-5".pricing."openrouter"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-5".pricing."poe"]
cache_read_input_token_cost = 2.6e-7
input_cost_per_token = 0.0000026
output_cost_per_token = 0.000013
[models."claude-sonnet-4-5".pricing."requesty"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-5".pricing."vercel"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-5".pricing."vertex_ai"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
input_cost_per_token_batches = 0.0000015
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
output_cost_per_token_batches = 0.0000075
[models."claude-sonnet-4-5".pricing."zenmux"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-sonnet-4-5-20250929"]
display_name = "claude-sonnet-4-5-20250929"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "anthropic"
providers = ["anthropic", "302ai", "abacus", "helicone", "jiekou", "qihang-ai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03"
release_date = "2025-09-29"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true
tool_use_system_prompt_tokens = 346

[models."claude-sonnet-4-5-20250929".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-sonnet-4-5-20250929".pricing."302ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-5-20250929".pricing."abacus"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-5-20250929".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
[models."claude-sonnet-4-5-20250929".pricing."helicone"]
cache_read_input_token_cost = 3.0000000000000004e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-5-20250929".pricing."jiekou"]
input_cost_per_token = 0.0000027
output_cost_per_token = 0.0000135
[models."claude-sonnet-4-5-20250929".pricing."qihang-ai"]
input_cost_per_token = 4.3e-7
output_cost_per_token = 0.0000021400000000000003

[models."claude-sonnet-4-5-20250929-thinking"]
display_name = "claude-sonnet-4-5-20250929-thinking"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "302ai"
providers = ["302ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-03"
release_date = "2025-09-30"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-sonnet-4-5-20250929-thinking".pricing."302ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-sonnet-4-5-20250929-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."claude-sonnet-4-5-20250929-v1:0".pricing."bedrock"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225

[models."claude-sonnet-4-5@20250929"]
display_name = "Claude Sonnet 4.5"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex-anthropic"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-07-31"
release_date = "2025-09-29"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
input_cost_per_token_batches = 0.0000015
output_cost_per_token_above_200k_tokens = 0.0000225
output_cost_per_token_batches = 0.0000075
supports_assistant_prefill = true
supports_computer_use = true
supports_native_streaming = true
supports_response_schema = true
supports_tool_choice = true

[models."claude-sonnet-4-5@20250929".pricing."google-vertex-anthropic"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-5@20250929".pricing."vertex_ai"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
input_cost_per_token_batches = 0.0000015
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
output_cost_per_token_batches = 0.0000075

[models."claude-sonnet-4-6"]
display_name = "Claude Sonnet 4.6"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "azure_ai"
providers = ["azure_ai", "aihubmix", "anthropic", "cloudflare-ai-gateway", "firmware", "github-copilot", "opencode", "openrouter", "poe", "venice", "vercel", "vertex_ai", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2026-02-17"
release_date = "2026-02-17"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_1hr = 0.000006
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."claude-sonnet-4-6".pricing."aihubmix"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-6".pricing."anthropic"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
[models."claude-sonnet-4-6".pricing."azure_ai"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_1hr = 0.000006
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-6".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-6".pricing."firmware"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-6".pricing."opencode"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-6".pricing."openrouter"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-6".pricing."poe"]
cache_read_input_token_cost = 2.6e-7
input_cost_per_token = 0.0000026
output_cost_per_token = 0.000013
[models."claude-sonnet-4-6".pricing."venice"]
cache_read_input_token_cost = 3.75e-7
input_cost_per_token = 0.00000375
output_cost_per_token = 0.00001875
[models."claude-sonnet-4-6".pricing."vercel"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-6".pricing."vertex_ai"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
[models."claude-sonnet-4-6".pricing."zenmux"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-sonnet-4-6-think"]
display_name = "Claude Sonnet 4.6 Think"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
litellm_provider = "aihubmix"
providers = ["aihubmix"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-08"
release_date = "2026-02-17"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-sonnet-4-6-think".pricing."aihubmix"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-sonnet-4-6@default"]
display_name = "Claude Sonnet 4.6"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex-anthropic"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-08"
release_date = "2026-02-17"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."claude-sonnet-4-6@default".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-sonnet-4-6@default".pricing."google-vertex-anthropic"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4-6@default".pricing."vertex_ai"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225

[models."claude-sonnet-4.5"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 1000000
max_tokens = 1000000
litellm_provider = "github_copilot"
providers = ["github_copilot", "gmi", "openrouter", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supported_endpoints = ["/v1/chat/completions"]
supports_assistant_prefill = true
supports_computer_use = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."claude-sonnet-4.5".pricing."gmi"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4.5".pricing."openrouter"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_image = 0.0048
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225
[models."claude-sonnet-4.5".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."claude-sonnet-45"]
display_name = "Claude Sonnet 4.5"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 198000
max_output_tokens = 49500
input_cost_per_token = 0.00000375
output_cost_per_token = 0.00001875
cache_read_input_token_cost = 3.75e-7
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-09"
release_date = "2025-01-15"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."claude-sonnet-45".pricing."venice"]
cache_read_input_token_cost = 3.75e-7
input_cost_per_token = 0.00000375
output_cost_per_token = 0.00001875

[models."claude-sonnet-4@20250514"]
display_name = "Claude Sonnet 4"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex-anthropic"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-05-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."claude-sonnet-4@20250514".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."claude-sonnet-4@20250514".pricing."google-vertex-anthropic"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."claude-sonnet-4@20250514".pricing."vertex_ai"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225

[models."code-bison"]
mode = "chat"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
providers = ["vertex_ai-code-text-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7
supports_tool_choice = true

[models."code-bison".pricing."vertex_ai-code-text-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."code-bison-32k@002"]
mode = "completion"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
providers = ["vertex_ai-code-text-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7

[models."code-bison-32k@002".pricing."vertex_ai-code-text-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."code-bison32k"]
mode = "completion"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
providers = ["vertex_ai-code-text-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7

[models."code-bison32k".pricing."vertex_ai-code-text-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."code-bison@001"]
mode = "completion"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
providers = ["vertex_ai-code-text-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7

[models."code-bison@001".pricing."vertex_ai-code-text-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."code-bison@002"]
mode = "completion"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
providers = ["vertex_ai-code-text-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7

[models."code-bison@002".pricing."vertex_ai-code-text-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."code-gecko"]
mode = "completion"
max_input_tokens = 2048
max_output_tokens = 64
max_tokens = 64
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
providers = ["vertex_ai-code-text-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."code-gecko".pricing."vertex_ai-code-text-models"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7

[models."code-gecko-latest"]
mode = "completion"
max_input_tokens = 2048
max_output_tokens = 64
max_tokens = 64
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
providers = ["vertex_ai-code-text-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."code-gecko-latest".pricing."vertex_ai-code-text-models"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7

[models."code-gecko@001"]
mode = "completion"
max_input_tokens = 2048
max_output_tokens = 64
max_tokens = 64
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
providers = ["vertex_ai-code-text-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."code-gecko@001".pricing."vertex_ai-code-text-models"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7

[models."code-gecko@002"]
mode = "completion"
max_input_tokens = 2048
max_output_tokens = 64
max_tokens = 64
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-text-models"
providers = ["vertex_ai-code-text-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."code-gecko@002".pricing."vertex_ai-code-text-models"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7

[models."codechat-bison"]
mode = "chat"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-chat-models"
providers = ["vertex_ai-code-chat-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7
supports_tool_choice = true

[models."codechat-bison".pricing."vertex_ai-code-chat-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."codechat-bison-32k"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-chat-models"
providers = ["vertex_ai-code-chat-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7
supports_tool_choice = true

[models."codechat-bison-32k".pricing."vertex_ai-code-chat-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."codechat-bison-32k@002"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-chat-models"
providers = ["vertex_ai-code-chat-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7
supports_tool_choice = true

[models."codechat-bison-32k@002".pricing."vertex_ai-code-chat-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."codechat-bison@001"]
mode = "chat"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-chat-models"
providers = ["vertex_ai-code-chat-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7
supports_tool_choice = true

[models."codechat-bison@001".pricing."vertex_ai-code-chat-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."codechat-bison@002"]
mode = "chat"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-chat-models"
providers = ["vertex_ai-code-chat-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7
supports_tool_choice = true

[models."codechat-bison@002".pricing."vertex_ai-code-chat-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."codechat-bison@latest"]
mode = "chat"
max_input_tokens = 6144
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-code-chat-models"
providers = ["vertex_ai-code-chat-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7
supports_tool_choice = true

[models."codechat-bison@latest".pricing."vertex_ai-code-chat-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."codegeex4"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = false

[models."codegeex4".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."codegemma"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]

[models."codegemma".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."codegemma-1-1-7b"]
display_name = "Codegemma 1.1 7b"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2024-04-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."codegemma-7b"]
display_name = "Codegemma 7b"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2024-03-21"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."codellama"]
mode = "completion"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]

[models."codellama".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."codellama-34b-instruct"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.0000014
litellm_provider = "perplexity"
providers = ["perplexity"]

[models."codellama-34b-instruct".pricing."perplexity"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.0000014

[models."codellama-70b-instruct"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000028
litellm_provider = "perplexity"
providers = ["perplexity"]

[models."codellama-70b-instruct".pricing."perplexity"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000028

[models."codellama-7b"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 6e-8
output_cost_per_token = 1.2e-7
litellm_provider = "llamagate"
providers = ["llamagate"]
supports_function_calling = true
supports_response_schema = true

[models."codellama-7b".pricing."llamagate"]
input_cost_per_token = 6e-8
output_cost_per_token = 1.2e-7

[models."codellama/CodeLlama-34b-Instruct-hf"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001
litellm_provider = "anyscale"
providers = ["anyscale"]

[models."codellama/CodeLlama-34b-Instruct-hf".pricing."anyscale"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001

[models."codellama/CodeLlama-70b-Instruct-hf"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001
litellm_provider = "anyscale"
providers = ["anyscale"]
source = "https://docs.anyscale.com/preview/endpoints/text-generation/supported-models/codellama-CodeLlama-70b-Instruct-hf"

[models."codellama/CodeLlama-70b-Instruct-hf".pricing."anyscale"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001

[models."codestral"]
display_name = "Codestral"
model_family = "codestral"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 4000
max_tokens = 4000
input_cost_per_token = 3e-7
output_cost_per_token = 9e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2024-05-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."codestral".pricing."vercel"]
input_cost_per_token = 3e-7
output_cost_per_token = 9.000000000000001e-7
[models."codestral".pricing."vercel_ai_gateway"]
input_cost_per_token = 3e-7
output_cost_per_token = 9e-7

[models."codestral-2"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 3e-7
output_cost_per_token = 9e-7
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_tool_choice = true

[models."codestral-2".pricing."vertex_ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 9e-7

[models."codestral-2405"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "codestral"
providers = ["codestral", "mistral", "text-completion-codestral"]
source = "https://docs.mistral.ai/capabilities/code_generation/"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."codestral-2405".pricing."codestral"]
input_cost_per_token = 0
output_cost_per_token = 0
[models."codestral-2405".pricing."mistral"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
[models."codestral-2405".pricing."text-completion-codestral"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."codestral-2501"]
display_name = "Codestral 25.01"
model_family = "codestral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "azure", "azure-cognitive-services"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-03"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."codestral-2501".pricing."azure"]
input_cost_per_token = 3e-7
output_cost_per_token = 9.000000000000001e-7
[models."codestral-2501".pricing."azure-cognitive-services"]
input_cost_per_token = 3e-7
output_cost_per_token = 9.000000000000001e-7
[models."codestral-2501".pricing."vertex_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7

[models."codestral-2508"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 3e-7
output_cost_per_token = 9e-7
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
source = "https://mistral.ai/news/codestral-25-08"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."codestral-2508".pricing."mistral"]
input_cost_per_token = 3e-7
output_cost_per_token = 9e-7

[models."codestral-2@001"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 3e-7
output_cost_per_token = 9e-7
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_tool_choice = true

[models."codestral-2@001".pricing."vertex_ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 9e-7

[models."codestral-embed"]
display_name = "Codestral Embed"
model_family = "codestral-embed"
mode = "embedding"
max_input_tokens = 8192
max_output_tokens = 0
max_tokens = 8192
input_cost_per_token = 1.5e-7
litellm_provider = "mistral"
providers = ["mistral", "vercel", "vercel_ai_gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-05-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."codestral-embed".pricing."mistral"]
input_cost_per_token = 1.5e-7
[models."codestral-embed".pricing."vercel"]
input_cost_per_token = 1.5e-7
[models."codestral-embed".pricing."vercel_ai_gateway"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0

[models."codestral-embed-2505"]
mode = "embedding"
max_input_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.5e-7
litellm_provider = "mistral"
providers = ["mistral"]

[models."codestral-embed-2505".pricing."mistral"]
input_cost_per_token = 1.5e-7

[models."codestral-latest"]
display_name = "Codestral"
model_family = "codestral"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "codestral"
providers = ["codestral", "mistral", "text-completion-codestral"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2024-05-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://docs.mistral.ai/capabilities/code_generation/"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."codestral-latest".pricing."codestral"]
input_cost_per_token = 0
output_cost_per_token = 0
[models."codestral-latest".pricing."mistral"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
[models."codestral-latest".pricing."text-completion-codestral"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."codestral-mamba-latest"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 2.5e-7
output_cost_per_token = 2.5e-7
litellm_provider = "mistral"
providers = ["mistral"]
source = "https://mistral.ai/technology/"
supports_assistant_prefill = true
supports_tool_choice = true

[models."codestral-mamba-latest".pricing."mistral"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 2.5e-7

[models."codestral@2405"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_tool_choice = true

[models."codestral@2405".pricing."vertex_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7

[models."codestral@latest"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_tool_choice = true

[models."codestral@latest".pricing."vertex_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7

[models."codex-mini"]
display_name = "Codex Mini"
model_family = "gpt-codex-mini"
mode = "responses"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000006
cache_read_input_token_cost = 3.75e-7
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-05-16"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/responses"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."codex-mini".pricing."azure"]
cache_read_input_token_cost = 3.75e-7
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000006
[models."codex-mini".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 3.75e-7
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000006
[models."codex-mini".pricing."vercel"]
cache_read_input_token_cost = 3.8e-7
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000006

[models."codex-mini-latest"]
display_name = "OpenAI Codex Mini Latest"
model_family = "gpt-codex-mini"
mode = "responses"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000006
cache_read_input_token_cost = 3.75e-7
litellm_provider = "openai"
providers = ["openai", "helicone"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-01-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/responses"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."codex-mini-latest".pricing."helicone"]
cache_read_input_token_cost = 3.75e-7
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000006
[models."codex-mini-latest".pricing."openai"]
cache_read_input_token_cost = 3.75e-7
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000006

[models."coding-glm-4-7"]
display_name = "Coding-GLM-4.7"
model_family = "glm"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.0000011
cache_read_input_token_cost = 5.480000000000001e-7
litellm_provider = "aihubmix"
providers = ["aihubmix"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-12-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."coding-glm-4-7".pricing."aihubmix"]
cache_read_input_token_cost = 5.480000000000001e-7
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.0000011

[models."coding-glm-4-7-free"]
display_name = "Coding GLM 4.7 Free"
model_family = "glm"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
litellm_provider = "aihubmix"
providers = ["aihubmix"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-12-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."coding-minimax-m2-1-free"]
display_name = "Coding MiniMax M2.1 Free"
model_family = "minimax"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
litellm_provider = "aihubmix"
providers = ["aihubmix"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."cognitivecomputations/dolphin-mistral-24b-venice-edition:free"]
display_name = "Uncensored (free)"
model_family = "mistral"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2025-07-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."cognitivecomputations/dolphin3-0-mistral-24b"]
display_name = "Dolphin3.0 Mistral 24B"
model_family = "mistral"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-02-13"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."cognitivecomputations/dolphin3-0-r1-mistral-24b"]
display_name = "Dolphin3.0 R1 Mistral 24B"
model_family = "mistral"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-02-13"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."cohere-command-a"]
display_name = "Cohere Command A"
model_family = "command-a"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "github-models"
providers = ["github-models", "azure", "azure-cognitive-services"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-03"
release_date = "2024-11-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."cohere-command-a".pricing."azure"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."cohere-command-a".pricing."azure-cognitive-services"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001

[models."cohere-command-r"]
display_name = "Cohere Command R"
model_family = "command-r"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-03"
release_date = "2024-03-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."cohere-command-r-08-2024"]
display_name = "Cohere Command R 08-2024"
model_family = "command-r"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "github-models"
providers = ["github-models", "azure", "azure-cognitive-services"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-03"
release_date = "2024-08-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."cohere-command-r-08-2024".pricing."azure"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."cohere-command-r-08-2024".pricing."azure-cognitive-services"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."cohere-command-r-plus"]
display_name = "Cohere Command R+"
model_family = "command-r"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-03"
release_date = "2024-04-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."cohere-command-r-plus-08-2024"]
display_name = "Cohere Command R+ 08-2024"
model_family = "command-r"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "github-models"
providers = ["github-models", "azure", "azure-cognitive-services"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-03"
release_date = "2024-08-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."cohere-command-r-plus-08-2024".pricing."azure"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."cohere-command-r-plus-08-2024".pricing."azure-cognitive-services"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001

[models."cohere-embed-v-4-0"]
display_name = "Embed v4"
model_family = "cohere-embed"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 1536
input_cost_per_token = 1.2e-7
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-04-15"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."cohere-embed-v-4-0".pricing."azure"]
input_cost_per_token = 1.2e-7
[models."cohere-embed-v-4-0".pricing."azure-cognitive-services"]
input_cost_per_token = 1.2e-7

[models."cohere-rerank-v3-english"]
mode = "rerank"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "azure_ai"
providers = ["azure_ai"]
input_cost_per_query = 0.002
max_query_tokens = 2048

[models."cohere-rerank-v3-english".pricing."azure_ai"]
input_cost_per_query = 0.002
input_cost_per_token = 0
output_cost_per_token = 0

[models."cohere-rerank-v3-multilingual"]
mode = "rerank"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "azure_ai"
providers = ["azure_ai"]
input_cost_per_query = 0.002
max_query_tokens = 2048

[models."cohere-rerank-v3-multilingual".pricing."azure_ai"]
input_cost_per_query = 0.002
input_cost_per_token = 0
output_cost_per_token = 0

[models."cohere-rerank-v3.5"]
mode = "rerank"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "azure_ai"
providers = ["azure_ai"]
input_cost_per_query = 0.002
max_query_tokens = 2048

[models."cohere-rerank-v3.5".pricing."azure_ai"]
input_cost_per_query = 0.002
input_cost_per_token = 0
output_cost_per_token = 0

[models."cohere-rerank-v4.0-fast"]
mode = "rerank"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "azure_ai"
providers = ["azure_ai"]
input_cost_per_query = 0.002
max_query_tokens = 4096

[models."cohere-rerank-v4.0-fast".pricing."azure_ai"]
input_cost_per_query = 0.002
input_cost_per_token = 0
output_cost_per_token = 0

[models."cohere-rerank-v4.0-pro"]
mode = "rerank"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "azure_ai"
providers = ["azure_ai"]
input_cost_per_query = 0.0025
max_query_tokens = 4096

[models."cohere-rerank-v4.0-pro".pricing."azure_ai"]
input_cost_per_query = 0.0025
input_cost_per_token = 0
output_cost_per_token = 0

[models."cohere.command-a-03-2025"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 4000
max_tokens = 4000
input_cost_per_token = 0.00000156
output_cost_per_token = 0.00000156
litellm_provider = "oci"
providers = ["oci"]
supports_function_calling = true
source = "https://www.oracle.com/cloud/ai/generative-ai/pricing/"
supports_response_schema = false

[models."cohere.command-a-03-2025".pricing."oci"]
input_cost_per_token = 0.00000156
output_cost_per_token = 0.00000156

[models."cohere.command-latest"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 4000
input_cost_per_token = 0.00000156
output_cost_per_token = 0.00000156
litellm_provider = "oci"
providers = ["oci"]
supports_function_calling = true
source = "https://www.oracle.com/cloud/ai/generative-ai/pricing/"
supports_response_schema = false

[models."cohere.command-latest".pricing."oci"]
input_cost_per_token = 0.00000156
output_cost_per_token = 0.00000156

[models."cohere.command-light-text-v14"]
display_name = "Command Light"
model_family = "command-light"
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 3e-7
output_cost_per_token = 6e-7
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-08"
release_date = "2023-11-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."cohere.command-light-text-v14".pricing."amazon-bedrock"]
input_cost_per_token = 3e-7
output_cost_per_token = 6e-7
[models."cohere.command-light-text-v14".pricing."bedrock"]
input_cost_per_token = 3e-7
output_cost_per_token = 6e-7

[models."cohere.command-plus-latest"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 4000
input_cost_per_token = 0.00000156
output_cost_per_token = 0.00000156
litellm_provider = "oci"
providers = ["oci"]
supports_function_calling = true
source = "https://www.oracle.com/cloud/ai/generative-ai/pricing/"
supports_response_schema = false

[models."cohere.command-plus-latest".pricing."oci"]
input_cost_per_token = 0.00000156
output_cost_per_token = 0.00000156

[models."cohere.command-r-plus-v1:0"]
display_name = "Command R+"
model_family = "command-r"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-04-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."cohere.command-r-plus-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."cohere.command-r-plus-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."cohere.command-r-v1:0"]
display_name = "Command R"
model_family = "command-r"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-03-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."cohere.command-r-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
[models."cohere.command-r-v1:0".pricing."bedrock"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."cohere.command-text-v14"]
display_name = "Command"
model_family = "command"
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-08"
release_date = "2023-11-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."cohere.command-text-v14".pricing."amazon-bedrock"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
[models."cohere.command-text-v14".pricing."bedrock"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002

[models."cohere.embed-english-v3"]
mode = "embedding"
max_input_tokens = 512
max_tokens = 512
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_embedding_image_input = true

[models."cohere.embed-english-v3".pricing."bedrock"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."cohere.embed-multilingual-v3"]
mode = "embedding"
max_input_tokens = 512
max_tokens = 512
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_embedding_image_input = true

[models."cohere.embed-multilingual-v3".pricing."bedrock"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."cohere.embed-v4:0"]
mode = "embedding"
max_input_tokens = 128000
max_tokens = 128000
input_cost_per_token = 1.2e-7
output_cost_per_token = 0
litellm_provider = "bedrock"
providers = ["bedrock"]
output_vector_size = 1536
supports_embedding_image_input = true

[models."cohere.embed-v4:0".pricing."bedrock"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 0

[models."cohere.rerank-v3-5:0"]
mode = "rerank"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_query = 0.002
max_document_chunks_per_query = 100
max_query_tokens = 32000
max_tokens_per_document_chunk = 512

[models."cohere.rerank-v3-5:0".pricing."bedrock"]
input_cost_per_query = 0.002
input_cost_per_token = 0
output_cost_per_token = 0

[models."command"]
mode = "completion"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
litellm_provider = "cohere"
providers = ["cohere"]

[models."command".pricing."cohere"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002

[models."command-a"]
display_name = "Command A"
model_family = "command"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 8000
max_tokens = 8000
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "kilo", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-03-13"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_tool_choice = true

[models."command-a".pricing."kilo"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."command-a".pricing."vercel"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."command-a".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001

[models."command-a-03-2025"]
display_name = "Command A"
model_family = "command-a"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 8000
max_tokens = 8000
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "cohere_chat"
providers = ["cohere_chat", "cohere"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-06-01"
release_date = "2025-03-13"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."command-a-03-2025".pricing."cohere"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."command-a-03-2025".pricing."cohere_chat"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001

[models."command-a-reasoning-08-2025"]
display_name = "Command A Reasoning"
model_family = "command-a"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 32000
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "cohere"
providers = ["cohere"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-06-01"
release_date = "2025-08-21"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."command-a-reasoning-08-2025".pricing."cohere"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001

[models."command-a-translate-08-2025"]
display_name = "Command A Translate"
model_family = "command-a"
mode = "chat"
max_input_tokens = 8000
max_output_tokens = 8000
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "cohere"
providers = ["cohere"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-06-01"
release_date = "2025-08-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."command-a-translate-08-2025".pricing."cohere"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001

[models."command-a-vision-07-2025"]
display_name = "Command A Vision"
model_family = "command-a"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8000
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "cohere"
providers = ["cohere"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-06-01"
release_date = "2025-07-31"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."command-a-vision-07-2025".pricing."cohere"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001

[models."command-light"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 3e-7
output_cost_per_token = 6e-7
litellm_provider = "cohere_chat"
providers = ["cohere_chat"]
supports_tool_choice = true

[models."command-light".pricing."cohere_chat"]
input_cost_per_token = 3e-7
output_cost_per_token = 6e-7

[models."command-nightly"]
mode = "completion"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
litellm_provider = "cohere"
providers = ["cohere"]

[models."command-nightly".pricing."cohere"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002

[models."command-r"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "cohere_chat"
providers = ["cohere_chat", "vercel_ai_gateway"]
supports_function_calling = true
supports_tool_choice = true

[models."command-r".pricing."cohere_chat"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."command-r".pricing."vercel_ai_gateway"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."command-r-08-2024"]
display_name = "Command R"
model_family = "command-r"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "cohere_chat"
providers = ["cohere_chat", "cohere", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-06-01"
release_date = "2024-08-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."command-r-08-2024".pricing."cohere"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."command-r-08-2024".pricing."cohere_chat"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."command-r-08-2024".pricing."kilo"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."command-r-plus"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "azure"
providers = ["azure", "cohere_chat", "vercel_ai_gateway"]
supports_function_calling = true
supports_tool_choice = true

[models."command-r-plus".pricing."azure"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."command-r-plus".pricing."cohere_chat"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."command-r-plus".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001

[models."command-r-plus-08-2024"]
display_name = "Command R+"
model_family = "command-r"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "cohere_chat"
providers = ["cohere_chat", "cohere", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-06-01"
release_date = "2024-08-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."command-r-plus-08-2024".pricing."cohere"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."command-r-plus-08-2024".pricing."cohere_chat"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."command-r-plus-08-2024".pricing."kilo"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001

[models."command-r7b-12-2024"]
display_name = "Command R7B"
model_family = "command-r"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.5e-7
output_cost_per_token = 3.75e-8
litellm_provider = "cohere_chat"
providers = ["cohere_chat", "cohere", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-06-01"
release_date = "2024-02-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://docs.cohere.com/v2/docs/command-r7b"
supports_tool_choice = true

[models."command-r7b-12-2024".pricing."cohere"]
input_cost_per_token = 3.75e-8
output_cost_per_token = 1.5e-7
[models."command-r7b-12-2024".pricing."cohere_chat"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 3.75e-8
[models."command-r7b-12-2024".pricing."kilo"]
input_cost_per_token = 3.75e-8
output_cost_per_token = 1.5e-7

[models."command-r7b-arabic-02-2025"]
display_name = "Command R7B Arabic"
model_family = "command-r"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4000
input_cost_per_token = 3.75e-8
output_cost_per_token = 1.5e-7
litellm_provider = "cohere"
providers = ["cohere"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-06-01"
release_date = "2025-02-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."command-r7b-arabic-02-2025".pricing."cohere"]
input_cost_per_token = 3.75e-8
output_cost_per_token = 1.5e-7

[models."computer-use-preview"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 0.000003
output_cost_per_token = 0.000012
litellm_provider = "azure"
providers = ["azure"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = false
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/responses"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."computer-use-preview".pricing."azure"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000012

[models."conservative"]
mode = "image_edit"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.04
supported_endpoints = ["/v1/images/edits"]

[models."conservative".pricing."stability"]
output_cost_per_image = 0.04

[models."container"]
mode = "chat"
litellm_provider = "azure"
providers = ["azure", "openai"]
code_interpreter_cost_per_session = 0.03

[models."container".pricing."azure"]
code_interpreter_cost_per_session = 0.03
[models."container".pricing."openai"]
code_interpreter_cost_per_session = 0.03

[models."core42/jais-13b-chat"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.0005
output_cost_per_token = 0.002
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = false
supports_vision = false
supports_parallel_function_calling = false

[models."core42/jais-13b-chat".pricing."watsonx"]
input_cost_per_token = 0.0005
output_cost_per_token = 0.002

[models."core42/jais-30b-chat"]
display_name = "JAIS 30b Chat"
model_family = "jais"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 2048
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-03"
release_date = "2023-08-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."cortecs/Llama-3-3-70B-Instruct-FP8-Dynamic"]
display_name = "Llama 3.3 70B"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 4.9e-7
output_cost_per_token = 7.1e-7
litellm_provider = "stackit"
providers = ["stackit"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-12-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."cortecs/Llama-3-3-70B-Instruct-FP8-Dynamic".pricing."stackit"]
input_cost_per_token = 4.9e-7
output_cost_per_token = 7.1e-7

[models."creative"]
mode = "image_edit"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.06
supported_endpoints = ["/v1/images/edits"]

[models."creative".pricing."stability"]
output_cost_per_image = 0.06

[models."dall-e-2"]
mode = "image_generation"
litellm_provider = "aiml"
providers = ["aiml", "openai"]
source = "https://docs.aimlapi.com/"
output_cost_per_image = 0.021
supported_endpoints = ["/v1/images/generations", "/v1/images/edits", "/v1/images/variations"]

[models."dall-e-2".metadata]
notes = "DALL-E 2 via AI/ML API - Reliable text-to-image generation"

[models."dall-e-2".pricing."aiml"]
output_cost_per_image = 0.021
[models."dall-e-2".pricing."openai"]
input_cost_per_image = 0.02

[models."dall-e-3"]
display_name = "DALL-E-3"
model_family = "dall-e"
mode = "image_generation"
max_input_tokens = 800
max_output_tokens = 0
litellm_provider = "aiml"
providers = ["aiml", "openai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2023-11-06"
supported_modalities = ["text"]
supported_output_modalities = ["image"]
source = "https://docs.aimlapi.com/"
output_cost_per_image = 0.042
supported_endpoints = ["/v1/images/generations"]

[models."dall-e-3".metadata]
notes = "DALL-E 3 via AI/ML API - High-quality text-to-image generation"

[models."dall-e-3".pricing."aiml"]
output_cost_per_image = 0.042
[models."dall-e-3".pricing."openai"]
input_cost_per_image = 0.04

[models."databricks-bge-large-en"]
mode = "embedding"
max_input_tokens = 512
max_tokens = 512
input_cost_per_token = 1.0003e-7
output_cost_per_token = 0
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
input_dbu_cost_per_token = 0.000001429
output_dbu_cost_per_token = 0
output_vector_size = 1024

[models."databricks-bge-large-en".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-bge-large-en".pricing."databricks"]
input_cost_per_token = 1.0003e-7
input_dbu_cost_per_token = 0.000001429
output_cost_per_token = 0
output_dbu_cost_per_token = 0

[models."databricks-claude-3-7-sonnet"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.0000029999900000000002
output_cost_per_token = 0.000015000020000000002
litellm_provider = "databricks"
providers = ["databricks"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
input_dbu_cost_per_token = 0.000042857
output_dbu_cost_per_token = 0.000214286
supports_assistant_prefill = true
supports_tool_choice = true

[models."databricks-claude-3-7-sonnet".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-claude-3-7-sonnet".pricing."databricks"]
input_cost_per_token = 0.0000029999900000000002
input_dbu_cost_per_token = 0.000042857
output_cost_per_token = 0.000015000020000000002
output_dbu_cost_per_token = 0.000214286

[models."databricks-claude-haiku-4-5"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.00000100002
output_cost_per_token = 0.00000500003
litellm_provider = "databricks"
providers = ["databricks"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
input_dbu_cost_per_token = 0.000014286
output_dbu_cost_per_token = 0.000071429
supports_assistant_prefill = true
supports_tool_choice = true

[models."databricks-claude-haiku-4-5".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-claude-haiku-4-5".pricing."databricks"]
input_cost_per_token = 0.00000100002
input_dbu_cost_per_token = 0.000014286
output_cost_per_token = 0.00000500003
output_dbu_cost_per_token = 0.000071429

[models."databricks-claude-opus-4"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000015000020000000002
output_cost_per_token = 0.00007500003000000001
litellm_provider = "databricks"
providers = ["databricks"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
input_dbu_cost_per_token = 0.000214286
output_dbu_cost_per_token = 0.001071429
supports_assistant_prefill = true
supports_tool_choice = true

[models."databricks-claude-opus-4".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-claude-opus-4".pricing."databricks"]
input_cost_per_token = 0.000015000020000000002
input_dbu_cost_per_token = 0.000214286
output_cost_per_token = 0.00007500003000000001
output_dbu_cost_per_token = 0.001071429

[models."databricks-claude-opus-4-1"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000015000020000000002
output_cost_per_token = 0.00007500003000000001
litellm_provider = "databricks"
providers = ["databricks"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
input_dbu_cost_per_token = 0.000214286
output_dbu_cost_per_token = 0.001071429
supports_assistant_prefill = true
supports_tool_choice = true

[models."databricks-claude-opus-4-1".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-claude-opus-4-1".pricing."databricks"]
input_cost_per_token = 0.000015000020000000002
input_dbu_cost_per_token = 0.000214286
output_cost_per_token = 0.00007500003000000001
output_dbu_cost_per_token = 0.001071429

[models."databricks-claude-opus-4-5"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.00000500003
output_cost_per_token = 0.000025000010000000002
litellm_provider = "databricks"
providers = ["databricks"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
input_dbu_cost_per_token = 0.000071429
output_dbu_cost_per_token = 0.000357143
supports_assistant_prefill = true
supports_tool_choice = true

[models."databricks-claude-opus-4-5".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-claude-opus-4-5".pricing."databricks"]
input_cost_per_token = 0.00000500003
input_dbu_cost_per_token = 0.000071429
output_cost_per_token = 0.000025000010000000002
output_dbu_cost_per_token = 0.000357143

[models."databricks-claude-sonnet-4"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000029999900000000002
output_cost_per_token = 0.000015000020000000002
litellm_provider = "databricks"
providers = ["databricks"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
input_dbu_cost_per_token = 0.000042857
output_dbu_cost_per_token = 0.000214286
supports_assistant_prefill = true
supports_tool_choice = true

[models."databricks-claude-sonnet-4".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-claude-sonnet-4".pricing."databricks"]
input_cost_per_token = 0.0000029999900000000002
input_dbu_cost_per_token = 0.000042857
output_cost_per_token = 0.000015000020000000002
output_dbu_cost_per_token = 0.000214286

[models."databricks-claude-sonnet-4-1"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000029999900000000002
output_cost_per_token = 0.000015000020000000002
litellm_provider = "databricks"
providers = ["databricks"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
input_dbu_cost_per_token = 0.000042857
output_dbu_cost_per_token = 0.000214286
supports_assistant_prefill = true
supports_tool_choice = true

[models."databricks-claude-sonnet-4-1".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-claude-sonnet-4-1".pricing."databricks"]
input_cost_per_token = 0.0000029999900000000002
input_dbu_cost_per_token = 0.000042857
output_cost_per_token = 0.000015000020000000002
output_dbu_cost_per_token = 0.000214286

[models."databricks-claude-sonnet-4-5"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000029999900000000002
output_cost_per_token = 0.000015000020000000002
litellm_provider = "databricks"
providers = ["databricks"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
input_dbu_cost_per_token = 0.000042857
output_dbu_cost_per_token = 0.000214286
supports_assistant_prefill = true
supports_tool_choice = true

[models."databricks-claude-sonnet-4-5".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-claude-sonnet-4-5".pricing."databricks"]
input_cost_per_token = 0.0000029999900000000002
input_dbu_cost_per_token = 0.000042857
output_cost_per_token = 0.000015000020000000002
output_dbu_cost_per_token = 0.000214286

[models."databricks-gemini-2-5-flash"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 3.0001999999999996e-7
output_cost_per_token = 0.00000249998
litellm_provider = "databricks"
providers = ["databricks"]
supports_function_calling = true
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
input_dbu_cost_per_token = 0.000004285999999999999
output_dbu_cost_per_token = 0.000035714
supports_tool_choice = true

[models."databricks-gemini-2-5-flash".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-gemini-2-5-flash".pricing."databricks"]
input_cost_per_token = 3.0001999999999996e-7
input_dbu_cost_per_token = 0.000004285999999999999
output_cost_per_token = 0.00000249998
output_dbu_cost_per_token = 0.000035714

[models."databricks-gemini-2-5-pro"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 0.00000124999
output_cost_per_token = 0.000009999990000000002
litellm_provider = "databricks"
providers = ["databricks"]
supports_function_calling = true
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
input_dbu_cost_per_token = 0.000017857
output_dbu_cost_per_token = 0.000142857
supports_tool_choice = true

[models."databricks-gemini-2-5-pro".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-gemini-2-5-pro".pricing."databricks"]
input_cost_per_token = 0.00000124999
input_dbu_cost_per_token = 0.000017857
output_cost_per_token = 0.000009999990000000002
output_dbu_cost_per_token = 0.000142857

[models."databricks-gemma-3-12b"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 1.5000999999999998e-7
output_cost_per_token = 5.0001e-7
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
input_dbu_cost_per_token = 0.0000021429999999999996
output_dbu_cost_per_token = 0.000007143

[models."databricks-gemma-3-12b".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-gemma-3-12b".pricing."databricks"]
input_cost_per_token = 1.5000999999999998e-7
input_dbu_cost_per_token = 0.0000021429999999999996
output_cost_per_token = 5.0001e-7
output_dbu_cost_per_token = 0.000007143

[models."databricks-gpt-5"]
mode = "chat"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000124999
output_cost_per_token = 0.000009999990000000002
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
input_dbu_cost_per_token = 0.000017857
output_dbu_cost_per_token = 0.000142857

[models."databricks-gpt-5".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-gpt-5".pricing."databricks"]
input_cost_per_token = 0.00000124999
input_dbu_cost_per_token = 0.000017857
output_cost_per_token = 0.000009999990000000002
output_dbu_cost_per_token = 0.000142857

[models."databricks-gpt-5-1"]
mode = "chat"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000124999
output_cost_per_token = 0.000009999990000000002
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
input_dbu_cost_per_token = 0.000017857
output_dbu_cost_per_token = 0.000142857

[models."databricks-gpt-5-1".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-gpt-5-1".pricing."databricks"]
input_cost_per_token = 0.00000124999
input_dbu_cost_per_token = 0.000017857
output_cost_per_token = 0.000009999990000000002
output_dbu_cost_per_token = 0.000142857

[models."databricks-gpt-5-mini"]
mode = "chat"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 2.4997000000000006e-7
output_cost_per_token = 0.0000019999700000000004
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
input_dbu_cost_per_token = 0.000003571
output_dbu_cost_per_token = 0.000028571

[models."databricks-gpt-5-mini".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-gpt-5-mini".pricing."databricks"]
input_cost_per_token = 2.4997000000000006e-7
input_dbu_cost_per_token = 0.000003571
output_cost_per_token = 0.0000019999700000000004
output_dbu_cost_per_token = 0.000028571

[models."databricks-gpt-5-nano"]
mode = "chat"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 4.998e-8
output_cost_per_token = 3.9998000000000007e-7
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/proprietary-foundation-model-serving"
input_dbu_cost_per_token = 7.14e-7
output_dbu_cost_per_token = 0.000005714000000000001

[models."databricks-gpt-5-nano".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-gpt-5-nano".pricing."databricks"]
input_cost_per_token = 4.998e-8
input_dbu_cost_per_token = 7.14e-7
output_cost_per_token = 3.9998000000000007e-7
output_dbu_cost_per_token = 0.000005714000000000001

[models."databricks-gpt-oss-120b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.5000999999999998e-7
output_cost_per_token = 5.9997e-7
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
input_dbu_cost_per_token = 0.0000021429999999999996
output_dbu_cost_per_token = 0.000008571

[models."databricks-gpt-oss-120b".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-gpt-oss-120b".pricing."databricks"]
input_cost_per_token = 1.5000999999999998e-7
input_dbu_cost_per_token = 0.0000021429999999999996
output_cost_per_token = 5.9997e-7
output_dbu_cost_per_token = 0.000008571

[models."databricks-gpt-oss-20b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 7e-8
output_cost_per_token = 3.0001999999999996e-7
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
input_dbu_cost_per_token = 0.000001
output_dbu_cost_per_token = 0.000004285999999999999

[models."databricks-gpt-oss-20b".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-gpt-oss-20b".pricing."databricks"]
input_cost_per_token = 7e-8
input_dbu_cost_per_token = 0.000001
output_cost_per_token = 3.0001999999999996e-7
output_dbu_cost_per_token = 0.000004285999999999999

[models."databricks-gte-large-en"]
mode = "embedding"
max_input_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.2999000000000001e-7
output_cost_per_token = 0
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
input_dbu_cost_per_token = 0.000001857
output_dbu_cost_per_token = 0
output_vector_size = 1024

[models."databricks-gte-large-en".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-gte-large-en".pricing."databricks"]
input_cost_per_token = 1.2999000000000001e-7
input_dbu_cost_per_token = 0.000001857
output_cost_per_token = 0
output_dbu_cost_per_token = 0

[models."databricks-llama-2-70b-chat"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 5.0001e-7
output_cost_per_token = 0.0000015000300000000002
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
input_dbu_cost_per_token = 0.000007143
output_dbu_cost_per_token = 0.000021429
supports_tool_choice = true

[models."databricks-llama-2-70b-chat".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-llama-2-70b-chat".pricing."databricks"]
input_cost_per_token = 5.0001e-7
input_dbu_cost_per_token = 0.000007143
output_cost_per_token = 0.0000015000300000000002
output_dbu_cost_per_token = 0.000021429

[models."databricks-llama-4-maverick"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 5.0001e-7
output_cost_per_token = 0.0000015000300000000002
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
input_dbu_cost_per_token = 0.000007143
output_dbu_cost_per_token = 0.000021429
supports_tool_choice = true

[models."databricks-llama-4-maverick".metadata]
notes = "Databricks documentation now provides both DBU costs (_dbu_cost_per_token) and dollar costs(_cost_per_token)."

[models."databricks-llama-4-maverick".pricing."databricks"]
input_cost_per_token = 5.0001e-7
input_dbu_cost_per_token = 0.000007143
output_cost_per_token = 0.0000015000300000000002
output_dbu_cost_per_token = 0.000021429

[models."databricks-meta-llama-3-1-405b-instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000500003
output_cost_per_token = 0.000015000020000000002
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
input_dbu_cost_per_token = 0.000071429
output_dbu_cost_per_token = 0.000214286
supports_tool_choice = true

[models."databricks-meta-llama-3-1-405b-instruct".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-meta-llama-3-1-405b-instruct".pricing."databricks"]
input_cost_per_token = 0.00000500003
input_dbu_cost_per_token = 0.000071429
output_cost_per_token = 0.000015000020000000002
output_dbu_cost_per_token = 0.000214286

[models."databricks-meta-llama-3-1-8b-instruct"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 1.5000999999999998e-7
output_cost_per_token = 4.5003000000000007e-7
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
input_dbu_cost_per_token = 0.0000021429999999999996
output_dbu_cost_per_token = 0.000006429000000000001

[models."databricks-meta-llama-3-1-8b-instruct".metadata]
notes = "Input/output cost per token is dbu cost * $0.070. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-meta-llama-3-1-8b-instruct".pricing."databricks"]
input_cost_per_token = 1.5000999999999998e-7
input_dbu_cost_per_token = 0.0000021429999999999996
output_cost_per_token = 4.5003000000000007e-7
output_dbu_cost_per_token = 0.000006429000000000001

[models."databricks-meta-llama-3-3-70b-instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 5.0001e-7
output_cost_per_token = 0.0000015000300000000002
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
input_dbu_cost_per_token = 0.000007143
output_dbu_cost_per_token = 0.000021429
supports_tool_choice = true

[models."databricks-meta-llama-3-3-70b-instruct".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-meta-llama-3-3-70b-instruct".pricing."databricks"]
input_cost_per_token = 5.0001e-7
input_dbu_cost_per_token = 0.000007143
output_cost_per_token = 0.0000015000300000000002
output_dbu_cost_per_token = 0.000021429

[models."databricks-meta-llama-3-70b-instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000100002
output_cost_per_token = 0.0000029999900000000002
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
input_dbu_cost_per_token = 0.000014286
output_dbu_cost_per_token = 0.000042857
supports_tool_choice = true

[models."databricks-meta-llama-3-70b-instruct".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-meta-llama-3-70b-instruct".pricing."databricks"]
input_cost_per_token = 0.00000100002
input_dbu_cost_per_token = 0.000014286
output_cost_per_token = 0.0000029999900000000002
output_dbu_cost_per_token = 0.000042857

[models."databricks-mixtral-8x7b-instruct"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 5.0001e-7
output_cost_per_token = 0.00000100002
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
input_dbu_cost_per_token = 0.000007143
output_dbu_cost_per_token = 0.000014286
supports_tool_choice = true

[models."databricks-mixtral-8x7b-instruct".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-mixtral-8x7b-instruct".pricing."databricks"]
input_cost_per_token = 5.0001e-7
input_dbu_cost_per_token = 0.000007143
output_cost_per_token = 0.00000100002
output_dbu_cost_per_token = 0.000014286

[models."databricks-mpt-30b-instruct"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000100002
output_cost_per_token = 0.00000100002
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
input_dbu_cost_per_token = 0.000014286
output_dbu_cost_per_token = 0.000014286
supports_tool_choice = true

[models."databricks-mpt-30b-instruct".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-mpt-30b-instruct".pricing."databricks"]
input_cost_per_token = 0.00000100002
input_dbu_cost_per_token = 0.000014286
output_cost_per_token = 0.00000100002
output_dbu_cost_per_token = 0.000014286

[models."databricks-mpt-7b-instruct"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5.0001e-7
output_cost_per_token = 0
litellm_provider = "databricks"
providers = ["databricks"]
source = "https://www.databricks.com/product/pricing/foundation-model-serving"
input_dbu_cost_per_token = 0.000007143
output_dbu_cost_per_token = 0
supports_tool_choice = true

[models."databricks-mpt-7b-instruct".metadata]
notes = "Input/output cost per token is dbu cost * $0.070, based on databricks Llama 3.1 70B conversion. Number provided for reference, '*_dbu_cost_per_token' used in actual calculation."

[models."databricks-mpt-7b-instruct".pricing."databricks"]
input_cost_per_token = 5.0001e-7
input_dbu_cost_per_token = 0.000007143
output_cost_per_token = 0
output_dbu_cost_per_token = 0

[models."davinci-002"]
mode = "completion"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002
litellm_provider = "text-completion-openai"
providers = ["text-completion-openai"]

[models."davinci-002".pricing."text-completion-openai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002

[models."deep-research-pro-preview-12-2025"]
mode = "image_generation"
max_input_tokens = 65536
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini", "vertex_ai"]
supports_function_calling = false
supports_vision = true
supports_prompt_caching = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
source = "https://ai.google.dev/gemini-api/docs/pricing"
input_cost_per_image = 0.0011
input_cost_per_token_batches = 0.000001
output_cost_per_image = 0.134
output_cost_per_image_token = 0.00012
output_cost_per_token_batches = 0.000006
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_response_schema = true
supports_system_messages = true
supports_web_search = true

[models."deep-research-pro-preview-12-2025".pricing."gemini"]
input_cost_per_image = 0.0011
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
output_cost_per_image = 0.134
output_cost_per_image_token = 0.00012
output_cost_per_token = 0.000012
output_cost_per_token_batches = 0.000006
[models."deep-research-pro-preview-12-2025".pricing."vertex_ai"]
input_cost_per_image = 0.0011
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
output_cost_per_image = 0.134
output_cost_per_image_token = 0.00012
output_cost_per_token = 0.000012
output_cost_per_token_batches = 0.000006
[models."deep-research-pro-preview-12-2025".pricing."vertex_ai-language-models"]
input_cost_per_image = 0.0011
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
output_cost_per_image = 0.134
output_cost_per_image_token = 0.00012
output_cost_per_token = 0.000012
output_cost_per_token_batches = 0.000006

[models."deepseek-ai/DeepSeek-OCR"]
display_name = "deepseek-ai/DeepSeek-OCR"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-10-20"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-ai/DeepSeek-R1"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000024
litellm_provider = "deepinfra"
providers = ["deepinfra", "hyperbolic", "together_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek-ai/DeepSeek-R1".pricing."deepinfra"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000024
[models."deepseek-ai/DeepSeek-R1".pricing."hyperbolic"]
input_cost_per_token = 4e-7
output_cost_per_token = 4e-7
[models."deepseek-ai/DeepSeek-R1".pricing."together_ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000007

[models."deepseek-ai/DeepSeek-R1-0528"]
display_name = "Deepseek R1 0528"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 5e-7
output_cost_per_token = 0.00000215
cache_read_input_token_cost = 4e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "huggingface", "hyperbolic", "io-net", "meganova", "nebius", "nvidia", "submodel", "wandb"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-11"
release_date = "2025-05-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
reasoning_cost_per_token = 0.0000024
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek-ai/DeepSeek-R1-0528".pricing."deepinfra"]
cache_read_input_token_cost = 4e-7
input_cost_per_token = 5e-7
output_cost_per_token = 0.00000215
[models."deepseek-ai/DeepSeek-R1-0528".pricing."huggingface"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000005
[models."deepseek-ai/DeepSeek-R1-0528".pricing."hyperbolic"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 2.5e-7
[models."deepseek-ai/DeepSeek-R1-0528".pricing."io-net"]
cache_read_input_token_cost = 0.000001
input_cost_per_token = 0.000002
output_cost_per_token = 0.00000875
[models."deepseek-ai/DeepSeek-R1-0528".pricing."meganova"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000021499999999999997
[models."deepseek-ai/DeepSeek-R1-0528".pricing."nebius"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.0000024
reasoning_cost_per_token = 0.0000024
[models."deepseek-ai/DeepSeek-R1-0528".pricing."submodel"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000021499999999999997
[models."deepseek-ai/DeepSeek-R1-0528".pricing."wandb"]
input_cost_per_token = 0.135
output_cost_per_token = 0.54

[models."deepseek-ai/DeepSeek-R1-0528-TEE"]
display_name = "DeepSeek R1 0528 TEE"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 65536
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.00000175
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-ai/DeepSeek-R1-0528-TEE".pricing."chutes"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.00000175

[models."deepseek-ai/DeepSeek-R1-0528-Turbo"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
litellm_provider = "deepinfra"
providers = ["deepinfra"]
supports_tool_choice = true

[models."deepseek-ai/DeepSeek-R1-0528-Turbo".pricing."deepinfra"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003

[models."deepseek-ai/DeepSeek-R1-0528-tput"]
mode = "chat"
max_input_tokens = 128000
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.00000219
litellm_provider = "together_ai"
providers = ["together_ai"]
supports_function_calling = true
source = "https://www.together.ai/models/deepseek-r1-0528-throughput"
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."deepseek-ai/DeepSeek-R1-0528-tput".pricing."together_ai"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.00000219

[models."deepseek-ai/DeepSeek-R1-Distill-Llama-70B"]
display_name = "DeepSeek R1 Distill Llama 70B"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "chutes", "fastrouter", "nscale"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = false

[models."deepseek-ai/DeepSeek-R1-Distill-Llama-70B".pricing."chutes"]
input_cost_per_token = 3e-8
output_cost_per_token = 1.1e-7
[models."deepseek-ai/DeepSeek-R1-Distill-Llama-70B".pricing."deepinfra"]
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7
[models."deepseek-ai/DeepSeek-R1-Distill-Llama-70B".pricing."fastrouter"]
input_cost_per_token = 3e-8
output_cost_per_token = 1.4e-7
[models."deepseek-ai/DeepSeek-R1-Distill-Llama-70B".pricing."nscale"]
input_cost_per_token = 3.75e-7
output_cost_per_token = 3.75e-7

[models."deepseek-ai/DeepSeek-R1-Distill-Llama-8B"]
mode = "chat"
input_cost_per_token = 2.5e-8
output_cost_per_token = 2.5e-8
litellm_provider = "nscale"
providers = ["nscale"]
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."deepseek-ai/DeepSeek-R1-Distill-Llama-8B".metadata]
notes = "Pricing listed as $0.05/1M tokens total. Assumed 50/50 split for input/output."

[models."deepseek-ai/DeepSeek-R1-Distill-Llama-8B".pricing."nscale"]
input_cost_per_token = 2.5e-8
output_cost_per_token = 2.5e-8

[models."deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"]
mode = "chat"
input_cost_per_token = 9e-8
output_cost_per_token = 9e-8
litellm_provider = "nscale"
providers = ["nscale"]
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B".metadata]
notes = "Pricing listed as $0.18/1M tokens total. Assumed 50/50 split for input/output."

[models."deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B".pricing."nscale"]
input_cost_per_token = 9e-8
output_cost_per_token = 9e-8

[models."deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"]
display_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 131000
input_cost_per_token = 7e-8
output_cost_per_token = 7e-8
litellm_provider = "nscale"
providers = ["nscale", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-01-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."deepseek-ai/DeepSeek-R1-Distill-Qwen-14B".metadata]
notes = "Pricing listed as $0.14/1M tokens total. Assumed 50/50 split for input/output."

[models."deepseek-ai/DeepSeek-R1-Distill-Qwen-14B".pricing."nscale"]
input_cost_per_token = 7e-8
output_cost_per_token = 7e-8
[models."deepseek-ai/DeepSeek-R1-Distill-Qwen-14B".pricing."siliconflow"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 1.0000000000000001e-7
[models."deepseek-ai/DeepSeek-R1-Distill-Qwen-14B".pricing."siliconflow-cn"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 1.0000000000000001e-7

[models."deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"]
display_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2.7e-7
output_cost_per_token = 2.7e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "nscale", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-01-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."deepseek-ai/DeepSeek-R1-Distill-Qwen-32B".pricing."deepinfra"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 2.7e-7
[models."deepseek-ai/DeepSeek-R1-Distill-Qwen-32B".pricing."nscale"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."deepseek-ai/DeepSeek-R1-Distill-Qwen-32B".pricing."siliconflow"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 1.8e-7
[models."deepseek-ai/DeepSeek-R1-Distill-Qwen-32B".pricing."siliconflow-cn"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 1.8e-7

[models."deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"]
mode = "chat"
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "nscale"
providers = ["nscale"]
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."deepseek-ai/DeepSeek-R1-Distill-Qwen-7B".metadata]
notes = "Pricing listed as $0.40/1M tokens total. Assumed 50/50 split for input/output."

[models."deepseek-ai/DeepSeek-R1-Distill-Qwen-7B".pricing."nscale"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."deepseek-ai/DeepSeek-R1-TEE"]
display_name = "DeepSeek R1 TEE"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-ai/DeepSeek-R1-TEE".pricing."chutes"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."deepseek-ai/DeepSeek-R1-Turbo"]
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 40960
max_tokens = 40960
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
litellm_provider = "deepinfra"
providers = ["deepinfra"]
supports_tool_choice = true

[models."deepseek-ai/DeepSeek-R1-Turbo".pricing."deepinfra"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003

[models."deepseek-ai/DeepSeek-V3"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 3.8e-7
output_cost_per_token = 8.9e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "hyperbolic", "together_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek-ai/DeepSeek-V3".pricing."deepinfra"]
input_cost_per_token = 3.8e-7
output_cost_per_token = 8.9e-7
[models."deepseek-ai/DeepSeek-V3".pricing."hyperbolic"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
[models."deepseek-ai/DeepSeek-V3".pricing."together_ai"]
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00000125

[models."deepseek-ai/DeepSeek-V3-0324"]
display_name = "DeepSeek-V3-0324"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 2.5e-7
output_cost_per_token = 8.8e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "gmi", "hyperbolic", "meganova", "nebius", "submodel", "wandb"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2025-03-24"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek-ai/DeepSeek-V3-0324".pricing."deepinfra"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 8.8e-7
[models."deepseek-ai/DeepSeek-V3-0324".pricing."gmi"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 8.8e-7
[models."deepseek-ai/DeepSeek-V3-0324".pricing."hyperbolic"]
input_cost_per_token = 4e-7
output_cost_per_token = 4e-7
[models."deepseek-ai/DeepSeek-V3-0324".pricing."meganova"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 8.8e-7
[models."deepseek-ai/DeepSeek-V3-0324".pricing."nebius"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
[models."deepseek-ai/DeepSeek-V3-0324".pricing."submodel"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7
[models."deepseek-ai/DeepSeek-V3-0324".pricing."wandb"]
input_cost_per_token = 0.114
output_cost_per_token = 0.275

[models."deepseek-ai/DeepSeek-V3-0324-TEE"]
display_name = "DeepSeek V3 0324 TEE"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 65536
input_cost_per_token = 1.9e-7
output_cost_per_token = 8.7e-7
cache_read_input_token_cost = 9.5e-8
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-ai/DeepSeek-V3-0324-TEE".pricing."chutes"]
cache_read_input_token_cost = 9.5e-8
input_cost_per_token = 1.9e-7
output_cost_per_token = 8.7e-7

[models."deepseek-ai/DeepSeek-V3-1"]
display_name = "DeepSeek V3.1"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000017
litellm_provider = "togetherai"
providers = ["togetherai", "meganova", "siliconflow", "submodel"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-08"
release_date = "2025-08-21"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-ai/DeepSeek-V3-1".pricing."meganova"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
[models."deepseek-ai/DeepSeek-V3-1".pricing."siliconflow"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
[models."deepseek-ai/DeepSeek-V3-1".pricing."submodel"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7
[models."deepseek-ai/DeepSeek-V3-1".pricing."togetherai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000017

[models."deepseek-ai/DeepSeek-V3-1-TEE"]
display_name = "DeepSeek V3.1 TEE"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 65536
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-ai/DeepSeek-V3-1-TEE".pricing."chutes"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7

[models."deepseek-ai/DeepSeek-V3-1-Terminus-TEE"]
display_name = "DeepSeek V3.1 Terminus TEE"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 65536
input_cost_per_token = 2.3000000000000002e-7
output_cost_per_token = 9.000000000000001e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-ai/DeepSeek-V3-1-Terminus-TEE".pricing."chutes"]
input_cost_per_token = 2.3000000000000002e-7
output_cost_per_token = 9.000000000000001e-7

[models."deepseek-ai/DeepSeek-V3-2-Exp"]
display_name = "deepseek-ai/DeepSeek-V3.2-Exp"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 164000
max_output_tokens = 164000
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.1e-7
litellm_provider = "siliconflow"
providers = ["siliconflow", "meganova"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-10-10"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-ai/DeepSeek-V3-2-Exp".pricing."meganova"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.0000000000000003e-7
[models."deepseek-ai/DeepSeek-V3-2-Exp".pricing."siliconflow"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.1e-7

[models."deepseek-ai/DeepSeek-V3-2-Speciale-TEE"]
display_name = "DeepSeek V3.2 Speciale TEE"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 65536
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.1e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-ai/DeepSeek-V3-2-Speciale-TEE".pricing."chutes"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.1e-7

[models."deepseek-ai/DeepSeek-V3-2-TEE"]
display_name = "DeepSeek V3.2 TEE"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 65536
input_cost_per_token = 2.5e-7
output_cost_per_token = 3.8e-7
cache_read_input_token_cost = 1.25e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-ai/DeepSeek-V3-2-TEE".pricing."chutes"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 2.5e-7
output_cost_per_token = 3.8e-7

[models."deepseek-ai/DeepSeek-V3.1"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
cache_read_input_token_cost = 2.16e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "together_ai", "wandb"]
supports_function_calling = true
supports_reasoning = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."deepseek-ai/DeepSeek-V3.1".pricing."deepinfra"]
cache_read_input_token_cost = 2.16e-7
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
[models."deepseek-ai/DeepSeek-V3.1".pricing."together_ai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000017
[models."deepseek-ai/DeepSeek-V3.1".pricing."wandb"]
input_cost_per_token = 0.055
output_cost_per_token = 0.165

[models."deepseek-ai/DeepSeek-V3.1-Terminus"]
display_name = "DeepSeek V3.1 Terminus"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
cache_read_input_token_cost = 2.16e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "abacus", "nvidia", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-09-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."deepseek-ai/DeepSeek-V3.1-Terminus".pricing."abacus"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
[models."deepseek-ai/DeepSeek-V3.1-Terminus".pricing."deepinfra"]
cache_read_input_token_cost = 2.16e-7
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
[models."deepseek-ai/DeepSeek-V3.1-Terminus".pricing."siliconflow"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
[models."deepseek-ai/DeepSeek-V3.1-Terminus".pricing."siliconflow-cn"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001

[models."deepseek-ai/DeepSeek-V3.2"]
display_name = "DeepSeek V3.2"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2.8e-7
output_cost_per_token = 4e-7
litellm_provider = "gmi"
providers = ["gmi", "abacus", "baseten", "deepinfra", "huggingface", "meganova", "nebius", "nvidia", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2025-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
reasoning_cost_per_token = 4.5000000000000003e-7

[models."deepseek-ai/DeepSeek-V3.2".pricing."abacus"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.0000000000000003e-7
[models."deepseek-ai/DeepSeek-V3.2".pricing."baseten"]
input_cost_per_token = 3e-7
output_cost_per_token = 4.5000000000000003e-7
[models."deepseek-ai/DeepSeek-V3.2".pricing."deepinfra"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 2.6e-7
output_cost_per_token = 3.8e-7
[models."deepseek-ai/DeepSeek-V3.2".pricing."gmi"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 4e-7
[models."deepseek-ai/DeepSeek-V3.2".pricing."huggingface"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 4.0000000000000003e-7
[models."deepseek-ai/DeepSeek-V3.2".pricing."meganova"]
input_cost_per_token = 2.6e-7
output_cost_per_token = 3.8e-7
[models."deepseek-ai/DeepSeek-V3.2".pricing."nebius"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 4.5000000000000003e-7
reasoning_cost_per_token = 4.5000000000000003e-7
[models."deepseek-ai/DeepSeek-V3.2".pricing."siliconflow"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.2e-7
[models."deepseek-ai/DeepSeek-V3.2".pricing."siliconflow-cn"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.2e-7

[models."deepseek-ai/deepseek-coder-6-7b-instruct"]
display_name = "Deepseek Coder 6.7b Instruct"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2023-10-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-ai/deepseek-ocr-maas"]
mode = "ocr"
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
source = "https://cloud.google.com/vertex-ai/pricing"
ocr_cost_per_page = 0.0003

[models."deepseek-ai/deepseek-ocr-maas".pricing."vertex_ai"]
input_cost_per_token = 3e-7
ocr_cost_per_page = 0.0003
output_cost_per_token = 0.0000012

[models."deepseek-ai/deepseek-r1"]
display_name = "Deepseek R1"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000375
output_cost_per_token = 0.00001
litellm_provider = "replicate"
providers = ["replicate", "abacus", "nvidia", "siliconflow", "siliconflow-cn", "togetherai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2025-01-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
output_cost_per_reasoning_token = 0.00001
supports_system_messages = true

[models."deepseek-ai/deepseek-r1".pricing."abacus"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000007
[models."deepseek-ai/deepseek-r1".pricing."replicate"]
input_cost_per_token = 0.00000375
output_cost_per_reasoning_token = 0.00001
output_cost_per_token = 0.00001
[models."deepseek-ai/deepseek-r1".pricing."siliconflow"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000021800000000000003
[models."deepseek-ai/deepseek-r1".pricing."siliconflow-cn"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000021800000000000003
[models."deepseek-ai/deepseek-r1".pricing."togetherai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000007

[models."deepseek-ai/deepseek-r1-0528-fast"]
display_name = "DeepSeek R1 0528 Fast"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
litellm_provider = "nebius"
providers = ["nebius"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-ai/deepseek-r1-0528-fast".pricing."nebius"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006

[models."deepseek-ai/deepseek-r1-0528-maas"]
mode = "chat"
max_input_tokens = 65336
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_assistant_prefill = true
supports_tool_choice = true

[models."deepseek-ai/deepseek-r1-0528-maas".pricing."vertex_ai"]
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054

[models."deepseek-ai/deepseek-v3"]
display_name = "deepseek-ai/DeepSeek-V3"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000145
output_cost_per_token = 0.00000145
litellm_provider = "replicate"
providers = ["replicate", "chutes", "siliconflow", "siliconflow-cn", "togetherai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2024-12-26"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."deepseek-ai/deepseek-v3".pricing."chutes"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."deepseek-ai/deepseek-v3".pricing."replicate"]
input_cost_per_token = 0.00000145
output_cost_per_token = 0.00000145
[models."deepseek-ai/deepseek-v3".pricing."siliconflow"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001
[models."deepseek-ai/deepseek-v3".pricing."siliconflow-cn"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001
[models."deepseek-ai/deepseek-v3".pricing."togetherai"]
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00000125

[models."deepseek-ai/deepseek-v3-0324-fast"]
display_name = "DeepSeek-V3-0324 (Fast)"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 7.5e-7
output_cost_per_token = 0.00000225
cache_read_input_token_cost = 7.5e-8
litellm_provider = "nebius"
providers = ["nebius"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2025-03-24"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-ai/deepseek-v3-0324-fast".pricing."nebius"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 7.5e-7
output_cost_per_token = 0.00000225

[models."deepseek-ai/deepseek-v3.1"]
display_name = "DeepSeek V3.1"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 6.72e-7
output_cost_per_token = 0.000002016
litellm_provider = "replicate"
providers = ["replicate", "nvidia"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2025-08-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."deepseek-ai/deepseek-v3.1".pricing."replicate"]
input_cost_per_token = 6.72e-7
output_cost_per_token = 0.000002016

[models."deepseek-ai/deepseek-v3.1-maas"]
display_name = "DeepSeek V3.1"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = true
release_date = "2025-08-28"
supported_modalities = ["text", "pdf"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supported_regions = ["us-west2"]
supports_assistant_prefill = true
supports_tool_choice = true

[models."deepseek-ai/deepseek-v3.1-maas".pricing."google-vertex"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000017
[models."deepseek-ai/deepseek-v3.1-maas".pricing."vertex_ai"]
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054

[models."deepseek-ai/deepseek-v3.2-maas"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000168
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
input_cost_per_token_batches = 2.8e-7
output_cost_per_token_batches = 8.4e-7
supported_regions = ["us-west2"]
supports_assistant_prefill = true
supports_tool_choice = true

[models."deepseek-ai/deepseek-v3.2-maas".pricing."vertex_ai"]
input_cost_per_token = 5.6e-7
input_cost_per_token_batches = 2.8e-7
output_cost_per_token = 0.00000168
output_cost_per_token_batches = 8.4e-7

[models."deepseek-ai/deepseek-vl2"]
display_name = "deepseek-ai/deepseek-vl2"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 4000
max_output_tokens = 4000
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "siliconflow"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-12-13"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-ai/deepseek-vl2".pricing."siliconflow"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."deepseek-ai/deepseek-vl2".pricing."siliconflow-cn"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7

[models."deepseek-chat"]
display_name = "Deepseek-Chat"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2.8e-7
output_cost_per_token = 4.2e-7
cache_read_input_token_cost = 2.8e-8
litellm_provider = "deepseek"
providers = ["deepseek", "302ai"]
supports_function_calling = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2024-11-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://api-docs.deepseek.com/quick_start/pricing"
supported_endpoints = ["/v1/chat/completions"]
supports_assistant_prefill = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek-chat".pricing."302ai"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 4.3e-7
[models."deepseek-chat".pricing."deepseek"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 2.8e-8
input_cost_per_token = 2.8e-7
input_cost_per_token_cache_hit = 2.8e-8
output_cost_per_token = 4.2e-7

[models."deepseek-coder"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.4e-7
output_cost_per_token = 2.8e-7
litellm_provider = "deepseek"
providers = ["deepseek"]
supports_function_calling = true
supports_prompt_caching = true
input_cost_per_token_cache_hit = 1.4e-8
supports_assistant_prefill = true
supports_tool_choice = true

[models."deepseek-coder".pricing."deepseek"]
input_cost_per_token = 1.4e-7
input_cost_per_token_cache_hit = 1.4e-8
output_cost_per_token = 2.8e-7

[models."deepseek-coder-6.7b"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 6e-8
output_cost_per_token = 1.2e-7
litellm_provider = "llamagate"
providers = ["llamagate"]
supports_function_calling = true
supports_response_schema = true

[models."deepseek-coder-6.7b".pricing."llamagate"]
input_cost_per_token = 6e-8
output_cost_per_token = 1.2e-7

[models."deepseek-coder-v2-base"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = true

[models."deepseek-coder-v2-base".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."deepseek-coder-v2-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = true

[models."deepseek-coder-v2-instruct".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."deepseek-coder-v2-lite-base"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = true

[models."deepseek-coder-v2-lite-base".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."deepseek-coder-v2-lite-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = true

[models."deepseek-coder-v2-lite-instruct".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."deepseek-llama3.3-70b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7
litellm_provider = "lambda_ai"
providers = ["lambda_ai"]
supports_function_calling = true
supports_reasoning = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek-llama3.3-70b".pricing."lambda_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7

[models."deepseek-r1"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054
litellm_provider = "azure_ai"
providers = ["azure_ai", "deepseek", "snowflake"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
source = "https://techcommunity.microsoft.com/blog/machinelearningblog/deepseek-r1-improved-performance-higher-limits-and-transparent-pricing/4386367"
supports_assistant_prefill = true
supports_tool_choice = true

[models."deepseek-r1".pricing."azure_ai"]
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054
[models."deepseek-r1".pricing."deepseek"]
input_cost_per_token = 5.5e-7
input_cost_per_token_cache_hit = 1.4e-7
output_cost_per_token = 0.00000219

[models."deepseek-r1-0528"]
display_name = "DeepSeek R1 0528"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7
litellm_provider = "lambda_ai"
providers = ["lambda_ai", "alibaba-cn", "azure", "azure-cognitive-services"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2025-05-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek-r1-0528".pricing."alibaba-cn"]
input_cost_per_token = 5.739999999999999e-7
output_cost_per_token = 0.000002294
[models."deepseek-r1-0528".pricing."azure"]
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054
[models."deepseek-r1-0528".pricing."azure-cognitive-services"]
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054
[models."deepseek-r1-0528".pricing."lambda_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7

[models."deepseek-r1-671b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 8e-7
output_cost_per_token = 8e-7
litellm_provider = "lambda_ai"
providers = ["lambda_ai"]
supports_function_calling = true
supports_reasoning = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek-r1-671b".pricing."lambda_ai"]
input_cost_per_token = 8e-7
output_cost_per_token = 8e-7

[models."deepseek-r1-7b-qwen"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 8e-8
output_cost_per_token = 1.5e-7
litellm_provider = "llamagate"
providers = ["llamagate"]
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true

[models."deepseek-r1-7b-qwen".pricing."llamagate"]
input_cost_per_token = 8e-8
output_cost_per_token = 1.5e-7

[models."deepseek-r1-8b"]
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1e-7
output_cost_per_token = 2e-7
litellm_provider = "llamagate"
providers = ["llamagate"]
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true

[models."deepseek-r1-8b".pricing."llamagate"]
input_cost_per_token = 1e-7
output_cost_per_token = 2e-7

[models."deepseek-r1-distill-llama-70b"]
mode = "chat"
max_tokens = 8000
input_cost_per_token = 9.9e-7
output_cost_per_token = 9.9e-7
litellm_provider = "gradient_ai"
providers = ["gradient_ai"]
supported_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions"]
supports_tool_choice = false

[models."deepseek-r1-distill-llama-70b".pricing."gradient_ai"]
input_cost_per_token = 9.9e-7
output_cost_per_token = 9.9e-7

[models."deepseek-r1-distill-llama-8b"]
display_name = "DeepSeek R1 Distill Llama 8B"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 16384
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-r1-distill-qwen-1-5b"]
display_name = "DeepSeek R1 Distill Qwen 1.5B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 16384
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-r1-distill-qwen-14b"]
display_name = "DeepSeek R1 Distill Qwen 14B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 16384
input_cost_per_token = 1.44e-7
output_cost_per_token = 4.31e-7
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-r1-distill-qwen-14b".pricing."alibaba-cn"]
input_cost_per_token = 1.44e-7
output_cost_per_token = 4.31e-7

[models."deepseek-r1-distill-qwen-32b"]
display_name = "DeepSeek R1 Distill Qwen 32B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 121808
max_output_tokens = 8192
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7
litellm_provider = "vultr"
providers = ["vultr", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-01-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-r1-distill-qwen-32b".pricing."alibaba-cn"]
input_cost_per_token = 2.8699999999999996e-7
output_cost_per_token = 8.61e-7
[models."deepseek-r1-distill-qwen-32b".pricing."vultr"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7

[models."deepseek-r1-distill-qwen-7b"]
display_name = "DeepSeek R1 Distill Qwen 7B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 16384
input_cost_per_token = 7.2e-8
output_cost_per_token = 1.44e-7
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-r1-distill-qwen-7b".pricing."alibaba-cn"]
input_cost_per_token = 7.2e-8
output_cost_per_token = 1.44e-7

[models."deepseek-reasoner"]
display_name = "Deepseek-Reasoner"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 2.8e-7
output_cost_per_token = 4.2e-7
cache_read_input_token_cost = 2.8e-8
litellm_provider = "deepseek"
providers = ["deepseek", "302ai", "helicone"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2025-01-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://api-docs.deepseek.com/quick_start/pricing"
supported_endpoints = ["/v1/chat/completions"]
supports_assistant_prefill = true
supports_native_streaming = true
supports_parallel_function_calling = false
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = false

[models."deepseek-reasoner".pricing."302ai"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 4.3e-7
[models."deepseek-reasoner".pricing."deepseek"]
cache_read_input_token_cost = 2.8e-8
input_cost_per_token = 2.8e-7
input_cost_per_token_cache_hit = 2.8e-8
output_cost_per_token = 4.2e-7
[models."deepseek-reasoner".pricing."helicone"]
cache_read_input_token_cost = 7e-8
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000168

[models."deepseek-tng-r1t2-chimera"]
display_name = "DeepSeek TNG R1T2 Chimera"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 130000
max_output_tokens = 163840
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
litellm_provider = "helicone"
providers = ["helicone"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-07"
release_date = "2025-07-02"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-tng-r1t2-chimera".pricing."helicone"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."deepseek-v3"]
display_name = "DeepSeek V3"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000114
output_cost_per_token = 0.00000456
litellm_provider = "azure_ai"
providers = ["azure_ai", "alibaba-cn", "deepseek", "helicone", "iflowcn"]
supports_function_calling = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2024-12"
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://techcommunity.microsoft.com/blog/machinelearningblog/announcing-deepseek-v3-on-azure-ai-foundry-and-github/4390438"
supports_assistant_prefill = true
supports_tool_choice = true

[models."deepseek-v3".pricing."alibaba-cn"]
input_cost_per_token = 2.8699999999999996e-7
output_cost_per_token = 0.000001147
[models."deepseek-v3".pricing."azure_ai"]
input_cost_per_token = 0.00000114
output_cost_per_token = 0.00000456
[models."deepseek-v3".pricing."deepseek"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 7e-8
input_cost_per_token = 2.7e-7
input_cost_per_token_cache_hit = 7e-8
output_cost_per_token = 0.0000011
[models."deepseek-v3".pricing."helicone"]
cache_read_input_token_cost = 7e-8
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000168

[models."deepseek-v3-0324"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.00000114
output_cost_per_token = 0.00000456
litellm_provider = "azure_ai"
providers = ["azure_ai", "lambda_ai"]
supports_function_calling = true
source = "https://techcommunity.microsoft.com/blog/machinelearningblog/announcing-deepseek-v3-on-azure-ai-foundry-and-github/4390438"
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek-v3-0324".pricing."azure_ai"]
input_cost_per_token = 0.00000114
output_cost_per_token = 0.00000456
[models."deepseek-v3-0324".pricing."lambda_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7

[models."deepseek-v3-1"]
display_name = "DeepSeek V3.1"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 65536
input_cost_per_token = 5.739999999999999e-7
output_cost_per_token = 0.0000017210000000000001
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn", "azure", "azure-cognitive-services"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-v3-1".pricing."alibaba-cn"]
input_cost_per_token = 5.739999999999999e-7
output_cost_per_token = 0.0000017210000000000001
[models."deepseek-v3-1".pricing."azure"]
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000168
[models."deepseek-v3-1".pricing."azure-cognitive-services"]
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000168

[models."deepseek-v3-1-terminus"]
display_name = "DeepSeek V3.1 Terminus"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
cache_read_input_token_cost = 2.1600000000000003e-7
litellm_provider = "helicone"
providers = ["helicone"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-09"
release_date = "2025-09-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-v3-1-terminus".pricing."helicone"]
cache_read_input_token_cost = 2.1600000000000003e-7
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001

[models."deepseek-v3-2-251201"]
mode = "chat"
max_input_tokens = 98304
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "volcengine"
providers = ["volcengine"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
supports_assistant_prefill = true
supports_tool_choice = true

[models."deepseek-v3-2-251201".pricing."volcengine"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."deepseek-v3-2-exp"]
display_name = "DeepSeek V3.2 Exp"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 65536
input_cost_per_token = 2.8699999999999996e-7
output_cost_per_token = 4.31e-7
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-v3-2-exp".pricing."alibaba-cn"]
input_cost_per_token = 2.8699999999999996e-7
output_cost_per_token = 4.31e-7

[models."deepseek-v3-2-fast"]
display_name = "DeepSeek-V3.2-Fast"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
input_cost_per_token = 0.0000011
output_cost_per_token = 0.00000329
litellm_provider = "aihubmix"
providers = ["aihubmix"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2025-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-v3-2-fast".pricing."aihubmix"]
input_cost_per_token = 0.0000011
output_cost_per_token = 0.00000329

[models."deepseek-v3-2-think"]
display_name = "DeepSeek-V3.2-Think"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 64000
input_cost_per_token = 3e-7
output_cost_per_token = 4.5000000000000003e-7
litellm_provider = "aihubmix"
providers = ["aihubmix"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2025-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-v3-2-think".pricing."aihubmix"]
input_cost_per_token = 3e-7
output_cost_per_token = 4.5000000000000003e-7

[models."deepseek-v3-2-thinking"]
display_name = "DeepSeek-V3.2-Thinking"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
input_cost_per_token = 2.9e-7
output_cost_per_token = 4.3e-7
litellm_provider = "302ai"
providers = ["302ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-12"
release_date = "2025-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek-v3-2-thinking".pricing."302ai"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 4.3e-7

[models."deepseek-v3.1:671b-cloud"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = true

[models."deepseek-v3.1:671b-cloud".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."deepseek-v3.2"]
display_name = "deepseek-v3.2"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 5.8e-7
output_cost_per_token = 0.00000168
litellm_provider = "azure_ai"
providers = ["azure_ai", "302ai", "aihubmix", "azure", "azure-cognitive-services", "deepseek", "helicone", "iflowcn", "venice", "vivgrid"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2024-12"
release_date = "2025-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_tool_choice = true

[models."deepseek-v3.2".pricing."302ai"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 4.3e-7
[models."deepseek-v3.2".pricing."aihubmix"]
input_cost_per_token = 3e-7
output_cost_per_token = 4.5000000000000003e-7
[models."deepseek-v3.2".pricing."azure"]
input_cost_per_token = 5.8e-7
output_cost_per_token = 0.00000168
[models."deepseek-v3.2".pricing."azure-cognitive-services"]
input_cost_per_token = 5.8e-7
output_cost_per_token = 0.00000168
[models."deepseek-v3.2".pricing."azure_ai"]
input_cost_per_token = 5.8e-7
output_cost_per_token = 0.00000168
[models."deepseek-v3.2".pricing."deepseek"]
input_cost_per_token = 2.8e-7
input_cost_per_token_cache_hit = 2.8e-8
output_cost_per_token = 4e-7
[models."deepseek-v3.2".pricing."helicone"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.1e-7
[models."deepseek-v3.2".pricing."venice"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000001
[models."deepseek-v3.2".pricing."vivgrid"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 4.2e-7

[models."deepseek-v3.2-speciale"]
display_name = "DeepSeek-V3.2-Speciale"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 5.8e-7
output_cost_per_token = 0.00000168
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2025-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_tool_choice = true

[models."deepseek-v3.2-speciale".pricing."azure"]
input_cost_per_token = 5.8e-7
output_cost_per_token = 0.00000168
[models."deepseek-v3.2-speciale".pricing."azure-cognitive-services"]
input_cost_per_token = 5.8e-7
output_cost_per_token = 0.00000168
[models."deepseek-v3.2-speciale".pricing."azure_ai"]
input_cost_per_token = 5.8e-7
output_cost_per_token = 0.00000168

[models."deepseek.r1-v1:0"]
display_name = "DeepSeek-R1"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054
litellm_provider = "amazon-bedrock"
providers = ["amazon-bedrock"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2025-01-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek.r1-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054

[models."deepseek.v3-2-v1:0"]
display_name = "DeepSeek-V3.2"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 81920
input_cost_per_token = 6.2e-7
output_cost_per_token = 0.00000185
litellm_provider = "amazon-bedrock"
providers = ["amazon-bedrock"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2026-02-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek.v3-2-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 6.2e-7
output_cost_per_token = 0.00000185

[models."deepseek.v3-v1:0"]
display_name = "DeepSeek-V3.1"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 81920
max_tokens = 81920
input_cost_per_token = 5.8e-7
output_cost_per_token = 0.00000168
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2025-09-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."deepseek.v3-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 5.8e-7
output_cost_per_token = 0.00000168
[models."deepseek.v3-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 5.8e-7
output_cost_per_token = 0.00000168

[models."deepseek.v3.2"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 6.2e-7
output_cost_per_token = 0.00000185
litellm_provider = "bedrock"
providers = ["bedrock", "bedrock_converse"]
supports_function_calling = true
supports_reasoning = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_tool_choice = true

[models."deepseek.v3.2".pricing."bedrock/us-east-1"]
input_cost_per_token = 6.2e-7
output_cost_per_token = 0.00000185
[models."deepseek.v3.2".pricing."bedrock/us-west-2"]
input_cost_per_token = 6.2e-7
output_cost_per_token = 0.00000185
[models."deepseek.v3.2".pricing."bedrock_converse"]
input_cost_per_token = 6.2e-7
output_cost_per_token = 0.00000185

[models."deepseek/deepseek-chat"]
display_name = "DeepSeek-V3.2 (Non-thinking Mode)"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.4e-7
output_cost_per_token = 2.8e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "zenmux"]
supports_function_calling = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2025-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."deepseek/deepseek-chat".pricing."kilo"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."deepseek/deepseek-chat".pricing."openrouter"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 2.8e-7
[models."deepseek/deepseek-chat".pricing."zenmux"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.8e-7
output_cost_per_token = 4.2e-7

[models."deepseek/deepseek-chat-v3-0324"]
display_name = "DeepSeek V3 0324"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.4e-7
output_cost_per_token = 2.8e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-03-24"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."deepseek/deepseek-chat-v3-0324".pricing."kilo"]
cache_read_input_token_cost = 9.5e-8
input_cost_per_token = 1.9e-7
output_cost_per_token = 8.7e-7
[models."deepseek/deepseek-chat-v3-0324".pricing."openrouter"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 2.8e-7

[models."deepseek/deepseek-chat-v3.1"]
display_name = "DeepSeek-V3.1"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 2e-7
output_cost_per_token = 8e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2025-08-21"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
input_cost_per_token_cache_hit = 2e-8
supports_assistant_prefill = true
supports_tool_choice = true

[models."deepseek/deepseek-chat-v3.1".pricing."kilo"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 7.5e-7
[models."deepseek/deepseek-chat-v3.1".pricing."openrouter"]
input_cost_per_token = 2e-7
input_cost_per_token_cache_hit = 2e-8
output_cost_per_token = 8e-7

[models."deepseek/deepseek-ocr"]
display_name = "DeepSeek-OCR"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3e-8
output_cost_per_token = 3e-8
litellm_provider = "novita"
providers = ["novita", "novita-ai"]
supports_function_calling = false
supports_vision = true
supports_reasoning = false
open_weights = true
release_date = "2025-10-24"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek/deepseek-ocr".pricing."novita"]
input_cost_per_token = 3e-8
output_cost_per_token = 3e-8
[models."deepseek/deepseek-ocr".pricing."novita-ai"]
input_cost_per_token = 3e-8
output_cost_per_token = 3e-8

[models."deepseek/deepseek-ocr-2"]
display_name = "deepseek/deepseek-ocr-2"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
input_cost_per_token = 3e-8
output_cost_per_token = 3e-8
litellm_provider = "novita-ai"
providers = ["novita-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2026-01-27"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek/deepseek-ocr-2".pricing."novita-ai"]
input_cost_per_token = 3e-8
output_cost_per_token = 3e-8

[models."deepseek/deepseek-prover-v2-671b"]
display_name = "Deepseek Prover V2 671B"
mode = "chat"
max_input_tokens = 160000
max_output_tokens = 160000
max_tokens = 160000
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000025
litellm_provider = "novita"
providers = ["novita", "novita-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-04-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."deepseek/deepseek-prover-v2-671b".pricing."novita"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000025
[models."deepseek/deepseek-prover-v2-671b".pricing."novita-ai"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000025

[models."deepseek/deepseek-r1"]
display_name = "DeepSeek-R1"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.00000219
litellm_provider = "openrouter"
providers = ["openrouter", "github-models", "kilo", "nano-gpt", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2025-01-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
input_cost_per_token_cache_hit = 1.4e-7
supports_assistant_prefill = true
supports_tool_choice = true

[models."deepseek/deepseek-r1".pricing."kilo"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000025
[models."deepseek/deepseek-r1".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
[models."deepseek/deepseek-r1".pricing."openrouter"]
input_cost_per_token = 5.5e-7
input_cost_per_token_cache_hit = 1.4e-7
output_cost_per_token = 0.00000219
[models."deepseek/deepseek-r1".pricing."vercel"]
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054
[models."deepseek/deepseek-r1".pricing."vercel_ai_gateway"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.00000219

[models."deepseek/deepseek-r1-0528"]
display_name = "DeepSeek R1 0528"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 5e-7
output_cost_per_token = 0.00000215
litellm_provider = "openrouter"
providers = ["openrouter", "github-models", "jiekou", "kilo", "novita", "novita-ai"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
input_cost_per_token_cache_hit = 1.4e-7
supports_assistant_prefill = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek/deepseek-r1-0528".pricing."jiekou"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000025
[models."deepseek/deepseek-r1-0528".pricing."kilo"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.00000175
[models."deepseek/deepseek-r1-0528".pricing."novita"]
cache_read_input_token_cost = 3.5e-7
input_cost_per_token = 7e-7
input_cost_per_token_cache_hit = 3.5e-7
output_cost_per_token = 0.0000025
[models."deepseek/deepseek-r1-0528".pricing."novita-ai"]
cache_read_input_token_cost = 3.5e-7
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000025
[models."deepseek/deepseek-r1-0528".pricing."openrouter"]
input_cost_per_token = 5e-7
input_cost_per_token_cache_hit = 1.4e-7
output_cost_per_token = 0.00000215

[models."deepseek/deepseek-r1-0528-qwen3-8b"]
display_name = "DeepSeek R1 0528 Qwen3 8B"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 6e-8
output_cost_per_token = 9e-8
litellm_provider = "novita"
providers = ["novita", "novita-ai"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
release_date = "2025-05-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."deepseek/deepseek-r1-0528-qwen3-8b".pricing."novita"]
input_cost_per_token = 6e-8
output_cost_per_token = 9e-8
[models."deepseek/deepseek-r1-0528-qwen3-8b".pricing."novita-ai"]
input_cost_per_token = 6e-8
output_cost_per_token = 9e-8

[models."deepseek/deepseek-r1-0528-qwen3-8b:free"]
display_name = "Deepseek R1 0528 Qwen3 8B (free)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-05"
release_date = "2025-05-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek/deepseek-r1-0528:free"]
display_name = "R1 0528 (free)"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-05"
release_date = "2025-05-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek/deepseek-r1-distill-llama-70b"]
display_name = "DeepSeek R1 Distill LLama 70B"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 7.5e-7
output_cost_per_token = 9.9e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "kilo", "novita", "novita-ai", "openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-01-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek/deepseek-r1-distill-llama-70b".pricing."kilo"]
cache_read_input_token_cost = 1.5e-8
input_cost_per_token = 3e-8
output_cost_per_token = 1.1e-7
[models."deepseek/deepseek-r1-distill-llama-70b".pricing."novita"]
input_cost_per_token = 8e-7
output_cost_per_token = 8e-7
[models."deepseek/deepseek-r1-distill-llama-70b".pricing."novita-ai"]
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 8.000000000000001e-7
[models."deepseek/deepseek-r1-distill-llama-70b".pricing."vercel_ai_gateway"]
input_cost_per_token = 7.5e-7
output_cost_per_token = 9.9e-7

[models."deepseek/deepseek-r1-distill-qwen-14b"]
display_name = "DeepSeek R1 Distill Qwen 14B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "novita"
providers = ["novita", "openrouter"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-01-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek/deepseek-r1-distill-qwen-14b".pricing."novita"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7

[models."deepseek/deepseek-r1-distill-qwen-32b"]
display_name = "DeepSeek: R1 Distill Qwen 32B"
mode = "chat"
max_input_tokens = 64000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 3e-7
output_cost_per_token = 3e-7
litellm_provider = "novita"
providers = ["novita", "kilo"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek/deepseek-r1-distill-qwen-32b".pricing."kilo"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 2.9e-7
[models."deepseek/deepseek-r1-distill-qwen-32b".pricing."novita"]
input_cost_per_token = 3e-7
output_cost_per_token = 3e-7

[models."deepseek/deepseek-r1-turbo"]
display_name = "DeepSeek R1 (Turbo)	"
mode = "chat"
max_input_tokens = 64000
max_output_tokens = 16000
max_tokens = 16000
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000025
litellm_provider = "novita"
providers = ["novita", "novita-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-03-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek/deepseek-r1-turbo".pricing."novita"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000025
[models."deepseek/deepseek-r1-turbo".pricing."novita-ai"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000025

[models."deepseek/deepseek-r1:free"]
display_name = "R1 (free)"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2025-01-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek/deepseek-v3"]
display_name = "DeepSeek V3 0324"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2024-12-26"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."deepseek/deepseek-v3".pricing."vercel"]
input_cost_per_token = 7.7e-7
output_cost_per_token = 7.7e-7
[models."deepseek/deepseek-v3".pricing."vercel_ai_gateway"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."deepseek/deepseek-v3-0324"]
display_name = "DeepSeek V3 0324"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.00000112
cache_read_input_token_cost = 1.35e-7
litellm_provider = "novita"
providers = ["novita", "github-models", "jiekou", "novita-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
input_cost_per_token_cache_hit = 1.35e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek/deepseek-v3-0324".pricing."jiekou"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 0.0000011399999999999999
[models."deepseek/deepseek-v3-0324".pricing."novita"]
cache_read_input_token_cost = 1.35e-7
input_cost_per_token = 2.7e-7
input_cost_per_token_cache_hit = 1.35e-7
output_cost_per_token = 0.00000112
[models."deepseek/deepseek-v3-0324".pricing."novita-ai"]
cache_read_input_token_cost = 1.35e-7
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.00000112

[models."deepseek/deepseek-v3-1-terminus:exacto"]
display_name = "DeepSeek V3.1 Terminus (exacto)"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 65536
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2025-09-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek/deepseek-v3-1-terminus:exacto".pricing."kilo"]
cache_read_input_token_cost = 1.6800000000000002e-7
input_cost_per_token = 2.1e-7
output_cost_per_token = 7.900000000000001e-7
[models."deepseek/deepseek-v3-1-terminus:exacto".pricing."openrouter"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001

[models."deepseek/deepseek-v3-2-speciale"]
display_name = "DeepSeek V3.2 Speciale"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 65536
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.1e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2025-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek/deepseek-v3-2-speciale".pricing."kilo"]
cache_read_input_token_cost = 1.35e-7
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.1e-7
[models."deepseek/deepseek-v3-2-speciale".pricing."openrouter"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.1e-7

[models."deepseek/deepseek-v3-2-thinking"]
display_name = "DeepSeek V3.2 Thinking"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 64000
input_cost_per_token = 2.8e-7
output_cost_per_token = 4.2e-7
cache_read_input_token_cost = 3e-8
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2025-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek/deepseek-v3-2-thinking".pricing."vercel"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.8e-7
output_cost_per_token = 4.2e-7

[models."deepseek/deepseek-v3-2:thinking"]
display_name = "Deepseek V3.2 Thinking"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
litellm_provider = "nano-gpt"
providers = ["nano-gpt"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek/deepseek-v3-2:thinking".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002

[models."deepseek/deepseek-v3-base:free"]
display_name = "DeepSeek V3 Base (free)"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-03"
release_date = "2025-03-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."deepseek/deepseek-v3-turbo"]
display_name = "DeepSeek V3 (Turbo)	"
mode = "chat"
max_input_tokens = 64000
max_output_tokens = 16000
max_tokens = 16000
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000013
litellm_provider = "novita"
providers = ["novita", "novita-ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-03-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek/deepseek-v3-turbo".pricing."novita"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000013
[models."deepseek/deepseek-v3-turbo".pricing."novita-ai"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000013

[models."deepseek/deepseek-v3.1"]
display_name = "DeepSeek V3.1"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
cache_read_input_token_cost = 1.35e-7
litellm_provider = "novita"
providers = ["novita", "abacus", "jiekou", "novita-ai", "vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
input_cost_per_token_cache_hit = 1.35e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek/deepseek-v3.1".pricing."abacus"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 2.8e-7
[models."deepseek/deepseek-v3.1".pricing."jiekou"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
[models."deepseek/deepseek-v3.1".pricing."novita"]
cache_read_input_token_cost = 1.35e-7
input_cost_per_token = 2.7e-7
input_cost_per_token_cache_hit = 1.35e-7
output_cost_per_token = 0.000001
[models."deepseek/deepseek-v3.1".pricing."novita-ai"]
cache_read_input_token_cost = 1.35e-7
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
[models."deepseek/deepseek-v3.1".pricing."vercel"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.000001

[models."deepseek/deepseek-v3.1-terminus"]
display_name = "DeepSeek V3.1 Terminus"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
cache_read_input_token_cost = 1.35e-7
litellm_provider = "novita"
providers = ["novita", "kilo", "novita-ai", "openrouter", "vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2025-09-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
input_cost_per_token_cache_hit = 1.35e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek/deepseek-v3.1-terminus".pricing."kilo"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 2.1e-7
output_cost_per_token = 7.900000000000001e-7
[models."deepseek/deepseek-v3.1-terminus".pricing."novita"]
cache_read_input_token_cost = 1.35e-7
input_cost_per_token = 2.7e-7
input_cost_per_token_cache_hit = 1.35e-7
output_cost_per_token = 0.000001
[models."deepseek/deepseek-v3.1-terminus".pricing."novita-ai"]
cache_read_input_token_cost = 1.35e-7
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
[models."deepseek/deepseek-v3.1-terminus".pricing."openrouter"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
[models."deepseek/deepseek-v3.1-terminus".pricing."vercel"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001

[models."deepseek/deepseek-v3.2"]
display_name = "DeepSeek V3.2"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 2.8e-7
output_cost_per_token = 4e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "novita", "novita-ai", "vercel", "zenmux"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2025-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
input_cost_per_token_cache_hit = 2.8e-8
supports_assistant_prefill = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek/deepseek-v3.2".pricing."kilo"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 2.5e-7
output_cost_per_token = 3.8e-7
[models."deepseek/deepseek-v3.2".pricing."novita"]
cache_read_input_token_cost = 1.345e-7
input_cost_per_token = 2.69e-7
input_cost_per_token_cache_hit = 1.345e-7
output_cost_per_token = 4e-7
[models."deepseek/deepseek-v3.2".pricing."novita-ai"]
cache_read_input_token_cost = 1.3450000000000002e-7
input_cost_per_token = 2.6900000000000004e-7
output_cost_per_token = 4.0000000000000003e-7
[models."deepseek/deepseek-v3.2".pricing."openrouter"]
input_cost_per_token = 2.8e-7
input_cost_per_token_cache_hit = 2.8e-8
output_cost_per_token = 4e-7
[models."deepseek/deepseek-v3.2".pricing."vercel"]
cache_read_input_token_cost = 2.2e-7
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.0000000000000003e-7
[models."deepseek/deepseek-v3.2".pricing."zenmux"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 4.3e-7

[models."deepseek/deepseek-v3.2-exp"]
display_name = "DeepSeek V3.2 Exp"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 2e-7
output_cost_per_token = 4e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "novita", "novita-ai", "vercel", "zenmux"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2025-09"
release_date = "2025-09-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
input_cost_per_token_cache_hit = 2e-8
supports_assistant_prefill = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."deepseek/deepseek-v3.2-exp".pricing."kilo"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.1e-7
[models."deepseek/deepseek-v3.2-exp".pricing."novita"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.1e-7
[models."deepseek/deepseek-v3.2-exp".pricing."novita-ai"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.1e-7
[models."deepseek/deepseek-v3.2-exp".pricing."openrouter"]
input_cost_per_token = 2e-7
input_cost_per_token_cache_hit = 2e-8
output_cost_per_token = 4e-7
[models."deepseek/deepseek-v3.2-exp".pricing."vercel"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.0000000000000003e-7
[models."deepseek/deepseek-v3.2-exp".pricing."zenmux"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 3.3e-7

[models."devstral-2-123b-instruct-2512"]
display_name = "Devstral 2 123B Instruct (2512)"
model_family = "devstral"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 8192
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002
litellm_provider = "scaleway"
providers = ["scaleway"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2026-01-07"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."devstral-2-123b-instruct-2512".pricing."scaleway"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002

[models."devstral-2512"]
display_name = "Devstral 2"
model_family = "devstral"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002
litellm_provider = "mistral"
providers = ["mistral", "cortecs"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-12"
release_date = "2025-12-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://mistral.ai/news/devstral-2-vibe-cli"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."devstral-2512".pricing."mistral"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002

[models."devstral-latest"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
source = "https://mistral.ai/news/devstral-2-vibe-cli"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."devstral-latest".pricing."mistral"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002

[models."devstral-medium-2507"]
display_name = "Devstral Medium"
model_family = "devstral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-05"
release_date = "2025-07-10"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://mistral.ai/news/devstral"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."devstral-medium-2507".pricing."mistral"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002

[models."devstral-medium-latest"]
display_name = "Devstral 2"
model_family = "devstral"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-12"
release_date = "2025-12-02"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://mistral.ai/news/devstral-2-vibe-cli"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."devstral-medium-latest".pricing."mistral"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002

[models."devstral-small"]
display_name = "Devstral Small 1.1"
model_family = "devstral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-05-07"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_tool_choice = true

[models."devstral-small".pricing."vercel"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
[models."devstral-small".pricing."vercel_ai_gateway"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7

[models."devstral-small-2505"]
display_name = "Devstral Small 2505"
model_family = "devstral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-05"
release_date = "2025-05-07"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://mistral.ai/news/devstral"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."devstral-small-2505".pricing."mistral"]
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7

[models."devstral-small-2507"]
display_name = "Devstral Small"
model_family = "devstral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-05"
release_date = "2025-07-10"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://mistral.ai/news/devstral"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."devstral-small-2507".pricing."mistral"]
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7

[models."devstral-small-2512"]
display_name = "Devstral Small 2 2512"
mode = "chat"
max_input_tokens = 262000
max_output_tokens = 262000
litellm_provider = "cortecs"
providers = ["cortecs"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-12"
release_date = "2025-12-09"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."devstral-small-latest"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
source = "https://docs.mistral.ai/models/devstral-small-2-25-12"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."devstral-small-latest".pricing."mistral"]
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7

[models."doc-intelligence/prebuilt-document"]
mode = "ocr"
litellm_provider = "azure_ai"
providers = ["azure_ai"]
source = "https://azure.microsoft.com/en-us/pricing/details/ai-document-intelligence/"
ocr_cost_per_page = 0.01
supported_endpoints = ["/v1/ocr"]

[models."doc-intelligence/prebuilt-document".pricing."azure_ai"]
ocr_cost_per_page = 0.01

[models."doc-intelligence/prebuilt-layout"]
mode = "ocr"
litellm_provider = "azure_ai"
providers = ["azure_ai"]
source = "https://azure.microsoft.com/en-us/pricing/details/ai-document-intelligence/"
ocr_cost_per_page = 0.01
supported_endpoints = ["/v1/ocr"]

[models."doc-intelligence/prebuilt-layout".pricing."azure_ai"]
ocr_cost_per_page = 0.01

[models."doc-intelligence/prebuilt-read"]
mode = "ocr"
litellm_provider = "azure_ai"
providers = ["azure_ai"]
source = "https://azure.microsoft.com/en-us/pricing/details/ai-document-intelligence/"
ocr_cost_per_page = 0.0015
supported_endpoints = ["/v1/ocr"]

[models."doc-intelligence/prebuilt-read".pricing."azure_ai"]
ocr_cost_per_page = 0.0015

[models."dolphin"]
mode = "completion"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7
litellm_provider = "nlp_cloud"
providers = ["nlp_cloud"]

[models."dolphin".pricing."nlp_cloud"]
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7

[models."dolphin3-8b"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 8e-8
output_cost_per_token = 1.5e-7
litellm_provider = "llamagate"
providers = ["llamagate"]
supports_function_calling = true
supports_response_schema = true

[models."dolphin3-8b".pricing."llamagate"]
input_cost_per_token = 8e-8
output_cost_per_token = 1.5e-7

[models."doubao-embedding"]
mode = "embedding"
max_input_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "volcengine"
providers = ["volcengine"]
output_vector_size = 2560

[models."doubao-embedding".metadata]
notes = "Volcengine Doubao embedding model - standard version with 2560 dimensions"

[models."doubao-embedding".pricing."volcengine"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."doubao-embedding-large"]
mode = "embedding"
max_input_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "volcengine"
providers = ["volcengine"]
output_vector_size = 2048

[models."doubao-embedding-large".metadata]
notes = "Volcengine Doubao embedding model - large version with 2048 dimensions"

[models."doubao-embedding-large".pricing."volcengine"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."doubao-embedding-large-text-240915"]
mode = "embedding"
max_input_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "volcengine"
providers = ["volcengine"]
output_vector_size = 4096

[models."doubao-embedding-large-text-240915".metadata]
notes = "Volcengine Doubao embedding model - text-240915 version with 4096 dimensions"

[models."doubao-embedding-large-text-240915".pricing."volcengine"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."doubao-embedding-large-text-250515"]
mode = "embedding"
max_input_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "volcengine"
providers = ["volcengine"]
output_vector_size = 2048

[models."doubao-embedding-large-text-250515".metadata]
notes = "Volcengine Doubao embedding model - text-250515 version with 2048 dimensions"

[models."doubao-embedding-large-text-250515".pricing."volcengine"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."doubao-embedding-text-240715"]
mode = "embedding"
max_input_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "volcengine"
providers = ["volcengine"]
output_vector_size = 2560

[models."doubao-embedding-text-240715".metadata]
notes = "Volcengine Doubao embedding model - text-240715 version with 2560 dimensions"

[models."doubao-embedding-text-240715".pricing."volcengine"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."doubao-seed-1-6-thinking-250715"]
display_name = "doubao-seed-1-6-thinking-250715"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 16000
input_cost_per_token = 1.2099999999999998e-7
output_cost_per_token = 0.00000121
litellm_provider = "302ai"
providers = ["302ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-07-15"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."doubao-seed-1-6-thinking-250715".pricing."302ai"]
input_cost_per_token = 1.2099999999999998e-7
output_cost_per_token = 0.00000121

[models."doubao-seed-1-6-vision-250815"]
display_name = "doubao-seed-1-6-vision-250815"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 32000
input_cost_per_token = 1.14e-7
output_cost_per_token = 0.0000011430000000000001
litellm_provider = "302ai"
providers = ["302ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-09-30"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."doubao-seed-1-6-vision-250815".pricing."302ai"]
input_cost_per_token = 1.14e-7
output_cost_per_token = 0.0000011430000000000001

[models."doubao-seed-1-8-251215"]
display_name = "doubao-seed-1-8-251215"
mode = "chat"
max_input_tokens = 224000
max_output_tokens = 64000
input_cost_per_token = 1.14e-7
output_cost_per_token = 2.86e-7
litellm_provider = "302ai"
providers = ["302ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-12-18"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."doubao-seed-1-8-251215".pricing."302ai"]
input_cost_per_token = 1.14e-7
output_cost_per_token = 2.86e-7

[models."doubao-seed-code-preview-251028"]
display_name = "doubao-seed-code-preview-251028"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 32000
input_cost_per_token = 1.7000000000000001e-7
output_cost_per_token = 0.0000011399999999999999
litellm_provider = "302ai"
providers = ["302ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-11-11"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."doubao-seed-code-preview-251028".pricing."302ai"]
input_cost_per_token = 1.7000000000000001e-7
output_cost_per_token = 0.0000011399999999999999

[models."duo-chat-gpt-5-1"]
display_name = "Agentic Chat (GPT-5.1)"
model_family = "gpt"
mode = "chat"
max_input_tokens = 400000
max_output_tokens = 128000
litellm_provider = "gitlab"
providers = ["gitlab"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-09-30"
release_date = "2026-01-22"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."duo-chat-gpt-5-2"]
display_name = "Agentic Chat (GPT-5.2)"
model_family = "gpt"
mode = "chat"
max_input_tokens = 400000
max_output_tokens = 128000
litellm_provider = "gitlab"
providers = ["gitlab"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-08-31"
release_date = "2026-01-23"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."duo-chat-gpt-5-2-codex"]
display_name = "Agentic Chat (GPT-5.2 Codex)"
model_family = "gpt-codex"
mode = "chat"
max_input_tokens = 400000
max_output_tokens = 128000
litellm_provider = "gitlab"
providers = ["gitlab"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-08-31"
release_date = "2026-01-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."duo-chat-gpt-5-codex"]
display_name = "Agentic Chat (GPT-5 Codex)"
model_family = "gpt-codex"
mode = "chat"
max_input_tokens = 400000
max_output_tokens = 128000
litellm_provider = "gitlab"
providers = ["gitlab"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-09-30"
release_date = "2026-01-22"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."duo-chat-gpt-5-mini"]
display_name = "Agentic Chat (GPT-5 Mini)"
model_family = "gpt-mini"
mode = "chat"
max_input_tokens = 400000
max_output_tokens = 128000
litellm_provider = "gitlab"
providers = ["gitlab"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-05-30"
release_date = "2026-01-22"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."duo-chat-haiku-4-5"]
display_name = "Agentic Chat (Claude Haiku 4.5)"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
litellm_provider = "gitlab"
providers = ["gitlab"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-02-28"
release_date = "2026-01-08"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."duo-chat-opus-4-5"]
display_name = "Agentic Chat (Claude Opus 4.5)"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
litellm_provider = "gitlab"
providers = ["gitlab"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2026-01-08"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."duo-chat-opus-4-6"]
display_name = "Agentic Chat (Claude Opus 4.6)"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
litellm_provider = "gitlab"
providers = ["gitlab"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2026-02-05"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."duo-chat-sonnet-4-5"]
display_name = "Agentic Chat (Claude Sonnet 4.5)"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
litellm_provider = "gitlab"
providers = ["gitlab"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-07-31"
release_date = "2026-01-08"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."duo-chat-sonnet-4-6"]
display_name = "Agentic Chat (Claude Sonnet 4.6)"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
litellm_provider = "gitlab"
providers = ["gitlab"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-08-31"
release_date = "2026-02-17"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."eleven_multilingual_v2"]
mode = "audio_speech"
litellm_provider = "elevenlabs"
providers = ["elevenlabs", "runwayml"]
source = "https://elevenlabs.io/pricing"
input_cost_per_character = 0.00018
supported_endpoints = ["/v1/audio/speech"]

[models."eleven_multilingual_v2".metadata]
calculation = "$0.18/1000 characters (Scale plan pricing, 1 credit per character)"
notes = "ElevenLabs Eleven Multilingual v2 - default TTS model with 29 languages support"

[models."eleven_multilingual_v2".pricing."elevenlabs"]
input_cost_per_character = 0.00018
[models."eleven_multilingual_v2".pricing."runwayml"]
input_cost_per_character = 3e-7

[models."eleven_v3"]
mode = "audio_speech"
litellm_provider = "elevenlabs"
providers = ["elevenlabs"]
source = "https://elevenlabs.io/pricing"
input_cost_per_character = 0.00018
supported_endpoints = ["/v1/audio/speech"]

[models."eleven_v3".metadata]
calculation = "$0.18/1000 characters (Scale plan pricing, 1 credit per character)"
notes = "ElevenLabs Eleven v3 - most expressive TTS model with 70+ languages and audio tags support"

[models."eleven_v3".pricing."elevenlabs"]
input_cost_per_character = 0.00018

[models."embed-english-light-v2.0"]
mode = "embedding"
max_input_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "cohere"
providers = ["cohere"]

[models."embed-english-light-v2.0".pricing."cohere"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."embed-english-light-v3.0"]
mode = "embedding"
max_input_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "cohere"
providers = ["cohere"]

[models."embed-english-light-v3.0".pricing."cohere"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."embed-english-v2.0"]
mode = "embedding"
max_input_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "cohere"
providers = ["cohere"]

[models."embed-english-v2.0".pricing."cohere"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."embed-english-v3.0"]
mode = "embedding"
max_input_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "cohere"
providers = ["cohere"]
input_cost_per_image = 0.0001
supports_embedding_image_input = true
supports_image_input = true

[models."embed-english-v3.0".metadata]
notes = "'supports_image_input' is a deprecated field. Use 'supports_embedding_image_input' instead."

[models."embed-english-v3.0".pricing."cohere"]
input_cost_per_image = 0.0001
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."embed-multilingual-light-v3.0"]
mode = "embedding"
max_input_tokens = 1024
max_tokens = 1024
input_cost_per_token = 0.0001
output_cost_per_token = 0
litellm_provider = "cohere"
providers = ["cohere"]
supports_embedding_image_input = true

[models."embed-multilingual-light-v3.0".pricing."cohere"]
input_cost_per_token = 0.0001
output_cost_per_token = 0

[models."embed-multilingual-v2.0"]
mode = "embedding"
max_input_tokens = 768
max_tokens = 768
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "cohere"
providers = ["cohere"]

[models."embed-multilingual-v2.0".pricing."cohere"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."embed-multilingual-v3.0"]
mode = "embedding"
max_input_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "cohere"
providers = ["cohere"]
supports_embedding_image_input = true

[models."embed-multilingual-v3.0".pricing."cohere"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."embed-v-4-0"]
mode = "embedding"
max_input_tokens = 128000
max_tokens = 128000
input_cost_per_token = 1.2e-7
output_cost_per_token = 0
litellm_provider = "azure_ai"
providers = ["azure_ai"]
supported_modalities = ["text", "image"]
source = "https://azuremarketplace.microsoft.com/pt-br/marketplace/apps/cohere.cohere-embed-4-offer?tab=PlansAndPrice"
output_vector_size = 3072
supported_endpoints = ["/v1/embeddings"]
supports_embedding_image_input = true

[models."embed-v-4-0".pricing."azure_ai"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 0

[models."embed-v4.0"]
display_name = "Embed v4.0"
model_family = "cohere-embed"
mode = "embedding"
max_input_tokens = 128000
max_output_tokens = 0
max_tokens = 128000
input_cost_per_token = 1.2e-7
output_cost_per_token = 0
litellm_provider = "cohere"
providers = ["cohere", "vercel", "vercel_ai_gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
output_vector_size = 1536
supports_embedding_image_input = true

[models."embed-v4.0".pricing."cohere"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 0
[models."embed-v4.0".pricing."vercel"]
input_cost_per_token = 1.2e-7
[models."embed-v4.0".pricing."vercel_ai_gateway"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 0

[models."enhanced"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00024167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."enhanced".metadata]
calculation = "$0.0145/60 seconds = $0.00024167 per second"
original_pricing_per_minute = 0.0145

[models."enhanced".pricing."deepgram"]
input_cost_per_second = 0.00024167
output_cost_per_second = 0

[models."enhanced-finance"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00024167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."enhanced-finance".metadata]
calculation = "$0.0145/60 seconds = $0.00024167 per second"
original_pricing_per_minute = 0.0145

[models."enhanced-finance".pricing."deepgram"]
input_cost_per_second = 0.00024167
output_cost_per_second = 0

[models."enhanced-general"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00024167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."enhanced-general".metadata]
calculation = "$0.0145/60 seconds = $0.00024167 per second"
original_pricing_per_minute = 0.0145

[models."enhanced-general".pricing."deepgram"]
input_cost_per_second = 0.00024167
output_cost_per_second = 0

[models."enhanced-meeting"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00024167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."enhanced-meeting".metadata]
calculation = "$0.0145/60 seconds = $0.00024167 per second"
original_pricing_per_minute = 0.0145

[models."enhanced-meeting".pricing."deepgram"]
input_cost_per_second = 0.00024167
output_cost_per_second = 0

[models."enhanced-phonecall"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00024167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."enhanced-phonecall".metadata]
calculation = "$0.0145/60 seconds = $0.00024167 per second"
original_pricing_per_minute = 0.0145

[models."enhanced-phonecall".pricing."deepgram"]
input_cost_per_second = 0.00024167
output_cost_per_second = 0

[models."erase"]
mode = "image_edit"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.005
supported_endpoints = ["/v1/images/edits"]

[models."erase".pricing."stability"]
output_cost_per_image = 0.005

[models."ernie-4-5-21b-a3b-thinking"]
display_name = "Baidu Ernie 4.5 21B A3B Thinking"
model_family = "ernie"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8000
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
litellm_provider = "helicone"
providers = ["helicone"]
supports_function_calling = false
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-03"
release_date = "2025-03-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."ernie-4-5-21b-a3b-thinking".pricing."helicone"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7

[models."essentialai/Rnj-1-Instruct"]
display_name = "Rnj-1 Instruct"
model_family = "rnj"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "togetherai"
providers = ["togetherai", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-12-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."essentialai/Rnj-1-Instruct".pricing."kilo"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."essentialai/Rnj-1-Instruct".pricing."togetherai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7

[models."eu-central-1/1-month-commitment/anthropic.claude-instant-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.01635
output_cost_per_second = 0.01635
supports_tool_choice = true

[models."eu-central-1/1-month-commitment/anthropic.claude-instant-v1".pricing."bedrock"]
input_cost_per_second = 0.01635
output_cost_per_second = 0.01635

[models."eu-central-1/1-month-commitment/anthropic.claude-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.0415
output_cost_per_second = 0.0415

[models."eu-central-1/1-month-commitment/anthropic.claude-v1".pricing."bedrock"]
input_cost_per_second = 0.0415
output_cost_per_second = 0.0415

[models."eu-central-1/1-month-commitment/anthropic.claude-v2:1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.0415
output_cost_per_second = 0.0415
supports_tool_choice = true

[models."eu-central-1/1-month-commitment/anthropic.claude-v2:1".pricing."bedrock"]
input_cost_per_second = 0.0415
output_cost_per_second = 0.0415

[models."eu-central-1/6-month-commitment/anthropic.claude-instant-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.009083
output_cost_per_second = 0.009083
supports_tool_choice = true

[models."eu-central-1/6-month-commitment/anthropic.claude-instant-v1".pricing."bedrock"]
input_cost_per_second = 0.009083
output_cost_per_second = 0.009083

[models."eu-central-1/6-month-commitment/anthropic.claude-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.02305
output_cost_per_second = 0.02305

[models."eu-central-1/6-month-commitment/anthropic.claude-v1".pricing."bedrock"]
input_cost_per_second = 0.02305
output_cost_per_second = 0.02305

[models."eu-central-1/6-month-commitment/anthropic.claude-v2:1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_second = 0.02305
output_cost_per_second = 0.02305
supports_tool_choice = true

[models."eu-central-1/6-month-commitment/anthropic.claude-v2:1".pricing."bedrock"]
input_cost_per_second = 0.02305
output_cost_per_second = 0.02305

[models."eu-central-1/anthropic.claude-instant-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.00000248
output_cost_per_token = 0.00000838
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_tool_choice = true

[models."eu-central-1/anthropic.claude-instant-v1".pricing."bedrock"]
input_cost_per_token = 0.00000248
output_cost_per_token = 0.00000838

[models."eu-central-1/anthropic.claude-v1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."eu-central-1/anthropic.claude-v1".pricing."bedrock"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024

[models."eu-central-1/anthropic.claude-v2:1"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_tool_choice = true

[models."eu-central-1/anthropic.claude-v2:1".pricing."bedrock"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024

[models."eu-central-1/minimax.minimax-m2.1"]
mode = "chat"
max_input_tokens = 196000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.00000144
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."eu-central-1/minimax.minimax-m2.1".pricing."bedrock"]
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.00000144

[models."eu-central-1/qwen.qwen3-coder-next"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-7
output_cost_per_token = 0.00000144
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."eu-central-1/qwen.qwen3-coder-next".pricing."bedrock"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.00000144

[models."eu-north-1/deepseek.v3.2"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 7.4e-7
output_cost_per_token = 0.00000222
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_reasoning = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_tool_choice = true

[models."eu-north-1/deepseek.v3.2".pricing."bedrock"]
input_cost_per_token = 7.4e-7
output_cost_per_token = 0.00000222

[models."eu-north-1/minimax.minimax-m2.1"]
mode = "chat"
max_input_tokens = 196000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.00000144
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."eu-north-1/minimax.minimax-m2.1".pricing."bedrock"]
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.00000144

[models."eu-north-1/moonshotai.kimi-k2.5"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 7.2e-7
output_cost_per_token = 0.0000036
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."eu-north-1/moonshotai.kimi-k2.5".pricing."bedrock"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 0.0000036

[models."eu-south-1/minimax.minimax-m2.1"]
mode = "chat"
max_input_tokens = 196000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.00000144
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."eu-south-1/minimax.minimax-m2.1".pricing."bedrock"]
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.00000144

[models."eu-south-1/qwen.qwen3-coder-next"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-7
output_cost_per_token = 0.00000144
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."eu-south-1/qwen.qwen3-coder-next".pricing."bedrock"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.00000144

[models."eu-west-2/meta.llama3-70b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000345
output_cost_per_token = 0.00000455
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."eu-west-2/meta.llama3-70b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 0.00000345
output_cost_per_token = 0.00000455

[models."eu-west-2/meta.llama3-8b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3.9e-7
output_cost_per_token = 7.8e-7
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."eu-west-2/meta.llama3-8b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 3.9e-7
output_cost_per_token = 7.8e-7

[models."eu-west-2/minimax.minimax-m2.1"]
mode = "chat"
max_input_tokens = 196000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 4.7e-7
output_cost_per_token = 0.00000186
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."eu-west-2/minimax.minimax-m2.1".pricing."bedrock"]
input_cost_per_token = 4.7e-7
output_cost_per_token = 0.00000186

[models."eu-west-2/qwen.qwen3-coder-next"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 7.8e-7
output_cost_per_token = 0.00000186
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."eu-west-2/qwen.qwen3-coder-next".pricing."bedrock"]
input_cost_per_token = 7.8e-7
output_cost_per_token = 0.00000186

[models."eu-west-3/mistral.mistral-7b-instruct-v0:2"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 2e-7
output_cost_per_token = 2.6e-7
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_tool_choice = true

[models."eu-west-3/mistral.mistral-7b-instruct-v0:2".pricing."bedrock"]
input_cost_per_token = 2e-7
output_cost_per_token = 2.6e-7

[models."eu-west-3/mistral.mistral-large-2402-v1:0"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.0000104
output_cost_per_token = 0.0000312
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true

[models."eu-west-3/mistral.mistral-large-2402-v1:0".pricing."bedrock"]
input_cost_per_token = 0.0000104
output_cost_per_token = 0.0000312

[models."eu-west-3/mistral.mixtral-8x7b-instruct-v0:1"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 5.9e-7
output_cost_per_token = 9.1e-7
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_tool_choice = true

[models."eu-west-3/mistral.mixtral-8x7b-instruct-v0:1".pricing."bedrock"]
input_cost_per_token = 5.9e-7
output_cost_per_token = 9.1e-7

[models."eu.amazon.nova-2-lite-v1:0"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 3.3e-7
output_cost_per_token = 0.00000275
cache_read_input_token_cost = 8.25e-8
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_response_schema = true
supports_video_input = true

[models."eu.amazon.nova-2-lite-v1:0".pricing."bedrock_converse"]
cache_read_input_token_cost = 8.25e-8
input_cost_per_token = 3.3e-7
output_cost_per_token = 0.00000275

[models."eu.amazon.nova-2-pro-preview-20251202-v1:0"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000021875
output_cost_per_token = 0.0000175
cache_read_input_token_cost = 5.46875e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
input_cost_per_audio_token = 0.0000021875
input_cost_per_image_token = 0.0000021875
supports_response_schema = true
supports_video_input = true

[models."eu.amazon.nova-2-pro-preview-20251202-v1:0".pricing."bedrock_converse"]
cache_read_input_token_cost = 5.46875e-7
input_cost_per_audio_token = 0.0000021875
input_cost_per_image_token = 0.0000021875
input_cost_per_token = 0.0000021875
output_cost_per_token = 0.0000175

[models."eu.amazon.nova-lite-v1:0"]
mode = "chat"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 7.8e-8
output_cost_per_token = 3.12e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supports_response_schema = true

[models."eu.amazon.nova-lite-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 7.8e-8
output_cost_per_token = 3.12e-7

[models."eu.amazon.nova-micro-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 4.6e-8
output_cost_per_token = 1.84e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true

[models."eu.amazon.nova-micro-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 4.6e-8
output_cost_per_token = 1.84e-7

[models."eu.amazon.nova-pro-v1:0"]
mode = "chat"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 0.00000105
output_cost_per_token = 0.0000042
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_response_schema = true

[models."eu.amazon.nova-pro-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 0.00000105
output_cost_per_token = 0.0000042

[models."eu.anthropic.claude-3-5-haiku-20241022-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_prompt_caching = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."eu.anthropic.claude-3-5-haiku-20241022-v1:0".pricing."bedrock"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125

[models."eu.anthropic.claude-3-5-sonnet-20240620-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true

[models."eu.anthropic.claude-3-5-sonnet-20240620-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."eu.anthropic.claude-3-5-sonnet-20241022-v2:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true

[models."eu.anthropic.claude-3-5-sonnet-20241022-v2:0".pricing."bedrock"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."eu.anthropic.claude-3-7-sonnet-20250219-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true

[models."eu.anthropic.claude-3-7-sonnet-20250219-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."eu.anthropic.claude-3-haiku-20240307-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true

[models."eu.anthropic.claude-3-haiku-20240307-v1:0".pricing."bedrock"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125

[models."eu.anthropic.claude-3-opus-20240229-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_response_schema = true
supports_tool_choice = true

[models."eu.anthropic.claude-3-opus-20240229-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."eu.anthropic.claude-3-sonnet-20240229-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true

[models."eu.anthropic.claude-3-sonnet-20240229-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."eu.anthropic.claude-haiku-4-5-20251001-v1:0"]
display_name = "Claude Haiku 4.5 (EU)"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000055
cache_read_input_token_cost = 1.1e-7
cache_creation_input_token_cost = 0.000001375
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-02-28"
release_date = "2025-10-15"
deprecation_date = "2026-10-15"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "https://aws.amazon.com/about-aws/whats-new/2025/10/claude-4-5-haiku-anthropic-amazon-bedrock"
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."eu.anthropic.claude-haiku-4-5-20251001-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."eu.anthropic.claude-haiku-4-5-20251001-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.000001375
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000055

[models."eu.anthropic.claude-opus-4-1-20250805-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
cache_creation_input_token_cost = 0.00001875
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."eu.anthropic.claude-opus-4-1-20250805-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."eu.anthropic.claude-opus-4-1-20250805-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."eu.anthropic.claude-opus-4-20250514-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
cache_creation_input_token_cost = 0.00001875
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."eu.anthropic.claude-opus-4-20250514-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."eu.anthropic.claude-opus-4-20250514-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."eu.anthropic.claude-opus-4-5-20251101-v1:0"]
display_name = "Claude Opus 4.5 (EU)"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 5e-7
cache_creation_input_token_cost = 0.00000625
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-11-24"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."eu.anthropic.claude-opus-4-5-20251101-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."eu.anthropic.claude-opus-4-5-20251101-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."eu.anthropic.claude-opus-4-5-20251101-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000625
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025

[models."eu.anthropic.claude-opus-4-6-v1"]
display_name = "Claude Opus 4.6 (EU)"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.0000055
output_cost_per_token = 0.0000275
cache_read_input_token_cost = 5.5e-7
cache_creation_input_token_cost = 0.000006875
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-05"
release_date = "2026-02-05"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.00001375
cache_read_input_token_cost_above_200k_tokens = 0.0000011
input_cost_per_token_above_200k_tokens = 0.000011
output_cost_per_token_above_200k_tokens = 0.00004125
supports_assistant_prefill = false
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."eu.anthropic.claude-opus-4-6-v1".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."eu.anthropic.claude-opus-4-6-v1".pricing."amazon-bedrock"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."eu.anthropic.claude-opus-4-6-v1".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.000006875
cache_creation_input_token_cost_above_200k_tokens = 0.00001375
cache_read_input_token_cost = 5.5e-7
cache_read_input_token_cost_above_200k_tokens = 0.0000011
input_cost_per_token = 0.0000055
input_cost_per_token_above_200k_tokens = 0.000011
output_cost_per_token = 0.0000275
output_cost_per_token_above_200k_tokens = 0.00004125

[models."eu.anthropic.claude-sonnet-4-20250514-v1:0"]
display_name = "Claude Sonnet 4 (EU)"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-05-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."eu.anthropic.claude-sonnet-4-20250514-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."eu.anthropic.claude-sonnet-4-20250514-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."eu.anthropic.claude-sonnet-4-20250514-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225

[models."eu.anthropic.claude-sonnet-4-5-20250929-v1:0"]
display_name = "Claude Sonnet 4.5 (EU)"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000033
output_cost_per_token = 0.0000165
cache_read_input_token_cost = 3.3e-7
cache_creation_input_token_cost = 0.000004125
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-07-31"
release_date = "2025-09-29"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token_above_200k_tokens = 0.0000066
output_cost_per_token_above_200k_tokens = 0.00002475
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."eu.anthropic.claude-sonnet-4-5-20250929-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."eu.anthropic.claude-sonnet-4-5-20250929-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."eu.anthropic.claude-sonnet-4-5-20250929-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.000004125
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost = 3.3e-7
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token = 0.0000033
input_cost_per_token_above_200k_tokens = 0.0000066
output_cost_per_token = 0.0000165
output_cost_per_token_above_200k_tokens = 0.00002475

[models."eu.anthropic.claude-sonnet-4-6"]
display_name = "Claude Sonnet 4.6 (EU)"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000033
output_cost_per_token = 0.0000165
cache_read_input_token_cost = 3.3e-7
cache_creation_input_token_cost = 0.000004125
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-08"
release_date = "2026-02-17"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token_above_200k_tokens = 0.0000066
output_cost_per_token_above_200k_tokens = 0.00002475
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."eu.anthropic.claude-sonnet-4-6".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."eu.anthropic.claude-sonnet-4-6".pricing."amazon-bedrock"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."eu.anthropic.claude-sonnet-4-6".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.000004125
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost = 3.3e-7
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token = 0.0000033
input_cost_per_token_above_200k_tokens = 0.0000066
output_cost_per_token = 0.0000165
output_cost_per_token_above_200k_tokens = 0.00002475

[models."eu.deepseek.v3.2"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 7.4e-7
output_cost_per_token = 0.00000222
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."eu.deepseek.v3.2".pricing."bedrock_converse"]
input_cost_per_token = 7.4e-7
output_cost_per_token = 0.00000222

[models."eu.meta.llama3-2-1b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.3e-7
output_cost_per_token = 1.3e-7
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_tool_choice = false

[models."eu.meta.llama3-2-1b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 1.3e-7

[models."eu.meta.llama3-2-3b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.9e-7
output_cost_per_token = 1.9e-7
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_tool_choice = false

[models."eu.meta.llama3-2-3b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 1.9e-7
output_cost_per_token = 1.9e-7

[models."eu.mistral.pixtral-large-2502-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_tool_choice = false

[models."eu.mistral.pixtral-large-2502-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006

[models."eu.twelvelabs.marengo-embed-2-7-v1:0"]
mode = "embedding"
max_input_tokens = 77
max_tokens = 77
input_cost_per_token = 0.00007
output_cost_per_token = 0
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_audio_per_second = 0.00014
input_cost_per_image = 0.0001
input_cost_per_video_per_second = 0.0007
output_vector_size = 1024
supports_embedding_image_input = true
supports_image_input = true

[models."eu.twelvelabs.marengo-embed-2-7-v1:0".pricing."bedrock"]
input_cost_per_audio_per_second = 0.00014
input_cost_per_image = 0.0001
input_cost_per_token = 0.00007
input_cost_per_video_per_second = 0.0007
output_cost_per_token = 0

[models."eu.twelvelabs.pegasus-1-2-v1:0"]
mode = "chat"
output_cost_per_token = 0.0000075
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_video_per_second = 0.00049
supports_video_input = true

[models."eu.twelvelabs.pegasus-1-2-v1:0".pricing."bedrock"]
input_cost_per_video_per_second = 0.00049
output_cost_per_token = 0.0000075

[models."fal-ai/bytedance/dreamina/v3.1/text-to-image"]
mode = "image_generation"
litellm_provider = "fal_ai"
providers = ["fal_ai"]
output_cost_per_image = 0.03
supported_endpoints = ["/v1/images/generations"]

[models."fal-ai/bytedance/dreamina/v3.1/text-to-image".pricing."fal_ai"]
output_cost_per_image = 0.03

[models."fal-ai/bytedance/seedream/v3/text-to-image"]
mode = "image_generation"
litellm_provider = "fal_ai"
providers = ["fal_ai"]
output_cost_per_image = 0.03
supported_endpoints = ["/v1/images/generations"]

[models."fal-ai/bytedance/seedream/v3/text-to-image".pricing."fal_ai"]
output_cost_per_image = 0.03

[models."fal-ai/flux-pro/v1.1"]
mode = "image_generation"
litellm_provider = "fal_ai"
providers = ["fal_ai"]
output_cost_per_image = 0.04
supported_endpoints = ["/v1/images/generations"]

[models."fal-ai/flux-pro/v1.1".pricing."fal_ai"]
output_cost_per_image = 0.04

[models."fal-ai/flux-pro/v1.1-ultra"]
mode = "image_generation"
litellm_provider = "fal_ai"
providers = ["fal_ai"]
output_cost_per_image = 0.06
supported_endpoints = ["/v1/images/generations"]

[models."fal-ai/flux-pro/v1.1-ultra".pricing."fal_ai"]
output_cost_per_image = 0.06

[models."fal-ai/flux/schnell"]
mode = "image_generation"
litellm_provider = "fal_ai"
providers = ["fal_ai"]
output_cost_per_image = 0.003
supported_endpoints = ["/v1/images/generations"]

[models."fal-ai/flux/schnell".pricing."fal_ai"]
output_cost_per_image = 0.003

[models."fal-ai/ideogram/v3"]
mode = "image_generation"
litellm_provider = "fal_ai"
providers = ["fal_ai"]
output_cost_per_image = 0.06
supported_endpoints = ["/v1/images/generations"]

[models."fal-ai/ideogram/v3".pricing."fal_ai"]
output_cost_per_image = 0.06

[models."fal-ai/imagen4/preview"]
mode = "image_generation"
litellm_provider = "fal_ai"
providers = ["fal_ai"]
output_cost_per_image = 0.0398
supported_endpoints = ["/v1/images/generations"]

[models."fal-ai/imagen4/preview".pricing."fal_ai"]
output_cost_per_image = 0.0398

[models."fal-ai/imagen4/preview/fast"]
mode = "image_generation"
litellm_provider = "fal_ai"
providers = ["fal_ai"]
output_cost_per_image = 0.02
supported_endpoints = ["/v1/images/generations"]

[models."fal-ai/imagen4/preview/fast".pricing."fal_ai"]
output_cost_per_image = 0.02

[models."fal-ai/imagen4/preview/ultra"]
mode = "image_generation"
litellm_provider = "fal_ai"
providers = ["fal_ai"]
output_cost_per_image = 0.06
supported_endpoints = ["/v1/images/generations"]

[models."fal-ai/imagen4/preview/ultra".pricing."fal_ai"]
output_cost_per_image = 0.06

[models."fal-ai/recraft/v3/text-to-image"]
mode = "image_generation"
litellm_provider = "fal_ai"
providers = ["fal_ai"]
output_cost_per_image = 0.0398
supported_endpoints = ["/v1/images/generations"]

[models."fal-ai/recraft/v3/text-to-image".pricing."fal_ai"]
output_cost_per_image = 0.0398

[models."fal-ai/stable-diffusion-v35-medium"]
mode = "image_generation"
litellm_provider = "fal_ai"
providers = ["fal_ai"]
output_cost_per_image = 0.0398
supported_endpoints = ["/v1/images/generations"]

[models."fal-ai/stable-diffusion-v35-medium".pricing."fal_ai"]
output_cost_per_image = 0.0398

[models."fast"]
mode = "image_edit"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.002
supported_endpoints = ["/v1/images/edits"]

[models."fast".pricing."stability"]
output_cost_per_image = 0.002

[models."featherless-ai/Qwerky-72B"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
litellm_provider = "featherless_ai"
providers = ["featherless_ai"]

[models."featherless-ai/Qwerky-QwQ-32B"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
litellm_provider = "featherless_ai"
providers = ["featherless_ai"]

[models."featherless/qwerky-72b"]
display_name = "Qwerky 72B"
model_family = "qwerky"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-03-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."fireworks-ai-4.1b-to-16b"]
mode = "chat"
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."fireworks-ai-4.1b-to-16b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."fireworks-ai-56b-to-176b"]
mode = "chat"
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."fireworks-ai-56b-to-176b".pricing."fireworks_ai"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."fireworks-ai-above-16b"]
mode = "chat"
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."fireworks-ai-above-16b".pricing."fireworks_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."fireworks-ai-default"]
mode = "chat"
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."fireworks-ai-default".pricing."fireworks_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."fireworks-ai-embedding-150m-to-350m"]
mode = "chat"
input_cost_per_token = 1.6e-8
output_cost_per_token = 0
litellm_provider = "fireworks_ai-embedding-models"
providers = ["fireworks_ai-embedding-models"]

[models."fireworks-ai-embedding-150m-to-350m".pricing."fireworks_ai-embedding-models"]
input_cost_per_token = 1.6e-8
output_cost_per_token = 0

[models."fireworks-ai-embedding-up-to-150m"]
mode = "chat"
input_cost_per_token = 8e-9
output_cost_per_token = 0
litellm_provider = "fireworks_ai-embedding-models"
providers = ["fireworks_ai-embedding-models"]

[models."fireworks-ai-embedding-up-to-150m".pricing."fireworks_ai-embedding-models"]
input_cost_per_token = 8e-9
output_cost_per_token = 0

[models."fireworks-ai-moe-up-to-56b"]
mode = "chat"
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."fireworks-ai-moe-up-to-56b".pricing."fireworks_ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 5e-7

[models."fireworks-ai-up-to-4b"]
mode = "chat"
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]

[models."fireworks-ai-up-to-4b".pricing."fireworks_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."flan-t5-xl-3b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-7
output_cost_per_token = 6e-7
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = false
supports_vision = false
supports_parallel_function_calling = false

[models."flan-t5-xl-3b".pricing."watsonx"]
input_cost_per_token = 6e-7
output_cost_per_token = 6e-7

[models."flux-pro"]
mode = "image_generation"
litellm_provider = "aiml"
providers = ["aiml"]
source = "https://docs.aimlapi.com/"
output_cost_per_image = 0.053
supported_endpoints = ["/v1/images/generations"]

[models."flux-pro".metadata]
notes = "Flux Dev - Development version optimized for experimentation"

[models."flux-pro".pricing."aiml"]
output_cost_per_image = 0.053

[models."flux-pro/v1.1"]
mode = "image_generation"
litellm_provider = "aiml"
providers = ["aiml"]
output_cost_per_image = 0.042
supported_endpoints = ["/v1/images/generations"]

[models."flux-pro/v1.1".pricing."aiml"]
output_cost_per_image = 0.042

[models."flux-pro/v1.1-ultra"]
mode = "image_generation"
litellm_provider = "aiml"
providers = ["aiml"]
output_cost_per_image = 0.063
supported_endpoints = ["/v1/images/generations"]

[models."flux-pro/v1.1-ultra".pricing."aiml"]
output_cost_per_image = 0.063

[models."flux-realism"]
mode = "image_generation"
litellm_provider = "aiml"
providers = ["aiml"]
source = "https://docs.aimlapi.com/"
output_cost_per_image = 0.037
supported_endpoints = ["/v1/images/generations"]

[models."flux-realism".metadata]
notes = "Flux Pro - Professional-grade image generation model"

[models."flux-realism".pricing."aiml"]
output_cost_per_image = 0.037

[models."flux.2-pro"]
mode = "image_generation"
litellm_provider = "azure_ai"
providers = ["azure_ai"]
source = "https://ai.azure.com/explore/models/flux.2-pro/version/1/registry/azureml-blackforestlabs"
output_cost_per_image = 0.04
supported_endpoints = ["/v1/images/generations"]

[models."flux.2-pro".pricing."azure_ai"]
output_cost_per_image = 0.04

[models."flux/dev"]
mode = "image_generation"
litellm_provider = "aiml"
providers = ["aiml"]
source = "https://docs.aimlapi.com/"
output_cost_per_image = 0.026
supported_endpoints = ["/v1/images/generations"]

[models."flux/dev".metadata]
notes = "Flux Dev - Development version optimized for experimentation"

[models."flux/dev".pricing."aiml"]
output_cost_per_image = 0.026

[models."flux/kontext-max/text-to-image"]
mode = "image_generation"
litellm_provider = "aiml"
providers = ["aiml"]
source = "https://docs.aimlapi.com/"
output_cost_per_image = 0.084
supported_endpoints = ["/v1/images/generations"]

[models."flux/kontext-max/text-to-image".metadata]
notes = "Flux Pro v1.1 - Enhanced version with improved capabilities and 6x faster inference speed"

[models."flux/kontext-max/text-to-image".pricing."aiml"]
output_cost_per_image = 0.084

[models."flux/kontext-pro/text-to-image"]
mode = "image_generation"
litellm_provider = "aiml"
providers = ["aiml"]
source = "https://docs.aimlapi.com/"
output_cost_per_image = 0.042
supported_endpoints = ["/v1/images/generations"]

[models."flux/kontext-pro/text-to-image".metadata]
notes = "Flux Pro v1.1 - Enhanced version with improved capabilities and 6x faster inference speed"

[models."flux/kontext-pro/text-to-image".pricing."aiml"]
output_cost_per_image = 0.042

[models."flux/schnell"]
mode = "image_generation"
litellm_provider = "aiml"
providers = ["aiml"]
source = "https://docs.aimlapi.com/"
output_cost_per_image = 0.003
supported_endpoints = ["/v1/images/generations"]

[models."flux/schnell".metadata]
notes = "Flux Schnell - Fast generation model optimized for speed"

[models."flux/schnell".pricing."aiml"]
output_cost_per_image = 0.003

[models."ft:babbage-002"]
mode = "completion"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.0000016
output_cost_per_token = 0.0000016
litellm_provider = "text-completion-openai"
providers = ["text-completion-openai"]
input_cost_per_token_batches = 2e-7
output_cost_per_token_batches = 2e-7

[models."ft:babbage-002".pricing."text-completion-openai"]
input_cost_per_token = 0.0000016
input_cost_per_token_batches = 2e-7
output_cost_per_token = 0.0000016
output_cost_per_token_batches = 2e-7

[models."ft:davinci-002"]
mode = "completion"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000012
output_cost_per_token = 0.000012
litellm_provider = "text-completion-openai"
providers = ["text-completion-openai"]
input_cost_per_token_batches = 0.000001
output_cost_per_token_batches = 0.000001

[models."ft:davinci-002".pricing."text-completion-openai"]
input_cost_per_token = 0.000012
input_cost_per_token_batches = 0.000001
output_cost_per_token = 0.000012
output_cost_per_token_batches = 0.000001

[models."ft:gpt-3.5-turbo"]
mode = "chat"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000006
litellm_provider = "openai"
providers = ["openai"]
input_cost_per_token_batches = 0.0000015
output_cost_per_token_batches = 0.000003
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-3.5-turbo".pricing."openai"]
input_cost_per_token = 0.000003
input_cost_per_token_batches = 0.0000015
output_cost_per_token = 0.000006
output_cost_per_token_batches = 0.000003

[models."ft:gpt-3.5-turbo-0125"]
mode = "chat"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000006
litellm_provider = "openai"
providers = ["openai"]
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-3.5-turbo-0125".pricing."openai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000006

[models."ft:gpt-3.5-turbo-0613"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000006
litellm_provider = "openai"
providers = ["openai"]
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-3.5-turbo-0613".pricing."openai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000006

[models."ft:gpt-3.5-turbo-1106"]
mode = "chat"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000006
litellm_provider = "openai"
providers = ["openai"]
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-3.5-turbo-1106".pricing."openai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000006

[models."ft:gpt-4-0613"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00003
output_cost_per_token = 0.00006
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
source = "OpenAI needs to add pricing for this ft model, will be updated when added by OpenAI. Defaulting to base model pricing"
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-4-0613".pricing."openai"]
input_cost_per_token = 0.00003
output_cost_per_token = 0.00006

[models."ft:gpt-4.1-2025-04-14"]
mode = "chat"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000003
output_cost_per_token = 0.000012
cache_read_input_token_cost = 7.5e-7
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_prompt_caching = true
input_cost_per_token_batches = 0.0000015
output_cost_per_token_batches = 0.000006
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-4.1-2025-04-14".pricing."openai"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
input_cost_per_token_batches = 0.0000015
output_cost_per_token = 0.000012
output_cost_per_token_batches = 0.000006

[models."ft:gpt-4.1-mini-2025-04-14"]
mode = "chat"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000032
cache_read_input_token_cost = 2e-7
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_prompt_caching = true
input_cost_per_token_batches = 4e-7
output_cost_per_token_batches = 0.0000016
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-4.1-mini-2025-04-14".pricing."openai"]
cache_read_input_token_cost = 2e-7
input_cost_per_token = 8e-7
input_cost_per_token_batches = 4e-7
output_cost_per_token = 0.0000032
output_cost_per_token_batches = 0.0000016

[models."ft:gpt-4.1-nano-2025-04-14"]
mode = "chat"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 8e-7
cache_read_input_token_cost = 5e-8
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_prompt_caching = true
input_cost_per_token_batches = 1e-7
output_cost_per_token_batches = 4e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-4.1-nano-2025-04-14".pricing."openai"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
input_cost_per_token_batches = 1e-7
output_cost_per_token = 8e-7
output_cost_per_token_batches = 4e-7

[models."ft:gpt-4o-2024-08-06"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.00000375
output_cost_per_token = 0.000015
cache_read_input_token_cost = 0.000001875
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
input_cost_per_token_batches = 0.000001875
output_cost_per_token_batches = 0.0000075
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-4o-2024-08-06".pricing."openai"]
cache_read_input_token_cost = 0.000001875
input_cost_per_token = 0.00000375
input_cost_per_token_batches = 0.000001875
output_cost_per_token = 0.000015
output_cost_per_token_batches = 0.0000075

[models."ft:gpt-4o-2024-11-20"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.00000375
output_cost_per_token = 0.000015
cache_creation_input_token_cost = 0.000001875
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_prompt_caching = true
supports_pdf_input = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-4o-2024-11-20".pricing."openai"]
cache_creation_input_token_cost = 0.000001875
input_cost_per_token = 0.00000375
output_cost_per_token = 0.000015

[models."ft:gpt-4o-mini-2024-07-18"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
cache_read_input_token_cost = 1.5e-7
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_prompt_caching = true
supports_pdf_input = true
input_cost_per_token_batches = 1.5e-7
output_cost_per_token_batches = 6e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."ft:gpt-4o-mini-2024-07-18".pricing."openai"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 3e-7
input_cost_per_token_batches = 1.5e-7
output_cost_per_token = 0.0000012
output_cost_per_token_batches = 6e-7

[models."ft:o4-mini-2025-04-16"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.000004
output_cost_per_token = 0.000016
cache_read_input_token_cost = 0.000001
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
input_cost_per_token_batches = 0.000002
output_cost_per_token_batches = 0.000008
supports_parallel_function_calling = false
supports_response_schema = true
supports_tool_choice = true

[models."ft:o4-mini-2025-04-16".pricing."openai"]
cache_read_input_token_cost = 0.000001
input_cost_per_token = 0.000004
input_cost_per_token_batches = 0.000002
output_cost_per_token = 0.000016
output_cost_per_token_batches = 0.000008

[models."gemini-1.0-pro"]
mode = "chat"
max_input_tokens = 32760
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models"]
supports_function_calling = true
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#google_models"
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_video_per_second = 0.002
output_cost_per_character = 3.75e-7
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.0-pro".pricing."vertex_ai-language-models"]
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015

[models."gemini-1.0-pro-001"]
mode = "chat"
max_input_tokens = 32760
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models"]
supports_function_calling = true
deprecation_date = "2025-04-09"
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_video_per_second = 0.002
output_cost_per_character = 3.75e-7
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.0-pro-001".pricing."vertex_ai-language-models"]
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015

[models."gemini-1.0-pro-002"]
mode = "chat"
max_input_tokens = 32760
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models"]
supports_function_calling = true
deprecation_date = "2025-04-09"
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_video_per_second = 0.002
output_cost_per_character = 3.75e-7
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.0-pro-002".pricing."vertex_ai-language-models"]
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015

[models."gemini-1.0-pro-vision"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "vertex_ai-vision-models"
providers = ["vertex_ai-vision-models"]
supports_function_calling = true
supports_vision = true
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_image = 0.0025
max_images_per_prompt = 16
max_video_length = 2
max_videos_per_prompt = 1
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.0-pro-vision".pricing."vertex_ai-vision-models"]
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."gemini-1.0-pro-vision-001"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "vertex_ai-vision-models"
providers = ["vertex_ai-vision-models"]
supports_function_calling = true
supports_vision = true
deprecation_date = "2025-04-09"
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_image = 0.0025
max_images_per_prompt = 16
max_video_length = 2
max_videos_per_prompt = 1
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.0-pro-vision-001".pricing."vertex_ai-vision-models"]
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."gemini-1.0-ultra"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models"]
supports_function_calling = true
source = "As of Jun, 2024. There is no available doc on vertex ai pricing gemini-1.0-ultra-001. Using gemini-1.0-pro pricing. Got max_tokens info here: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_video_per_second = 0.002
output_cost_per_character = 3.75e-7
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.0-ultra".pricing."vertex_ai-language-models"]
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015

[models."gemini-1.0-ultra-001"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models"]
supports_function_calling = true
source = "As of Jun, 2024. There is no available doc on vertex ai pricing gemini-1.0-ultra-001. Using gemini-1.0-pro pricing. Got max_tokens info here: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_video_per_second = 0.002
output_cost_per_character = 3.75e-7
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-1.0-ultra-001".pricing."vertex_ai-language-models"]
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015

[models."gemini-1.5-flash"]
display_name = "Gemini 1.5 Flash"
model_family = "gemini-flash"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini", "google"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-05-14"
deprecation_date = "2025-09-29"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_character = 7.5e-8
output_cost_per_character_above_128k_tokens = 1.5e-7
output_cost_per_token_above_128k_tokens = 6e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gemini-1.5-flash".pricing."gemini"]
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 1.5e-7
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
[models."gemini-1.5-flash".pricing."google"]
cache_read_input_token_cost = 1.875e-8
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."gemini-1.5-flash".pricing."vertex_ai-language-models"]
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
output_cost_per_character = 7.5e-8
output_cost_per_character_above_128k_tokens = 1.5e-7
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7

[models."gemini-1.5-flash-001"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
deprecation_date = "2025-05-24"
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_character = 7.5e-8
output_cost_per_character_above_128k_tokens = 1.5e-7
output_cost_per_token_above_128k_tokens = 6e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gemini-1.5-flash-001".pricing."gemini"]
cache_creation_input_token_cost = 0.000001
cache_read_input_token_cost = 1.875e-8
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 1.5e-7
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
[models."gemini-1.5-flash-001".pricing."vertex_ai-language-models"]
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
output_cost_per_character = 7.5e-8
output_cost_per_character_above_128k_tokens = 1.5e-7
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7

[models."gemini-1.5-flash-002"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
deprecation_date = "2025-09-24"
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-1.5-flash"
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_character = 7.5e-8
output_cost_per_character_above_128k_tokens = 1.5e-7
output_cost_per_token_above_128k_tokens = 6e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gemini-1.5-flash-002".pricing."gemini"]
cache_creation_input_token_cost = 0.000001
cache_read_input_token_cost = 1.875e-8
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 1.5e-7
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7
[models."gemini-1.5-flash-002".pricing."vertex_ai-language-models"]
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
output_cost_per_character = 7.5e-8
output_cost_per_character_above_128k_tokens = 1.5e-7
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7

[models."gemini-1.5-flash-8b"]
display_name = "Gemini 1.5 Flash-8B"
model_family = "gemini-flash"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "gemini"
providers = ["gemini", "google"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-10-03"
deprecation_date = "2025-09-29"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://ai.google.dev/pricing"
input_cost_per_token_above_128k_tokens = 0
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_token_above_128k_tokens = 0
rpm = 4000
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
tpm = 4000000

[models."gemini-1.5-flash-8b".pricing."gemini"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
[models."gemini-1.5-flash-8b".pricing."google"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 3.75e-8
output_cost_per_token = 1.5e-7

[models."gemini-1.5-flash-8b-exp-0827"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "gemini"
providers = ["gemini"]
supports_function_calling = true
supports_vision = true
deprecation_date = "2025-09-29"
source = "https://ai.google.dev/pricing"
input_cost_per_token_above_128k_tokens = 0
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_token_above_128k_tokens = 0
rpm = 4000
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
tpm = 4000000

[models."gemini-1.5-flash-8b-exp-0827".pricing."gemini"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0

[models."gemini-1.5-flash-8b-exp-0924"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "gemini"
providers = ["gemini"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
deprecation_date = "2025-09-29"
source = "https://ai.google.dev/pricing"
input_cost_per_token_above_128k_tokens = 0
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_token_above_128k_tokens = 0
rpm = 4000
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
tpm = 4000000

[models."gemini-1.5-flash-8b-exp-0924".pricing."gemini"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0

[models."gemini-1.5-flash-exp-0827"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 4.688e-9
output_cost_per_token = 4.6875e-9
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini"]
supports_function_calling = true
supports_vision = true
deprecation_date = "2025-09-29"
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_character = 1.875e-8
output_cost_per_character_above_128k_tokens = 3.75e-8
output_cost_per_token_above_128k_tokens = 9.375e-9
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gemini-1.5-flash-exp-0827".pricing."gemini"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
[models."gemini-1.5-flash-exp-0827".pricing."vertex_ai-language-models"]
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token = 4.688e-9
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
output_cost_per_character = 1.875e-8
output_cost_per_character_above_128k_tokens = 3.75e-8
output_cost_per_token = 4.6875e-9
output_cost_per_token_above_128k_tokens = 9.375e-9

[models."gemini-1.5-flash-latest"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
litellm_provider = "gemini"
providers = ["gemini"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
deprecation_date = "2025-09-29"
source = "https://ai.google.dev/pricing"
input_cost_per_token_above_128k_tokens = 1.5e-7
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_token_above_128k_tokens = 6e-7
rpm = 2000
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
tpm = 4000000

[models."gemini-1.5-flash-latest".pricing."gemini"]
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 1.5e-7
output_cost_per_token = 3e-7
output_cost_per_token_above_128k_tokens = 6e-7

[models."gemini-1.5-flash-preview-0514"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 7.5e-8
output_cost_per_token = 4.6875e-9
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models"]
supports_function_calling = true
supports_vision = true
deprecation_date = "2025-09-29"
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_character = 1.875e-8
output_cost_per_character_above_128k_tokens = 3.75e-8
output_cost_per_token_above_128k_tokens = 9.375e-9
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gemini-1.5-flash-preview-0514".pricing."vertex_ai-language-models"]
input_cost_per_audio_per_second = 0.000002
input_cost_per_audio_per_second_above_128k_tokens = 0.000004
input_cost_per_character = 1.875e-8
input_cost_per_character_above_128k_tokens = 2.5e-7
input_cost_per_image = 0.00002
input_cost_per_image_above_128k_tokens = 0.00004
input_cost_per_token = 7.5e-8
input_cost_per_token_above_128k_tokens = 0.000001
input_cost_per_video_per_second = 0.00002
input_cost_per_video_per_second_above_128k_tokens = 0.00004
output_cost_per_character = 1.875e-8
output_cost_per_character_above_128k_tokens = 3.75e-8
output_cost_per_token = 4.6875e-9
output_cost_per_token_above_128k_tokens = 9.375e-9

[models."gemini-1.5-pro"]
display_name = "Gemini 1.5 Pro"
model_family = "gemini-pro"
mode = "chat"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000125
output_cost_per_token = 0.000005
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini", "google"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-02-15"
deprecation_date = "2025-09-29"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token_above_128k_tokens = 0.0000025
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token_above_128k_tokens = 0.00001
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gemini-1.5-pro".pricing."gemini"]
input_cost_per_token = 0.0000035
input_cost_per_token_above_128k_tokens = 0.000007
output_cost_per_token = 0.0000105
output_cost_per_token_above_128k_tokens = 0.000021
[models."gemini-1.5-pro".pricing."google"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.000005
[models."gemini-1.5-pro".pricing."vertex_ai-language-models"]
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 0.00000125
input_cost_per_token_above_128k_tokens = 0.0000025
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 0.000005
output_cost_per_token_above_128k_tokens = 0.00001

[models."gemini-1.5-pro-001"]
mode = "chat"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000125
output_cost_per_token = 0.000005
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
deprecation_date = "2025-05-24"
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token_above_128k_tokens = 0.0000025
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token_above_128k_tokens = 0.00001
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gemini-1.5-pro-001".pricing."gemini"]
input_cost_per_token = 0.0000035
input_cost_per_token_above_128k_tokens = 0.000007
output_cost_per_token = 0.0000105
output_cost_per_token_above_128k_tokens = 0.000021
[models."gemini-1.5-pro-001".pricing."vertex_ai-language-models"]
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 0.00000125
input_cost_per_token_above_128k_tokens = 0.0000025
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 0.000005
output_cost_per_token_above_128k_tokens = 0.00001

[models."gemini-1.5-pro-002"]
mode = "chat"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000125
output_cost_per_token = 0.000005
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
deprecation_date = "2025-09-24"
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-1.5-pro"
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token_above_128k_tokens = 0.0000025
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token_above_128k_tokens = 0.00001
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gemini-1.5-pro-002".pricing."gemini"]
input_cost_per_token = 0.0000035
input_cost_per_token_above_128k_tokens = 0.000007
output_cost_per_token = 0.0000105
output_cost_per_token_above_128k_tokens = 0.000021
[models."gemini-1.5-pro-002".pricing."vertex_ai-language-models"]
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 0.00000125
input_cost_per_token_above_128k_tokens = 0.0000025
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 0.000005
output_cost_per_token_above_128k_tokens = 0.00001

[models."gemini-1.5-pro-exp-0801"]
mode = "chat"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.0000035
output_cost_per_token = 0.0000105
litellm_provider = "gemini"
providers = ["gemini"]
supports_function_calling = true
supports_vision = true
deprecation_date = "2025-09-29"
source = "https://ai.google.dev/pricing"
input_cost_per_token_above_128k_tokens = 0.000007
output_cost_per_token_above_128k_tokens = 0.000021
rpm = 1000
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
tpm = 4000000

[models."gemini-1.5-pro-exp-0801".pricing."gemini"]
input_cost_per_token = 0.0000035
input_cost_per_token_above_128k_tokens = 0.000007
output_cost_per_token = 0.0000105
output_cost_per_token_above_128k_tokens = 0.000021

[models."gemini-1.5-pro-exp-0827"]
mode = "chat"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "gemini"
providers = ["gemini"]
supports_function_calling = true
supports_vision = true
deprecation_date = "2025-09-29"
source = "https://ai.google.dev/pricing"
input_cost_per_token_above_128k_tokens = 0
output_cost_per_token_above_128k_tokens = 0
rpm = 1000
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
tpm = 4000000

[models."gemini-1.5-pro-exp-0827".pricing."gemini"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0

[models."gemini-1.5-pro-latest"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.0000035
output_cost_per_token = 0.00000105
litellm_provider = "gemini"
providers = ["gemini"]
supports_function_calling = true
supports_vision = true
deprecation_date = "2025-09-29"
source = "https://ai.google.dev/pricing"
input_cost_per_token_above_128k_tokens = 0.000007
output_cost_per_token_above_128k_tokens = 0.000021
rpm = 1000
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
tpm = 4000000

[models."gemini-1.5-pro-latest".pricing."gemini"]
input_cost_per_token = 0.0000035
input_cost_per_token_above_128k_tokens = 0.000007
output_cost_per_token = 0.00000105
output_cost_per_token_above_128k_tokens = 0.000021

[models."gemini-1.5-pro-preview-0215"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 7.8125e-8
output_cost_per_token = 3.125e-7
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models"]
supports_function_calling = true
deprecation_date = "2025-09-29"
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token_above_128k_tokens = 1.5625e-7
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token_above_128k_tokens = 6.25e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gemini-1.5-pro-preview-0215".pricing."vertex_ai-language-models"]
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 7.8125e-8
input_cost_per_token_above_128k_tokens = 1.5625e-7
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 3.125e-7
output_cost_per_token_above_128k_tokens = 6.25e-7

[models."gemini-1.5-pro-preview-0409"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 7.8125e-8
output_cost_per_token = 3.125e-7
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models"]
supports_function_calling = true
deprecation_date = "2025-09-29"
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token_above_128k_tokens = 1.5625e-7
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token_above_128k_tokens = 6.25e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."gemini-1.5-pro-preview-0409".pricing."vertex_ai-language-models"]
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 7.8125e-8
input_cost_per_token_above_128k_tokens = 1.5625e-7
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 3.125e-7
output_cost_per_token_above_128k_tokens = 6.25e-7

[models."gemini-1.5-pro-preview-0514"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 7.8125e-8
output_cost_per_token = 3.125e-7
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models"]
supports_function_calling = true
deprecation_date = "2025-09-29"
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token_above_128k_tokens = 1.5625e-7
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token_above_128k_tokens = 6.25e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gemini-1.5-pro-preview-0514".pricing."vertex_ai-language-models"]
input_cost_per_audio_per_second = 0.00003125
input_cost_per_audio_per_second_above_128k_tokens = 0.0000625
input_cost_per_character = 3.125e-7
input_cost_per_character_above_128k_tokens = 6.25e-7
input_cost_per_image = 0.00032875
input_cost_per_image_above_128k_tokens = 0.0006575
input_cost_per_token = 7.8125e-8
input_cost_per_token_above_128k_tokens = 1.5625e-7
input_cost_per_video_per_second = 0.00032875
input_cost_per_video_per_second_above_128k_tokens = 0.0006575
output_cost_per_character = 0.00000125
output_cost_per_character_above_128k_tokens = 0.0000025
output_cost_per_token = 3.125e-7
output_cost_per_token_above_128k_tokens = 6.25e-7

[models."gemini-2-0-flash-exp:free"]
display_name = "Gemini 2.0 Flash Experimental (free)"
model_family = "gemini-flash"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 1048576
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-12"
release_date = "2024-12-11"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemini-2-5-flash-nothink"]
display_name = "gemini-2.5-flash-nothink"
model_family = "gemini-flash"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 65536
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
litellm_provider = "302ai"
providers = ["302ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-06-24"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemini-2-5-flash-nothink".pricing."302ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025

[models."gemini-2-5-pro-preview"]
display_name = "Google: Gemini 2.5 Pro Preview 06-05"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65536
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-06-05"
supported_modalities = ["image", "text", "audio"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemini-2-5-pro-preview".pricing."kilo"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001

[models."gemini-2.0-flash"]
display_name = "Gemini 2.0 Flash"
model_family = "gemini-flash"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
cache_read_input_token_cost = 2.5e-8
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini", "google", "google-vertex", "poe", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2024-06"
release_date = "2024-12-11"
deprecation_date = "2026-03-31"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
source = "https://ai.google.dev/pricing#2_0flash"
input_cost_per_audio_token = 7e-7
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true

[models."gemini-2.0-flash".pricing."gemini"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
[models."gemini-2.0-flash".pricing."google"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.0-flash".pricing."google-vertex"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gemini-2.0-flash".pricing."poe"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.2e-7
[models."gemini-2.0-flash".pricing."vercel"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.0-flash".pricing."vercel_ai_gateway"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gemini-2.0-flash".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7

[models."gemini-2.0-flash-001"]
display_name = "Gemini 2.0 Flash"
model_family = "gemini-flash"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 1000000
max_tokens = 1000000
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "abacus", "gemini", "kilo", "openrouter", "vertex_ai-language-models"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2024-06"
release_date = "2025-02-05"
deprecation_date = "2026-03-31"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
supports_audio_output = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gemini-2.0-flash-001".pricing."abacus"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.0-flash-001".pricing."deepinfra"]
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
[models."gemini-2.0-flash-001".pricing."gemini"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
[models."gemini-2.0-flash-001".pricing."kilo"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.0-flash-001".pricing."openrouter"]
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
[models."gemini-2.0-flash-001".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."gemini-2.0-flash-exp"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
cache_read_input_token_cost = 3.75e-8
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token_above_128k_tokens = 0
supports_audio_output = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gemini-2.0-flash-exp".pricing."gemini"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
[models."gemini-2.0-flash-exp".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 1.5e-7
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 6e-7
output_cost_per_token_above_128k_tokens = 0

[models."gemini-2.0-flash-exp-image-generation"]
mode = "image_generation"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "gemini"
providers = ["gemini"]
supports_vision = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
source = "https://ai.google.dev/pricing"
max_images_per_prompt = 3000
output_cost_per_image = 0.039

[models."gemini-2.0-flash-exp-image-generation".pricing."gemini"]
input_cost_per_token = 0
output_cost_per_image = 0.039
output_cost_per_token = 0

[models."gemini-2.0-flash-lite"]
display_name = "gemini-2.0-flash-lite"
model_family = "gemini-flash-lite"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
cache_read_input_token_cost = 1.875e-8
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "302ai", "gemini", "google", "google-vertex", "poe", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-06-16"
deprecation_date = "2026-03-31"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
input_cost_per_audio_token = 7.5e-8
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 50
max_video_length = 1
max_videos_per_prompt = 10
supports_audio_output = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gemini-2.0-flash-lite".pricing."302ai"]
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."gemini-2.0-flash-lite".pricing."gemini"]
cache_read_input_token_cost = 1.875e-8
input_cost_per_audio_token = 7.5e-8
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."gemini-2.0-flash-lite".pricing."google"]
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."gemini-2.0-flash-lite".pricing."google-vertex"]
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."gemini-2.0-flash-lite".pricing."poe"]
input_cost_per_token = 5.1999999999999996e-8
output_cost_per_token = 2.1e-7
[models."gemini-2.0-flash-lite".pricing."vercel"]
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."gemini-2.0-flash-lite".pricing."vercel_ai_gateway"]
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."gemini-2.0-flash-lite".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 1.875e-8
input_cost_per_audio_token = 7.5e-8
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7

[models."gemini-2.0-flash-lite-001"]
display_name = "Google: Gemini 2.0 Flash Lite"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
cache_read_input_token_cost = 1.875e-8
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini", "kilo"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
release_date = "2024-12-11"
deprecation_date = "2026-03-31"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
input_cost_per_audio_token = 7.5e-8
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 50
max_video_length = 1
max_videos_per_prompt = 10
supports_audio_output = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gemini-2.0-flash-lite-001".pricing."gemini"]
cache_read_input_token_cost = 1.875e-8
input_cost_per_audio_token = 7.5e-8
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."gemini-2.0-flash-lite-001".pricing."kilo"]
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."gemini-2.0-flash-lite-001".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 1.875e-8
input_cost_per_audio_token = 7.5e-8
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7

[models."gemini-2.0-flash-lite-preview-02-05"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
cache_read_input_token_cost = 1.875e-8
litellm_provider = "gemini"
providers = ["gemini"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
deprecation_date = "2025-12-02"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash-lite"
input_cost_per_audio_token = 7.5e-8
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
rpm = 60000
supports_audio_output = false
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true
tpm = 10000000

[models."gemini-2.0-flash-lite-preview-02-05".pricing."gemini"]
cache_read_input_token_cost = 1.875e-8
input_cost_per_audio_token = 7.5e-8
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7

[models."gemini-2.0-flash-live-001"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.0000015
cache_read_input_token_cost = 7.5e-8
litellm_provider = "gemini"
providers = ["gemini"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
deprecation_date = "2025-12-09"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2-0-flash-live-001"
input_cost_per_audio_token = 0.0000021
input_cost_per_image = 0.0000021
input_cost_per_video_per_second = 0.0000021
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_audio_token = 0.0000085
rpm = 10
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supports_audio_output = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true
tpm = 250000

[models."gemini-2.0-flash-live-001".pricing."gemini"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.0000021
input_cost_per_image = 0.0000021
input_cost_per_token = 3.5e-7
input_cost_per_video_per_second = 0.0000021
output_cost_per_audio_token = 0.0000085
output_cost_per_token = 0.0000015

[models."gemini-2.0-flash-live-preview-04-09"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002
cache_read_input_token_cost = 7.5e-8
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
source = "https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini#gemini-2-0-flash-live-preview-04-09"
input_cost_per_audio_token = 0.000003
input_cost_per_image = 0.000003
input_cost_per_video_per_second = 0.000003
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_audio_token = 0.000012
rpm = 10
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supports_audio_output = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true
tpm = 250000

[models."gemini-2.0-flash-live-preview-04-09".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000003
input_cost_per_image = 0.000003
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.000003
output_cost_per_audio_token = 0.000012
output_cost_per_token = 0.000002

[models."gemini-2.0-flash-preview-image-generation"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
cache_read_input_token_cost = 2.5e-8
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
deprecation_date = "2025-11-14"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
source = "https://ai.google.dev/pricing#2_0flash"
input_cost_per_audio_token = 7e-7
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gemini-2.0-flash-preview-image-generation".pricing."gemini"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
[models."gemini-2.0-flash-preview-image-generation".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 7e-7
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7

[models."gemini-2.0-flash-thinking-exp"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 0
output_cost_per_token = 0
cache_read_input_token_cost = 0
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
deprecation_date = "2025-12-02"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token_above_128k_tokens = 0
supports_audio_output = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gemini-2.0-flash-thinking-exp".pricing."gemini"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
[models."gemini-2.0-flash-thinking-exp".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0

[models."gemini-2.0-flash-thinking-exp-01-21"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 0
output_cost_per_token = 0
cache_read_input_token_cost = 0
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
deprecation_date = "2025-12-02"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash"
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token_above_128k_tokens = 0
supports_audio_output = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gemini-2.0-flash-thinking-exp-01-21".pricing."gemini"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
[models."gemini-2.0-flash-thinking-exp-01-21".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0

[models."gemini-2.0-pro-exp-02-05"]
display_name = "Gemini 2.0 Pro Exp"
model_family = "gemini-pro"
mode = "chat"
max_input_tokens = 2097152
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 3.125e-7
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
release_date = "2025-02-05"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
input_cost_per_token_above_200k_tokens = 0.0000025
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_token_above_200k_tokens = 0.000015
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supports_audio_input = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_web_search = true

[models."gemini-2.0-pro-exp-02-05".pricing."gemini"]
cache_read_input_token_cost = 0
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
[models."gemini-2.0-pro-exp-02-05".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015

[models."gemini-2.5-computer-use-preview-10-2025"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini"]
supports_function_calling = true
supports_vision = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/computer-use"
input_cost_per_token_above_200k_tokens = 0.0000025
max_images_per_prompt = 3000
output_cost_per_token_above_200k_tokens = 0.000015
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supports_computer_use = true
supports_system_messages = true
supports_tool_choice = true

[models."gemini-2.5-computer-use-preview-10-2025".pricing."gemini"]
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
[models."gemini-2.5-computer-use-preview-10-2025".pricing."vertex_ai-language-models"]
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015

[models."gemini-2.5-flash"]
display_name = "Gemini 2.5 Flash"
model_family = "gemini-flash"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 1000000
max_tokens = 1000000
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
litellm_provider = "deepinfra"
providers = ["deepinfra", "302ai", "abacus", "aihubmix", "fastrouter", "firmware", "gemini", "google", "google-vertex", "helicone", "jiekou", "kilo", "openrouter", "perplexity", "poe", "qihang-ai", "replicate", "requesty", "sap-ai-core", "vercel", "vercel_ai_gateway", "vertex_ai-language-models", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-07-17"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_output = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true

[models."gemini-2.5-flash".pricing."302ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."abacus"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."aihubmix"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."gemini-2.5-flash".pricing."deepinfra"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."fastrouter"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."firmware"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."gemini"]
cache_read_input_token_cost = 3e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."google"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."google-vertex"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."helicone"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."jiekou"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.00000225
[models."gemini-2.5-flash".pricing."kilo"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."openrouter"]
input_cost_per_audio_token = 7e-7
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."poe"]
cache_read_input_token_cost = 2.1000000000000003e-8
input_cost_per_token = 2.1e-7
output_cost_per_token = 0.0000018000000000000001
[models."gemini-2.5-flash".pricing."qihang-ai"]
input_cost_per_token = 9e-8
output_cost_per_token = 7.1e-7
[models."gemini-2.5-flash".pricing."replicate"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."requesty"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."sap-ai-core"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."vercel"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."vercel_ai_gateway"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 3e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash".pricing."zenmux"]
cache_read_input_token_cost = 7e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025

[models."gemini-2.5-flash-image"]
display_name = "gemini-2.5-flash-image"
model_family = "gemini-flash"
mode = "image_generation"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
cache_read_input_token_cost = 3e-8
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "302ai", "gemini", "google", "vercel", "vertex_ai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-10-08"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
source = "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-flash-image"
input_cost_per_audio_token = 0.000001
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_image = 0.039
output_cost_per_image_token = 0.00003
output_cost_per_reasoning_token = 0.0000025
rpm = 100000
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_output = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true
tpm = 8000000

[models."gemini-2.5-flash-image".pricing."302ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.00003
[models."gemini-2.5-flash-image".pricing."gemini"]
cache_read_input_token_cost = 3e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_image = 0.039
output_cost_per_image_token = 0.00003
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash-image".pricing."google"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.00003
[models."gemini-2.5-flash-image".pricing."vercel"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash-image".pricing."vertex_ai"]
cache_read_input_token_cost = 3e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_image = 0.039
output_cost_per_image_token = 0.00003
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash-image".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 3e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_image = 0.039
output_cost_per_image_token = 0.00003
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025

[models."gemini-2.5-flash-image-preview"]
display_name = "Nano Banana Preview (Gemini 2.5 Flash Image Preview)"
model_family = "gemini-flash"
mode = "image_generation"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 3e-7
output_cost_per_token = 0.00003
cache_read_input_token_cost = 7.5e-8
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini", "google", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-03-20"
deprecation_date = "2026-01-15"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "image"]
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
input_cost_per_audio_token = 0.000001
input_cost_per_image_token = 3e-7
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_image = 0.039
output_cost_per_image_token = 0.00003
output_cost_per_reasoning_token = 0.00003
rpm = 100000
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_output = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true
tpm = 8000000

[models."gemini-2.5-flash-image-preview".pricing."gemini"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_image = 0.039
output_cost_per_image_token = 0.00003
output_cost_per_reasoning_token = 0.00003
output_cost_per_token = 0.00003
[models."gemini-2.5-flash-image-preview".pricing."google"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.00003
[models."gemini-2.5-flash-image-preview".pricing."vercel"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash-image-preview".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_image_token = 3e-7
input_cost_per_token = 3e-7
output_cost_per_image = 0.039
output_cost_per_image_token = 0.00003
output_cost_per_reasoning_token = 0.00003
output_cost_per_token = 0.00003

[models."gemini-2.5-flash-lite"]
display_name = "gemini-2.5-flash-lite"
model_family = "gemini-flash-lite"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
cache_read_input_token_cost = 1e-8
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini", "google", "google-vertex", "helicone", "jiekou", "kilo", "openrouter", "poe", "vercel", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2026-01"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
input_cost_per_audio_token = 3e-7
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_reasoning_token = 4e-7
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_output = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true

[models."gemini-2.5-flash-lite".pricing."gemini"]
cache_read_input_token_cost = 1e-8
input_cost_per_audio_token = 3e-7
input_cost_per_token = 1e-7
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
[models."gemini-2.5-flash-lite".pricing."google"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.5-flash-lite".pricing."google-vertex"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.5-flash-lite".pricing."helicone"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
[models."gemini-2.5-flash-lite".pricing."jiekou"]
input_cost_per_token = 9e-8
output_cost_per_token = 3.6e-7
[models."gemini-2.5-flash-lite".pricing."kilo"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.5-flash-lite".pricing."openrouter"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.5-flash-lite".pricing."poe"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
[models."gemini-2.5-flash-lite".pricing."vercel"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.5-flash-lite".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 1e-8
input_cost_per_audio_token = 3e-7
input_cost_per_token = 1e-7
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
[models."gemini-2.5-flash-lite".pricing."zenmux"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7

[models."gemini-2.5-flash-lite-preview-06-17"]
display_name = "gemini-2.5-flash-lite-preview-06-17"
model_family = "gemini-flash-lite"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
cache_read_input_token_cost = 2.5e-8
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini", "google", "google-vertex", "jiekou"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2026-01"
deprecation_date = "2025-11-18"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
input_cost_per_audio_token = 5e-7
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_reasoning_token = 4e-7
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_output = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true

[models."gemini-2.5-flash-lite-preview-06-17".pricing."gemini"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 5e-7
input_cost_per_token = 1e-7
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
[models."gemini-2.5-flash-lite-preview-06-17".pricing."google"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.5-flash-lite-preview-06-17".pricing."google-vertex"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.5-flash-lite-preview-06-17".pricing."jiekou"]
input_cost_per_token = 9e-8
output_cost_per_token = 3.6e-7
[models."gemini-2.5-flash-lite-preview-06-17".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_audio_token = 5e-7
input_cost_per_token = 1e-7
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7

[models."gemini-2.5-flash-lite-preview-09-2025"]
display_name = "gemini-2.5-flash-lite-preview-09-2025"
model_family = "gemini-flash-lite"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
cache_read_input_token_cost = 1e-8
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "302ai", "gemini", "google", "google-vertex", "jiekou", "kilo", "openrouter", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-09-26"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"
input_cost_per_audio_token = 3e-7
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_reasoning_token = 4e-7
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_output = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true

[models."gemini-2.5-flash-lite-preview-09-2025".pricing."302ai"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.5-flash-lite-preview-09-2025".pricing."gemini"]
cache_read_input_token_cost = 1e-8
input_cost_per_audio_token = 3e-7
input_cost_per_token = 1e-7
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
[models."gemini-2.5-flash-lite-preview-09-2025".pricing."google"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.5-flash-lite-preview-09-2025".pricing."google-vertex"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.5-flash-lite-preview-09-2025".pricing."jiekou"]
input_cost_per_token = 9e-8
output_cost_per_token = 3.6e-7
[models."gemini-2.5-flash-lite-preview-09-2025".pricing."kilo"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.5-flash-lite-preview-09-2025".pricing."openrouter"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.5-flash-lite-preview-09-2025".pricing."vercel"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-2.5-flash-lite-preview-09-2025".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 1e-8
input_cost_per_audio_token = 3e-7
input_cost_per_token = 1e-7
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7

[models."gemini-2.5-flash-native-audio-latest"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
litellm_provider = "gemini"
providers = ["gemini"]
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
source = "https://ai.google.dev/pricing"
input_cost_per_audio_token = 0.000001
supported_endpoints = ["/v1/realtime"]
supports_audio_input = true
supports_audio_output = true

[models."gemini-2.5-flash-native-audio-latest".pricing."gemini"]
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025

[models."gemini-2.5-flash-native-audio-preview-09-2025"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
litellm_provider = "gemini"
providers = ["gemini"]
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
source = "https://ai.google.dev/pricing"
input_cost_per_audio_token = 0.000001
supported_endpoints = ["/v1/realtime"]
supports_audio_input = true
supports_audio_output = true

[models."gemini-2.5-flash-native-audio-preview-09-2025".pricing."gemini"]
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025

[models."gemini-2.5-flash-native-audio-preview-12-2025"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
litellm_provider = "gemini"
providers = ["gemini"]
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
source = "https://ai.google.dev/pricing"
input_cost_per_audio_token = 0.000001
supported_endpoints = ["/v1/realtime"]
supports_audio_input = true
supports_audio_output = true

[models."gemini-2.5-flash-native-audio-preview-12-2025".pricing."gemini"]
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025

[models."gemini-2.5-flash-preview-04-17"]
display_name = "Gemini 2.5 Flash Preview 04-17"
model_family = "gemini-flash"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
cache_read_input_token_cost = 3.75e-8
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini", "google", "google-vertex"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-04-17"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
input_cost_per_audio_token = 0.000001
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_reasoning_token = 0.0000035
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_output = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gemini-2.5-flash-preview-04-17".pricing."gemini"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 1.5e-7
output_cost_per_reasoning_token = 0.0000035
output_cost_per_token = 6e-7
[models."gemini-2.5-flash-preview-04-17".pricing."google"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gemini-2.5-flash-preview-04-17".pricing."google-vertex"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gemini-2.5-flash-preview-04-17".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 1.5e-7
output_cost_per_reasoning_token = 0.0000035
output_cost_per_token = 6e-7

[models."gemini-2.5-flash-preview-05-20"]
display_name = "gemini-2.5-flash-preview-05-20"
model_family = "gemini-flash"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
cache_read_input_token_cost = 7.5e-8
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini", "google", "google-vertex", "jiekou"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2026-01"
deprecation_date = "2025-11-18"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
input_cost_per_audio_token = 0.000001
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_reasoning_token = 0.0000025
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_output = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true

[models."gemini-2.5-flash-preview-05-20".pricing."gemini"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash-preview-05-20".pricing."google"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gemini-2.5-flash-preview-05-20".pricing."google-vertex"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gemini-2.5-flash-preview-05-20".pricing."jiekou"]
input_cost_per_token = 1.35e-7
output_cost_per_token = 0.00000315
[models."gemini-2.5-flash-preview-05-20".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025

[models."gemini-2.5-flash-preview-09-2025"]
display_name = "gemini-2.5-flash-preview-09-2025"
model_family = "gemini-flash"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
cache_read_input_token_cost = 7.5e-8
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "302ai", "gemini", "google", "google-vertex", "kilo", "openrouter", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-09-26"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"
input_cost_per_audio_token = 0.000001
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_reasoning_token = 0.0000025
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_output = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true

[models."gemini-2.5-flash-preview-09-2025".pricing."302ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash-preview-09-2025".pricing."gemini"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash-preview-09-2025".pricing."google"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash-preview-09-2025".pricing."google-vertex"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash-preview-09-2025".pricing."kilo"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash-preview-09-2025".pricing."openrouter"]
cache_read_input_token_cost = 3.1e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash-preview-09-2025".pricing."vercel"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash-preview-09-2025".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025

[models."gemini-2.5-flash-preview-tts"]
display_name = "Gemini 2.5 Flash Preview TTS"
model_family = "gemini-flash"
mode = "audio_speech"
max_input_tokens = 8000
max_output_tokens = 16000
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
litellm_provider = "gemini"
providers = ["gemini", "google"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-05-01"
supported_modalities = ["text"]
supported_output_modalities = ["audio"]
source = "https://ai.google.dev/pricing"
rpm = 10
supported_endpoints = ["/v1/audio/speech"]
tpm = 4000000

[models."gemini-2.5-flash-preview-tts".pricing."gemini"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-2.5-flash-preview-tts".pricing."google"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.00001

[models."gemini-2.5-pro"]
display_name = "Gemini 2.5 Pro"
model_family = "gemini-pro"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 1000000
max_tokens = 1000000
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
litellm_provider = "deepinfra"
providers = ["deepinfra", "302ai", "abacus", "aihubmix", "cortecs", "fastrouter", "firmware", "gemini", "github_copilot", "github-copilot", "google", "google-vertex", "helicone", "jiekou", "kilo", "openrouter", "perplexity", "poe", "requesty", "sap-ai-core", "vercel", "vercel_ai_gateway", "vertex_ai-language-models", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-03-20"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_web_search = true

[models."gemini-2.5-pro".pricing."302ai"]
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro".pricing."abacus"]
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro".pricing."aihubmix"]
cache_read_input_token_cost = 3.1e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.000005
[models."gemini-2.5-pro".pricing."cortecs"]
input_cost_per_token = 0.000001654
output_cost_per_token = 0.000011024
[models."gemini-2.5-pro".pricing."deepinfra"]
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro".pricing."fastrouter"]
cache_read_input_token_cost = 3.1e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro".pricing."firmware"]
cache_read_input_token_cost = 3.1e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro".pricing."gemini"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
input_cost_per_token_above_200k_tokens_priority = 0.0000025
input_cost_per_token_priority = 0.00000125
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
output_cost_per_token_above_200k_tokens_priority = 0.000015
output_cost_per_token_priority = 0.00001
[models."gemini-2.5-pro".pricing."google"]
cache_read_input_token_cost = 3.1e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro".pricing."google-vertex"]
cache_read_input_token_cost = 3.1e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro".pricing."helicone"]
cache_read_input_token_cost = 3.125e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro".pricing."jiekou"]
input_cost_per_token = 0.000001125
output_cost_per_token = 0.000009
[models."gemini-2.5-pro".pricing."kilo"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro".pricing."openrouter"]
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro".pricing."poe"]
cache_read_input_token_cost = 8.7e-8
input_cost_per_token = 8.7e-7
output_cost_per_token = 0.000007
[models."gemini-2.5-pro".pricing."requesty"]
cache_read_input_token_cost = 3.1e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro".pricing."sap-ai-core"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro".pricing."vercel"]
cache_read_input_token_cost = 3.1e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gemini-2.5-pro".pricing."vertex_ai-language-models"]
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
[models."gemini-2.5-pro".pricing."zenmux"]
cache_read_input_token_cost = 3.1e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001

[models."gemini-2.5-pro-exp-03-25"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_token_above_200k_tokens = 0.0000025
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_token_above_200k_tokens = 0.000015
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supports_audio_input = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_web_search = true

[models."gemini-2.5-pro-exp-03-25".pricing."gemini"]
cache_read_input_token_cost = 0
input_cost_per_token = 0
input_cost_per_token_above_200k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_200k_tokens = 0
[models."gemini-2.5-pro-exp-03-25".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015

[models."gemini-2.5-pro-preview-03-25"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
deprecation_date = "2025-12-02"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_audio_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_token_above_200k_tokens = 0.000015
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_output = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gemini-2.5-pro-preview-03-25".pricing."gemini"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
[models."gemini-2.5-pro-preview-03-25".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_audio_token = 0.00000125
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015

[models."gemini-2.5-pro-preview-05-06"]
display_name = "Gemini 2.5 Pro Preview 05-06"
model_family = "gemini-pro"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini", "google", "google-vertex", "kilo", "openrouter"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-05-06"
deprecation_date = "2025-12-02"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_audio_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_token_above_200k_tokens = 0.000015
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supported_regions = ["global"]
supports_audio_output = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true

[models."gemini-2.5-pro-preview-05-06".pricing."gemini"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
[models."gemini-2.5-pro-preview-05-06".pricing."google"]
cache_read_input_token_cost = 3.1e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro-preview-05-06".pricing."google-vertex"]
cache_read_input_token_cost = 3.1e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro-preview-05-06".pricing."kilo"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro-preview-05-06".pricing."openrouter"]
cache_read_input_token_cost = 3.1e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro-preview-05-06".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_audio_token = 0.00000125
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015

[models."gemini-2.5-pro-preview-06-05"]
display_name = "gemini-2.5-pro-preview-06-05"
model_family = "gemini-pro"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini", "google", "google-vertex", "jiekou", "openrouter"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2026-01"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview"
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_audio_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_token_above_200k_tokens = 0.000015
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_output = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true

[models."gemini-2.5-pro-preview-06-05".pricing."gemini"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
[models."gemini-2.5-pro-preview-06-05".pricing."google"]
cache_read_input_token_cost = 3.1e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro-preview-06-05".pricing."google-vertex"]
cache_read_input_token_cost = 3.1e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro-preview-06-05".pricing."jiekou"]
input_cost_per_token = 0.000001125
output_cost_per_token = 0.000009
[models."gemini-2.5-pro-preview-06-05".pricing."openrouter"]
cache_read_input_token_cost = 3.1e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gemini-2.5-pro-preview-06-05".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_audio_token = 0.00000125
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015

[models."gemini-2.5-pro-preview-tts"]
display_name = "Gemini 2.5 Pro Preview TTS"
model_family = "gemini-flash"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini", "google"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-05-01"
supported_modalities = ["text"]
supported_output_modalities = ["audio"]
source = "https://ai.google.dev/gemini-api/docs/pricing#gemini-2.5-pro-preview"
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_audio_token = 7e-7
input_cost_per_token_above_200k_tokens = 0.0000025
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_token_above_200k_tokens = 0.000015
supports_audio_output = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gemini-2.5-pro-preview-tts".pricing."gemini"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015
[models."gemini-2.5-pro-preview-tts".pricing."google"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.00002
[models."gemini-2.5-pro-preview-tts".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_audio_token = 7e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015

[models."gemini-3-1-pro"]
display_name = "Gemini 3.1 Pro Preview"
model_family = "gemini-pro"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65536
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
cache_read_input_token_cost = 2.0000000000000002e-7
litellm_provider = "opencode"
providers = ["opencode"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2026-02-19"
supported_modalities = ["text", "image", "video", "audio", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemini-3-1-pro".pricing."opencode"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012

[models."gemini-3-1-pro-preview"]
display_name = "Gemini 3.1 Pro Preview"
model_family = "gemini-pro"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 65000
input_cost_per_token = 0.0000025
output_cost_per_token = 0.000015
cache_read_input_token_cost = 5e-7
litellm_provider = "venice"
providers = ["venice", "google", "google-vertex", "openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2026-02-19"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"
reasoning_cost_per_token = 0.000012

[models."gemini-3-1-pro-preview".pricing."google"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3-1-pro-preview".pricing."google-vertex"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3-1-pro-preview".pricing."openrouter"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
reasoning_cost_per_token = 0.000012
[models."gemini-3-1-pro-preview".pricing."venice"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.0000025
output_cost_per_token = 0.000015

[models."gemini-3-flash"]
display_name = "Gemini 3 Flash"
model_family = "gemini-flash"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
input_cost_per_token = 5e-7
output_cost_per_token = 0.000003
cache_read_input_token_cost = 5.0000000000000004e-8
litellm_provider = "vercel"
providers = ["vercel", "opencode", "poe"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-03"
release_date = "2025-12-17"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemini-3-flash".pricing."opencode"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 5e-7
output_cost_per_token = 0.000003
[models."gemini-3-flash".pricing."poe"]
cache_read_input_token_cost = 4e-8
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000024
[models."gemini-3-flash".pricing."vercel"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 5e-7
output_cost_per_token = 0.000003

[models."gemini-3-flash-preview"]
display_name = "Gemini 3 Flash Preview"
model_family = "gemini-flash"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 5e-7
output_cost_per_token = 0.000003
cache_read_input_token_cost = 5e-8
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "302ai", "abacus", "firmware", "gemini", "github-copilot", "gmi", "google", "google-vertex", "jiekou", "kilo", "openrouter", "perplexity", "qihang-ai", "requesty", "venice", "vertex_ai-language-models", "vivgrid", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-12-17"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
cache_read_input_token_cost_priority = 9e-8
input_cost_per_audio_token = 0.000001
input_cost_per_audio_token_priority = 0.0000018
input_cost_per_token_priority = 9e-7
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_token_priority = 0.0000054
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_input = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_video_input = true
supports_web_search = true

[models."gemini-3-flash-preview".pricing."302ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.000003
[models."gemini-3-flash-preview".pricing."abacus"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.000003
[models."gemini-3-flash-preview".pricing."firmware"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 5e-7
output_cost_per_token = 0.000003
[models."gemini-3-flash-preview".pricing."gemini"]
cache_read_input_token_cost = 5e-8
cache_read_input_token_cost_priority = 9e-8
input_cost_per_audio_token = 0.000001
input_cost_per_audio_token_priority = 0.0000018
input_cost_per_token = 5e-7
input_cost_per_token_priority = 9e-7
output_cost_per_reasoning_token = 0.000003
output_cost_per_token = 0.000003
output_cost_per_token_priority = 0.0000054
[models."gemini-3-flash-preview".pricing."gmi"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.000003
[models."gemini-3-flash-preview".pricing."google"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 5e-7
output_cost_per_token = 0.000003
[models."gemini-3-flash-preview".pricing."google-vertex"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 5e-7
output_cost_per_token = 0.000003
[models."gemini-3-flash-preview".pricing."jiekou"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.000003
[models."gemini-3-flash-preview".pricing."kilo"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 5e-7
output_cost_per_token = 0.000003
[models."gemini-3-flash-preview".pricing."openrouter"]
cache_read_input_token_cost = 5e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 5e-7
output_cost_per_reasoning_token = 0.000003
output_cost_per_token = 0.000003
[models."gemini-3-flash-preview".pricing."qihang-ai"]
input_cost_per_token = 7e-8
output_cost_per_token = 4.3e-7
[models."gemini-3-flash-preview".pricing."requesty"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 5e-7
output_cost_per_token = 0.000003
[models."gemini-3-flash-preview".pricing."venice"]
cache_read_input_token_cost = 7e-8
input_cost_per_token = 7e-7
output_cost_per_token = 0.00000375
[models."gemini-3-flash-preview".pricing."vertex_ai"]
cache_read_input_token_cost = 5e-8
cache_read_input_token_cost_priority = 9e-8
input_cost_per_audio_token = 0.000001
input_cost_per_audio_token_priority = 0.0000018
input_cost_per_token = 5e-7
input_cost_per_token_priority = 9e-7
output_cost_per_token = 0.000003
output_cost_per_token_priority = 0.0000054
[models."gemini-3-flash-preview".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 5e-8
cache_read_input_token_cost_priority = 9e-8
input_cost_per_audio_token = 0.000001
input_cost_per_audio_token_priority = 0.0000018
input_cost_per_token = 5e-7
input_cost_per_token_priority = 9e-7
output_cost_per_reasoning_token = 0.000003
output_cost_per_token = 0.000003
output_cost_per_token_priority = 0.0000054
[models."gemini-3-flash-preview".pricing."vivgrid"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 5e-7
output_cost_per_token = 0.000003
[models."gemini-3-flash-preview".pricing."zenmux"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 5e-7
output_cost_per_token = 0.000003

[models."gemini-3-pro"]
display_name = "Gemini 3 Pro"
model_family = "gemini-pro"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65536
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
litellm_provider = "replicate"
providers = ["replicate", "opencode", "poe"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-11-18"
supported_modalities = ["text", "image", "video", "audio", "pdf"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gemini-3-pro".pricing."opencode"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3-pro".pricing."poe"]
cache_read_input_token_cost = 1.6e-7
input_cost_per_token = 0.0000016000000000000001
output_cost_per_token = 0.0000096
[models."gemini-3-pro".pricing."replicate"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012

[models."gemini-3-pro-image"]
display_name = "Nano Banana Pro (Gemini 3 Pro Image)"
model_family = "gemini-pro"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 32768
input_cost_per_token = 0.000002
output_cost_per_token = 0.00012
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-03"
release_date = "2025-09"
supported_modalities = ["text"]
supported_output_modalities = ["text", "image"]
source = "modelsdev"

[models."gemini-3-pro-image".pricing."vercel"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.00012

[models."gemini-3-pro-image-preview"]
display_name = "gemini-3-pro-image-preview"
mode = "image_generation"
max_input_tokens = 65536
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "302ai", "gemini", "vertex_ai"]
supports_function_calling = false
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2025-06"
release_date = "2025-11-20"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
source = "https://ai.google.dev/gemini-api/docs/pricing"
input_cost_per_image = 0.0011
input_cost_per_token_batches = 0.000001
output_cost_per_image = 0.134
output_cost_per_image_token = 0.00012
output_cost_per_token_batches = 0.000006
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_response_schema = true
supports_system_messages = true
supports_web_search = true

[models."gemini-3-pro-image-preview".pricing."302ai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.00012
[models."gemini-3-pro-image-preview".pricing."gemini"]
input_cost_per_image = 0.0011
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
output_cost_per_image = 0.134
output_cost_per_image_token = 0.00012
output_cost_per_token = 0.000012
output_cost_per_token_batches = 0.000006
[models."gemini-3-pro-image-preview".pricing."vertex_ai"]
input_cost_per_image = 0.0011
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
output_cost_per_image = 0.134
output_cost_per_image_token = 0.00012
output_cost_per_token = 0.000012
output_cost_per_token_batches = 0.000006
[models."gemini-3-pro-image-preview".pricing."vertex_ai-language-models"]
input_cost_per_image = 0.0011
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
output_cost_per_image = 0.134
output_cost_per_image_token = 0.00012
output_cost_per_token = 0.000012
output_cost_per_token_batches = 0.000006

[models."gemini-3-pro-preview"]
display_name = "Gemini 3 Pro Preview"
model_family = "gemini-pro"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
cache_read_input_token_cost = 2e-7
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "302ai", "abacus", "aihubmix", "firmware", "gemini", "github_copilot", "github-copilot", "gmi", "google", "google-vertex", "helicone", "jiekou", "kilo", "openrouter", "perplexity", "qihang-ai", "requesty", "venice", "vercel", "vertex_ai", "vivgrid", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-11-18"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
cache_read_input_token_cost_above_200k_tokens_priority = 7.2e-7
cache_read_input_token_cost_priority = 3.6e-7
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_above_200k_tokens_priority = 0.0000072
input_cost_per_token_batches = 0.000001
input_cost_per_token_priority = 0.0000036
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_above_200k_tokens_priority = 0.0000324
output_cost_per_token_batches = 0.000006
output_cost_per_token_priority = 0.0000216
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_input = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_web_search = true

[models."gemini-3-pro-preview".pricing."302ai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3-pro-preview".pricing."abacus"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3-pro-preview".pricing."aihubmix"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3-pro-preview".pricing."firmware"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3-pro-preview".pricing."gemini"]
cache_read_input_token_cost = 2e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
cache_read_input_token_cost_above_200k_tokens_priority = 7.2e-7
cache_read_input_token_cost_priority = 3.6e-7
input_cost_per_token = 0.000002
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_above_200k_tokens_priority = 0.0000072
input_cost_per_token_batches = 0.000001
input_cost_per_token_priority = 0.0000036
output_cost_per_token = 0.000012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_above_200k_tokens_priority = 0.0000324
output_cost_per_token_batches = 0.000006
output_cost_per_token_priority = 0.0000216
[models."gemini-3-pro-preview".pricing."gmi"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3-pro-preview".pricing."google"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3-pro-preview".pricing."google-vertex"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3-pro-preview".pricing."helicone"]
cache_read_input_token_cost = 2e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3-pro-preview".pricing."jiekou"]
input_cost_per_token = 0.0000018000000000000001
output_cost_per_token = 0.0000108
[models."gemini-3-pro-preview".pricing."kilo"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3-pro-preview".pricing."openrouter"]
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost = 2e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
input_cost_per_token = 0.000002
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_batches = 0.000001
output_cost_per_token = 0.000012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_batches = 0.000006
[models."gemini-3-pro-preview".pricing."qihang-ai"]
input_cost_per_token = 5.699999999999999e-7
output_cost_per_token = 0.00000343
[models."gemini-3-pro-preview".pricing."requesty"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3-pro-preview".pricing."venice"]
cache_read_input_token_cost = 6.25e-7
input_cost_per_token = 0.0000025
output_cost_per_token = 0.000015
[models."gemini-3-pro-preview".pricing."vercel"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3-pro-preview".pricing."vertex_ai"]
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost = 2e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
cache_read_input_token_cost_above_200k_tokens_priority = 7.2e-7
cache_read_input_token_cost_priority = 3.6e-7
input_cost_per_token = 0.000002
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_above_200k_tokens_priority = 0.0000072
input_cost_per_token_batches = 0.000001
input_cost_per_token_priority = 0.0000036
output_cost_per_token = 0.000012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_above_200k_tokens_priority = 0.0000324
output_cost_per_token_batches = 0.000006
output_cost_per_token_priority = 0.0000216
[models."gemini-3-pro-preview".pricing."vertex_ai-language-models"]
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost = 2e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
cache_read_input_token_cost_above_200k_tokens_priority = 7.2e-7
cache_read_input_token_cost_priority = 3.6e-7
input_cost_per_token = 0.000002
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_above_200k_tokens_priority = 0.0000072
input_cost_per_token_batches = 0.000001
input_cost_per_token_priority = 0.0000036
output_cost_per_token = 0.000012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_above_200k_tokens_priority = 0.0000324
output_cost_per_token_batches = 0.000006
output_cost_per_token_priority = 0.0000216
[models."gemini-3-pro-preview".pricing."vivgrid"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3-pro-preview".pricing."zenmux"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012

[models."gemini-3-pro-preview-search"]
display_name = "Gemini 3 Pro Preview Search"
model_family = "gemini-pro"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 65000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
cache_read_input_token_cost = 5e-7
litellm_provider = "aihubmix"
providers = ["aihubmix"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-11"
release_date = "2025-11-19"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemini-3-pro-preview-search".pricing."aihubmix"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012

[models."gemini-3.1-pro-preview"]
display_name = "Gemini 3.1 Pro Preview"
model_family = "gemini-pro"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
cache_read_input_token_cost = 2e-7
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini", "github-copilot", "vercel", "vertex_ai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2026-02-19"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#gemini-models"
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
cache_read_input_token_cost_above_200k_tokens_priority = 7.2e-7
cache_read_input_token_cost_priority = 3.6e-7
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_above_200k_tokens_priority = 0.0000072
input_cost_per_token_batches = 0.000001
input_cost_per_token_priority = 0.0000036
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_image = 0.00012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_above_200k_tokens_priority = 0.0000324
output_cost_per_token_batches = 0.000006
output_cost_per_token_priority = 0.0000216
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_input = true
supports_native_streaming = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_video_input = true
supports_web_search = true

[models."gemini-3.1-pro-preview".pricing."gemini"]
cache_read_input_token_cost = 2e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
cache_read_input_token_cost_above_200k_tokens_priority = 7.2e-7
cache_read_input_token_cost_priority = 3.6e-7
input_cost_per_token = 0.000002
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_above_200k_tokens_priority = 0.0000072
input_cost_per_token_batches = 0.000001
input_cost_per_token_priority = 0.0000036
output_cost_per_token = 0.000012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_above_200k_tokens_priority = 0.0000324
output_cost_per_token_batches = 0.000006
output_cost_per_token_priority = 0.0000216
[models."gemini-3.1-pro-preview".pricing."vercel"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3.1-pro-preview".pricing."vertex_ai"]
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost = 2e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
cache_read_input_token_cost_above_200k_tokens_priority = 7.2e-7
cache_read_input_token_cost_priority = 3.6e-7
input_cost_per_token = 0.000002
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_above_200k_tokens_priority = 0.0000072
input_cost_per_token_batches = 0.000001
input_cost_per_token_priority = 0.0000036
output_cost_per_image = 0.00012
output_cost_per_token = 0.000012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_above_200k_tokens_priority = 0.0000324
output_cost_per_token_batches = 0.000006
output_cost_per_token_priority = 0.0000216
[models."gemini-3.1-pro-preview".pricing."vertex_ai-language-models"]
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost = 2e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
cache_read_input_token_cost_above_200k_tokens_priority = 7.2e-7
cache_read_input_token_cost_priority = 3.6e-7
input_cost_per_token = 0.000002
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_above_200k_tokens_priority = 0.0000072
input_cost_per_token_batches = 0.000001
input_cost_per_token_priority = 0.0000036
output_cost_per_image = 0.00012
output_cost_per_token = 0.000012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_above_200k_tokens_priority = 0.0000324
output_cost_per_token_batches = 0.000006
output_cost_per_token_priority = 0.0000216

[models."gemini-3.1-pro-preview-customtools"]
display_name = "Gemini 3.1 Pro Preview Custom Tools"
model_family = "gemini-pro"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
cache_read_input_token_cost = 2e-7
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini", "google", "google-vertex", "vertex_ai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2026-02-19"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#gemini-models"
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_batches = 0.000001
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_image = 0.00012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_batches = 0.000006
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_input = true
supports_native_streaming = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_video_input = true
supports_web_search = true

[models."gemini-3.1-pro-preview-customtools".pricing."gemini"]
cache_read_input_token_cost = 2e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
cache_read_input_token_cost_above_200k_tokens_priority = 7.2e-7
cache_read_input_token_cost_priority = 3.6e-7
input_cost_per_token = 0.000002
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_above_200k_tokens_priority = 0.0000072
input_cost_per_token_batches = 0.000001
input_cost_per_token_priority = 0.0000036
output_cost_per_token = 0.000012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_above_200k_tokens_priority = 0.0000324
output_cost_per_token_batches = 0.000006
output_cost_per_token_priority = 0.0000216
[models."gemini-3.1-pro-preview-customtools".pricing."google"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3.1-pro-preview-customtools".pricing."google-vertex"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012
[models."gemini-3.1-pro-preview-customtools".pricing."vertex_ai"]
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost = 2e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
cache_read_input_token_cost_above_200k_tokens_priority = 7.2e-7
cache_read_input_token_cost_priority = 3.6e-7
input_cost_per_token = 0.000002
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_above_200k_tokens_priority = 0.0000072
input_cost_per_token_batches = 0.000001
input_cost_per_token_priority = 0.0000036
output_cost_per_image = 0.00012
output_cost_per_token = 0.000012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_above_200k_tokens_priority = 0.0000324
output_cost_per_token_batches = 0.000006
output_cost_per_token_priority = 0.0000216
[models."gemini-3.1-pro-preview-customtools".pricing."vertex_ai-language-models"]
cache_creation_input_token_cost_above_200k_tokens = 2.5e-7
cache_read_input_token_cost = 2e-7
cache_read_input_token_cost_above_200k_tokens = 4e-7
input_cost_per_token = 0.000002
input_cost_per_token_above_200k_tokens = 0.000004
input_cost_per_token_batches = 0.000001
output_cost_per_image = 0.00012
output_cost_per_token = 0.000012
output_cost_per_token_above_200k_tokens = 0.000018
output_cost_per_token_batches = 0.000006

[models."gemini-deep-research"]
display_name = "gemini-deep-research"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 0
input_cost_per_token = 0.0000016000000000000001
output_cost_per_token = 0.0000096
litellm_provider = "poe"
providers = ["poe"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-12-11"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemini-deep-research".pricing."poe"]
input_cost_per_token = 0.0000016000000000000001
output_cost_per_token = 0.0000096

[models."gemini-embedding-001"]
display_name = "Gemini Embedding 001"
model_family = "gemini-embedding"
mode = "embedding"
max_input_tokens = 2048
max_output_tokens = 0
max_tokens = 2048
input_cost_per_token = 1.5e-7
output_cost_per_token = 0
litellm_provider = "vertex_ai-embedding-models"
providers = ["vertex_ai-embedding-models", "gemini", "google", "google-vertex", "vercel", "vercel_ai_gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-05"
release_date = "2025-05-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"
output_vector_size = 3072

[models."gemini-embedding-001".pricing."gemini"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0
[models."gemini-embedding-001".pricing."google"]
input_cost_per_token = 1.5e-7
[models."gemini-embedding-001".pricing."google-vertex"]
input_cost_per_token = 1.5e-7
[models."gemini-embedding-001".pricing."vercel"]
input_cost_per_token = 1.5e-7
[models."gemini-embedding-001".pricing."vercel_ai_gateway"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0
[models."gemini-embedding-001".pricing."vertex_ai-embedding-models"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0

[models."gemini-exp-1114"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "gemini"
providers = ["gemini"]
supports_function_calling = true
supports_vision = true
source = "https://ai.google.dev/pricing"
input_cost_per_token_above_128k_tokens = 0
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_token_above_128k_tokens = 0
rpm = 1000
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
tpm = 4000000

[models."gemini-exp-1114".metadata]
notes = "Rate limits not documented for gemini-exp-1114. Assuming same as gemini-1.5-pro."
supports_tool_choice = true

[models."gemini-exp-1114".pricing."gemini"]
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0

[models."gemini-exp-1206"]
mode = "chat"
max_input_tokens = 2097152
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "gemini"
providers = ["gemini"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://ai.google.dev/pricing"
input_cost_per_token_above_128k_tokens = 0
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_token_above_128k_tokens = 0
rpm = 1000
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true
tpm = 4000000

[models."gemini-exp-1206".metadata]
notes = "Rate limits not documented for gemini-exp-1206. Assuming same as gemini-1.5-pro."
supports_tool_choice = true

[models."gemini-exp-1206".pricing."gemini"]
cache_read_input_token_cost = 3e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025

[models."gemini-flash-experimental"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models"]
supports_function_calling = false
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/gemini-experimental"
input_cost_per_character = 0
output_cost_per_character = 0
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-flash-experimental".pricing."vertex_ai-language-models"]
input_cost_per_character = 0
input_cost_per_token = 0
output_cost_per_character = 0
output_cost_per_token = 0

[models."gemini-flash-latest"]
display_name = "Gemini Flash Latest"
model_family = "gemini-flash"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
cache_read_input_token_cost = 7.5e-8
litellm_provider = "gemini"
providers = ["gemini", "google", "google-vertex"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-09-25"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"
input_cost_per_audio_token = 0.000001
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_reasoning_token = 0.0000025
rpm = 15
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_output = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true
tpm = 250000

[models."gemini-flash-latest".pricing."gemini"]
cache_read_input_token_cost = 3e-8
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
[models."gemini-flash-latest".pricing."google"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
[models."gemini-flash-latest".pricing."google-vertex"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025

[models."gemini-flash-lite-latest"]
display_name = "Gemini Flash-Lite Latest"
model_family = "gemini-flash-lite"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
cache_read_input_token_cost = 2.5e-8
litellm_provider = "gemini"
providers = ["gemini", "google", "google-vertex"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-09-25"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/"
input_cost_per_audio_token = 3e-7
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_reasoning_token = 4e-7
rpm = 15
supported_endpoints = ["/v1/chat/completions", "/v1/completions", "/v1/batch"]
supports_audio_output = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true
tpm = 250000

[models."gemini-flash-lite-latest".pricing."gemini"]
cache_read_input_token_cost = 1e-8
input_cost_per_audio_token = 3e-7
input_cost_per_token = 1e-7
output_cost_per_reasoning_token = 4e-7
output_cost_per_token = 4e-7
[models."gemini-flash-lite-latest".pricing."google"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gemini-flash-lite-latest".pricing."google-vertex"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7

[models."gemini-gemma-2-27b-it"]
mode = "chat"
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000105
litellm_provider = "gemini"
providers = ["gemini"]
supports_function_calling = true
supports_vision = true
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
rpm = 10
supports_tool_choice = true
tpm = 250000

[models."gemini-gemma-2-27b-it".pricing."gemini"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000105

[models."gemini-gemma-2-9b-it"]
mode = "chat"
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000105
litellm_provider = "gemini"
providers = ["gemini"]
supports_function_calling = true
supports_vision = true
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
rpm = 10
supports_tool_choice = true
tpm = 250000

[models."gemini-gemma-2-9b-it".pricing."gemini"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000105

[models."gemini-live-2-5-flash"]
display_name = "Gemini Live 2.5 Flash"
model_family = "gemini-flash"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8000
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002
litellm_provider = "google"
providers = ["google"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-09-01"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
source = "modelsdev"

[models."gemini-live-2-5-flash".pricing."google"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002

[models."gemini-live-2-5-flash-preview-native-audio"]
display_name = "Gemini Live 2.5 Flash Preview Native Audio"
model_family = "gemini-flash"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 65536
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002
litellm_provider = "google"
providers = ["google"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-06-17"
supported_modalities = ["text", "audio", "video"]
supported_output_modalities = ["text", "audio"]
source = "modelsdev"

[models."gemini-live-2-5-flash-preview-native-audio".pricing."google"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002

[models."gemini-live-2.5-flash-preview-native-audio-09-2025"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 3e-7
output_cost_per_token = 0.000002
cache_read_input_token_cost = 7.5e-8
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
source = "https://ai.google.dev/gemini-api/docs/pricing"
input_cost_per_audio_token = 0.000003
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_audio_token = 0.000012
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true

[models."gemini-live-2.5-flash-preview-native-audio-09-2025".pricing."gemini"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000003
input_cost_per_token = 3e-7
output_cost_per_audio_token = 0.000012
output_cost_per_token = 0.000002
[models."gemini-live-2.5-flash-preview-native-audio-09-2025".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_audio_token = 0.000003
input_cost_per_token = 3e-7
output_cost_per_audio_token = 0.000012
output_cost_per_token = 0.000002

[models."gemini-pro"]
mode = "chat"
max_input_tokens = 32760
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini"]
supports_function_calling = true
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_video_per_second = 0.002
output_cost_per_character = 3.75e-7
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-pro".pricing."gemini"]
input_cost_per_token = 3.5e-7
input_cost_per_token_above_128k_tokens = 7e-7
output_cost_per_token = 0.00000105
output_cost_per_token_above_128k_tokens = 0.0000021
[models."gemini-pro".pricing."vertex_ai-language-models"]
input_cost_per_character = 1.25e-7
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
input_cost_per_video_per_second = 0.002
output_cost_per_character = 3.75e-7
output_cost_per_token = 0.0000015

[models."gemini-pro-experimental"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models"]
supports_function_calling = false
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/gemini-experimental"
input_cost_per_character = 0
output_cost_per_character = 0
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-pro-experimental".pricing."vertex_ai-language-models"]
input_cost_per_character = 0
input_cost_per_token = 0
output_cost_per_character = 0
output_cost_per_token = 0

[models."gemini-pro-latest"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "gemini"
providers = ["gemini"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_token_above_200k_tokens = 0.0000025
max_audio_length_hours = 8.4
max_audio_per_prompt = 1
max_images_per_prompt = 3000
max_pdf_size_mb = 30
max_video_length = 1
max_videos_per_prompt = 10
output_cost_per_token_above_200k_tokens = 0.000015
rpm = 2000
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supports_audio_input = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true
supports_web_search = true
tpm = 800000

[models."gemini-pro-latest".pricing."gemini"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_above_200k_tokens = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_above_200k_tokens = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_above_200k_tokens = 0.000015

[models."gemini-pro-vision"]
mode = "chat"
max_input_tokens = 30720
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "vertex_ai-vision-models"
providers = ["vertex_ai-vision-models", "gemini"]
supports_function_calling = true
supports_vision = true
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_image = 0.0025
max_images_per_prompt = 16
max_video_length = 2
max_videos_per_prompt = 1
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gemini-pro-vision".pricing."gemini"]
input_cost_per_token = 3.5e-7
input_cost_per_token_above_128k_tokens = 7e-7
output_cost_per_token = 0.00000105
output_cost_per_token_above_128k_tokens = 0.0000021
[models."gemini-pro-vision".pricing."vertex_ai-vision-models"]
input_cost_per_image = 0.0025
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."gemini-robotics-er-1.5-preview"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 65535
max_tokens = 65535
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
cache_read_input_token_cost = 0
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models", "gemini"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = false
supported_modalities = ["text", "image", "video", "audio"]
supported_output_modalities = ["text"]
source = "https://ai.google.dev/gemini-api/docs/models#gemini-robotics-er-1-5-preview"
input_cost_per_audio_token = 0.000001
output_cost_per_reasoning_token = 0.0000025
supported_endpoints = ["/v1/chat/completions", "/v1/completions"]
supports_audio_output = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_url_context = true
supports_web_search = true

[models."gemini-robotics-er-1.5-preview".pricing."gemini"]
cache_read_input_token_cost = 0
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025
[models."gemini-robotics-er-1.5-preview".pricing."vertex_ai-language-models"]
cache_read_input_token_cost = 0
input_cost_per_audio_token = 0.000001
input_cost_per_token = 3e-7
output_cost_per_reasoning_token = 0.0000025
output_cost_per_token = 0.0000025

[models."gemma-2-27b-it"]
display_name = "Gemma 2 27b It"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-06-24"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemma-2-27b-it".pricing."kilo"]
input_cost_per_token = 6.5e-7
output_cost_per_token = 6.5e-7

[models."gemma-2-2b-it"]
display_name = "Gemma 2 2b It"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia", "nebius"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-06"
release_date = "2024-07-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemma-2-2b-it".pricing."nebius"]
cache_read_input_token_cost = 2e-9
input_cost_per_token = 2e-8
output_cost_per_token = 6e-8

[models."gemma-2-9b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_tool_choice = true

[models."gemma-2-9b".pricing."vercel_ai_gateway"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."gemma-2-9b-it"]
display_name = "Gemma 2 9B"
model_family = "gemma"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
input_cost_per_token = 3e-8
output_cost_per_token = 9e-8
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-06"
release_date = "2024-06-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemma-2-9b-it".pricing."kilo"]
input_cost_per_token = 3e-8
output_cost_per_token = 9e-8
[models."gemma-2-9b-it".pricing."openrouter"]
input_cost_per_token = 3e-8
output_cost_per_token = 9e-8

[models."gemma-2-9b-it-fast"]
display_name = "Gemma-2-9b-it (Fast)"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 4096
input_cost_per_token = 3e-8
output_cost_per_token = 9e-8
cache_read_input_token_cost = 3e-9
litellm_provider = "nebius"
providers = ["nebius"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-06"
release_date = "2024-06-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemma-2-9b-it-fast".pricing."nebius"]
cache_read_input_token_cost = 3e-9
input_cost_per_token = 3e-8
output_cost_per_token = 9e-8

[models."gemma-3"]
display_name = "Google Gemma 3"
model_family = "gemma"
mode = "chat"
max_input_tokens = 125000
max_output_tokens = 4096
input_cost_per_token = 1.5e-7
output_cost_per_token = 3e-7
litellm_provider = "inference"
providers = ["inference"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2025-01-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemma-3".pricing."inference"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 3e-7

[models."gemma-3-12b-it"]
display_name = "Gemma 3 12b It"
model_family = "gemma"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 5e-8
output_cost_per_token = 1e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "helicone", "kilo", "novita", "nvidia", "openrouter"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2025-03-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gemma-3-12b-it".pricing."deepinfra"]
input_cost_per_token = 5e-8
output_cost_per_token = 1e-7
[models."gemma-3-12b-it".pricing."helicone"]
input_cost_per_token = 5e-8
output_cost_per_token = 1e-7
[models."gemma-3-12b-it".pricing."kilo"]
cache_read_input_token_cost = 1.5e-8
input_cost_per_token = 3e-8
output_cost_per_token = 1.0000000000000001e-7
[models."gemma-3-12b-it".pricing."novita"]
input_cost_per_token = 5e-8
output_cost_per_token = 1e-7
[models."gemma-3-12b-it".pricing."openrouter"]
input_cost_per_token = 3e-8
output_cost_per_token = 1.0000000000000001e-7

[models."gemma-3-12b-it:free"]
display_name = "Gemma 3 12B (free)"
model_family = "gemma"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-03-13"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemma-3-1b-it"]
display_name = "Gemma 3 1b It"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-03-10"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemma-3-27b"]
display_name = "Gemma 3 27B"
model_family = "gemma"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
litellm_provider = "privatemode-ai"
providers = ["privatemode-ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-03-12"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemma-3-27b-it"]
display_name = "Gemma-3-27B-IT"
model_family = "gemma"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 9e-8
output_cost_per_token = 1.6e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "gemini", "kilo", "nebius", "novita", "novita-ai", "nvidia", "openrouter", "scaleway", "stackit"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-12"
release_date = "2024-12-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gemma-3-27b-it".pricing."deepinfra"]
input_cost_per_token = 9e-8
output_cost_per_token = 1.6e-7
[models."gemma-3-27b-it".pricing."gemini"]
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0
[models."gemma-3-27b-it".pricing."kilo"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 4e-8
output_cost_per_token = 1.5e-7
[models."gemma-3-27b-it".pricing."nebius"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
[models."gemma-3-27b-it".pricing."novita"]
input_cost_per_token = 1.19e-7
output_cost_per_token = 2e-7
[models."gemma-3-27b-it".pricing."novita-ai"]
input_cost_per_token = 1.19e-7
output_cost_per_token = 2.0000000000000002e-7
[models."gemma-3-27b-it".pricing."openrouter"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.5e-7
[models."gemma-3-27b-it".pricing."scaleway"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 5e-7
[models."gemma-3-27b-it".pricing."stackit"]
input_cost_per_token = 4.9e-7
output_cost_per_token = 7.1e-7

[models."gemma-3-27b-it-fast"]
display_name = "Gemma-3-27b-it (Fast)"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7
cache_read_input_token_cost = 2e-8
litellm_provider = "nebius"
providers = ["nebius"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-10"
release_date = "2026-01-20"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemma-3-27b-it-fast".pricing."nebius"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7

[models."gemma-3-27b-it:free"]
display_name = "Gemma 3 27B (free)"
model_family = "gemma"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-03-12"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemma-3-4b-it"]
display_name = "Gemma 3 4B"
model_family = "gemma"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 4e-8
output_cost_per_token = 8e-8
litellm_provider = "deepinfra"
providers = ["deepinfra", "kilo", "openrouter"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-03-13"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."gemma-3-4b-it".pricing."deepinfra"]
input_cost_per_token = 4e-8
output_cost_per_token = 8e-8
[models."gemma-3-4b-it".pricing."kilo"]
input_cost_per_token = 1.703e-8
output_cost_per_token = 6.8154e-8
[models."gemma-3-4b-it".pricing."openrouter"]
input_cost_per_token = 1.703e-8
output_cost_per_token = 6.815e-8

[models."gemma-3-4b-it:free"]
display_name = "Gemma 3 4B (free)"
model_family = "gemma"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-03-13"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemma-3n-e2b-it"]
display_name = "Gemma 3n E2b It"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-06"
release_date = "2025-06-12"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemma-3n-e2b-it:free"]
display_name = "Gemma 3n 2B (free)"
model_family = "gemma"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 2000
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-06"
release_date = "2025-07-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemma-3n-e4b-it"]
display_name = "Gemma 3n E4b It"
model_family = "gemma"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia", "kilo", "openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-06"
release_date = "2025-06-03"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemma-3n-e4b-it".pricing."kilo"]
input_cost_per_token = 2e-8
output_cost_per_token = 4e-8
[models."gemma-3n-e4b-it".pricing."openrouter"]
input_cost_per_token = 2e-8
output_cost_per_token = 4e-8

[models."gemma-3n-e4b-it:free"]
display_name = "Gemma 3n 4B (free)"
model_family = "gemma"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 2000
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-06"
release_date = "2025-05-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemma-7b"]
mode = "chat"
max_input_tokens = 8000
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake"]

[models."gemma-7b-it"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "anyscale"
providers = ["anyscale", "groq"]
supports_function_calling = true
source = "https://docs.anyscale.com/preview/endpoints/text-generation/supported-models/google-gemma-7b-it"
supports_tool_choice = true

[models."gemma-7b-it".pricing."anyscale"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."gemma-7b-it".pricing."groq"]
input_cost_per_token = 5e-8
output_cost_per_token = 8e-8

[models."gemma2-9b-it"]
display_name = "Gemma 2 9B"
model_family = "gemma"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7
litellm_provider = "groq"
providers = ["groq", "helicone"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-06"
release_date = "2024-06-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gemma2-9b-it".pricing."groq"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7
[models."gemma2-9b-it".pricing."helicone"]
input_cost_per_token = 1e-8
output_cost_per_token = 3e-8

[models."gemma3-4b"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3e-8
output_cost_per_token = 8e-8
litellm_provider = "llamagate"
providers = ["llamagate"]
supports_function_calling = true
supports_vision = true
supports_response_schema = true

[models."gemma3-4b".pricing."llamagate"]
input_cost_per_token = 3e-8
output_cost_per_token = 8e-8

[models."gen3a_turbo"]
mode = "video_generation"
litellm_provider = "runwayml"
providers = ["runwayml"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["video"]
source = "https://docs.dev.runwayml.com/guides/pricing/"
output_cost_per_video_per_second = 0.05
supported_resolutions = ["1280x720", "720x1280"]

[models."gen3a_turbo".metadata]
comment = "5 credits per second @ $0.01 per credit = $0.05 per second"

[models."gen3a_turbo".pricing."runwayml"]
output_cost_per_video_per_second = 0.05

[models."gen4_aleph"]
mode = "video_generation"
litellm_provider = "runwayml"
providers = ["runwayml"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["video"]
source = "https://docs.dev.runwayml.com/guides/pricing/"
output_cost_per_video_per_second = 0.15
supported_resolutions = ["1280x720", "720x1280"]

[models."gen4_aleph".metadata]
comment = "15 credits per second @ $0.01 per credit = $0.15 per second"

[models."gen4_aleph".pricing."runwayml"]
output_cost_per_video_per_second = 0.15

[models."gen4_image"]
mode = "image_generation"
litellm_provider = "runwayml"
providers = ["runwayml"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["image"]
source = "https://docs.dev.runwayml.com/guides/pricing/"
input_cost_per_image = 0.05
output_cost_per_image = 0.05
supported_resolutions = ["1280x720", "1920x1080"]

[models."gen4_image".metadata]
comment = "5 credits per 720p image or 8 credits per 1080p image @ $0.01 per credit. Using 5 credits ($0.05) as base cost"

[models."gen4_image".pricing."runwayml"]
input_cost_per_image = 0.05
output_cost_per_image = 0.05

[models."gen4_image_turbo"]
mode = "image_generation"
litellm_provider = "runwayml"
providers = ["runwayml"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["image"]
source = "https://docs.dev.runwayml.com/guides/pricing/"
input_cost_per_image = 0.02
output_cost_per_image = 0.02
supported_resolutions = ["1280x720", "1920x1080"]

[models."gen4_image_turbo".metadata]
comment = "2 credits per image (any resolution) @ $0.01 per credit = $0.02 per image"

[models."gen4_image_turbo".pricing."runwayml"]
input_cost_per_image = 0.02
output_cost_per_image = 0.02

[models."gen4_turbo"]
mode = "video_generation"
litellm_provider = "runwayml"
providers = ["runwayml"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["video"]
source = "https://docs.dev.runwayml.com/guides/pricing/"
output_cost_per_video_per_second = 0.05
supported_resolutions = ["1280x720", "720x1280"]

[models."gen4_turbo".metadata]
comment = "5 credits per second @ $0.01 per credit = $0.05 per second"

[models."gen4_turbo".pricing."runwayml"]
output_cost_per_video_per_second = 0.05

[models."generative"]
mode = "audio_speech"
litellm_provider = "aws_polly"
providers = ["aws_polly"]
source = "https://aws.amazon.com/polly/pricing/"
input_cost_per_character = 0.00003
supported_endpoints = ["/v1/audio/speech"]

[models."generative".pricing."aws_polly"]
input_cost_per_character = 0.00003

[models."glm-4-32b-0414-128k"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "zai"
providers = ["zai"]
supports_function_calling = true
source = "https://docs.z.ai/guides/overview/pricing"
supports_tool_choice = true

[models."glm-4-32b-0414-128k".pricing."zai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."glm-4-6v"]
display_name = "GLM-4.6V"
model_family = "glm"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
input_cost_per_token = 1.45e-7
output_cost_per_token = 4.3e-7
litellm_provider = "302ai"
providers = ["302ai", "aihubmix", "zai", "zai-coding-plan", "zhipuai", "zhipuai-coding-plan"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-03"
release_date = "2025-12-08"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."glm-4-6v".pricing."302ai"]
input_cost_per_token = 1.45e-7
output_cost_per_token = 4.3e-7
[models."glm-4-6v".pricing."aihubmix"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 4.1e-7
[models."glm-4-6v".pricing."zai"]
input_cost_per_token = 3e-7
output_cost_per_token = 9.000000000000001e-7
[models."glm-4-6v".pricing."zhipuai"]
input_cost_per_token = 3e-7
output_cost_per_token = 9.000000000000001e-7

[models."glm-4-6v-flash"]
display_name = "GLM-4.6V-Flash"
model_family = "glm"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
litellm_provider = "zhipuai-coding-plan"
providers = ["zhipuai-coding-plan"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-12-08"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."glm-4-7-251222"]
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "volcengine"
providers = ["volcengine"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
supports_assistant_prefill = true
supports_tool_choice = true

[models."glm-4-7-251222".pricing."volcengine"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."glm-4-7-flash"]
display_name = "GLM-4.7-Flash"
model_family = "glm-flash"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 131072
litellm_provider = "zai-coding-plan"
providers = ["zai-coding-plan", "zai", "zhipuai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2026-01-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."glm-4-7-flashx"]
display_name = "GLM-4.7-FlashX"
model_family = "glm-flash"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 131072
input_cost_per_token = 7e-8
output_cost_per_token = 4.0000000000000003e-7
cache_read_input_token_cost = 1e-8
litellm_provider = "zai-coding-plan"
providers = ["zai-coding-plan"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2026-01-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."glm-4-7-flashx".pricing."zai-coding-plan"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 7e-8
output_cost_per_token = 4.0000000000000003e-7

[models."glm-4-7-free"]
display_name = "GLM-4.7 Free"
model_family = "glm-free"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
litellm_provider = "opencode"
providers = ["opencode"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-12-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."glm-4.5"]
display_name = "GLM-4.5"
model_family = "glm"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
litellm_provider = "zai"
providers = ["zai", "302ai", "zai-coding-plan", "zhipuai", "zhipuai-coding-plan"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-07-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://docs.z.ai/guides/overview/pricing"
supports_tool_choice = true

[models."glm-4.5".pricing."302ai"]
input_cost_per_token = 2.86e-7
output_cost_per_token = 0.0000011419999999999998
[models."glm-4.5".pricing."zai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."glm-4.5".pricing."zhipuai"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022

[models."glm-4.5-air"]
display_name = "GLM-4.5-Air"
model_family = "glm-air"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
input_cost_per_token = 2e-7
output_cost_per_token = 0.0000011
litellm_provider = "zai"
providers = ["zai", "zai-coding-plan", "zhipuai", "zhipuai-coding-plan"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://docs.z.ai/guides/overview/pricing"
supports_tool_choice = true

[models."glm-4.5-air".pricing."zai"]
input_cost_per_token = 2e-7
output_cost_per_token = 0.0000011
[models."glm-4.5-air".pricing."zhipuai"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000011

[models."glm-4.5-airx"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000045
litellm_provider = "zai"
providers = ["zai"]
supports_function_calling = true
source = "https://docs.z.ai/guides/overview/pricing"
supports_tool_choice = true

[models."glm-4.5-airx".pricing."zai"]
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000045

[models."glm-4.5-flash"]
display_name = "GLM-4.5-Flash"
model_family = "glm-flash"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "zai"
providers = ["zai", "zai-coding-plan", "zhipuai", "zhipuai-coding-plan"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://docs.z.ai/guides/overview/pricing"
supports_tool_choice = true

[models."glm-4.5-flash".pricing."zai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."glm-4.5-x"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
input_cost_per_token = 0.0000022
output_cost_per_token = 0.0000089
litellm_provider = "zai"
providers = ["zai"]
supports_function_calling = true
source = "https://docs.z.ai/guides/overview/pricing"
supports_tool_choice = true

[models."glm-4.5-x".pricing."zai"]
input_cost_per_token = 0.0000022
output_cost_per_token = 0.0000089

[models."glm-4.5v"]
display_name = "GLM-4.5V"
model_family = "glm"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000018
litellm_provider = "zai"
providers = ["zai", "302ai", "zai-coding-plan", "zhipuai", "zhipuai-coding-plan"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-07-29"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://docs.z.ai/guides/overview/pricing"
supports_tool_choice = true

[models."glm-4.5v".pricing."302ai"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 8.6e-7
[models."glm-4.5v".pricing."zai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000018
[models."glm-4.5v".pricing."zhipuai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000018000000000000001

[models."glm-4.6"]
display_name = "glm-4.6"
model_family = "glm"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 128000
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
cache_read_input_token_cost = 1.1e-7
cache_creation_input_token_cost = 0
litellm_provider = "zai"
providers = ["zai", "302ai", "helicone", "iflowcn", "opencode", "zai-coding-plan", "zhipuai", "zhipuai-coding-plan"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2025-03"
release_date = "2025-09-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://docs.z.ai/guides/overview/pricing"
supports_tool_choice = true

[models."glm-4.6".pricing."302ai"]
input_cost_per_token = 2.86e-7
output_cost_per_token = 0.0000011419999999999998
[models."glm-4.6".pricing."helicone"]
input_cost_per_token = 4.5e-7
output_cost_per_token = 0.0000015
[models."glm-4.6".pricing."opencode"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."glm-4.6".pricing."zai"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."glm-4.6".pricing."zhipuai"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022

[models."glm-4.7"]
display_name = "glm-4.7"
model_family = "glm"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 128000
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
cache_read_input_token_cost = 1.1e-7
cache_creation_input_token_cost = 0
litellm_provider = "zai"
providers = ["zai", "302ai", "aihubmix", "kuae-cloud-coding-plan", "moark", "opencode", "zai-coding-plan", "zhipuai", "zhipuai-coding-plan"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2025-06"
release_date = "2025-12-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://docs.z.ai/guides/overview/pricing"
supports_tool_choice = true

[models."glm-4.7".pricing."302ai"]
input_cost_per_token = 2.86e-7
output_cost_per_token = 0.0000011419999999999998
[models."glm-4.7".pricing."aihubmix"]
cache_read_input_token_cost = 5.480000000000001e-7
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.0000011
[models."glm-4.7".pricing."moark"]
input_cost_per_token = 0.0000035
output_cost_per_token = 0.000014
[models."glm-4.7".pricing."opencode"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."glm-4.7".pricing."zai"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."glm-4.7".pricing."zhipuai"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022

[models."glm-4p5"]
display_name = "GLM 4.5"
model_family = "glm"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 6.7e-7
output_cost_per_token = 0.00000246
litellm_provider = "cortecs"
providers = ["cortecs"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."glm-4p5".pricing."cortecs"]
input_cost_per_token = 6.7e-7
output_cost_per_token = 0.00000246

[models."glm-4p5-air"]
display_name = "GLM 4.5 Air"
model_family = "glm-air"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 2.2e-7
output_cost_per_token = 0.00000134
litellm_provider = "cortecs"
providers = ["cortecs"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-08-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."glm-4p5-air".pricing."cortecs"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 0.00000134

[models."glm-4p7"]
display_name = "GLM 4.7"
model_family = "glm"
mode = "chat"
max_input_tokens = 202800
max_output_tokens = 202800
max_tokens = 202800
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
cache_read_input_token_cost = 3e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai", "cortecs"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-12-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://fireworks.ai/models/fireworks/glm-4p7"
supports_response_schema = true
supports_tool_choice = true

[models."glm-4p7".pricing."cortecs"]
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.00000223
[models."glm-4p7".pricing."fireworks_ai"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022

[models."glm-5"]
display_name = "GLM-5"
model_family = "glm"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
litellm_provider = "zai-coding-plan"
providers = ["zai-coding-plan", "opencode", "vivgrid", "zai", "zhipuai", "zhipuai-coding-plan"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2026-02-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."glm-5".pricing."opencode"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003
[models."glm-5".pricing."vivgrid"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003
[models."glm-5".pricing."zai"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003
[models."glm-5".pricing."zhipuai"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003

[models."glm-5-free"]
display_name = "GLM-5 Free"
model_family = "glm-free"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
litellm_provider = "opencode"
providers = ["opencode"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2026-02-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."global.amazon.nova-2-lite-v1:0"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
cache_read_input_token_cost = 7.5e-8
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_response_schema = true
supports_video_input = true

[models."global.amazon.nova-2-lite-v1:0".pricing."bedrock_converse"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025

[models."global.anthropic.claude-haiku-4-5-20251001-v1:0"]
display_name = "Claude Haiku 4.5 (Global)"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
cache_read_input_token_cost = 1e-7
cache_creation_input_token_cost = 0.00000125
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-02-28"
release_date = "2025-10-15"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "https://aws.amazon.com/about-aws/whats-new/2025/10/claude-4-5-haiku-anthropic-amazon-bedrock"
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."global.anthropic.claude-haiku-4-5-20251001-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."global.anthropic.claude-haiku-4-5-20251001-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000125
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005

[models."global.anthropic.claude-opus-4-5-20251101-v1:0"]
display_name = "Claude Opus 4.5 (Global)"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 5e-7
cache_creation_input_token_cost = 0.00000625
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-11-24"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."global.anthropic.claude-opus-4-5-20251101-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."global.anthropic.claude-opus-4-5-20251101-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."global.anthropic.claude-opus-4-5-20251101-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000625
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025

[models."global.anthropic.claude-opus-4-6-v1"]
display_name = "Claude Opus 4.6 (Global)"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 5e-7
cache_creation_input_token_cost = 0.00000625
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-05"
release_date = "2026-02-05"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.0000125
cache_read_input_token_cost_above_200k_tokens = 0.000001
input_cost_per_token_above_200k_tokens = 0.00001
output_cost_per_token_above_200k_tokens = 0.0000375
supports_assistant_prefill = false
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."global.anthropic.claude-opus-4-6-v1".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."global.anthropic.claude-opus-4-6-v1".pricing."amazon-bedrock"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."global.anthropic.claude-opus-4-6-v1".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000625
cache_creation_input_token_cost_above_200k_tokens = 0.0000125
cache_read_input_token_cost = 5e-7
cache_read_input_token_cost_above_200k_tokens = 0.000001
input_cost_per_token = 0.000005
input_cost_per_token_above_200k_tokens = 0.00001
output_cost_per_token = 0.000025
output_cost_per_token_above_200k_tokens = 0.0000375

[models."global.anthropic.claude-sonnet-4-20250514-v1:0"]
display_name = "Claude Sonnet 4 (Global)"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-05-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."global.anthropic.claude-sonnet-4-20250514-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."global.anthropic.claude-sonnet-4-20250514-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."global.anthropic.claude-sonnet-4-20250514-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225

[models."global.anthropic.claude-sonnet-4-5-20250929-v1:0"]
display_name = "Claude Sonnet 4.5 (Global)"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-07-31"
release_date = "2025-09-29"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."global.anthropic.claude-sonnet-4-5-20250929-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."global.anthropic.claude-sonnet-4-5-20250929-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."global.anthropic.claude-sonnet-4-5-20250929-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225

[models."global.anthropic.claude-sonnet-4-6"]
display_name = "Claude Sonnet 4.6 (Global)"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-08"
release_date = "2026-02-17"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."global.anthropic.claude-sonnet-4-6".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."global.anthropic.claude-sonnet-4-6".pricing."amazon-bedrock"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."global.anthropic.claude-sonnet-4-6".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225

[models."google-gemma-3-27b-it"]
display_name = "Google Gemma 3 27B Instruct"
model_family = "gemma"
mode = "chat"
max_input_tokens = 198000
max_output_tokens = 49500
input_cost_per_token = 1.2e-7
output_cost_per_token = 2.0000000000000002e-7
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2025-11-04"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."google-gemma-3-27b-it".pricing."venice"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 2.0000000000000002e-7

[models."google.gemma-3-12b-it"]
display_name = "Google Gemma 3 12B"
model_family = "gemma"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 9e-8
output_cost_per_token = 2.9e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = false
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-12"
release_date = "2024-12-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."google.gemma-3-12b-it".pricing."amazon-bedrock"]
input_cost_per_token = 5e-8
output_cost_per_token = 1e-7
[models."google.gemma-3-12b-it".pricing."bedrock_converse"]
input_cost_per_token = 9e-8
output_cost_per_token = 2.9e-7

[models."google.gemma-3-27b-it"]
display_name = "Google Gemma 3 27B Instruct"
model_family = "gemma"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2.3e-7
output_cost_per_token = 3.8e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2025-07-27"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."google.gemma-3-27b-it".pricing."amazon-bedrock"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 2.0000000000000002e-7
[models."google.gemma-3-27b-it".pricing."bedrock_converse"]
input_cost_per_token = 2.3e-7
output_cost_per_token = 3.8e-7

[models."google.gemma-3-4b-it"]
display_name = "Gemma 3 4B IT"
model_family = "gemma"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 4e-8
output_cost_per_token = 8e-8
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
release_date = "2024-12-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."google.gemma-3-4b-it".pricing."amazon-bedrock"]
input_cost_per_token = 4e-8
output_cost_per_token = 8e-8
[models."google.gemma-3-4b-it".pricing."bedrock_converse"]
input_cost_per_token = 4e-8
output_cost_per_token = 8e-8

[models."gpt-3-5-turbo-raw"]
display_name = "GPT-3.5-Turbo-Raw"
model_family = "gpt"
mode = "chat"
max_input_tokens = 4524
max_output_tokens = 2048
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.0000014
litellm_provider = "poe"
providers = ["poe"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2023-09-27"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gpt-3-5-turbo-raw".pricing."poe"]
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.0000014

[models."gpt-3.5-turbo"]
display_name = "GPT-3.5 Turbo"
model_family = "gpt"
mode = "chat"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "azure"
providers = ["azure", "cloudflare-ai-gateway", "github_copilot", "kilo", "openai", "openrouter", "poe", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2021-09"
release_date = "2023-03-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo".pricing."azure"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
[models."gpt-3.5-turbo".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
[models."gpt-3.5-turbo".pricing."kilo"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
[models."gpt-3.5-turbo".pricing."openai"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
[models."gpt-3.5-turbo".pricing."openrouter"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
[models."gpt-3.5-turbo".pricing."poe"]
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.0000014
[models."gpt-3.5-turbo".pricing."vercel"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
[models."gpt-3.5-turbo".pricing."vercel_ai_gateway"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."gpt-3.5-turbo-0125"]
display_name = "GPT-3.5 Turbo 0125"
model_family = "gpt"
mode = "chat"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services", "openai"]
supports_function_calling = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2021-08"
release_date = "2024-01-25"
deprecation_date = "2025-03-31"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-0125".pricing."azure"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
[models."gpt-3.5-turbo-0125".pricing."azure-cognitive-services"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
[models."gpt-3.5-turbo-0125".pricing."openai"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."gpt-3.5-turbo-0301"]
display_name = "GPT-3.5 Turbo 0301"
model_family = "gpt"
mode = "chat"
max_input_tokens = 4097
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
litellm_provider = "openai"
providers = ["openai", "azure", "azure-cognitive-services"]
supports_function_calling = false
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2021-08"
release_date = "2023-03-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-0301".pricing."azure"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
[models."gpt-3.5-turbo-0301".pricing."azure-cognitive-services"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
[models."gpt-3.5-turbo-0301".pricing."openai"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002

[models."gpt-3.5-turbo-0613"]
display_name = "GPT-3.5 Turbo 0613"
model_family = "gpt"
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 4096
litellm_provider = "github_copilot"
providers = ["github_copilot", "azure", "azure-cognitive-services", "kilo", "openai"]
supports_function_calling = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2021-08"
release_date = "2023-06-13"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-0613".pricing."azure"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000004
[models."gpt-3.5-turbo-0613".pricing."azure-cognitive-services"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000004
[models."gpt-3.5-turbo-0613".pricing."kilo"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
[models."gpt-3.5-turbo-0613".pricing."openai"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002

[models."gpt-3.5-turbo-1106"]
display_name = "GPT-3.5 Turbo 1106"
model_family = "gpt"
mode = "chat"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
litellm_provider = "openai"
providers = ["openai", "azure", "azure-cognitive-services"]
supports_function_calling = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2021-08"
release_date = "2023-11-06"
deprecation_date = "2026-09-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-1106".pricing."azure"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
[models."gpt-3.5-turbo-1106".pricing."azure-cognitive-services"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
[models."gpt-3.5-turbo-1106".pricing."openai"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002

[models."gpt-3.5-turbo-16k"]
mode = "chat"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 16383
input_cost_per_token = 0.000003
output_cost_per_token = 0.000004
litellm_provider = "openai"
providers = ["openai", "openrouter"]
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-16k".pricing."openai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000004
[models."gpt-3.5-turbo-16k".pricing."openrouter"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000004

[models."gpt-3.5-turbo-16k-0613"]
mode = "chat"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000004
litellm_provider = "openai"
providers = ["openai"]
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-3.5-turbo-16k-0613".pricing."openai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000004

[models."gpt-3.5-turbo-instruct"]
display_name = "GPT-3.5 Turbo Instruct"
model_family = "gpt"
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
litellm_provider = "text-completion-openai"
providers = ["text-completion-openai", "azure", "azure-cognitive-services", "kilo", "poe", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2021-09"
release_date = "2023-03-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."gpt-3.5-turbo-instruct".pricing."azure"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
[models."gpt-3.5-turbo-instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
[models."gpt-3.5-turbo-instruct".pricing."kilo"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
[models."gpt-3.5-turbo-instruct".pricing."poe"]
input_cost_per_token = 0.0000014
output_cost_per_token = 0.0000018000000000000001
[models."gpt-3.5-turbo-instruct".pricing."text-completion-openai"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
[models."gpt-3.5-turbo-instruct".pricing."vercel"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
[models."gpt-3.5-turbo-instruct".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002

[models."gpt-3.5-turbo-instruct-0914"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 4097
max_tokens = 4097
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
litellm_provider = "azure"
providers = ["azure", "text-completion-openai"]

[models."gpt-3.5-turbo-instruct-0914".pricing."azure"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
[models."gpt-3.5-turbo-instruct-0914".pricing."text-completion-openai"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002

[models."gpt-35-turbo"]
mode = "chat"
max_input_tokens = 4097
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "azure"
providers = ["azure"]
supports_function_calling = true
supports_tool_choice = true

[models."gpt-35-turbo".pricing."azure"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."gpt-35-turbo-0125"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "azure"
providers = ["azure"]
supports_function_calling = true
deprecation_date = "2025-05-31"
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gpt-35-turbo-0125".pricing."azure"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."gpt-35-turbo-0301"]
mode = "chat"
max_input_tokens = 4097
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2e-7
output_cost_per_token = 0.000002
litellm_provider = "azure"
providers = ["azure"]
supports_function_calling = true
deprecation_date = "2025-02-13"
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gpt-35-turbo-0301".pricing."azure"]
input_cost_per_token = 2e-7
output_cost_per_token = 0.000002

[models."gpt-35-turbo-0613"]
mode = "chat"
max_input_tokens = 4097
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
litellm_provider = "azure"
providers = ["azure"]
supports_function_calling = true
deprecation_date = "2025-02-13"
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gpt-35-turbo-0613".pricing."azure"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002

[models."gpt-35-turbo-1106"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
litellm_provider = "azure"
providers = ["azure"]
supports_function_calling = true
deprecation_date = "2025-03-31"
supports_parallel_function_calling = true
supports_tool_choice = true

[models."gpt-35-turbo-1106".pricing."azure"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002

[models."gpt-35-turbo-16k"]
mode = "chat"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000004
litellm_provider = "azure"
providers = ["azure"]
supports_tool_choice = true

[models."gpt-35-turbo-16k".pricing."azure"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000004

[models."gpt-35-turbo-16k-0613"]
mode = "chat"
max_input_tokens = 16385
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000004
litellm_provider = "azure"
providers = ["azure"]
supports_function_calling = true
supports_tool_choice = true

[models."gpt-35-turbo-16k-0613".pricing."azure"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000004

[models."gpt-35-turbo-instruct"]
mode = "completion"
max_input_tokens = 4097
max_tokens = 4097
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
litellm_provider = "azure"
providers = ["azure"]

[models."gpt-35-turbo-instruct".pricing."azure"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002

[models."gpt-35-turbo-instruct-0914"]
mode = "completion"
max_input_tokens = 4097
max_tokens = 4097
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002
litellm_provider = "azure"
providers = ["azure"]

[models."gpt-35-turbo-instruct-0914".pricing."azure"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000002

[models."gpt-4"]
display_name = "GPT-4"
model_family = "gpt"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 8192
input_cost_per_token = 0.00003
output_cost_per_token = 0.00006
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services", "cloudflare-ai-gateway", "github_copilot", "kilo", "openai", "openrouter"]
supports_function_calling = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2023-11"
release_date = "2023-03-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4".pricing."azure"]
input_cost_per_token = 0.00003
output_cost_per_token = 0.00006
[models."gpt-4".pricing."azure-cognitive-services"]
input_cost_per_token = 0.00006
output_cost_per_token = 0.00012
[models."gpt-4".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 0.00003
output_cost_per_token = 0.00006
[models."gpt-4".pricing."kilo"]
input_cost_per_token = 0.00003
output_cost_per_token = 0.00006
[models."gpt-4".pricing."openai"]
input_cost_per_token = 0.00003
output_cost_per_token = 0.00006
[models."gpt-4".pricing."openrouter"]
input_cost_per_token = 0.00003
output_cost_per_token = 0.00006

[models."gpt-4-0125-preview"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_prompt_caching = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-0125-preview".pricing."azure"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
[models."gpt-4-0125-preview".pricing."openai"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003

[models."gpt-4-0314"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00003
output_cost_per_token = 0.00006
litellm_provider = "openai"
providers = ["openai"]
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-0314".pricing."openai"]
input_cost_per_token = 0.00003
output_cost_per_token = 0.00006

[models."gpt-4-0613"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00003
output_cost_per_token = 0.00006
litellm_provider = "azure"
providers = ["azure", "github_copilot", "openai"]
supports_function_calling = true
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-0613".pricing."azure"]
input_cost_per_token = 0.00003
output_cost_per_token = 0.00006
[models."gpt-4-0613".pricing."openai"]
input_cost_per_token = 0.00003
output_cost_per_token = 0.00006

[models."gpt-4-1106-preview"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_prompt_caching = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-1106-preview".pricing."azure"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
[models."gpt-4-1106-preview".pricing."openai"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003

[models."gpt-4-1106-vision-preview"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
deprecation_date = "2024-12-06"
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-1106-vision-preview".pricing."openai"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003

[models."gpt-4-32k"]
display_name = "GPT-4 32K"
model_family = "gpt"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00006
output_cost_per_token = 0.00012
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services", "openai"]
supports_function_calling = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2023-11"
release_date = "2023-03-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-32k".pricing."azure"]
input_cost_per_token = 0.00006
output_cost_per_token = 0.00012
[models."gpt-4-32k".pricing."azure-cognitive-services"]
input_cost_per_token = 0.00006
output_cost_per_token = 0.00012
[models."gpt-4-32k".pricing."openai"]
input_cost_per_token = 0.00006
output_cost_per_token = 0.00012

[models."gpt-4-32k-0314"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00006
output_cost_per_token = 0.00012
litellm_provider = "openai"
providers = ["openai"]
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-32k-0314".pricing."openai"]
input_cost_per_token = 0.00006
output_cost_per_token = 0.00012

[models."gpt-4-32k-0613"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00006
output_cost_per_token = 0.00012
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_prompt_caching = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-32k-0613".pricing."azure"]
input_cost_per_token = 0.00006
output_cost_per_token = 0.00012
[models."gpt-4-32k-0613".pricing."openai"]
input_cost_per_token = 0.00006
output_cost_per_token = 0.00012

[models."gpt-4-classic"]
display_name = "GPT-4-Classic"
model_family = "gpt"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 4096
input_cost_per_token = 0.000027
output_cost_per_token = 0.000054
litellm_provider = "poe"
providers = ["poe"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-03-25"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gpt-4-classic".pricing."poe"]
input_cost_per_token = 0.000027
output_cost_per_token = 0.000054

[models."gpt-4-classic-0314"]
display_name = "GPT-4-Classic-0314"
model_family = "gpt"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 4096
input_cost_per_token = 0.000027
output_cost_per_token = 0.000054
litellm_provider = "poe"
providers = ["poe"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-08-26"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gpt-4-classic-0314".pricing."poe"]
input_cost_per_token = 0.000027
output_cost_per_token = 0.000054

[models."gpt-4-o-preview"]
mode = "chat"
max_input_tokens = 64000
max_output_tokens = 4096
max_tokens = 4096
litellm_provider = "github_copilot"
providers = ["github_copilot"]
supports_function_calling = true
supports_parallel_function_calling = true

[models."gpt-4-turbo"]
display_name = "GPT-4 Turbo"
model_family = "gpt"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services", "cloudflare-ai-gateway", "kilo", "openai", "poe", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2023-12"
release_date = "2023-11-06"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-turbo".pricing."azure"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
[models."gpt-4-turbo".pricing."azure-cognitive-services"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
[models."gpt-4-turbo".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
[models."gpt-4-turbo".pricing."kilo"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
[models."gpt-4-turbo".pricing."openai"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
[models."gpt-4-turbo".pricing."poe"]
input_cost_per_token = 0.000009
output_cost_per_token = 0.000027
[models."gpt-4-turbo".pricing."vercel"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
[models."gpt-4-turbo".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003

[models."gpt-4-turbo-2024-04-09"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-turbo-2024-04-09".pricing."azure"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
[models."gpt-4-turbo-2024-04-09".pricing."openai"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003

[models."gpt-4-turbo-preview"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_prompt_caching = true
supports_pdf_input = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-turbo-preview".pricing."openai"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003

[models."gpt-4-turbo-vision"]
display_name = "GPT-4 Turbo Vision"
model_family = "gpt"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-11"
release_date = "2023-11-06"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gpt-4-turbo-vision".pricing."azure"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
[models."gpt-4-turbo-vision".pricing."azure-cognitive-services"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003

[models."gpt-4-turbo-vision-preview"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
litellm_provider = "azure"
providers = ["azure"]
supports_vision = true
supports_tool_choice = true

[models."gpt-4-turbo-vision-preview".pricing."azure"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003

[models."gpt-4-vision-preview"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
deprecation_date = "2024-12-06"
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4-vision-preview".pricing."openai"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00003

[models."gpt-4.1"]
display_name = "gpt-4.1"
model_family = "gpt"
mode = "chat"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
cache_read_input_token_cost = 5e-7
litellm_provider = "azure"
providers = ["azure", "302ai", "abacus", "aihubmix", "azure-cognitive-services", "cortecs", "fastrouter", "github_copilot", "github-copilot", "github-models", "helicone", "kilo", "openai", "openrouter", "poe", "replicate", "requesty", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-04-14"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_token_batches = 0.000001
output_cost_per_token_batches = 0.000004
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = false

[models."gpt-4.1".pricing."302ai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."gpt-4.1".pricing."abacus"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."gpt-4.1".pricing."aihubmix"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."gpt-4.1".pricing."azure"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
output_cost_per_token = 0.000008
output_cost_per_token_batches = 0.000004
[models."gpt-4.1".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."gpt-4.1".pricing."cortecs"]
input_cost_per_token = 0.0000023540000000000002
output_cost_per_token = 0.000009417
[models."gpt-4.1".pricing."fastrouter"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."gpt-4.1".pricing."helicone"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."gpt-4.1".pricing."kilo"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."gpt-4.1".pricing."openai"]
cache_read_input_token_cost = 5e-7
cache_read_input_token_cost_priority = 8.75e-7
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
input_cost_per_token_priority = 0.0000035
output_cost_per_token = 0.000008
output_cost_per_token_batches = 0.000004
output_cost_per_token_priority = 0.000014
[models."gpt-4.1".pricing."openrouter"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."gpt-4.1".pricing."poe"]
cache_read_input_token_cost = 4.5000000000000003e-7
input_cost_per_token = 0.0000018000000000000001
output_cost_per_token = 0.0000072000000000000005
[models."gpt-4.1".pricing."replicate"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."gpt-4.1".pricing."requesty"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."gpt-4.1".pricing."vercel"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."gpt-4.1".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008

[models."gpt-4.1-2025-04-14"]
mode = "chat"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
cache_read_input_token_cost = 5e-7
litellm_provider = "azure"
providers = ["azure", "github_copilot", "openai"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
deprecation_date = "2026-11-04"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_token_batches = 0.000001
output_cost_per_token_batches = 0.000004
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = false

[models."gpt-4.1-2025-04-14".pricing."azure"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
output_cost_per_token = 0.000008
output_cost_per_token_batches = 0.000004
[models."gpt-4.1-2025-04-14".pricing."azure/us"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000022
input_cost_per_token_batches = 0.0000011
output_cost_per_token = 0.0000088
output_cost_per_token_batches = 0.0000044
[models."gpt-4.1-2025-04-14".pricing."openai"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
output_cost_per_token = 0.000008
output_cost_per_token_batches = 0.000004

[models."gpt-4.1-mini"]
display_name = "gpt-4.1-mini"
model_family = "gpt-mini"
mode = "chat"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000016
cache_read_input_token_cost = 1e-7
litellm_provider = "azure"
providers = ["azure", "302ai", "abacus", "aihubmix", "azure-cognitive-services", "github-models", "helicone", "kilo", "openai", "openrouter", "poe", "replicate", "requesty", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-04-14"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_token_batches = 2e-7
output_cost_per_token_batches = 8e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = false

[models."gpt-4.1-mini".pricing."302ai"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000016000000000000001
[models."gpt-4.1-mini".pricing."abacus"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000016000000000000001
[models."gpt-4.1-mini".pricing."aihubmix"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000016000000000000001
[models."gpt-4.1-mini".pricing."azure"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 4e-7
input_cost_per_token_batches = 2e-7
output_cost_per_token = 0.0000016
output_cost_per_token_batches = 8e-7
[models."gpt-4.1-mini".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000016000000000000001
[models."gpt-4.1-mini".pricing."helicone"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000016
[models."gpt-4.1-mini".pricing."kilo"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000016000000000000001
[models."gpt-4.1-mini".pricing."openai"]
cache_read_input_token_cost = 1e-7
cache_read_input_token_cost_priority = 1.75e-7
input_cost_per_token = 4e-7
input_cost_per_token_batches = 2e-7
input_cost_per_token_priority = 7e-7
output_cost_per_token = 0.0000016
output_cost_per_token_batches = 8e-7
output_cost_per_token_priority = 0.0000028
[models."gpt-4.1-mini".pricing."openrouter"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000016
[models."gpt-4.1-mini".pricing."poe"]
cache_read_input_token_cost = 9e-8
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.0000014
[models."gpt-4.1-mini".pricing."replicate"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000016
[models."gpt-4.1-mini".pricing."requesty"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000016000000000000001
[models."gpt-4.1-mini".pricing."vercel"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000016000000000000001
[models."gpt-4.1-mini".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 1e-7
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000016

[models."gpt-4.1-mini-2025-04-14"]
display_name = "OpenAI GPT-4.1 Mini"
model_family = "gpt-mini"
mode = "chat"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000016
cache_read_input_token_cost = 1e-7
litellm_provider = "azure"
providers = ["azure", "helicone", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-04-14"
deprecation_date = "2026-11-04"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_token_batches = 2e-7
output_cost_per_token_batches = 8e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = false

[models."gpt-4.1-mini-2025-04-14".pricing."azure"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 4e-7
input_cost_per_token_batches = 2e-7
output_cost_per_token = 0.0000016
output_cost_per_token_batches = 8e-7
[models."gpt-4.1-mini-2025-04-14".pricing."azure/us"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 4.4e-7
input_cost_per_token_batches = 2.2e-7
output_cost_per_token = 0.00000176
output_cost_per_token_batches = 8.8e-7
[models."gpt-4.1-mini-2025-04-14".pricing."helicone"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000016
[models."gpt-4.1-mini-2025-04-14".pricing."openai"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 4e-7
input_cost_per_token_batches = 2e-7
output_cost_per_token = 0.0000016
output_cost_per_token_batches = 8e-7

[models."gpt-4.1-nano"]
display_name = "gpt-4.1-nano"
model_family = "gpt-nano"
mode = "chat"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
cache_read_input_token_cost = 2.5e-8
litellm_provider = "azure"
providers = ["azure", "302ai", "abacus", "aihubmix", "azure-cognitive-services", "github-models", "helicone", "kilo", "openai", "openrouter", "poe", "replicate", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-04-14"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_token_batches = 5e-8
output_cost_per_token_batches = 2e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4.1-nano".pricing."302ai"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gpt-4.1-nano".pricing."abacus"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gpt-4.1-nano".pricing."aihubmix"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gpt-4.1-nano".pricing."azure"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 1e-7
input_cost_per_token_batches = 5e-8
output_cost_per_token = 4e-7
output_cost_per_token_batches = 2e-7
[models."gpt-4.1-nano".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gpt-4.1-nano".pricing."helicone"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
[models."gpt-4.1-nano".pricing."kilo"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gpt-4.1-nano".pricing."openai"]
cache_read_input_token_cost = 2.5e-8
cache_read_input_token_cost_priority = 5e-8
input_cost_per_token = 1e-7
input_cost_per_token_batches = 5e-8
input_cost_per_token_priority = 2e-7
output_cost_per_token = 4e-7
output_cost_per_token_batches = 2e-7
output_cost_per_token_priority = 8e-7
[models."gpt-4.1-nano".pricing."openrouter"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
[models."gpt-4.1-nano".pricing."poe"]
cache_read_input_token_cost = 2.2e-8
input_cost_per_token = 9e-8
output_cost_per_token = 3.6e-7
[models."gpt-4.1-nano".pricing."replicate"]
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
[models."gpt-4.1-nano".pricing."vercel"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."gpt-4.1-nano".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7

[models."gpt-4.1-nano-2025-04-14"]
mode = "chat"
max_input_tokens = 1047576
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
cache_read_input_token_cost = 2.5e-8
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
deprecation_date = "2026-11-04"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_token_batches = 5e-8
output_cost_per_token_batches = 2e-7
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4.1-nano-2025-04-14".pricing."azure"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 1e-7
input_cost_per_token_batches = 5e-8
output_cost_per_token = 4e-7
output_cost_per_token_batches = 2e-7
[models."gpt-4.1-nano-2025-04-14".pricing."azure/us"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 1.1e-7
input_cost_per_token_batches = 6e-8
output_cost_per_token = 4.4e-7
output_cost_per_token_batches = 2.2e-7
[models."gpt-4.1-nano-2025-04-14".pricing."openai"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 1e-7
input_cost_per_token_batches = 5e-8
output_cost_per_token = 4e-7
output_cost_per_token_batches = 2e-7

[models."gpt-4.5-preview"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.000075
output_cost_per_token = 0.00015
cache_read_input_token_cost = 0.0000375
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
input_cost_per_token_batches = 0.0000375
output_cost_per_token_batches = 0.000075
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4.5-preview".pricing."azure"]
cache_read_input_token_cost = 0.0000375
input_cost_per_token = 0.000075
input_cost_per_token_batches = 0.0000375
output_cost_per_token = 0.00015
output_cost_per_token_batches = 0.000075
[models."gpt-4.5-preview".pricing."openai"]
cache_read_input_token_cost = 0.0000375
input_cost_per_token = 0.000075
input_cost_per_token_batches = 0.0000375
output_cost_per_token = 0.00015
output_cost_per_token_batches = 0.000075

[models."gpt-4.5-preview-2025-02-27"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.000075
output_cost_per_token = 0.00015
cache_read_input_token_cost = 0.0000375
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
deprecation_date = "2025-07-14"
input_cost_per_token_batches = 0.0000375
output_cost_per_token_batches = 0.000075
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4.5-preview-2025-02-27".pricing."openai"]
cache_read_input_token_cost = 0.0000375
input_cost_per_token = 0.000075
input_cost_per_token_batches = 0.0000375
output_cost_per_token = 0.00015
output_cost_per_token_batches = 0.000075

[models."gpt-41-copilot"]
mode = "completion"
litellm_provider = "github_copilot"
providers = ["github_copilot"]

[models."gpt-4o"]
display_name = "GPT-4o"
model_family = "gpt"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
cache_read_input_token_cost = 0.00000125
litellm_provider = "azure"
providers = ["azure", "302ai", "aihubmix", "azure-cognitive-services", "cloudflare-ai-gateway", "firmware", "github_copilot", "github-copilot", "github-models", "gmi", "helicone", "kilo", "openai", "openrouter", "replicate", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2023-09"
release_date = "2024-05-13"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o".pricing."302ai"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o".pricing."aihubmix"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o".pricing."azure"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o".pricing."firmware"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o".pricing."gmi"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o".pricing."helicone"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o".pricing."kilo"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o".pricing."openai"]
cache_read_input_token_cost = 0.00000125
cache_read_input_token_cost_priority = 0.000002125
input_cost_per_token = 0.0000025
input_cost_per_token_batches = 0.00000125
input_cost_per_token_priority = 0.00000425
output_cost_per_token = 0.00001
output_cost_per_token_batches = 0.000005
output_cost_per_token_priority = 0.000017
[models."gpt-4o".pricing."openrouter"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o".pricing."replicate"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o".pricing."vercel"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001

[models."gpt-4o-2024-05-13"]
display_name = "GPT-4o (2024-05-13)"
model_family = "gpt"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000005
output_cost_per_token = 0.000015
litellm_provider = "azure"
providers = ["azure", "github_copilot", "kilo", "openai", "openrouter"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2023-09"
release_date = "2024-05-13"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-2024-05-13".pricing."azure"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000015
[models."gpt-4o-2024-05-13".pricing."kilo"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000015
[models."gpt-4o-2024-05-13".pricing."openai"]
input_cost_per_token = 0.000005
input_cost_per_token_batches = 0.0000025
input_cost_per_token_priority = 0.00000875
output_cost_per_token = 0.000015
output_cost_per_token_batches = 0.0000075
output_cost_per_token_priority = 0.00002625
[models."gpt-4o-2024-05-13".pricing."openrouter"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000015

[models."gpt-4o-2024-08-06"]
display_name = "GPT-4o (2024-08-06)"
model_family = "gpt"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.00000275
output_cost_per_token = 0.000011
cache_read_input_token_cost = 0.000001375
litellm_provider = "azure"
providers = ["azure", "github_copilot", "kilo", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2023-09"
release_date = "2024-08-06"
deprecation_date = "2026-02-27"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-2024-08-06".pricing."azure"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o-2024-08-06".pricing."azure/eu"]
cache_read_input_token_cost = 0.000001375
input_cost_per_token = 0.00000275
output_cost_per_token = 0.000011
[models."gpt-4o-2024-08-06".pricing."azure/global"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o-2024-08-06".pricing."azure/global-standard"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o-2024-08-06".pricing."azure/us"]
cache_read_input_token_cost = 0.000001375
input_cost_per_token = 0.00000275
output_cost_per_token = 0.000011
[models."gpt-4o-2024-08-06".pricing."kilo"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o-2024-08-06".pricing."openai"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
input_cost_per_token_batches = 0.00000125
output_cost_per_token = 0.00001
output_cost_per_token_batches = 0.000005

[models."gpt-4o-2024-11-20"]
display_name = "GPT-4o (2024-11-20)"
model_family = "gpt"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.00000275
output_cost_per_token = 0.000011
cache_creation_input_token_cost = 0.00000138
litellm_provider = "azure"
providers = ["azure", "abacus", "github_copilot", "kilo", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-11-20"
deprecation_date = "2026-03-01"
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-2024-11-20".pricing."abacus"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o-2024-11-20".pricing."azure"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.00000275
output_cost_per_token = 0.000011
[models."gpt-4o-2024-11-20".pricing."azure/eu"]
cache_creation_input_token_cost = 0.00000138
input_cost_per_token = 0.00000275
output_cost_per_token = 0.000011
[models."gpt-4o-2024-11-20".pricing."azure/global"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o-2024-11-20".pricing."azure/global-standard"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o-2024-11-20".pricing."azure/us"]
cache_creation_input_token_cost = 0.00000138
input_cost_per_token = 0.00000275
output_cost_per_token = 0.000011
[models."gpt-4o-2024-11-20".pricing."kilo"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o-2024-11-20".pricing."openai"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
input_cost_per_token_batches = 0.00000125
output_cost_per_token = 0.00001
output_cost_per_token_batches = 0.000005

[models."gpt-4o-audio-preview"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
input_cost_per_audio_token = 0.00004
output_cost_per_audio_token = 0.00008
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-audio-preview".pricing."openai"]
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.0000025
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00001

[models."gpt-4o-audio-preview-2024-10-01"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
input_cost_per_audio_token = 0.00004
output_cost_per_audio_token = 0.00008
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-audio-preview-2024-10-01".pricing."openai"]
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.0000025
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00001

[models."gpt-4o-audio-preview-2024-12-17"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = false
supports_reasoning = false
supports_prompt_caching = false
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
input_cost_per_audio_token = 0.00004
output_cost_per_audio_token = 0.00008
supported_endpoints = ["/v1/chat/completions"]
supports_audio_input = true
supports_audio_output = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = false
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-audio-preview-2024-12-17".pricing."azure"]
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.0000025
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00001
[models."gpt-4o-audio-preview-2024-12-17".pricing."openai"]
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.0000025
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00001

[models."gpt-4o-audio-preview-2025-06-03"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
input_cost_per_audio_token = 0.00004
output_cost_per_audio_token = 0.00008
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-audio-preview-2025-06-03".pricing."openai"]
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.0000025
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00001

[models."gpt-4o-aug"]
display_name = "GPT-4o-Aug"
model_family = "gpt"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 0.0000022
output_cost_per_token = 0.000009
cache_read_input_token_cost = 0.0000011
litellm_provider = "poe"
providers = ["poe"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-11-21"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gpt-4o-aug".pricing."poe"]
cache_read_input_token_cost = 0.0000011
input_cost_per_token = 0.0000022
output_cost_per_token = 0.000009

[models."gpt-4o-mini"]
display_name = "GPT-4o Mini"
model_family = "gpt"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "azure"
providers = ["azure", "abacus", "azure-cognitive-services", "cloudflare-ai-gateway", "github_copilot", "github-models", "gmi", "helicone", "kilo", "openai", "openrouter", "poe", "replicate", "requesty", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-07-18"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini".pricing."abacus"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-4o-mini".pricing."azure"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.65e-7
output_cost_per_token = 6.6e-7
[models."gpt-4o-mini".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-4o-mini".pricing."azure/global-standard"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-4o-mini".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-4o-mini".pricing."gmi"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-4o-mini".pricing."helicone"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-4o-mini".pricing."kilo"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-4o-mini".pricing."openai"]
cache_read_input_token_cost = 7.5e-8
cache_read_input_token_cost_priority = 1.25e-7
input_cost_per_token = 1.5e-7
input_cost_per_token_batches = 7.5e-8
input_cost_per_token_priority = 2.5e-7
output_cost_per_token = 6e-7
output_cost_per_token_batches = 3e-7
output_cost_per_token_priority = 0.000001
[models."gpt-4o-mini".pricing."openrouter"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-4o-mini".pricing."poe"]
cache_read_input_token_cost = 6.8e-8
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.4e-7
[models."gpt-4o-mini".pricing."replicate"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-4o-mini".pricing."requesty"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-4o-mini".pricing."vercel"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-4o-mini".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."gpt-4o-mini-2024-07-18"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1.65e-7
output_cost_per_token = 6.6e-7
cache_read_input_token_cost = 8.3e-8
litellm_provider = "azure"
providers = ["azure", "github_copilot", "openai"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini-2024-07-18".pricing."azure"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.65e-7
output_cost_per_token = 6.6e-7
[models."gpt-4o-mini-2024-07-18".pricing."azure/eu"]
cache_read_input_token_cost = 8.3e-8
input_cost_per_token = 1.65e-7
output_cost_per_token = 6.6e-7
[models."gpt-4o-mini-2024-07-18".pricing."azure/us"]
cache_read_input_token_cost = 8.3e-8
input_cost_per_token = 1.65e-7
output_cost_per_token = 6.6e-7
[models."gpt-4o-mini-2024-07-18".pricing."openai"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.5e-7
input_cost_per_token_batches = 7.5e-8
output_cost_per_token = 6e-7
output_cost_per_token_batches = 3e-7

[models."gpt-4o-mini-audio-preview"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
input_cost_per_audio_token = 0.00001
output_cost_per_audio_token = 0.00002
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini-audio-preview".pricing."openai"]
input_cost_per_audio_token = 0.00001
input_cost_per_token = 1.5e-7
output_cost_per_audio_token = 0.00002
output_cost_per_token = 6e-7

[models."gpt-4o-mini-audio-preview-2024-12-17"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = false
supports_reasoning = false
supports_prompt_caching = false
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
input_cost_per_audio_token = 0.00004
output_cost_per_audio_token = 0.00008
supported_endpoints = ["/v1/chat/completions"]
supports_audio_input = true
supports_audio_output = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = false
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini-audio-preview-2024-12-17".pricing."azure"]
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.0000025
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00001
[models."gpt-4o-mini-audio-preview-2024-12-17".pricing."openai"]
input_cost_per_audio_token = 0.00001
input_cost_per_token = 1.5e-7
output_cost_per_audio_token = 0.00002
output_cost_per_token = 6e-7

[models."gpt-4o-mini-realtime-preview"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000024
cache_read_input_token_cost = 3e-7
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
cache_creation_input_audio_token_cost = 3e-7
input_cost_per_audio_token = 0.00001
output_cost_per_audio_token = 0.00002
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini-realtime-preview".pricing."openai"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_token_cost = 3e-7
input_cost_per_audio_token = 0.00001
input_cost_per_token = 6e-7
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024

[models."gpt-4o-mini-realtime-preview-2024-12-17"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 6.6e-7
output_cost_per_token = 0.00000264
cache_read_input_token_cost = 3.3e-7
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
cache_creation_input_audio_token_cost = 3.3e-7
input_cost_per_audio_token = 0.000011
output_cost_per_audio_token = 0.000022
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini-realtime-preview-2024-12-17".pricing."azure"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_token_cost = 3e-7
input_cost_per_audio_token = 0.00001
input_cost_per_token = 6e-7
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024
[models."gpt-4o-mini-realtime-preview-2024-12-17".pricing."azure/eu"]
cache_creation_input_audio_token_cost = 3.3e-7
cache_read_input_token_cost = 3.3e-7
input_cost_per_audio_token = 0.000011
input_cost_per_token = 6.6e-7
output_cost_per_audio_token = 0.000022
output_cost_per_token = 0.00000264
[models."gpt-4o-mini-realtime-preview-2024-12-17".pricing."azure/us"]
cache_creation_input_audio_token_cost = 3.3e-7
cache_read_input_token_cost = 3.3e-7
input_cost_per_audio_token = 0.000011
input_cost_per_token = 6.6e-7
output_cost_per_audio_token = 0.000022
output_cost_per_token = 0.00000264
[models."gpt-4o-mini-realtime-preview-2024-12-17".pricing."openai"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_token_cost = 3e-7
input_cost_per_audio_token = 0.00001
input_cost_per_token = 6e-7
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024

[models."gpt-4o-mini-search"]
display_name = "GPT-4o-mini-Search"
model_family = "gpt-mini"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.4e-7
litellm_provider = "poe"
providers = ["poe"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-03-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gpt-4o-mini-search".pricing."poe"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.4e-7

[models."gpt-4o-mini-search-preview"]
display_name = "GPT 4o Mini Search Preview"
model_family = "gpt-mini"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
cache_read_input_token_cost = 7.5e-8
litellm_provider = "openai"
providers = ["openai", "kilo", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2023-09"
release_date = "2025-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
input_cost_per_token_batches = 7.5e-8
output_cost_per_token_batches = 3e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gpt-4o-mini-search-preview".search_context_cost_per_query]
search_context_size_high = 0.03
search_context_size_low = 0.025
search_context_size_medium = 0.0275

[models."gpt-4o-mini-search-preview".pricing."kilo"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-4o-mini-search-preview".pricing."openai"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.5e-7
input_cost_per_token_batches = 7.5e-8
output_cost_per_token = 6e-7
output_cost_per_token_batches = 3e-7
[models."gpt-4o-mini-search-preview".pricing."vercel"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."gpt-4o-mini-search-preview-2025-03-11"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
cache_read_input_token_cost = 7.5e-8
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
input_cost_per_token_batches = 7.5e-8
output_cost_per_token_batches = 3e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-mini-search-preview-2025-03-11".pricing."openai"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.5e-7
input_cost_per_token_batches = 7.5e-8
output_cost_per_token = 6e-7
output_cost_per_token_batches = 3e-7

[models."gpt-4o-mini-transcribe"]
mode = "audio_transcription"
max_input_tokens = 16000
max_output_tokens = 2000
input_cost_per_token = 0.00000125
output_cost_per_token = 0.000005
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_audio_token = 0.000003
supported_endpoints = ["/v1/audio/transcriptions"]

[models."gpt-4o-mini-transcribe".pricing."azure"]
input_cost_per_audio_token = 0.000003
input_cost_per_token = 0.00000125
output_cost_per_token = 0.000005
[models."gpt-4o-mini-transcribe".pricing."openai"]
input_cost_per_audio_token = 0.000003
input_cost_per_token = 0.00000125
output_cost_per_token = 0.000005

[models."gpt-4o-mini-transcribe-2025-03-20"]
mode = "audio_transcription"
max_input_tokens = 16000
max_output_tokens = 2000
input_cost_per_token = 0.00000125
output_cost_per_token = 0.000005
litellm_provider = "openai"
providers = ["openai"]
input_cost_per_audio_token = 0.000003
supported_endpoints = ["/v1/audio/transcriptions"]

[models."gpt-4o-mini-transcribe-2025-03-20".pricing."openai"]
input_cost_per_audio_token = 0.000003
input_cost_per_token = 0.00000125
output_cost_per_token = 0.000005

[models."gpt-4o-mini-transcribe-2025-12-15"]
mode = "audio_transcription"
max_input_tokens = 16000
max_output_tokens = 2000
input_cost_per_token = 0.00000125
output_cost_per_token = 0.000005
litellm_provider = "openai"
providers = ["openai"]
input_cost_per_audio_token = 0.000003
supported_endpoints = ["/v1/audio/transcriptions"]

[models."gpt-4o-mini-transcribe-2025-12-15".pricing."openai"]
input_cost_per_audio_token = 0.000003
input_cost_per_token = 0.00000125
output_cost_per_token = 0.000005

[models."gpt-4o-mini-tts"]
mode = "audio_speech"
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "azure"
providers = ["azure", "openai"]
supported_modalities = ["text", "audio"]
supported_output_modalities = ["audio"]
output_cost_per_audio_token = 0.000012
output_cost_per_second = 0.00025
supported_endpoints = ["/v1/audio/speech"]

[models."gpt-4o-mini-tts".pricing."azure"]
input_cost_per_token = 0.0000025
output_cost_per_audio_token = 0.000012
output_cost_per_second = 0.00025
output_cost_per_token = 0.00001
[models."gpt-4o-mini-tts".pricing."openai"]
input_cost_per_token = 0.0000025
output_cost_per_audio_token = 0.000012
output_cost_per_second = 0.00025
output_cost_per_token = 0.00001

[models."gpt-4o-mini-tts-2025-03-20"]
mode = "audio_speech"
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "openai"
providers = ["openai"]
supported_modalities = ["text", "audio"]
supported_output_modalities = ["audio"]
output_cost_per_audio_token = 0.000012
output_cost_per_second = 0.00025
supported_endpoints = ["/v1/audio/speech"]

[models."gpt-4o-mini-tts-2025-03-20".pricing."openai"]
input_cost_per_token = 0.0000025
output_cost_per_audio_token = 0.000012
output_cost_per_second = 0.00025
output_cost_per_token = 0.00001

[models."gpt-4o-mini-tts-2025-12-15"]
mode = "audio_speech"
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "openai"
providers = ["openai"]
supported_modalities = ["text", "audio"]
supported_output_modalities = ["audio"]
output_cost_per_audio_token = 0.000012
output_cost_per_second = 0.00025
supported_endpoints = ["/v1/audio/speech"]

[models."gpt-4o-mini-tts-2025-12-15".pricing."openai"]
input_cost_per_token = 0.0000025
output_cost_per_audio_token = 0.000012
output_cost_per_second = 0.00025
output_cost_per_token = 0.00001

[models."gpt-4o-realtime-preview"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000005
output_cost_per_token = 0.00002
cache_read_input_token_cost = 0.0000025
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
input_cost_per_audio_token = 0.00004
output_cost_per_audio_token = 0.00008
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-realtime-preview".pricing."openai"]
cache_read_input_token_cost = 0.0000025
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.000005
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00002

[models."gpt-4o-realtime-preview-2024-10-01"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.0000055
output_cost_per_token = 0.000022
cache_read_input_token_cost = 0.00000275
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
cache_creation_input_audio_token_cost = 0.000022
input_cost_per_audio_token = 0.00011
output_cost_per_audio_token = 0.00022
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-realtime-preview-2024-10-01".pricing."azure"]
cache_creation_input_audio_token_cost = 0.00002
cache_read_input_token_cost = 0.0000025
input_cost_per_audio_token = 0.0001
input_cost_per_token = 0.000005
output_cost_per_audio_token = 0.0002
output_cost_per_token = 0.00002
[models."gpt-4o-realtime-preview-2024-10-01".pricing."azure/eu"]
cache_creation_input_audio_token_cost = 0.000022
cache_read_input_token_cost = 0.00000275
input_cost_per_audio_token = 0.00011
input_cost_per_token = 0.0000055
output_cost_per_audio_token = 0.00022
output_cost_per_token = 0.000022
[models."gpt-4o-realtime-preview-2024-10-01".pricing."azure/us"]
cache_creation_input_audio_token_cost = 0.000022
cache_read_input_token_cost = 0.00000275
input_cost_per_audio_token = 0.00011
input_cost_per_token = 0.0000055
output_cost_per_audio_token = 0.00022
output_cost_per_token = 0.000022
[models."gpt-4o-realtime-preview-2024-10-01".pricing."openai"]
cache_creation_input_audio_token_cost = 0.00002
cache_read_input_token_cost = 0.0000025
input_cost_per_audio_token = 0.0001
input_cost_per_token = 0.000005
output_cost_per_audio_token = 0.0002
output_cost_per_token = 0.00002

[models."gpt-4o-realtime-preview-2024-12-17"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.0000055
output_cost_per_token = 0.000022
cache_read_input_token_cost = 0.00000275
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
cache_read_input_audio_token_cost = 0.0000025
input_cost_per_audio_token = 0.000044
output_cost_per_audio_token = 0.00008
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-realtime-preview-2024-12-17".pricing."azure"]
cache_read_input_token_cost = 0.0000025
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.000005
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00002
[models."gpt-4o-realtime-preview-2024-12-17".pricing."azure/eu"]
cache_read_input_audio_token_cost = 0.0000025
cache_read_input_token_cost = 0.00000275
input_cost_per_audio_token = 0.000044
input_cost_per_token = 0.0000055
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.000022
[models."gpt-4o-realtime-preview-2024-12-17".pricing."azure/us"]
cache_read_input_audio_token_cost = 0.0000025
cache_read_input_token_cost = 0.00000275
input_cost_per_audio_token = 0.000044
input_cost_per_token = 0.0000055
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.000022
[models."gpt-4o-realtime-preview-2024-12-17".pricing."openai"]
cache_read_input_token_cost = 0.0000025
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.000005
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00002

[models."gpt-4o-realtime-preview-2025-06-03"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000005
output_cost_per_token = 0.00002
cache_read_input_token_cost = 0.0000025
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
input_cost_per_audio_token = 0.00004
output_cost_per_audio_token = 0.00008
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-realtime-preview-2025-06-03".pricing."openai"]
cache_read_input_token_cost = 0.0000025
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.000005
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00002

[models."gpt-4o-search"]
display_name = "GPT-4o-Search"
model_family = "gpt"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 0.0000022
output_cost_per_token = 0.000009
litellm_provider = "poe"
providers = ["poe"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-03-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gpt-4o-search".pricing."poe"]
input_cost_per_token = 0.0000022
output_cost_per_token = 0.000009

[models."gpt-4o-search-preview"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
cache_read_input_token_cost = 0.00000125
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
input_cost_per_token_batches = 0.00000125
output_cost_per_token_batches = 0.000005
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gpt-4o-search-preview".search_context_cost_per_query]
search_context_size_high = 0.05
search_context_size_low = 0.03
search_context_size_medium = 0.035

[models."gpt-4o-search-preview".pricing."openai"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
input_cost_per_token_batches = 0.00000125
output_cost_per_token = 0.00001
output_cost_per_token_batches = 0.000005

[models."gpt-4o-search-preview-2025-03-11"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
cache_read_input_token_cost = 0.00000125
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
input_cost_per_token_batches = 0.00000125
output_cost_per_token_batches = 0.000005
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-4o-search-preview-2025-03-11".pricing."openai"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.0000025
input_cost_per_token_batches = 0.00000125
output_cost_per_token = 0.00001
output_cost_per_token_batches = 0.000005

[models."gpt-4o-transcribe"]
mode = "audio_transcription"
max_input_tokens = 16000
max_output_tokens = 2000
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_audio_token = 0.000006
supported_endpoints = ["/v1/audio/transcriptions"]

[models."gpt-4o-transcribe".pricing."azure"]
input_cost_per_audio_token = 0.000006
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o-transcribe".pricing."openai"]
input_cost_per_audio_token = 0.000006
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001

[models."gpt-4o-transcribe-diarize"]
mode = "audio_transcription"
max_input_tokens = 16000
max_output_tokens = 2000
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_audio_token = 0.000006
supported_endpoints = ["/v1/audio/transcriptions"]

[models."gpt-4o-transcribe-diarize".pricing."azure"]
input_cost_per_audio_token = 0.000006
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."gpt-4o-transcribe-diarize".pricing."openai"]
input_cost_per_audio_token = 0.000006
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001

[models."gpt-4o:extended"]
display_name = "OpenAI: GPT-4o (extended)"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 64000
input_cost_per_token = 0.000006
output_cost_per_token = 0.000018
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-05-13"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gpt-4o:extended".pricing."kilo"]
input_cost_per_token = 0.000006
output_cost_per_token = 0.000018

[models."gpt-5"]
display_name = "GPT-5"
model_family = "gpt"
mode = "chat"
max_input_tokens = 409600
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "azure"
providers = ["azure", "302ai", "abacus", "aihubmix", "azure-cognitive-services", "fastrouter", "firmware", "github_copilot", "github-copilot", "gmi", "helicone", "kilo", "openai", "opencode", "openrouter", "poe", "replicate", "requesty", "sap-ai-core", "vercel", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-09-30"
release_date = "2025-08-07"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5".pricing."302ai"]
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5".pricing."abacus"]
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5".pricing."aihubmix"]
cache_read_input_token_cost = 0.0000025
input_cost_per_token = 0.000005
output_cost_per_token = 0.00002
[models."gpt-5".pricing."azure"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5".pricing."fastrouter"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5".pricing."firmware"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5".pricing."gmi"]
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5".pricing."helicone"]
cache_read_input_token_cost = 1.2500000000000002e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5".pricing."kilo"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5".pricing."openai"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_flex = 6.25e-8
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_flex = 6.25e-7
input_cost_per_token_priority = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_flex = 0.000005
output_cost_per_token_priority = 0.00002
[models."gpt-5".pricing."opencode"]
cache_read_input_token_cost = 1.0699999999999999e-7
input_cost_per_token = 0.0000010700000000000001
output_cost_per_token = 0.0000085
[models."gpt-5".pricing."openrouter"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5".pricing."poe"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.000009
[models."gpt-5".pricing."replicate"]
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5".pricing."requesty"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5".pricing."sap-ai-core"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5".pricing."vercel"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5".pricing."zenmux"]
cache_read_input_token_cost = 1.2e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001

[models."gpt-5-1-instant"]
display_name = "GPT-5.1 Instant"
model_family = "gpt"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.3e-7
litellm_provider = "vercel"
providers = ["vercel", "poe"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-08-07"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text", "image"]
source = "modelsdev"

[models."gpt-5-1-instant".pricing."poe"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.000009
[models."gpt-5-1-instant".pricing."vercel"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001

[models."gpt-5-1-thinking"]
display_name = "GPT 5.1 Thinking"
model_family = "gpt"
mode = "chat"
max_input_tokens = 400000
max_output_tokens = 128000
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.3e-7
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-08-07"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text", "image"]
source = "modelsdev"

[models."gpt-5-1-thinking".pricing."vercel"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001

[models."gpt-5-2-instant"]
display_name = "GPT-5.2-Instant"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 0.0000016000000000000001
output_cost_per_token = 0.000013
cache_read_input_token_cost = 1.6e-7
litellm_provider = "poe"
providers = ["poe"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-12-11"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gpt-5-2-instant".pricing."poe"]
cache_read_input_token_cost = 1.6e-7
input_cost_per_token = 0.0000016000000000000001
output_cost_per_token = 0.000013

[models."gpt-5-2025-08-07"]
mode = "chat"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000001375
output_cost_per_token = 0.000011
cache_read_input_token_cost = 1.375e-7
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5-2025-08-07".pricing."azure"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5-2025-08-07".pricing."azure/eu"]
cache_read_input_token_cost = 1.375e-7
input_cost_per_token = 0.000001375
output_cost_per_token = 0.000011
[models."gpt-5-2025-08-07".pricing."azure/us"]
cache_read_input_token_cost = 1.375e-7
input_cost_per_token = 0.000001375
output_cost_per_token = 0.000011
[models."gpt-5-2025-08-07".pricing."openai"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_flex = 6.25e-8
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_flex = 6.25e-7
input_cost_per_token_priority = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_flex = 0.000005
output_cost_per_token_priority = 0.00002

[models."gpt-5-3-codex-spark"]
display_name = "GPT-5.3 Codex Spark"
model_family = "gpt-codex-spark"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
cache_read_input_token_cost = 1.75e-7
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-08-31"
release_date = "2026-02-05"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gpt-5-3-codex-spark".pricing."openai"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014

[models."gpt-5-chat"]
display_name = "GPT-5 Chat"
model_family = "gpt"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services", "kilo", "openai", "openrouter", "poe", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-08-07"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/blog/gpt-5-in-azure-ai-foundry-the-future-of-ai-apps-and-agents-starts-here/"
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5-chat".pricing."azure"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5-chat".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5-chat".pricing."kilo"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5-chat".pricing."openai"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5-chat".pricing."openrouter"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5-chat".pricing."poe"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.000009
[models."gpt-5-chat".pricing."vercel"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001

[models."gpt-5-chat-latest"]
display_name = "gpt-5-chat-latest"
model_family = "gpt"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "azure"
providers = ["azure", "helicone", "jiekou", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-09"
release_date = "2026-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5-chat-latest".pricing."azure"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5-chat-latest".pricing."helicone"]
cache_read_input_token_cost = 1.2500000000000002e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5-chat-latest".pricing."jiekou"]
input_cost_per_token = 0.000001125
output_cost_per_token = 0.000009
[models."gpt-5-chat-latest".pricing."openai"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001

[models."gpt-5-codex"]
display_name = "gpt-5-codex"
model_family = "gpt-codex"
mode = "responses"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "azure"
providers = ["azure", "aihubmix", "azure-cognitive-services", "helicone", "jiekou", "kilo", "openai", "opencode", "openrouter", "poe", "vercel", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-09-30"
release_date = "2026-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5-codex".pricing."aihubmix"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5-codex".pricing."azure"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5-codex".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5-codex".pricing."helicone"]
cache_read_input_token_cost = 1.2500000000000002e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5-codex".pricing."jiekou"]
input_cost_per_token = 0.000001125
output_cost_per_token = 0.000009
[models."gpt-5-codex".pricing."kilo"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5-codex".pricing."openai"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5-codex".pricing."opencode"]
cache_read_input_token_cost = 1.0699999999999999e-7
input_cost_per_token = 0.0000010700000000000001
output_cost_per_token = 0.0000085
[models."gpt-5-codex".pricing."openrouter"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5-codex".pricing."poe"]
input_cost_per_token = 0.0000011
output_cost_per_token = 0.000009
[models."gpt-5-codex".pricing."vercel"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5-codex".pricing."zenmux"]
cache_read_input_token_cost = 1.2e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001

[models."gpt-5-image"]
display_name = "GPT-5 Image"
model_family = "gpt"
mode = "chat"
max_input_tokens = 400000
max_output_tokens = 128000
input_cost_per_token = 0.000005
output_cost_per_token = 0.00001
cache_read_input_token_cost = 0.00000125
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10-01"
release_date = "2025-10-14"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text", "image"]
source = "modelsdev"

[models."gpt-5-image".pricing."openrouter"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.000005
output_cost_per_token = 0.00001

[models."gpt-5-mini"]
display_name = "GPT-5 Mini"
model_family = "gpt-mini"
mode = "chat"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
cache_read_input_token_cost = 2.5e-8
litellm_provider = "azure"
providers = ["azure", "302ai", "abacus", "aihubmix", "azure-cognitive-services", "fastrouter", "firmware", "github_copilot", "github-copilot", "helicone", "jiekou", "kilo", "openai", "openrouter", "perplexity", "poe", "qihang-ai", "replicate", "requesty", "sap-ai-core", "vercel", "vivgrid"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-05-30"
release_date = "2025-08-07"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gpt-5-mini".pricing."302ai"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5-mini".pricing."abacus"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5-mini".pricing."aihubmix"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000006
[models."gpt-5-mini".pricing."azure"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5-mini".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5-mini".pricing."fastrouter"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5-mini".pricing."firmware"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5-mini".pricing."helicone"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5-mini".pricing."jiekou"]
input_cost_per_token = 2.2500000000000002e-7
output_cost_per_token = 0.0000018000000000000001
[models."gpt-5-mini".pricing."kilo"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5-mini".pricing."openai"]
cache_read_input_token_cost = 2.5e-8
cache_read_input_token_cost_flex = 1.25e-8
cache_read_input_token_cost_priority = 4.5e-8
input_cost_per_token = 2.5e-7
input_cost_per_token_flex = 1.25e-7
input_cost_per_token_priority = 4.5e-7
output_cost_per_token = 0.000002
output_cost_per_token_flex = 0.000001
output_cost_per_token_priority = 0.0000036
[models."gpt-5-mini".pricing."openrouter"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5-mini".pricing."poe"]
cache_read_input_token_cost = 2.2e-8
input_cost_per_token = 2.2e-7
output_cost_per_token = 0.0000018000000000000001
[models."gpt-5-mini".pricing."qihang-ai"]
input_cost_per_token = 4e-8
output_cost_per_token = 2.9e-7
[models."gpt-5-mini".pricing."replicate"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5-mini".pricing."requesty"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5-mini".pricing."sap-ai-core"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5-mini".pricing."vercel"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5-mini".pricing."vivgrid"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002

[models."gpt-5-mini-2025-08-07"]
mode = "chat"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 2.75e-7
output_cost_per_token = 0.0000022
cache_read_input_token_cost = 2.75e-8
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5-mini-2025-08-07".pricing."azure"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5-mini-2025-08-07".pricing."azure/eu"]
cache_read_input_token_cost = 2.75e-8
input_cost_per_token = 2.75e-7
output_cost_per_token = 0.0000022
[models."gpt-5-mini-2025-08-07".pricing."azure/us"]
cache_read_input_token_cost = 2.75e-8
input_cost_per_token = 2.75e-7
output_cost_per_token = 0.0000022
[models."gpt-5-mini-2025-08-07".pricing."openai"]
cache_read_input_token_cost = 2.5e-8
cache_read_input_token_cost_flex = 1.25e-8
cache_read_input_token_cost_priority = 4.5e-8
input_cost_per_token = 2.5e-7
input_cost_per_token_flex = 1.25e-7
input_cost_per_token_priority = 4.5e-7
output_cost_per_token = 0.000002
output_cost_per_token_flex = 0.000001
output_cost_per_token_priority = 0.0000036

[models."gpt-5-nano"]
display_name = "GPT-5 Nano"
model_family = "gpt-nano"
mode = "chat"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 5e-8
output_cost_per_token = 4e-7
cache_read_input_token_cost = 5e-9
litellm_provider = "azure"
providers = ["azure", "abacus", "aihubmix", "azure-cognitive-services", "fastrouter", "firmware", "helicone", "jiekou", "kilo", "openai", "opencode", "openrouter", "poe", "replicate", "requesty", "sap-ai-core", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-05-30"
release_date = "2025-08-07"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5-nano".pricing."abacus"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 4.0000000000000003e-7
[models."gpt-5-nano".pricing."aihubmix"]
cache_read_input_token_cost = 2.5e-7
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002
[models."gpt-5-nano".pricing."azure"]
cache_read_input_token_cost = 5e-9
input_cost_per_token = 5e-8
output_cost_per_token = 4e-7
[models."gpt-5-nano".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 4.0000000000000003e-7
[models."gpt-5-nano".pricing."fastrouter"]
cache_read_input_token_cost = 5e-9
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 4.0000000000000003e-7
[models."gpt-5-nano".pricing."firmware"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 4.0000000000000003e-7
[models."gpt-5-nano".pricing."helicone"]
cache_read_input_token_cost = 5e-9
input_cost_per_token = 5e-8
output_cost_per_token = 4e-7
[models."gpt-5-nano".pricing."jiekou"]
input_cost_per_token = 4.5e-8
output_cost_per_token = 3.6e-7
[models."gpt-5-nano".pricing."kilo"]
cache_read_input_token_cost = 5e-9
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 4.0000000000000003e-7
[models."gpt-5-nano".pricing."openai"]
cache_read_input_token_cost = 5e-9
cache_read_input_token_cost_flex = 2.5e-9
input_cost_per_token = 5e-8
input_cost_per_token_flex = 2.5e-8
input_cost_per_token_priority = 0.0000025
output_cost_per_token = 4e-7
output_cost_per_token_flex = 2e-7
[models."gpt-5-nano".pricing."openrouter"]
cache_read_input_token_cost = 5e-9
input_cost_per_token = 5e-8
output_cost_per_token = 4e-7
[models."gpt-5-nano".pricing."poe"]
cache_read_input_token_cost = 4.5e-9
input_cost_per_token = 4.5e-8
output_cost_per_token = 3.6e-7
[models."gpt-5-nano".pricing."replicate"]
input_cost_per_token = 5e-8
output_cost_per_token = 4e-7
[models."gpt-5-nano".pricing."requesty"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 4.0000000000000003e-7
[models."gpt-5-nano".pricing."sap-ai-core"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 4.0000000000000003e-7
[models."gpt-5-nano".pricing."vercel"]
cache_read_input_token_cost = 5e-9
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 4.0000000000000003e-7

[models."gpt-5-nano-2025-08-07"]
mode = "chat"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 5.5e-8
output_cost_per_token = 4.4e-7
cache_read_input_token_cost = 5.5e-9
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5-nano-2025-08-07".pricing."azure"]
cache_read_input_token_cost = 5e-9
input_cost_per_token = 5e-8
output_cost_per_token = 4e-7
[models."gpt-5-nano-2025-08-07".pricing."azure/eu"]
cache_read_input_token_cost = 5.5e-9
input_cost_per_token = 5.5e-8
output_cost_per_token = 4.4e-7
[models."gpt-5-nano-2025-08-07".pricing."azure/us"]
cache_read_input_token_cost = 5.5e-9
input_cost_per_token = 5.5e-8
output_cost_per_token = 4.4e-7
[models."gpt-5-nano-2025-08-07".pricing."openai"]
cache_read_input_token_cost = 5e-9
cache_read_input_token_cost_flex = 2.5e-9
input_cost_per_token = 5e-8
input_cost_per_token_flex = 2.5e-8
output_cost_per_token = 4e-7
output_cost_per_token_flex = 2e-7

[models."gpt-5-pro"]
display_name = "gpt-5-pro"
model_family = "gpt-pro"
mode = "responses"
max_input_tokens = 272000
max_output_tokens = 272000
max_tokens = 272000
input_cost_per_token = 0.000015
output_cost_per_token = 0.00012
litellm_provider = "azure"
providers = ["azure", "302ai", "aihubmix", "azure-cognitive-services", "helicone", "jiekou", "kilo", "openai", "openrouter", "poe", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-10-08"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/concepts/models-sold-directly-by-azure?pivots=azure-openai&tabs=global-standard-aoai%2Cstandard-chat-completions%2Cglobal-standard#gpt-5"
supported_endpoints = ["/v1/responses", "/v1/batch"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gpt-5-pro".pricing."302ai"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.00012
[models."gpt-5-pro".pricing."aihubmix"]
cache_read_input_token_cost = 0.0000035
input_cost_per_token = 0.000007
output_cost_per_token = 0.000028
[models."gpt-5-pro".pricing."azure"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.00012
[models."gpt-5-pro".pricing."azure-cognitive-services"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.00012
[models."gpt-5-pro".pricing."helicone"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.00012
[models."gpt-5-pro".pricing."jiekou"]
input_cost_per_token = 0.0000135
output_cost_per_token = 0.000108
[models."gpt-5-pro".pricing."kilo"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.00012
[models."gpt-5-pro".pricing."openai"]
input_cost_per_token = 0.000015
input_cost_per_token_batches = 0.0000075
output_cost_per_token = 0.00012
output_cost_per_token_batches = 0.00006
[models."gpt-5-pro".pricing."openrouter"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.00012
[models."gpt-5-pro".pricing."poe"]
input_cost_per_token = 0.000014
output_cost_per_token = 0.00011
[models."gpt-5-pro".pricing."vercel"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.00012

[models."gpt-5-pro-2025-10-06"]
mode = "responses"
max_input_tokens = 128000
max_output_tokens = 272000
max_tokens = 272000
input_cost_per_token = 0.000015
output_cost_per_token = 0.00012
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_token_batches = 0.0000075
output_cost_per_token_batches = 0.00006
supported_endpoints = ["/v1/batch", "/v1/responses"]
supports_native_streaming = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gpt-5-pro-2025-10-06".pricing."openai"]
input_cost_per_token = 0.000015
input_cost_per_token_batches = 0.0000075
output_cost_per_token = 0.00012
output_cost_per_token_batches = 0.00006

[models."gpt-5-search-api"]
mode = "chat"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gpt-5-search-api".pricing."openai"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001

[models."gpt-5-search-api-2025-10-14"]
mode = "chat"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gpt-5-search-api-2025-10-14".pricing."openai"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001

[models."gpt-5-thinking"]
display_name = "gpt-5-thinking"
mode = "chat"
max_input_tokens = 400000
max_output_tokens = 128000
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
litellm_provider = "302ai"
providers = ["302ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-08-08"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gpt-5-thinking".pricing."302ai"]
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001

[models."gpt-5.1"]
display_name = "gpt-5.1"
model_family = "gpt"
mode = "chat"
max_input_tokens = 409600
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000138
output_cost_per_token = 0.000011
cache_read_input_token_cost = 1.4e-7
litellm_provider = "azure"
providers = ["azure", "302ai", "abacus", "aihubmix", "azure-cognitive-services", "cloudflare-ai-gateway", "github_copilot", "github-copilot", "gmi", "helicone", "jiekou", "kilo", "openai", "opencode", "openrouter", "perplexity", "poe", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-11-14"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gpt-5.1".pricing."302ai"]
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1".pricing."abacus"]
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1".pricing."aihubmix"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1".pricing."azure"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1".pricing."azure/eu"]
cache_read_input_token_cost = 1.4e-7
input_cost_per_token = 0.00000138
output_cost_per_token = 0.000011
[models."gpt-5.1".pricing."azure/global"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1".pricing."azure/us"]
cache_read_input_token_cost = 1.4e-7
input_cost_per_token = 0.00000138
output_cost_per_token = 0.000011
[models."gpt-5.1".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1".pricing."gmi"]
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1".pricing."helicone"]
cache_read_input_token_cost = 1.2500000000000002e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1".pricing."jiekou"]
input_cost_per_token = 0.000001125
output_cost_per_token = 0.000009
[models."gpt-5.1".pricing."kilo"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1".pricing."openai"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002
[models."gpt-5.1".pricing."opencode"]
cache_read_input_token_cost = 1.0699999999999999e-7
input_cost_per_token = 0.0000010700000000000001
output_cost_per_token = 0.0000085
[models."gpt-5.1".pricing."openrouter"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1".pricing."poe"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.000009
[models."gpt-5.1".pricing."zenmux"]
cache_read_input_token_cost = 1.2e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001

[models."gpt-5.1-2025-11-13"]
mode = "chat"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token_priority = 0.0000025
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5.1-2025-11-13".pricing."azure"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002
[models."gpt-5.1-2025-11-13".pricing."openai"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002

[models."gpt-5.1-chat"]
display_name = "GPT-5.1 Chat"
model_family = "gpt-codex"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000138
output_cost_per_token = 0.000011
cache_read_input_token_cost = 1.4e-7
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services", "kilo", "openrouter", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-09-30"
release_date = "2025-11-14"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5.1-chat".pricing."azure"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-chat".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-chat".pricing."azure/eu"]
cache_read_input_token_cost = 1.4e-7
input_cost_per_token = 0.00000138
output_cost_per_token = 0.000011
[models."gpt-5.1-chat".pricing."azure/global"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-chat".pricing."azure/us"]
cache_read_input_token_cost = 1.4e-7
input_cost_per_token = 0.00000138
output_cost_per_token = 0.000011
[models."gpt-5.1-chat".pricing."kilo"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-chat".pricing."openrouter"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-chat".pricing."zenmux"]
cache_read_input_token_cost = 1.2e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001

[models."gpt-5.1-chat-2025-11-13"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "azure"
providers = ["azure"]
supports_function_calling = false
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token_priority = 0.0000025
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = false
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = false

[models."gpt-5.1-chat-2025-11-13".pricing."azure"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002

[models."gpt-5.1-chat-latest"]
display_name = "gpt-5.1-chat-latest"
model_family = "gpt"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "openai"
providers = ["openai", "302ai", "abacus", "helicone"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-11-14"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token_priority = 0.0000025
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = false
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = false

[models."gpt-5.1-chat-latest".pricing."302ai"]
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-chat-latest".pricing."abacus"]
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-chat-latest".pricing."helicone"]
cache_read_input_token_cost = 1.2500000000000002e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-chat-latest".pricing."openai"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002

[models."gpt-5.1-codex"]
display_name = "GPT-5.1-Codex"
model_family = "gpt-5-codex"
mode = "chat"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "openai"
providers = ["openai", "aihubmix", "azure", "azure-cognitive-services", "cloudflare-ai-gateway", "github-copilot", "helicone", "jiekou", "kilo", "opencode", "openrouter", "poe", "vercel", "vivgrid", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-09-30"
release_date = "2025-11-13"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
source = "custom"
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token_priority = 0.0000025
model_vendor = "openai"
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5.1-codex".pricing."aihubmix"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-codex".pricing."azure"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-codex".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-codex".pricing."azure/eu"]
cache_read_input_token_cost = 1.4e-7
input_cost_per_token = 0.00000138
output_cost_per_token = 0.000011
[models."gpt-5.1-codex".pricing."azure/global"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-codex".pricing."azure/us"]
cache_read_input_token_cost = 1.4e-7
input_cost_per_token = 0.00000138
output_cost_per_token = 0.000011
[models."gpt-5.1-codex".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-codex".pricing."helicone"]
cache_read_input_token_cost = 1.2500000000000002e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-codex".pricing."jiekou"]
input_cost_per_token = 0.000001125
output_cost_per_token = 0.000009
[models."gpt-5.1-codex".pricing."kilo"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-codex".pricing."openai"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002
[models."gpt-5.1-codex".pricing."opencode"]
cache_read_input_token_cost = 1.0699999999999999e-7
input_cost_per_token = 0.0000010700000000000001
output_cost_per_token = 0.0000085
[models."gpt-5.1-codex".pricing."openrouter"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-codex".pricing."poe"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.000009
[models."gpt-5.1-codex".pricing."vercel"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-codex".pricing."vivgrid"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-codex".pricing."zenmux"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001

[models."gpt-5.1-codex-2025-11-13"]
mode = "responses"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "azure"
providers = ["azure"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token_priority = 0.0000025
output_cost_per_token_priority = 0.00002
supported_endpoints = ["/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true

[models."gpt-5.1-codex-2025-11-13".pricing."azure"]
cache_read_input_token_cost = 1.25e-7
cache_read_input_token_cost_priority = 2.5e-7
input_cost_per_token = 0.00000125
input_cost_per_token_priority = 0.0000025
output_cost_per_token = 0.00001
output_cost_per_token_priority = 0.00002

[models."gpt-5.1-codex-max"]
display_name = "GPT-5.1-Codex-max"
model_family = "gpt-codex"
mode = "responses"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
cache_read_input_token_cost = 1.25e-7
litellm_provider = "azure"
providers = ["azure", "aihubmix", "chatgpt", "github_copilot", "github-copilot", "jiekou", "kilo", "openai", "opencode", "openrouter", "poe", "vercel", "vivgrid"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-09-30"
release_date = "2025-12-04"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true

[models."gpt-5.1-codex-max".pricing."aihubmix"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-codex-max".pricing."azure"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-codex-max".pricing."jiekou"]
input_cost_per_token = 0.000001125
output_cost_per_token = 0.000009
[models."gpt-5.1-codex-max".pricing."kilo"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-codex-max".pricing."openai"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-codex-max".pricing."opencode"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-codex-max".pricing."openrouter"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.000009
[models."gpt-5.1-codex-max".pricing."poe"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.000009
[models."gpt-5.1-codex-max".pricing."vercel"]
cache_read_input_token_cost = 1.3e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001
[models."gpt-5.1-codex-max".pricing."vivgrid"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00001

[models."gpt-5.1-codex-mini"]
display_name = "GPT-5.1-Codex-mini"
model_family = "gpt-codex"
mode = "responses"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 2.75e-7
output_cost_per_token = 0.0000022
cache_read_input_token_cost = 2.8e-8
litellm_provider = "azure"
providers = ["azure", "aihubmix", "azure-cognitive-services", "chatgpt", "github-copilot", "helicone", "jiekou", "kilo", "openai", "opencode", "openrouter", "poe", "vercel", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-09-30"
release_date = "2025-11-13"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true

[models."gpt-5.1-codex-mini".pricing."aihubmix"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5.1-codex-mini".pricing."azure"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5.1-codex-mini".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5.1-codex-mini".pricing."azure/eu"]
cache_read_input_token_cost = 2.8e-8
input_cost_per_token = 2.75e-7
output_cost_per_token = 0.0000022
[models."gpt-5.1-codex-mini".pricing."azure/global"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5.1-codex-mini".pricing."azure/us"]
cache_read_input_token_cost = 2.8e-8
input_cost_per_token = 2.75e-7
output_cost_per_token = 0.0000022
[models."gpt-5.1-codex-mini".pricing."helicone"]
cache_read_input_token_cost = 2.5e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5.1-codex-mini".pricing."jiekou"]
input_cost_per_token = 2.2500000000000002e-7
output_cost_per_token = 0.0000018000000000000001
[models."gpt-5.1-codex-mini".pricing."kilo"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5.1-codex-mini".pricing."openai"]
cache_read_input_token_cost = 2.5e-8
cache_read_input_token_cost_priority = 4.5e-8
input_cost_per_token = 2.5e-7
input_cost_per_token_priority = 4.5e-7
output_cost_per_token = 0.000002
output_cost_per_token_priority = 0.0000036
[models."gpt-5.1-codex-mini".pricing."opencode"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5.1-codex-mini".pricing."openrouter"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5.1-codex-mini".pricing."poe"]
cache_read_input_token_cost = 2.2e-8
input_cost_per_token = 2.2e-7
output_cost_per_token = 0.0000018000000000000001
[models."gpt-5.1-codex-mini".pricing."vercel"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5.1-codex-mini".pricing."zenmux"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002

[models."gpt-5.1-codex-mini-2025-11-13"]
mode = "responses"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
cache_read_input_token_cost = 2.5e-8
litellm_provider = "azure"
providers = ["azure"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
cache_read_input_token_cost_priority = 4.5e-8
input_cost_per_token_priority = 4.5e-7
output_cost_per_token_priority = 0.0000036
supported_endpoints = ["/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = false
supports_tool_choice = true

[models."gpt-5.1-codex-mini-2025-11-13".pricing."azure"]
cache_read_input_token_cost = 2.5e-8
cache_read_input_token_cost_priority = 4.5e-8
input_cost_per_token = 2.5e-7
input_cost_per_token_priority = 4.5e-7
output_cost_per_token = 0.000002
output_cost_per_token_priority = 0.0000036

[models."gpt-5.2"]
display_name = "GPT-5.2"
model_family = "gpt"
mode = "chat"
max_input_tokens = 409600
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
cache_read_input_token_cost = 1.75e-7
litellm_provider = "azure"
providers = ["azure", "302ai", "abacus", "aihubmix", "azure-cognitive-services", "chatgpt", "cloudflare-ai-gateway", "firmware", "github_copilot", "github-copilot", "gmi", "jiekou", "kilo", "openai", "opencode", "openrouter", "perplexity", "poe", "qihang-ai", "vercel", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-08-31"
release_date = "2025-12-11"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gpt-5.2".pricing."302ai"]
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2".pricing."abacus"]
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2".pricing."aihubmix"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2".pricing."azure"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2".pricing."firmware"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2".pricing."gmi"]
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2".pricing."jiekou"]
input_cost_per_token = 0.000001575
output_cost_per_token = 0.0000126
[models."gpt-5.2".pricing."kilo"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2".pricing."openai"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028
[models."gpt-5.2".pricing."opencode"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2".pricing."openrouter"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_image = 0
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2".pricing."poe"]
cache_read_input_token_cost = 1.6e-7
input_cost_per_token = 0.0000016000000000000001
output_cost_per_token = 0.000013
[models."gpt-5.2".pricing."qihang-ai"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000002
[models."gpt-5.2".pricing."vercel"]
cache_read_input_token_cost = 1.8e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2".pricing."zenmux"]
cache_read_input_token_cost = 1.7000000000000001e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014

[models."gpt-5.2-2025-12-11"]
mode = "chat"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
cache_read_input_token_cost = 1.75e-7
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token_priority = 0.0000035
output_cost_per_token_priority = 0.000028
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5.2-2025-12-11".pricing."azure"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028
[models."gpt-5.2-2025-12-11".pricing."openai"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028

[models."gpt-5.2-chat"]
display_name = "GPT-5.2 Chat"
model_family = "gpt"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
cache_read_input_token_cost = 1.75e-7
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services", "kilo", "openrouter", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-08-07"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token_priority = 0.0000035
output_cost_per_token_priority = 0.000028
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5.2-chat".pricing."azure"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028
[models."gpt-5.2-chat".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2-chat".pricing."kilo"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2-chat".pricing."openrouter"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_image = 0
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2-chat".pricing."vercel"]
cache_read_input_token_cost = 1.8e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014

[models."gpt-5.2-chat-2025-12-11"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
cache_read_input_token_cost = 1.75e-7
litellm_provider = "azure"
providers = ["azure"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token_priority = 0.0000035
output_cost_per_token_priority = 0.000028
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5.2-chat-2025-12-11".pricing."azure"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028

[models."gpt-5.2-chat-latest"]
display_name = "gpt-5.2-chat-latest"
model_family = "gpt"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
cache_read_input_token_cost = 1.75e-7
litellm_provider = "openai"
providers = ["openai", "302ai", "abacus"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-12-12"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token_priority = 0.0000035
output_cost_per_token_priority = 0.000028
supported_endpoints = ["/v1/chat/completions", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5.2-chat-latest".pricing."302ai"]
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2-chat-latest".pricing."abacus"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000012
[models."gpt-5.2-chat-latest".pricing."openai"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028

[models."gpt-5.2-codex"]
display_name = "GPT-5.2 Codex"
model_family = "gpt-codex"
mode = "chat"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
cache_read_input_token_cost = 1.75e-7
litellm_provider = "openai"
providers = ["openai", "aihubmix", "azure", "azure-cognitive-services", "chatgpt", "github-copilot", "jiekou", "kilo", "opencode", "openrouter", "poe", "qihang-ai", "vercel", "vivgrid", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-08-31"
release_date = "2025-12-11"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
source = "custom"
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token_priority = 0.0000035
output_cost_per_token_priority = 0.000028
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5.2-codex".pricing."aihubmix"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2-codex".pricing."azure"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2-codex".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2-codex".pricing."jiekou"]
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2-codex".pricing."kilo"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2-codex".pricing."openai"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028
[models."gpt-5.2-codex".pricing."opencode"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2-codex".pricing."openrouter"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2-codex".pricing."poe"]
cache_read_input_token_cost = 1.6e-7
input_cost_per_token = 0.0000016000000000000001
output_cost_per_token = 0.000013
[models."gpt-5.2-codex".pricing."qihang-ai"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 0.0000011399999999999999
[models."gpt-5.2-codex".pricing."vercel"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2-codex".pricing."vivgrid"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.2-codex".pricing."zenmux"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014

[models."gpt-5.2-pro"]
display_name = "gpt-5.2-pro"
model_family = "gpt-pro"
mode = "responses"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000021
output_cost_per_token = 0.000168
litellm_provider = "azure"
providers = ["azure", "jiekou", "kilo", "openai", "openrouter", "poe", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2026-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gpt-5.2-pro".pricing."azure"]
input_cost_per_token = 0.000021
output_cost_per_token = 0.000168
[models."gpt-5.2-pro".pricing."jiekou"]
input_cost_per_token = 0.0000189
output_cost_per_token = 0.0001512
[models."gpt-5.2-pro".pricing."kilo"]
input_cost_per_token = 0.000021
output_cost_per_token = 0.000168
[models."gpt-5.2-pro".pricing."openai"]
input_cost_per_token = 0.000021
output_cost_per_token = 0.000168
[models."gpt-5.2-pro".pricing."openrouter"]
input_cost_per_image = 0
input_cost_per_token = 0.000021
output_cost_per_token = 0.000168
[models."gpt-5.2-pro".pricing."poe"]
input_cost_per_token = 0.000019
output_cost_per_token = 0.00015
[models."gpt-5.2-pro".pricing."vercel"]
input_cost_per_token = 0.000021
output_cost_per_token = 0.000168

[models."gpt-5.2-pro-2025-12-11"]
mode = "responses"
max_input_tokens = 272000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000021
output_cost_per_token = 0.000168
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gpt-5.2-pro-2025-12-11".pricing."azure"]
input_cost_per_token = 0.000021
output_cost_per_token = 0.000168
[models."gpt-5.2-pro-2025-12-11".pricing."openai"]
input_cost_per_token = 0.000021
output_cost_per_token = 0.000168

[models."gpt-5.3-codex"]
display_name = "GPT-5.3 Codex"
model_family = "gpt-codex"
mode = "chat"
max_input_tokens = 400000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
cache_read_input_token_cost = 1.75e-7
litellm_provider = "openai"
providers = ["openai", "azure", "azure-cognitive-services", "github_copilot", "opencode", "openrouter", "poe", "vercel", "vivgrid", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-08-31"
release_date = "2026-02-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
source = "custom"
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token_priority = 0.0000035
output_cost_per_token_priority = 0.000028
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-5.3-codex".pricing."azure"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.3-codex".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.3-codex".pricing."openai"]
cache_read_input_token_cost = 1.75e-7
cache_read_input_token_cost_priority = 3.5e-7
input_cost_per_token = 0.00000175
input_cost_per_token_priority = 0.0000035
output_cost_per_token = 0.000014
output_cost_per_token_priority = 0.000028
[models."gpt-5.3-codex".pricing."opencode"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.3-codex".pricing."openrouter"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.3-codex".pricing."poe"]
cache_read_input_token_cost = 1.6e-7
input_cost_per_token = 0.0000016000000000000001
output_cost_per_token = 0.000013
[models."gpt-5.3-codex".pricing."vercel"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.3-codex".pricing."vivgrid"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014
[models."gpt-5.3-codex".pricing."zenmux"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 0.00000175
output_cost_per_token = 0.000014

[models."gpt-audio"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_vision = false
supports_reasoning = false
supports_prompt_caching = false
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
input_cost_per_audio_token = 0.000032
output_cost_per_audio_token = 0.000064
supported_endpoints = ["/v1/chat/completions", "/v1/responses", "/v1/realtime", "/v1/batch"]
supports_audio_input = true
supports_audio_output = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = false
supports_system_messages = true
supports_tool_choice = true

[models."gpt-audio".pricing."openai"]
input_cost_per_audio_token = 0.000032
input_cost_per_token = 0.0000025
output_cost_per_audio_token = 0.000064
output_cost_per_token = 0.00001

[models."gpt-audio-2025-08-28"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = false
supports_reasoning = false
supports_prompt_caching = false
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
input_cost_per_audio_token = 0.00004
output_cost_per_audio_token = 0.00008
supported_endpoints = ["/v1/chat/completions", "/v1/responses", "/v1/realtime", "/v1/batch"]
supports_audio_input = true
supports_audio_output = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = false
supports_system_messages = true
supports_tool_choice = true

[models."gpt-audio-2025-08-28".pricing."azure"]
input_cost_per_audio_token = 0.00004
input_cost_per_token = 0.0000025
output_cost_per_audio_token = 0.00008
output_cost_per_token = 0.00001
[models."gpt-audio-2025-08-28".pricing."openai"]
input_cost_per_audio_token = 0.000032
input_cost_per_token = 0.0000025
output_cost_per_audio_token = 0.000064
output_cost_per_token = 0.00001

[models."gpt-audio-mini"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000024
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_vision = false
supports_reasoning = false
supports_prompt_caching = false
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
input_cost_per_audio_token = 0.00001
output_cost_per_audio_token = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/responses", "/v1/realtime", "/v1/batch"]
supports_audio_input = true
supports_audio_output = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = false
supports_system_messages = true
supports_tool_choice = true

[models."gpt-audio-mini".pricing."openai"]
input_cost_per_audio_token = 0.00001
input_cost_per_token = 6e-7
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024

[models."gpt-audio-mini-2025-10-06"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000024
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = false
supports_reasoning = false
supports_prompt_caching = false
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
input_cost_per_audio_token = 0.00001
output_cost_per_audio_token = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/responses", "/v1/realtime", "/v1/batch"]
supports_audio_input = true
supports_audio_output = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = false
supports_system_messages = true
supports_tool_choice = true

[models."gpt-audio-mini-2025-10-06".pricing."azure"]
input_cost_per_audio_token = 0.00001
input_cost_per_token = 6e-7
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024
[models."gpt-audio-mini-2025-10-06".pricing."openai"]
input_cost_per_audio_token = 0.00001
input_cost_per_token = 6e-7
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024

[models."gpt-audio-mini-2025-12-15"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000024
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_vision = false
supports_reasoning = false
supports_prompt_caching = false
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text", "audio"]
input_cost_per_audio_token = 0.00001
output_cost_per_audio_token = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/responses", "/v1/realtime", "/v1/batch"]
supports_audio_input = true
supports_audio_output = true
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = false
supports_system_messages = true
supports_tool_choice = true

[models."gpt-audio-mini-2025-12-15".pricing."openai"]
input_cost_per_audio_token = 0.00001
input_cost_per_token = 6e-7
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024

[models."gpt-image-1"]
display_name = "GPT-Image-1"
model_family = "gpt"
mode = "image_generation"
max_input_tokens = 128000
max_output_tokens = 0
input_cost_per_token = 0.000005
cache_read_input_token_cost = 0.00000125
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-03-31"
supported_modalities = ["text", "image"]
supported_output_modalities = ["image"]
cache_read_input_image_token_cost = 0.0000025
input_cost_per_image_token = 0.00001
output_cost_per_image_token = 0.00004
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."gpt-image-1".pricing."azure"]
cache_read_input_image_token_cost = 0.0000025
cache_read_input_token_cost = 0.00000125
input_cost_per_image_token = 0.00001
input_cost_per_token = 0.000005
output_cost_per_image_token = 0.00004
[models."gpt-image-1".pricing."openai"]
cache_read_input_image_token_cost = 0.0000025
cache_read_input_token_cost = 0.00000125
input_cost_per_image_token = 0.00001
input_cost_per_token = 0.000005
output_cost_per_image_token = 0.00004

[models."gpt-image-1-mini"]
display_name = "GPT-Image-1-Mini"
model_family = "gpt"
mode = "image_generation"
max_input_tokens = 0
max_output_tokens = 0
input_cost_per_token = 0.000002
cache_read_input_token_cost = 2e-7
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-08-26"
supported_modalities = ["text", "image"]
supported_output_modalities = ["image"]
cache_read_input_image_token_cost = 2.5e-7
input_cost_per_image_token = 0.0000025
output_cost_per_image_token = 0.000008
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."gpt-image-1-mini".pricing."azure"]
cache_read_input_image_token_cost = 2.5e-7
cache_read_input_token_cost = 2e-7
input_cost_per_image_token = 0.0000025
input_cost_per_token = 0.000002
output_cost_per_image_token = 0.000008
[models."gpt-image-1-mini".pricing."openai"]
cache_read_input_image_token_cost = 2.5e-7
cache_read_input_token_cost = 2e-7
input_cost_per_image_token = 0.0000025
input_cost_per_token = 0.000002
output_cost_per_image_token = 0.000008

[models."gpt-image-1.5"]
display_name = "gpt-image-1.5"
mode = "image_generation"
max_input_tokens = 128000
max_output_tokens = 0
input_cost_per_token = 0.000005
cache_read_input_token_cost = 0.00000125
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_pdf_input = true
open_weights = false
release_date = "2025-12-16"
supported_modalities = ["text", "image"]
supported_output_modalities = ["image"]
cache_read_input_image_token_cost = 0.000002
input_cost_per_image_token = 0.000008
output_cost_per_image_token = 0.000032
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."gpt-image-1.5".pricing."azure"]
cache_read_input_image_token_cost = 0.000002
cache_read_input_token_cost = 0.00000125
input_cost_per_image_token = 0.000008
input_cost_per_token = 0.000005
output_cost_per_image_token = 0.000032
[models."gpt-image-1.5".pricing."openai"]
cache_read_input_image_token_cost = 0.000002
cache_read_input_token_cost = 0.00000125
input_cost_per_image_token = 0.000008
input_cost_per_token = 0.000005
output_cost_per_image_token = 0.000032
output_cost_per_token = 0.00001

[models."gpt-image-1.5-2025-12-16"]
mode = "image_generation"
input_cost_per_token = 0.000005
cache_read_input_token_cost = 0.00000125
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_vision = true
supports_pdf_input = true
cache_read_input_image_token_cost = 0.000002
input_cost_per_image_token = 0.000008
output_cost_per_image_token = 0.000032
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."gpt-image-1.5-2025-12-16".pricing."azure"]
cache_read_input_image_token_cost = 0.000002
cache_read_input_token_cost = 0.00000125
input_cost_per_image_token = 0.000008
input_cost_per_token = 0.000005
output_cost_per_image_token = 0.000032
[models."gpt-image-1.5-2025-12-16".pricing."openai"]
cache_read_input_image_token_cost = 0.000002
cache_read_input_token_cost = 0.00000125
input_cost_per_image_token = 0.000008
input_cost_per_token = 0.000005
output_cost_per_image_token = 0.000032
output_cost_per_token = 0.00001

[models."gpt-oss-120b"]
display_name = "GPT OSS 120B"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "abacus", "berget", "cerebras", "cloudferro-sherlock", "cortecs", "deepinfra", "evroc", "fastrouter", "firmware", "groq", "helicone", "io-net", "kilo", "nano-gpt", "nebius", "novita", "novita-ai", "nvidia", "openrouter", "ovhcloud", "privatemode-ai", "replicate", "sambanova", "scaleway", "siliconflow", "stackit", "submodel", "together_ai", "togetherai", "vercel", "vultr", "wandb", "watsonx"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-08"
release_date = "2025-08-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/"
reasoning_cost_per_token = 6e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gpt-oss-120b".pricing."abacus"]
input_cost_per_token = 8e-8
output_cost_per_token = 4.4e-7
[models."gpt-oss-120b".pricing."azure_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-oss-120b".pricing."berget"]
input_cost_per_token = 3e-7
output_cost_per_token = 9.000000000000001e-7
[models."gpt-oss-120b".pricing."cerebras"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 7.5e-7
[models."gpt-oss-120b".pricing."cloudferro-sherlock"]
input_cost_per_token = 0.00000292
output_cost_per_token = 0.00000292
[models."gpt-oss-120b".pricing."deepinfra"]
input_cost_per_token = 5e-8
output_cost_per_token = 4.5e-7
[models."gpt-oss-120b".pricing."evroc"]
input_cost_per_token = 2.4e-7
output_cost_per_token = 9.399999999999999e-7
[models."gpt-oss-120b".pricing."fastrouter"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-oss-120b".pricing."firmware"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-oss-120b".pricing."groq"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-oss-120b".pricing."helicone"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.6e-7
[models."gpt-oss-120b".pricing."io-net"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 4e-8
output_cost_per_token = 4.0000000000000003e-7
[models."gpt-oss-120b".pricing."kilo"]
input_cost_per_token = 3.9e-8
output_cost_per_token = 1.9e-7
[models."gpt-oss-120b".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
[models."gpt-oss-120b".pricing."nebius"]
cache_read_input_token_cost = 1.5e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
reasoning_cost_per_token = 6e-7
[models."gpt-oss-120b".pricing."novita"]
input_cost_per_token = 5e-8
output_cost_per_token = 2.5e-7
[models."gpt-oss-120b".pricing."novita-ai"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.5e-7
[models."gpt-oss-120b".pricing."openrouter"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 8e-7
[models."gpt-oss-120b".pricing."ovhcloud"]
input_cost_per_token = 8e-8
output_cost_per_token = 4e-7
[models."gpt-oss-120b".pricing."replicate"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 7.2e-7
[models."gpt-oss-120b".pricing."sambanova"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.0000045
[models."gpt-oss-120b".pricing."scaleway"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-oss-120b".pricing."siliconflow"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 4.5000000000000003e-7
[models."gpt-oss-120b".pricing."stackit"]
input_cost_per_token = 4.9e-7
output_cost_per_token = 7.1e-7
[models."gpt-oss-120b".pricing."submodel"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 5e-7
[models."gpt-oss-120b".pricing."together_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-oss-120b".pricing."togetherai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."gpt-oss-120b".pricing."vercel"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 5e-7
[models."gpt-oss-120b".pricing."vultr"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7
[models."gpt-oss-120b".pricing."wandb"]
input_cost_per_token = 0.015
output_cost_per_token = 0.06
[models."gpt-oss-120b".pricing."watsonx"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."gpt-oss-120b-TEE"]
display_name = "gpt oss 120b TEE"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 65536
input_cost_per_token = 4e-8
output_cost_per_token = 1.8e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gpt-oss-120b-TEE".pricing."chutes"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.8e-7

[models."gpt-oss-120b-maas"]
display_name = "GPT OSS 120B"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-08-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://console.cloud.google.com/vertex-ai/publishers/openai/model-garden/gpt-oss-120b-maas"

[models."gpt-oss-120b-maas".pricing."google-vertex"]
input_cost_per_token = 9e-8
output_cost_per_token = 3.6e-7
[models."gpt-oss-120b-maas".pricing."vertex_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."gpt-oss-120b-mxfp-GGUF"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "lemonade"
providers = ["lemonade"]
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."gpt-oss-120b-mxfp-GGUF".pricing."lemonade"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."gpt-oss-120b:exacto"]
display_name = "GPT OSS 120B (exacto)"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.4e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-08-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gpt-oss-120b:exacto".pricing."kilo"]
input_cost_per_token = 3.9e-8
output_cost_per_token = 1.9e-7
[models."gpt-oss-120b:exacto".pricing."openrouter"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.4e-7

[models."gpt-oss-120b:free"]
display_name = "gpt-oss-120b (free)"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-08-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gpt-oss-20b"]
display_name = "GPT OSS 20B"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 4e-8
output_cost_per_token = 1.5e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "chutes", "fastrouter", "firmware", "groq", "helicone", "io-net", "kilo", "lmstudio", "nebius", "novita", "novita-ai", "openrouter", "ovhcloud", "siliconflow", "together_ai", "vercel", "wandb"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "1970-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."gpt-oss-20b".pricing."chutes"]
input_cost_per_token = 2e-8
output_cost_per_token = 1.0000000000000001e-7
[models."gpt-oss-20b".pricing."deepinfra"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.5e-7
[models."gpt-oss-20b".pricing."fastrouter"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.0000000000000002e-7
[models."gpt-oss-20b".pricing."firmware"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.0000000000000002e-7
[models."gpt-oss-20b".pricing."groq"]
cache_read_input_token_cost = 3.75e-8
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."gpt-oss-20b".pricing."helicone"]
input_cost_per_token = 5e-8
output_cost_per_token = 2e-7
[models."gpt-oss-20b".pricing."io-net"]
cache_read_input_token_cost = 1.5e-8
input_cost_per_token = 3e-8
output_cost_per_token = 1.4e-7
[models."gpt-oss-20b".pricing."kilo"]
input_cost_per_token = 3e-8
output_cost_per_token = 1.4e-7
[models."gpt-oss-20b".pricing."nebius"]
cache_read_input_token_cost = 5e-9
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.0000000000000002e-7
[models."gpt-oss-20b".pricing."novita"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.5e-7
[models."gpt-oss-20b".pricing."novita-ai"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.5e-7
[models."gpt-oss-20b".pricing."openrouter"]
input_cost_per_token = 2e-8
output_cost_per_token = 1e-7
[models."gpt-oss-20b".pricing."ovhcloud"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.5e-7
[models."gpt-oss-20b".pricing."siliconflow"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.8e-7
[models."gpt-oss-20b".pricing."together_ai"]
input_cost_per_token = 5e-8
output_cost_per_token = 2e-7
[models."gpt-oss-20b".pricing."vercel"]
input_cost_per_token = 7e-8
output_cost_per_token = 3e-7
[models."gpt-oss-20b".pricing."wandb"]
input_cost_per_token = 0.005
output_cost_per_token = 0.02

[models."gpt-oss-20b-maas"]
display_name = "GPT OSS 20B"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-08-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://console.cloud.google.com/vertex-ai/publishers/openai/model-garden/gpt-oss-120b-maas"

[models."gpt-oss-20b-maas".pricing."google-vertex"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.5e-7
[models."gpt-oss-20b-maas".pricing."vertex_ai"]
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7

[models."gpt-oss-20b-mxfp4-GGUF"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "lemonade"
providers = ["lemonade"]
supports_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."gpt-oss-20b-mxfp4-GGUF".pricing."lemonade"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."gpt-oss-20b:free"]
display_name = "gpt-oss-20b (free)"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-08-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gpt-oss-safeguard-20b"]
display_name = "gpt-oss-safeguard-20b"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
cache_read_input_token_cost = 3.7e-8
litellm_provider = "groq"
providers = ["groq", "kilo", "openrouter", "vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."gpt-oss-safeguard-20b".pricing."groq"]
cache_read_input_token_cost = 3.7e-8
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."gpt-oss-safeguard-20b".pricing."kilo"]
cache_read_input_token_cost = 3.7e-8
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."gpt-oss-safeguard-20b".pricing."openrouter"]
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."gpt-oss-safeguard-20b".pricing."vercel"]
cache_read_input_token_cost = 4e-8
input_cost_per_token = 8e-8
output_cost_per_token = 3e-7

[models."gpt-oss:120b-cloud"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = true

[models."gpt-oss:120b-cloud".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."gpt-oss:20b-cloud"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = true

[models."gpt-oss:20b-cloud".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."gpt-realtime"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000004
output_cost_per_token = 0.000016
cache_read_input_token_cost = 4e-7
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text", "audio"]
cache_creation_input_audio_token_cost = 4e-7
input_cost_per_audio_token = 0.000032
input_cost_per_image = 0.000005
output_cost_per_audio_token = 0.000064
supported_endpoints = ["/v1/realtime"]
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-realtime".pricing."openai"]
cache_creation_input_audio_token_cost = 4e-7
cache_read_input_token_cost = 4e-7
input_cost_per_audio_token = 0.000032
input_cost_per_image = 0.000005
input_cost_per_token = 0.000004
output_cost_per_audio_token = 0.000064
output_cost_per_token = 0.000016

[models."gpt-realtime-2025-08-28"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000004
output_cost_per_token = 0.000016
cache_read_input_token_cost = 0.000004
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text", "audio"]
cache_creation_input_audio_token_cost = 0.000004
input_cost_per_audio_token = 0.000032
input_cost_per_image = 0.000005
output_cost_per_audio_token = 0.000064
supported_endpoints = ["/v1/realtime"]
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-realtime-2025-08-28".pricing."azure"]
cache_creation_input_audio_token_cost = 0.000004
cache_read_input_token_cost = 0.000004
input_cost_per_audio_token = 0.000032
input_cost_per_image = 0.000005
input_cost_per_token = 0.000004
output_cost_per_audio_token = 0.000064
output_cost_per_token = 0.000016
[models."gpt-realtime-2025-08-28".pricing."openai"]
cache_creation_input_audio_token_cost = 4e-7
cache_read_input_token_cost = 4e-7
input_cost_per_audio_token = 0.000032
input_cost_per_image = 0.000005
input_cost_per_token = 0.000004
output_cost_per_audio_token = 0.000064
output_cost_per_token = 0.000016

[models."gpt-realtime-mini"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000024
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text", "audio"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_audio_token_cost = 3e-7
input_cost_per_audio_token = 0.00001
output_cost_per_audio_token = 0.00002
supported_endpoints = ["/v1/realtime"]
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-realtime-mini".pricing."openai"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_audio_token_cost = 3e-7
input_cost_per_audio_token = 0.00001
input_cost_per_token = 6e-7
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024

[models."gpt-realtime-mini-2025-10-06"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000024
cache_read_input_token_cost = 6e-8
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text", "audio"]
cache_creation_input_audio_token_cost = 3e-7
input_cost_per_audio_token = 0.00001
input_cost_per_image = 8e-7
output_cost_per_audio_token = 0.00002
supported_endpoints = ["/v1/realtime"]
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-realtime-mini-2025-10-06".pricing."azure"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_token_cost = 6e-8
input_cost_per_audio_token = 0.00001
input_cost_per_image = 8e-7
input_cost_per_token = 6e-7
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024
[models."gpt-realtime-mini-2025-10-06".pricing."openai"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_audio_token_cost = 3e-7
cache_read_input_token_cost = 6e-8
input_cost_per_audio_token = 0.00001
input_cost_per_image = 8e-7
input_cost_per_token = 6e-7
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024

[models."gpt-realtime-mini-2025-12-15"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000024
cache_read_input_token_cost = 6e-8
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text", "audio"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_audio_token_cost = 3e-7
input_cost_per_audio_token = 0.00001
input_cost_per_image = 8e-7
output_cost_per_audio_token = 0.00002
supported_endpoints = ["/v1/realtime"]
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."gpt-realtime-mini-2025-12-15".pricing."openai"]
cache_creation_input_audio_token_cost = 3e-7
cache_read_input_audio_token_cost = 3e-7
cache_read_input_token_cost = 6e-8
input_cost_per_audio_token = 0.00001
input_cost_per_image = 8e-7
input_cost_per_token = 6e-7
output_cost_per_audio_token = 0.00002
output_cost_per_token = 0.0000024

[models."grok-2"]
display_name = "Grok 2"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-08"
release_date = "2024-08-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true
supports_web_search = true

[models."grok-2".pricing."xai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001

[models."grok-2-1212"]
display_name = "Grok 2 (1212)"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-08"
release_date = "2024-12-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true
supports_web_search = true

[models."grok-2-1212".pricing."xai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001

[models."grok-2-latest"]
display_name = "Grok 2 Latest"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-08"
release_date = "2024-08-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true
supports_web_search = true

[models."grok-2-latest".pricing."xai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001

[models."grok-2-vision"]
display_name = "Grok 2 Vision"
model_family = "grok"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-08"
release_date = "2024-08-20"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_image = 0.000002
supports_tool_choice = true
supports_web_search = true

[models."grok-2-vision".pricing."xai"]
input_cost_per_image = 0.000002
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001

[models."grok-2-vision-1212"]
display_name = "Grok 2 Vision (1212)"
model_family = "grok"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-08"
release_date = "2024-08-20"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_image = 0.000002
supports_tool_choice = true
supports_web_search = true

[models."grok-2-vision-1212".pricing."xai"]
input_cost_per_image = 0.000002
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001

[models."grok-2-vision-latest"]
display_name = "Grok 2 Vision Latest"
model_family = "grok"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-08"
release_date = "2024-08-20"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_image = 0.000002
supports_tool_choice = true
supports_web_search = true

[models."grok-2-vision-latest".pricing."xai"]
input_cost_per_image = 0.000002
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001

[models."grok-3"]
display_name = "Grok 3"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services", "helicone", "xai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-02-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://devblogs.microsoft.com/foundry/announcing-grok-3-and-grok-3-mini-on-azure-ai-foundry/"
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."grok-3".pricing."azure"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."grok-3".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."grok-3".pricing."azure_ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."grok-3".pricing."azure_ai/global"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."grok-3".pricing."helicone"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."grok-3".pricing."xai"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."grok-3-beta"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 7.5e-7
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
source = "https://x.ai/api#pricing"
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."grok-3-beta".pricing."xai"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."grok-3-fast-beta"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 0.00000125
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
source = "https://x.ai/api#pricing"
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."grok-3-fast-beta".pricing."xai"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025

[models."grok-3-fast-latest"]
display_name = "Grok 3 Fast Latest"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
cache_read_input_token_cost = 0.00000125
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-02-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://x.ai/api#pricing"
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."grok-3-fast-latest".pricing."xai"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025

[models."grok-3-latest"]
display_name = "Grok 3 Latest"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 7.5e-7
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-02-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://x.ai/api#pricing"
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."grok-3-latest".pricing."xai"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."grok-3-mini"]
display_name = "Grok 3 Mini"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000127
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services", "helicone", "xai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-02-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://devblogs.microsoft.com/foundry/announcing-grok-3-and-grok-3-mini-on-azure-ai-foundry/"
reasoning_cost_per_token = 5e-7
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."grok-3-mini".pricing."azure"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7
reasoning_cost_per_token = 5e-7
[models."grok-3-mini".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7
reasoning_cost_per_token = 5e-7
[models."grok-3-mini".pricing."azure_ai"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000127
[models."grok-3-mini".pricing."azure_ai/global"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000127
[models."grok-3-mini".pricing."helicone"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7
[models."grok-3-mini".pricing."xai"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7

[models."grok-3-mini-beta"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 7.5e-8
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_reasoning = true
source = "https://x.ai/api#pricing"
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."grok-3-mini-beta".pricing."xai"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7

[models."grok-3-mini-fast"]
display_name = "Grok 3 Mini Fast"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 6e-7
output_cost_per_token = 0.000004
cache_read_input_token_cost = 1.5e-7
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-02-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://x.ai/api#pricing"
reasoning_cost_per_token = 0.000004
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."grok-3-mini-fast".pricing."xai"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.000004

[models."grok-3-mini-fast-beta"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 6e-7
output_cost_per_token = 0.000004
cache_read_input_token_cost = 1.5e-7
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_reasoning = true
source = "https://x.ai/api#pricing"
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."grok-3-mini-fast-beta".pricing."xai"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.000004

[models."grok-3-mini-fast-latest"]
display_name = "Grok 3 Mini Fast Latest"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 6e-7
output_cost_per_token = 0.000004
cache_read_input_token_cost = 1.5e-7
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-02-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://x.ai/api#pricing"
reasoning_cost_per_token = 0.000004
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."grok-3-mini-fast-latest".pricing."xai"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.000004

[models."grok-3-mini-latest"]
display_name = "Grok 3 Mini Latest"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 7.5e-8
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-02-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://x.ai/api#pricing"
reasoning_cost_per_token = 5e-7
supports_response_schema = false
supports_tool_choice = true
supports_web_search = true

[models."grok-3-mini-latest".pricing."xai"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7

[models."grok-4"]
display_name = "Grok 4"
model_family = "grok"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services", "helicone", "xai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-07"
release_date = "2025-07-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/pricing/details/ai-foundry-models/grok/"
reasoning_cost_per_token = 0.000015
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."grok-4".pricing."azure"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
reasoning_cost_per_token = 0.000015
[models."grok-4".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
reasoning_cost_per_token = 0.000015
[models."grok-4".pricing."azure_ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."grok-4".pricing."helicone"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."grok-4".pricing."xai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."grok-4-0709"]
display_name = "grok-4-0709"
model_family = "grok"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "xai"
providers = ["xai", "abacus", "jiekou"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2026-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://docs.x.ai/docs/models"
input_cost_per_token_above_128k_tokens = 0.000006
output_cost_per_token_above_128k_tokens = 0.00003
supports_tool_choice = true
supports_web_search = true

[models."grok-4-0709".pricing."abacus"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."grok-4-0709".pricing."jiekou"]
input_cost_per_token = 0.0000027
output_cost_per_token = 0.0000135
[models."grok-4-0709".pricing."xai"]
input_cost_per_token = 0.000003
input_cost_per_token_above_128k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_128k_tokens = 0.00003

[models."grok-4-1"]
display_name = "grok-4.1"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001
litellm_provider = "302ai"
providers = ["302ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-06"
release_date = "2025-11-18"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."grok-4-1".pricing."302ai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001

[models."grok-4-1-fast"]
display_name = "Grok 4.1 Fast"
model_family = "grok"
mode = "chat"
max_input_tokens = 2000000
max_output_tokens = 2000000
max_tokens = 2000000
input_cost_per_token = 2e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 5e-8
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-07"
release_date = "2025-11-19"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://docs.x.ai/docs/models/grok-4-1-fast-reasoning"
input_cost_per_token_above_128k_tokens = 4e-7
output_cost_per_token_above_128k_tokens = 0.000001
supports_audio_input = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."grok-4-1-fast".pricing."xai"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
input_cost_per_token_above_128k_tokens = 4e-7
output_cost_per_token = 5e-7
output_cost_per_token_above_128k_tokens = 0.000001

[models."grok-4-1-fast-non-reasoning"]
display_name = "grok-4-1-fast-non-reasoning"
model_family = "grok"
mode = "chat"
max_input_tokens = 2000000
max_output_tokens = 2000000
max_tokens = 2000000
input_cost_per_token = 2e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 5e-8
litellm_provider = "xai"
providers = ["xai", "302ai", "abacus", "helicone", "jiekou"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-06"
release_date = "2025-11-20"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://docs.x.ai/docs/models/grok-4-1-fast-non-reasoning"
input_cost_per_token_above_128k_tokens = 4e-7
output_cost_per_token_above_128k_tokens = 0.000001
supports_audio_input = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."grok-4-1-fast-non-reasoning".pricing."302ai"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
[models."grok-4-1-fast-non-reasoning".pricing."abacus"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
[models."grok-4-1-fast-non-reasoning".pricing."helicone"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
output_cost_per_token = 5e-7
[models."grok-4-1-fast-non-reasoning".pricing."jiekou"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 4.5000000000000003e-7
[models."grok-4-1-fast-non-reasoning".pricing."xai"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
input_cost_per_token_above_128k_tokens = 4e-7
output_cost_per_token = 5e-7
output_cost_per_token_above_128k_tokens = 0.000001

[models."grok-4-1-fast-non-reasoning-latest"]
mode = "chat"
max_input_tokens = 2000000
max_output_tokens = 2000000
max_tokens = 2000000
input_cost_per_token = 2e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 5e-8
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_vision = true
source = "https://docs.x.ai/docs/models/grok-4-1-fast-non-reasoning"
input_cost_per_token_above_128k_tokens = 4e-7
output_cost_per_token_above_128k_tokens = 0.000001
supports_audio_input = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."grok-4-1-fast-non-reasoning-latest".pricing."xai"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
input_cost_per_token_above_128k_tokens = 4e-7
output_cost_per_token = 5e-7
output_cost_per_token_above_128k_tokens = 0.000001

[models."grok-4-1-fast-reasoning"]
display_name = "grok-4-1-fast-reasoning"
model_family = "grok"
mode = "chat"
max_input_tokens = 2000000
max_output_tokens = 2000000
max_tokens = 2000000
input_cost_per_token = 2e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 5e-8
litellm_provider = "xai"
providers = ["xai", "302ai", "helicone", "jiekou"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-06"
release_date = "2025-11-20"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://docs.x.ai/docs/models/grok-4-1-fast-reasoning"
input_cost_per_token_above_128k_tokens = 4e-7
output_cost_per_token_above_128k_tokens = 0.000001
supports_audio_input = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."grok-4-1-fast-reasoning".pricing."302ai"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
[models."grok-4-1-fast-reasoning".pricing."helicone"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
output_cost_per_token = 5e-7
[models."grok-4-1-fast-reasoning".pricing."jiekou"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 4.5000000000000003e-7
[models."grok-4-1-fast-reasoning".pricing."xai"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
input_cost_per_token_above_128k_tokens = 4e-7
output_cost_per_token = 5e-7
output_cost_per_token_above_128k_tokens = 0.000001

[models."grok-4-1-fast-reasoning-latest"]
mode = "chat"
max_input_tokens = 2000000
max_output_tokens = 2000000
max_tokens = 2000000
input_cost_per_token = 2e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 5e-8
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
source = "https://docs.x.ai/docs/models/grok-4-1-fast-reasoning"
input_cost_per_token_above_128k_tokens = 4e-7
output_cost_per_token_above_128k_tokens = 0.000001
supports_audio_input = true
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."grok-4-1-fast-reasoning-latest".pricing."xai"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
input_cost_per_token_above_128k_tokens = 4e-7
output_cost_per_token = 5e-7
output_cost_per_token_above_128k_tokens = 0.000001

[models."grok-4-fast"]
display_name = "Grok 4 Fast"
model_family = "grok"
mode = "chat"
max_input_tokens = 2000000
max_output_tokens = 30000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 5.0000000000000004e-8
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-07"
release_date = "2025-09-19"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."grok-4-fast".pricing."xai"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7

[models."grok-4-fast-non-reasoning"]
display_name = "grok-4-fast-non-reasoning"
model_family = "grok"
mode = "chat"
max_input_tokens = 2000000
max_output_tokens = 2000000
max_tokens = 2000000
input_cost_per_token = 2e-7
output_cost_per_token = 5e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "302ai", "abacus", "azure", "azure-cognitive-services", "helicone", "jiekou", "xai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-06"
release_date = "2025-09-23"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."grok-4-fast-non-reasoning".pricing."302ai"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
[models."grok-4-fast-non-reasoning".pricing."abacus"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
[models."grok-4-fast-non-reasoning".pricing."azure"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
[models."grok-4-fast-non-reasoning".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
[models."grok-4-fast-non-reasoning".pricing."azure_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 5e-7
[models."grok-4-fast-non-reasoning".pricing."helicone"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
output_cost_per_token = 5e-7
[models."grok-4-fast-non-reasoning".pricing."jiekou"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 4.5000000000000003e-7
[models."grok-4-fast-non-reasoning".pricing."xai"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
input_cost_per_token_above_128k_tokens = 4e-7
output_cost_per_token = 5e-7
output_cost_per_token_above_128k_tokens = 0.000001

[models."grok-4-fast-reasoning"]
display_name = "grok-4-fast-reasoning"
model_family = "grok"
mode = "chat"
max_input_tokens = 2000000
max_output_tokens = 2000000
max_tokens = 2000000
input_cost_per_token = 2e-7
output_cost_per_token = 5e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "302ai", "azure", "azure-cognitive-services", "helicone", "jiekou", "xai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-06"
release_date = "2025-09-23"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/pricing/details/ai-foundry-models/grok/"
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."grok-4-fast-reasoning".pricing."302ai"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
[models."grok-4-fast-reasoning".pricing."azure"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
[models."grok-4-fast-reasoning".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
[models."grok-4-fast-reasoning".pricing."azure_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 5e-7
[models."grok-4-fast-reasoning".pricing."helicone"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
output_cost_per_token = 5e-7
[models."grok-4-fast-reasoning".pricing."jiekou"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 4.5000000000000003e-7
[models."grok-4-fast-reasoning".pricing."xai"]
cache_read_input_token_cost = 5e-8
input_cost_per_token = 2e-7
input_cost_per_token_above_128k_tokens = 4e-7
output_cost_per_token = 5e-7
output_cost_per_token_above_128k_tokens = 0.000001

[models."grok-4-latest"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
source = "https://docs.x.ai/docs/models"
input_cost_per_token_above_128k_tokens = 0.000006
output_cost_per_token_above_128k_tokens = 0.00003
supports_tool_choice = true
supports_web_search = true

[models."grok-4-latest".pricing."xai"]
input_cost_per_token = 0.000003
input_cost_per_token_above_128k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_128k_tokens = 0.00003

[models."grok-41-fast"]
display_name = "Grok 4.1 Fast"
model_family = "grok"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 64000
input_cost_per_token = 5e-7
output_cost_per_token = 0.00000125
cache_read_input_token_cost = 1.25e-7
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-07"
release_date = "2025-12-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."grok-41-fast".pricing."venice"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 5e-7
output_cost_per_token = 0.00000125

[models."grok-beta"]
display_name = "Grok Beta"
model_family = "grok-beta"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000005
output_cost_per_token = 0.000015
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-08"
release_date = "2024-11-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true
supports_web_search = true

[models."grok-beta".pricing."xai"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000015

[models."grok-code"]
display_name = "Grok Code Fast 1"
model_family = "grok"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
litellm_provider = "opencode"
providers = ["opencode"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-08-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."grok-code-fast"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 2e-7
output_cost_per_token = 0.0000015
cache_read_input_token_cost = 2e-8
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_reasoning = true
source = "https://docs.x.ai/docs/models"
supports_tool_choice = true

[models."grok-code-fast".pricing."xai"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2e-7
output_cost_per_token = 0.0000015

[models."grok-code-fast-1"]
display_name = "Grok Code Fast 1"
model_family = "grok"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 2e-7
output_cost_per_token = 0.0000015
litellm_provider = "azure_ai"
providers = ["azure_ai", "abacus", "azure", "azure-cognitive-services", "github-copilot", "helicone", "jiekou", "venice", "xai"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2023-10"
release_date = "2025-08-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/pricing/details/ai-foundry-models/grok/"
supports_response_schema = true
supports_tool_choice = true
supports_web_search = true

[models."grok-code-fast-1".pricing."abacus"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015
[models."grok-code-fast-1".pricing."azure"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015
[models."grok-code-fast-1".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015
[models."grok-code-fast-1".pricing."azure_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 0.0000015
[models."grok-code-fast-1".pricing."helicone"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2e-7
output_cost_per_token = 0.0000015
[models."grok-code-fast-1".pricing."jiekou"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 0.00000135
[models."grok-code-fast-1".pricing."venice"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000187
[models."grok-code-fast-1".pricing."xai"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2e-7
output_cost_per_token = 0.0000015

[models."grok-code-fast-1-0825"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 2e-7
output_cost_per_token = 0.0000015
cache_read_input_token_cost = 2e-8
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_reasoning = true
source = "https://docs.x.ai/docs/models"
supports_tool_choice = true

[models."grok-code-fast-1-0825".pricing."xai"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2e-7
output_cost_per_token = 0.0000015

[models."grok-vision-beta"]
display_name = "Grok Vision Beta"
model_family = "grok-vision"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000005
output_cost_per_token = 0.000015
litellm_provider = "xai"
providers = ["xai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-08"
release_date = "2024-11-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_image = 0.000005
supports_tool_choice = true
supports_web_search = true

[models."grok-vision-beta".pricing."xai"]
input_cost_per_image = 0.000005
input_cost_per_token = 0.000005
output_cost_per_token = 0.000015

[models."groq-llama-4-maverick-17b-128e-instruct"]
display_name = "Groq-Llama-4-Maverick-17B-128E-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "llama"
providers = ["llama"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2025-04-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."gryphe/mythomax-l2-13b"]
display_name = "Mythomax L2 13B"
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 3200
max_tokens = 8192
input_cost_per_token = 0.000001875
output_cost_per_token = 0.000001875
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "novita", "novita-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2024-04-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true
supports_tool_choice = true

[models."gryphe/mythomax-l2-13b".pricing."kilo"]
input_cost_per_token = 6e-8
output_cost_per_token = 6e-8
[models."gryphe/mythomax-l2-13b".pricing."novita"]
input_cost_per_token = 9e-8
output_cost_per_token = 9e-8
[models."gryphe/mythomax-l2-13b".pricing."novita-ai"]
input_cost_per_token = 9e-8
output_cost_per_token = 9e-8
[models."gryphe/mythomax-l2-13b".pricing."openrouter"]
input_cost_per_token = 0.000001875
output_cost_per_token = 0.000001875

[models."hd/1024-x-1024/dall-e-3"]
mode = "image_generation"
output_cost_per_token = 0
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 7.629e-8

[models."hd/1024-x-1024/dall-e-3".pricing."azure"]
input_cost_per_pixel = 7.629e-8
output_cost_per_token = 0
[models."hd/1024-x-1024/dall-e-3".pricing."openai"]
input_cost_per_pixel = 7.629e-8
output_cost_per_pixel = 0

[models."hd/1024-x-1792/dall-e-3"]
mode = "image_generation"
output_cost_per_token = 0
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 6.539e-8

[models."hd/1024-x-1792/dall-e-3".pricing."azure"]
input_cost_per_pixel = 6.539e-8
output_cost_per_token = 0
[models."hd/1024-x-1792/dall-e-3".pricing."openai"]
input_cost_per_pixel = 6.539e-8
output_cost_per_pixel = 0

[models."hd/1792-x-1024/dall-e-3"]
mode = "image_generation"
output_cost_per_token = 0
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 6.539e-8

[models."hd/1792-x-1024/dall-e-3".pricing."azure"]
input_cost_per_pixel = 6.539e-8
output_cost_per_token = 0
[models."hd/1792-x-1024/dall-e-3".pricing."openai"]
input_cost_per_pixel = 6.539e-8
output_cost_per_pixel = 0

[models."hermes-2-pro-llama-3-8b"]
display_name = "Hermes 2 Pro Llama 3 8B"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 1.4e-7
output_cost_per_token = 1.4e-7
litellm_provider = "helicone"
providers = ["helicone"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-05"
release_date = "2024-05-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hermes-2-pro-llama-3-8b".pricing."helicone"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 1.4e-7

[models."hermes-3-llama-3-1-405b"]
display_name = "Hermes 3 Llama 3.1 405b"
model_family = "hermes"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
input_cost_per_token = 0.0000011
output_cost_per_token = 0.000003
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2025-09-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hermes-3-llama-3-1-405b".pricing."venice"]
input_cost_per_token = 0.0000011
output_cost_per_token = 0.000003

[models."hermes3-405b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 8e-7
output_cost_per_token = 8e-7
litellm_provider = "lambda_ai"
providers = ["lambda_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hermes3-405b".pricing."lambda_ai"]
input_cost_per_token = 8e-7
output_cost_per_token = 8e-7

[models."hermes3-70b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7
litellm_provider = "lambda_ai"
providers = ["lambda_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hermes3-70b".pricing."lambda_ai"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7

[models."hermes3-8b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2.5e-8
output_cost_per_token = 4e-8
litellm_provider = "lambda_ai"
providers = ["lambda_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."hermes3-8b".pricing."lambda_ai"]
input_cost_per_token = 2.5e-8
output_cost_per_token = 4e-8

[models."hf:MiniMaxAI/MiniMax-M2"]
display_name = "MiniMax-M2"
model_family = "minimax"
mode = "chat"
max_input_tokens = 196608
max_output_tokens = 131000
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-10-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:MiniMaxAI/MiniMax-M2".pricing."synthetic"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998

[models."hf:MiniMaxAI/MiniMax-M2-1"]
display_name = "MiniMax-M2.1"
model_family = "minimax"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:MiniMaxAI/MiniMax-M2-1".pricing."synthetic"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998

[models."hf:Qwen/Qwen2-5-Coder-32B-Instruct"]
display_name = "Qwen2.5-Coder-32B-Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 8.000000000000001e-7
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2024-11-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:Qwen/Qwen2-5-Coder-32B-Instruct".pricing."synthetic"]
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 8.000000000000001e-7

[models."hf:Qwen/Qwen3-235B-A22B-Instruct-2507"]
display_name = "Qwen 3 235B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 32000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:Qwen/Qwen3-235B-A22B-Instruct-2507".pricing."synthetic"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7

[models."hf:Qwen/Qwen3-235B-A22B-Thinking-2507"]
display_name = "Qwen3 235B A22B Thinking 2507"
model_family = "qwen"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 32000
input_cost_per_token = 6.5e-7
output_cost_per_token = 0.000003
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:Qwen/Qwen3-235B-A22B-Thinking-2507".pricing."synthetic"]
input_cost_per_token = 6.5e-7
output_cost_per_token = 0.000003

[models."hf:Qwen/Qwen3-Coder-480B-A35B-Instruct"]
display_name = "Qwen 3 Coder 480B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 32000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:Qwen/Qwen3-Coder-480B-A35B-Instruct".pricing."synthetic"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002

[models."hf:deepseek-ai/DeepSeek-R1"]
display_name = "DeepSeek R1"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2025-01-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:deepseek-ai/DeepSeek-R1".pricing."synthetic"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998

[models."hf:deepseek-ai/DeepSeek-R1-0528"]
display_name = "DeepSeek R1 (0528)"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000008
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-08-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:deepseek-ai/DeepSeek-R1-0528".pricing."synthetic"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000008

[models."hf:deepseek-ai/DeepSeek-V3"]
display_name = "DeepSeek V3"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00000125
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2025-01-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:deepseek-ai/DeepSeek-V3".pricing."synthetic"]
input_cost_per_token = 0.00000125
output_cost_per_token = 0.00000125

[models."hf:deepseek-ai/DeepSeek-V3-0324"]
display_name = "DeepSeek V3 (0324)"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-08-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:deepseek-ai/DeepSeek-V3-0324".pricing."synthetic"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."hf:deepseek-ai/DeepSeek-V3-1"]
display_name = "DeepSeek V3.1"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000168
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-08-21"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:deepseek-ai/DeepSeek-V3-1".pricing."synthetic"]
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000168

[models."hf:deepseek-ai/DeepSeek-V3-1-Terminus"]
display_name = "DeepSeek V3.1 Terminus"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-09-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:deepseek-ai/DeepSeek-V3-1-Terminus".pricing."synthetic"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."hf:deepseek-ai/DeepSeek-V3-2"]
display_name = "DeepSeek V3.2"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 162816
max_output_tokens = 8000
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.0000000000000003e-7
cache_read_input_token_cost = 2.7e-7
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:deepseek-ai/DeepSeek-V3-2".pricing."synthetic"]
cache_read_input_token_cost = 2.7e-7
input_cost_per_token = 2.7e-7
output_cost_per_token = 4.0000000000000003e-7

[models."hf:meta-llama/Llama-3-1-405B-Instruct"]
display_name = "Llama-3.1-405B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
input_cost_per_token = 0.000003
output_cost_per_token = 0.000003
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:meta-llama/Llama-3-1-405B-Instruct".pricing."synthetic"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000003

[models."hf:meta-llama/Llama-3-1-70B-Instruct"]
display_name = "Llama-3.1-70B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
input_cost_per_token = 9.000000000000001e-7
output_cost_per_token = 9.000000000000001e-7
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:meta-llama/Llama-3-1-70B-Instruct".pricing."synthetic"]
input_cost_per_token = 9.000000000000001e-7
output_cost_per_token = 9.000000000000001e-7

[models."hf:meta-llama/Llama-3-1-8B-Instruct"]
display_name = "Llama-3.1-8B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:meta-llama/Llama-3-1-8B-Instruct".pricing."synthetic"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7

[models."hf:meta-llama/Llama-3-3-70B-Instruct"]
display_name = "Llama-3.3-70B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
input_cost_per_token = 9.000000000000001e-7
output_cost_per_token = 9.000000000000001e-7
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-12-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:meta-llama/Llama-3-3-70B-Instruct".pricing."synthetic"]
input_cost_per_token = 9.000000000000001e-7
output_cost_per_token = 9.000000000000001e-7

[models."hf:meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"]
display_name = "Llama-4-Maverick-17B-128E-Instruct-FP8"
model_family = "llama"
mode = "chat"
max_input_tokens = 524000
max_output_tokens = 4096
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-04-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8".pricing."synthetic"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7

[models."hf:meta-llama/Llama-4-Scout-17B-16E-Instruct"]
display_name = "Llama-4-Scout-17B-16E-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 328000
max_output_tokens = 4096
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-04-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:meta-llama/Llama-4-Scout-17B-16E-Instruct".pricing."synthetic"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."hf:moonshotai/Kimi-K2-5"]
display_name = "Kimi K2.5"
model_family = "kimi"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65536
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2026-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:moonshotai/Kimi-K2-5".pricing."synthetic"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998

[models."hf:moonshotai/Kimi-K2-Instruct-0905"]
display_name = "Kimi K2 0905"
model_family = "kimi"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 32768
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-09-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:moonshotai/Kimi-K2-Instruct-0905".pricing."synthetic"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."hf:moonshotai/Kimi-K2-Thinking"]
display_name = "Kimi K2 Thinking"
model_family = "kimi-thinking"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-11"
release_date = "2025-11-07"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:moonshotai/Kimi-K2-Thinking".pricing."synthetic"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998

[models."hf:nvidia/Kimi-K2-5-NVFP4"]
display_name = "Kimi K2.5 (NVFP4)"
model_family = "kimi"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65536
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2026-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:nvidia/Kimi-K2-5-NVFP4".pricing."synthetic"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998

[models."hf:openai/gpt-oss-120b"]
display_name = "GPT OSS 120B"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 1.0000000000000001e-7
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-08-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:openai/gpt-oss-120b".pricing."synthetic"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 1.0000000000000001e-7

[models."hf:zai-org/GLM-4-6"]
display_name = "GLM 4.6"
model_family = "glm"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-09-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:zai-org/GLM-4-6".pricing."synthetic"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998

[models."hf:zai-org/GLM-4-7"]
display_name = "GLM 4.7"
model_family = "glm"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998
litellm_provider = "synthetic"
providers = ["synthetic"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-12-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."hf:zai-org/GLM-4-7".pricing."synthetic"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998

[models."high/1024-x-1024/gpt-image-1"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 1.59263611e-7
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."high/1024-x-1024/gpt-image-1".pricing."azure"]
input_cost_per_pixel = 1.59263611e-7
output_cost_per_pixel = 0
[models."high/1024-x-1024/gpt-image-1".pricing."openai"]
input_cost_per_image = 0.167
input_cost_per_pixel = 1.59263611e-7
output_cost_per_pixel = 0

[models."high/1024-x-1024/gpt-image-1-mini"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure"]
input_cost_per_pixel = 3.173828125e-8
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."high/1024-x-1024/gpt-image-1-mini".pricing."azure"]
input_cost_per_pixel = 3.173828125e-8
output_cost_per_pixel = 0

[models."high/1024-x-1024/gpt-image-1.5"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.133
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."high/1024-x-1024/gpt-image-1.5".pricing."openai"]
input_cost_per_image = 0.133

[models."high/1024-x-1024/gpt-image-1.5-2025-12-16"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.133
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."high/1024-x-1024/gpt-image-1.5-2025-12-16".pricing."openai"]
input_cost_per_image = 0.133

[models."high/1024-x-1536/gpt-image-1"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 1.58945719e-7
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."high/1024-x-1536/gpt-image-1".pricing."azure"]
input_cost_per_pixel = 1.58945719e-7
output_cost_per_pixel = 0
[models."high/1024-x-1536/gpt-image-1".pricing."openai"]
input_cost_per_image = 0.25
input_cost_per_pixel = 1.58945719e-7
output_cost_per_pixel = 0

[models."high/1024-x-1536/gpt-image-1-mini"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure"]
input_cost_per_pixel = 3.173828125e-8
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."high/1024-x-1536/gpt-image-1-mini".pricing."azure"]
input_cost_per_pixel = 3.173828125e-8
output_cost_per_pixel = 0

[models."high/1024-x-1536/gpt-image-1.5"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.2
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."high/1024-x-1536/gpt-image-1.5".pricing."openai"]
input_cost_per_image = 0.2

[models."high/1024-x-1536/gpt-image-1.5-2025-12-16"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.2
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."high/1024-x-1536/gpt-image-1.5-2025-12-16".pricing."openai"]
input_cost_per_image = 0.2

[models."high/1536-x-1024/gpt-image-1"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 1.58945719e-7
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."high/1536-x-1024/gpt-image-1".pricing."azure"]
input_cost_per_pixel = 1.58945719e-7
output_cost_per_pixel = 0
[models."high/1536-x-1024/gpt-image-1".pricing."openai"]
input_cost_per_image = 0.25
input_cost_per_pixel = 1.58945719e-7
output_cost_per_pixel = 0

[models."high/1536-x-1024/gpt-image-1-mini"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure"]
input_cost_per_pixel = 3.1575520833e-8
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."high/1536-x-1024/gpt-image-1-mini".pricing."azure"]
input_cost_per_pixel = 3.1575520833e-8
output_cost_per_pixel = 0

[models."high/1536-x-1024/gpt-image-1.5"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.2
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."high/1536-x-1024/gpt-image-1.5".pricing."openai"]
input_cost_per_image = 0.2

[models."high/1536-x-1024/gpt-image-1.5-2025-12-16"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.2
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."high/1536-x-1024/gpt-image-1.5-2025-12-16".pricing."openai"]
input_cost_per_image = 0.2

[models."ibm-granite/granite-3.3-8b-instruct"]
mode = "chat"
input_cost_per_token = 3e-8
output_cost_per_token = 2.5e-7
litellm_provider = "replicate"
providers = ["replicate"]
supports_function_calling = true
supports_system_messages = true

[models."ibm-granite/granite-3.3-8b-instruct".pricing."replicate"]
input_cost_per_token = 3e-8
output_cost_per_token = 2.5e-7

[models."ibm/granite-13b-chat-v2"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-7
output_cost_per_token = 6e-7
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = false
supports_vision = false
supports_parallel_function_calling = false

[models."ibm/granite-13b-chat-v2".pricing."watsonx"]
input_cost_per_token = 6e-7
output_cost_per_token = 6e-7

[models."ibm/granite-13b-instruct-v2"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-7
output_cost_per_token = 6e-7
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = false
supports_vision = false
supports_parallel_function_calling = false

[models."ibm/granite-13b-instruct-v2".pricing."watsonx"]
input_cost_per_token = 6e-7
output_cost_per_token = 6e-7

[models."ibm/granite-3-3-8b-instruct"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = true
supports_vision = false
supports_parallel_function_calling = true

[models."ibm/granite-3-3-8b-instruct".pricing."watsonx"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."ibm/granite-3-8b-instruct"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = true
supports_vision = false
supports_prompt_caching = true
supports_audio_input = false
supports_audio_output = false
supports_parallel_function_calling = false
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."ibm/granite-3-8b-instruct".pricing."watsonx"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."ibm/granite-4-h-small"]
mode = "chat"
max_input_tokens = 20480
max_output_tokens = 20480
max_tokens = 20480
input_cost_per_token = 6e-8
output_cost_per_token = 2.5e-7
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = true
supports_vision = false
supports_parallel_function_calling = true

[models."ibm/granite-4-h-small".pricing."watsonx"]
input_cost_per_token = 6e-8
output_cost_per_token = 2.5e-7

[models."ibm/granite-guardian-3-2-2b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = false
supports_vision = false
supports_parallel_function_calling = false

[models."ibm/granite-guardian-3-2-2b".pricing."watsonx"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."ibm/granite-guardian-3-3-8b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = false
supports_vision = false
supports_parallel_function_calling = false

[models."ibm/granite-guardian-3-3-8b".pricing."watsonx"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."ibm/granite-ttm-1024-96-r2"]
mode = "chat"
max_input_tokens = 512
max_output_tokens = 512
max_tokens = 512
input_cost_per_token = 3.8e-7
output_cost_per_token = 3.8e-7
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = false
supports_vision = false
supports_parallel_function_calling = false

[models."ibm/granite-ttm-1024-96-r2".pricing."watsonx"]
input_cost_per_token = 3.8e-7
output_cost_per_token = 3.8e-7

[models."ibm/granite-ttm-1536-96-r2"]
mode = "chat"
max_input_tokens = 512
max_output_tokens = 512
max_tokens = 512
input_cost_per_token = 3.8e-7
output_cost_per_token = 3.8e-7
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = false
supports_vision = false
supports_parallel_function_calling = false

[models."ibm/granite-ttm-1536-96-r2".pricing."watsonx"]
input_cost_per_token = 3.8e-7
output_cost_per_token = 3.8e-7

[models."ibm/granite-ttm-512-96-r2"]
mode = "chat"
max_input_tokens = 512
max_output_tokens = 512
max_tokens = 512
input_cost_per_token = 3.8e-7
output_cost_per_token = 3.8e-7
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = false
supports_vision = false
supports_parallel_function_calling = false

[models."ibm/granite-ttm-512-96-r2".pricing."watsonx"]
input_cost_per_token = 3.8e-7
output_cost_per_token = 3.8e-7

[models."ibm/granite-vision-3-2-2b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = false
supports_vision = true
supports_parallel_function_calling = false

[models."ibm/granite-vision-3-2-2b".pricing."watsonx"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."imagegeneration@006"]
mode = "image_generation"
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
output_cost_per_image = 0.02

[models."imagegeneration@006".pricing."vertex_ai"]
output_cost_per_image = 0.02

[models."imagen-3.0-capability-001"]
mode = "image_generation"
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/image/edit-insert-objects"
output_cost_per_image = 0.04

[models."imagen-3.0-capability-001".pricing."vertex_ai"]
output_cost_per_image = 0.04

[models."imagen-3.0-fast-generate-001"]
mode = "image_generation"
litellm_provider = "gemini"
providers = ["gemini", "vertex_ai"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
output_cost_per_image = 0.02

[models."imagen-3.0-fast-generate-001".pricing."gemini"]
output_cost_per_image = 0.02
[models."imagen-3.0-fast-generate-001".pricing."vertex_ai"]
output_cost_per_image = 0.02

[models."imagen-3.0-generate-001"]
mode = "image_generation"
litellm_provider = "gemini"
providers = ["gemini", "vertex_ai"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
output_cost_per_image = 0.04

[models."imagen-3.0-generate-001".pricing."gemini"]
output_cost_per_image = 0.04
[models."imagen-3.0-generate-001".pricing."vertex_ai"]
output_cost_per_image = 0.04

[models."imagen-3.0-generate-002"]
mode = "image_generation"
litellm_provider = "gemini"
providers = ["gemini", "vertex_ai"]
deprecation_date = "2025-11-10"
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
output_cost_per_image = 0.04

[models."imagen-3.0-generate-002".pricing."gemini"]
output_cost_per_image = 0.04
[models."imagen-3.0-generate-002".pricing."vertex_ai"]
output_cost_per_image = 0.04

[models."imagen-4.0-fast-generate-001"]
display_name = "Imagen 4 Fast"
model_family = "imagen"
mode = "image_generation"
max_input_tokens = 480
max_output_tokens = 0
litellm_provider = "gemini"
providers = ["gemini", "vertex_ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-06"
supported_modalities = ["text"]
supported_output_modalities = ["image"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
output_cost_per_image = 0.02

[models."imagen-4.0-fast-generate-001".pricing."gemini"]
output_cost_per_image = 0.02
[models."imagen-4.0-fast-generate-001".pricing."vertex_ai"]
output_cost_per_image = 0.02

[models."imagen-4.0-generate-001"]
display_name = "Imagen 4"
model_family = "imagen"
mode = "image_generation"
max_input_tokens = 480
max_output_tokens = 0
litellm_provider = "gemini"
providers = ["gemini", "vertex_ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-05-22"
supported_modalities = ["text"]
supported_output_modalities = ["image"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
output_cost_per_image = 0.04

[models."imagen-4.0-generate-001".pricing."gemini"]
output_cost_per_image = 0.04
[models."imagen-4.0-generate-001".pricing."vertex_ai"]
output_cost_per_image = 0.04

[models."imagen-4.0-ultra-generate-001"]
display_name = "Imagen 4 Ultra"
model_family = "imagen"
mode = "image_generation"
max_input_tokens = 480
max_output_tokens = 0
litellm_provider = "aiml"
providers = ["aiml", "gemini", "vertex_ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-05-24"
supported_modalities = ["text"]
supported_output_modalities = ["image"]
source = "https://docs.aimlapi.com/api-references/image-models/google/imagen-4-ultra-generate"
output_cost_per_image = 0.063
supported_endpoints = ["/v1/images/generations"]

[models."imagen-4.0-ultra-generate-001".metadata]
notes = "Imagen 4.0 Ultra Generate API - Photorealistic image generation with precise text rendering"

[models."imagen-4.0-ultra-generate-001".pricing."aiml"]
output_cost_per_image = 0.063
[models."imagen-4.0-ultra-generate-001".pricing."gemini"]
output_cost_per_image = 0.06
[models."imagen-4.0-ultra-generate-001".pricing."vertex_ai"]
output_cost_per_image = 0.06

[models."inception/mercury"]
display_name = "Inception: Mercury"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-06-26"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."inception/mercury".pricing."kilo"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001

[models."inception/mercury-coder"]
display_name = "Inception: Mercury Coder"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-02-26"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."inception/mercury-coder".pricing."kilo"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001

[models."inception/mercury-coder-small"]
display_name = "Mercury Coder Small Beta"
model_family = "mercury"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-02-26"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."inception/mercury-coder-small".pricing."vercel"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001
[models."inception/mercury-coder-small".pricing."vercel_ai_gateway"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001

[models."inclusionAI/Ling-flash-2-0"]
display_name = "inclusionAI/Ling-flash-2.0"
model_family = "ling"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 131000
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "siliconflow"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-09-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."inclusionAI/Ling-flash-2-0".pricing."siliconflow"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7
[models."inclusionAI/Ling-flash-2-0".pricing."siliconflow-cn"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7

[models."inclusionAI/Ling-mini-2-0"]
display_name = "inclusionAI/Ling-mini-2.0"
model_family = "ling"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 131000
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "siliconflow"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-09-10"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."inclusionAI/Ling-mini-2-0".pricing."siliconflow"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
[models."inclusionAI/Ling-mini-2-0".pricing."siliconflow-cn"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7

[models."inclusionAI/Ring-flash-2-0"]
display_name = "inclusionAI/Ring-flash-2.0"
model_family = "ring"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 131000
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "siliconflow"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-09-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."inclusionAI/Ring-flash-2-0".pricing."siliconflow"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7
[models."inclusionAI/Ring-flash-2-0".pricing."siliconflow-cn"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7

[models."inclusionai/ling-1t"]
display_name = "Ling-1T"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 64000
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000224
cache_read_input_token_cost = 1.1e-7
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2025-10-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."inclusionai/ling-1t".pricing."zenmux"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000224

[models."inclusionai/ring-1t"]
display_name = "Ring-1T"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 64000
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000224
cache_read_input_token_cost = 1.1e-7
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2025-10-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."inclusionai/ring-1t".pricing."zenmux"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000224

[models."inpaint"]
mode = "image_edit"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.005
supported_endpoints = ["/v1/images/edits"]

[models."inpaint".pricing."stability"]
output_cost_per_image = 0.005

[models."intellect-3"]
display_name = "INTELLECT 3"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
input_cost_per_token = 2.19e-7
output_cost_per_token = 0.0000012019999999999999
litellm_provider = "cortecs"
providers = ["cortecs"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-11"
release_date = "2025-11-26"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."intellect-3".pricing."cortecs"]
input_cost_per_token = 2.19e-7
output_cost_per_token = 0.0000012019999999999999

[models."internlm2_5-20b-chat"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = true

[models."internlm2_5-20b-chat".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."intfloat/e5-mistral-7b-instruct"]
display_name = "e5-mistral-7b-instruct"
model_family = "text-embedding"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 0
input_cost_per_token = 1e-8
litellm_provider = "nebius"
providers = ["nebius", "stackit"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."intfloat/e5-mistral-7b-instruct".pricing."nebius"]
input_cost_per_token = 1e-8
[models."intfloat/e5-mistral-7b-instruct".pricing."stackit"]
input_cost_per_token = 2e-8
output_cost_per_token = 2e-8

[models."intfloat/multilingual-e5-large"]
display_name = "Multilingual-E5-large"
model_family = "text-embedding"
mode = "chat"
max_input_tokens = 512
max_output_tokens = 1024
input_cost_per_token = 2e-8
litellm_provider = "berget"
providers = ["berget"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-09"
release_date = "2025-09-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."intfloat/multilingual-e5-large".pricing."berget"]
input_cost_per_token = 2e-8

[models."intfloat/multilingual-e5-large-instruct"]
display_name = "E5 Multi-Lingual Large Embeddings 0.6B"
model_family = "text-embedding"
mode = "chat"
max_input_tokens = 512
max_output_tokens = 512
input_cost_per_token = 1.2e-7
output_cost_per_token = 1.2e-7
litellm_provider = "evroc"
providers = ["evroc", "berget"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2024-06-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."intfloat/multilingual-e5-large-instruct".pricing."berget"]
input_cost_per_token = 2e-8
[models."intfloat/multilingual-e5-large-instruct".pricing."evroc"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 1.2e-7

[models."invoke/anthropic.claude-3-5-sonnet-20240620-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_response_schema = true
supports_tool_choice = true

[models."invoke/anthropic.claude-3-5-sonnet-20240620-v1:0".metadata]
notes = "Anthropic via Invoke route does not currently support pdf input."

[models."invoke/anthropic.claude-3-5-sonnet-20240620-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."j2-light"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000003
litellm_provider = "ai21"
providers = ["ai21"]

[models."j2-light".pricing."ai21"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000003

[models."j2-mid"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00001
output_cost_per_token = 0.00001
litellm_provider = "ai21"
providers = ["ai21"]

[models."j2-mid".pricing."ai21"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00001

[models."j2-ultra"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000015
output_cost_per_token = 0.000015
litellm_provider = "ai21"
providers = ["ai21"]

[models."j2-ultra".pricing."ai21"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.000015

[models."jais-30b-chat"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.0032
output_cost_per_token = 0.00971
litellm_provider = "azure_ai"
providers = ["azure_ai"]
source = "https://azure.microsoft.com/en-us/products/ai-services/ai-foundry/models/jais-30b-chat"

[models."jais-30b-chat".pricing."azure_ai"]
input_cost_per_token = 0.0032
output_cost_per_token = 0.00971

[models."jamba-1.5"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 2e-7
output_cost_per_token = 4e-7
litellm_provider = "ai21"
providers = ["ai21", "vertex_ai"]
supports_tool_choice = true

[models."jamba-1.5".pricing."ai21"]
input_cost_per_token = 2e-7
output_cost_per_token = 4e-7
[models."jamba-1.5".pricing."vertex_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 4e-7

[models."jamba-1.5-large"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
litellm_provider = "ai21"
providers = ["ai21", "snowflake", "vertex_ai"]
supports_tool_choice = true

[models."jamba-1.5-large".pricing."ai21"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."jamba-1.5-large".pricing."vertex_ai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008

[models."jamba-1.5-large@001"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
litellm_provider = "ai21"
providers = ["ai21", "vertex_ai"]
supports_tool_choice = true

[models."jamba-1.5-large@001".pricing."ai21"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."jamba-1.5-large@001".pricing."vertex_ai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008

[models."jamba-1.5-mini"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 2e-7
output_cost_per_token = 4e-7
litellm_provider = "ai21"
providers = ["ai21", "snowflake", "vertex_ai"]
supports_tool_choice = true

[models."jamba-1.5-mini".pricing."ai21"]
input_cost_per_token = 2e-7
output_cost_per_token = 4e-7
[models."jamba-1.5-mini".pricing."vertex_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 4e-7

[models."jamba-1.5-mini@001"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 2e-7
output_cost_per_token = 4e-7
litellm_provider = "ai21"
providers = ["ai21", "vertex_ai"]
supports_tool_choice = true

[models."jamba-1.5-mini@001".pricing."ai21"]
input_cost_per_token = 2e-7
output_cost_per_token = 4e-7
[models."jamba-1.5-mini@001".pricing."vertex_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 4e-7

[models."jamba-instruct"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5e-7
output_cost_per_token = 7e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "snowflake"]
supports_tool_choice = true

[models."jamba-instruct".pricing."azure_ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 7e-7

[models."jamba-large-1.6"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
litellm_provider = "ai21"
providers = ["ai21"]
supports_tool_choice = true

[models."jamba-large-1.6".pricing."ai21"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008

[models."jamba-large-1.7"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
litellm_provider = "ai21"
providers = ["ai21"]
supports_tool_choice = true

[models."jamba-large-1.7".pricing."ai21"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008

[models."jamba-mini-1.6"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 2e-7
output_cost_per_token = 4e-7
litellm_provider = "ai21"
providers = ["ai21"]
supports_tool_choice = true

[models."jamba-mini-1.6".pricing."ai21"]
input_cost_per_token = 2e-7
output_cost_per_token = 4e-7

[models."jamba-mini-1.7"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 2e-7
output_cost_per_token = 4e-7
litellm_provider = "ai21"
providers = ["ai21"]
supports_tool_choice = true

[models."jamba-mini-1.7".pricing."ai21"]
input_cost_per_token = 2e-7
output_cost_per_token = 4e-7

[models."jina-reranker-v2-base-multilingual"]
mode = "rerank"
max_input_tokens = 1024
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1.8e-8
output_cost_per_token = 1.8e-8
litellm_provider = "jina_ai"
providers = ["jina_ai"]
max_document_chunks_per_query = 2048

[models."jina-reranker-v2-base-multilingual".pricing."jina_ai"]
input_cost_per_token = 1.8e-8
output_cost_per_token = 1.8e-8

[models."jp.anthropic.claude-haiku-4-5-20251001-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000055
cache_read_input_token_cost = 1.1e-7
cache_creation_input_token_cost = 0.000001375
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
source = "https://aws.amazon.com/about-aws/whats-new/2025/10/claude-4-5-haiku-anthropic-amazon-bedrock"
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."jp.anthropic.claude-haiku-4-5-20251001-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.000001375
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000055

[models."jp.anthropic.claude-sonnet-4-5-20250929-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000033
output_cost_per_token = 0.0000165
cache_read_input_token_cost = 3.3e-7
cache_creation_input_token_cost = 0.000004125
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token_above_200k_tokens = 0.0000066
output_cost_per_token_above_200k_tokens = 0.00002475
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."jp.anthropic.claude-sonnet-4-5-20250929-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."jp.anthropic.claude-sonnet-4-5-20250929-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.000004125
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost = 3.3e-7
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token = 0.0000033
input_cost_per_token_above_200k_tokens = 0.0000066
output_cost_per_token = 0.0000165
output_cost_per_token_above_200k_tokens = 0.00002475

[models."k2p5"]
display_name = "Kimi K2.5"
model_family = "kimi-thinking"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 32768
litellm_provider = "kimi-for-coding"
providers = ["kimi-for-coding"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2026-01"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."kilo/auto"]
display_name = "Kilo: Auto"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2024-06-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."kilo/auto".pricing."kilo"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001

[models."kimi-k2"]
display_name = "Kimi K2"
model_family = "kimi"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000025
cache_read_input_token_cost = 4.0000000000000003e-7
litellm_provider = "opencode"
providers = ["opencode", "iflowcn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-09-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."kimi-k2".pricing."opencode"]
cache_read_input_token_cost = 4.0000000000000003e-7
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000025

[models."kimi-k2-0711"]
display_name = "Kimi K2 (07/11)"
model_family = "kimi"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 16384
input_cost_per_token = 5.7e-7
output_cost_per_token = 0.0000023
litellm_provider = "helicone"
providers = ["helicone"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."kimi-k2-0711".pricing."helicone"]
input_cost_per_token = 5.7e-7
output_cost_per_token = 0.0000023

[models."kimi-k2-0711-preview"]
display_name = "Kimi K2 0711"
model_family = "kimi"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
cache_read_input_token_cost = 1.5e-7
litellm_provider = "moonshot"
providers = ["moonshot", "moonshotai", "moonshotai-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-07-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://platform.moonshot.ai/docs/pricing/chat#generation-model-kimi-k2"
supports_tool_choice = true
supports_web_search = true

[models."kimi-k2-0711-preview".pricing."moonshot"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."kimi-k2-0711-preview".pricing."moonshotai"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."kimi-k2-0711-preview".pricing."moonshotai-cn"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025

[models."kimi-k2-0905"]
display_name = "Kimi K2 (09/05)"
model_family = "kimi"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 16384
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002
cache_read_input_token_cost = 4e-7
litellm_provider = "helicone"
providers = ["helicone", "aihubmix", "iflowcn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-09"
release_date = "2025-09-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."kimi-k2-0905".pricing."aihubmix"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000021899999999999998
[models."kimi-k2-0905".pricing."helicone"]
cache_read_input_token_cost = 4e-7
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002

[models."kimi-k2-0905-preview"]
display_name = "Kimi K2 0905"
model_family = "kimi"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
cache_read_input_token_cost = 1.5e-7
litellm_provider = "moonshot"
providers = ["moonshot", "302ai", "moonshotai", "moonshotai-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-09-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://platform.moonshot.ai/docs/pricing/chat#generation-model-kimi-k2"
supports_tool_choice = true
supports_web_search = true

[models."kimi-k2-0905-preview".pricing."302ai"]
input_cost_per_token = 6.32e-7
output_cost_per_token = 0.00000253
[models."kimi-k2-0905-preview".pricing."moonshot"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."kimi-k2-0905-preview".pricing."moonshotai"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."kimi-k2-0905-preview".pricing."moonshotai-cn"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025

[models."kimi-k2-5"]
display_name = "Kimi K2.5"
model_family = "kimi"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 64000
input_cost_per_token = 7.5e-7
output_cost_per_token = 0.00000375
cache_read_input_token_cost = 1.25e-7
litellm_provider = "venice"
providers = ["venice", "aihubmix", "azure", "azure-cognitive-services", "opencode"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2026-01-27"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."kimi-k2-5".pricing."aihubmix"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
[models."kimi-k2-5".pricing."azure"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
[models."kimi-k2-5".pricing."azure-cognitive-services"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
[models."kimi-k2-5".pricing."opencode"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
[models."kimi-k2-5".pricing."venice"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 7.5e-7
output_cost_per_token = 0.00000375

[models."kimi-k2-5-free"]
display_name = "Kimi K2.5 Free"
model_family = "kimi-free"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
litellm_provider = "opencode"
providers = ["opencode"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2026-01-27"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."kimi-k2-instruct"]
display_name = "Kimi K2 Instruct"
model_family = "kimi"
mode = "chat"
max_input_tokens = 58904
max_output_tokens = 4096
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7
litellm_provider = "vultr"
providers = ["vultr", "cortecs"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2024-07-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."kimi-k2-instruct".pricing."cortecs"]
input_cost_per_token = 5.51e-7
output_cost_per_token = 0.0000026459999999999997
[models."kimi-k2-instruct".pricing."vultr"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7

[models."kimi-k2-thinking"]
display_name = "Kimi K2 Thinking"
model_family = "kimi-thinking"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
cache_read_input_token_cost = 1.5e-7
litellm_provider = "moonshot"
providers = ["moonshot", "302ai", "alibaba-cn", "azure", "azure-cognitive-services", "cortecs", "helicone", "kimi-for-coding", "moonshotai", "moonshotai-cn", "opencode", "venice"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-11-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://platform.moonshot.ai/docs/pricing/chat#generation-model-kimi-k2"
supports_tool_choice = true
supports_web_search = true

[models."kimi-k2-thinking".pricing."302ai"]
input_cost_per_token = 5.75e-7
output_cost_per_token = 0.0000023
[models."kimi-k2-thinking".pricing."alibaba-cn"]
input_cost_per_token = 5.739999999999999e-7
output_cost_per_token = 0.000002294
[models."kimi-k2-thinking".pricing."azure"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."kimi-k2-thinking".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."kimi-k2-thinking".pricing."cortecs"]
input_cost_per_token = 6.56e-7
output_cost_per_token = 0.0000027309999999999998
[models."kimi-k2-thinking".pricing."helicone"]
input_cost_per_token = 4.8e-7
output_cost_per_token = 0.000002
[models."kimi-k2-thinking".pricing."moonshot"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."kimi-k2-thinking".pricing."moonshotai"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."kimi-k2-thinking".pricing."moonshotai-cn"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."kimi-k2-thinking".pricing."opencode"]
cache_read_input_token_cost = 4.0000000000000003e-7
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000025
[models."kimi-k2-thinking".pricing."venice"]
cache_read_input_token_cost = 3.75e-7
input_cost_per_token = 7.5e-7
output_cost_per_token = 0.0000032000000000000003

[models."kimi-k2-thinking-251104"]
mode = "chat"
max_input_tokens = 229376
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "volcengine"
providers = ["volcengine"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
supports_assistant_prefill = true
supports_tool_choice = true

[models."kimi-k2-thinking-251104".pricing."volcengine"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."kimi-k2-thinking-turbo"]
display_name = "Kimi K2 Thinking Turbo"
model_family = "kimi-thinking"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 0.00000115
output_cost_per_token = 0.000008
cache_read_input_token_cost = 1.5e-7
litellm_provider = "moonshot"
providers = ["moonshot", "302ai", "moonshotai", "moonshotai-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-11-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://platform.moonshot.ai/docs/pricing/chat#generation-model-kimi-k2"
supports_tool_choice = true
supports_web_search = true

[models."kimi-k2-thinking-turbo".pricing."302ai"]
input_cost_per_token = 0.000001265
output_cost_per_token = 0.000009119
[models."kimi-k2-thinking-turbo".pricing."moonshot"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 0.00000115
output_cost_per_token = 0.000008
[models."kimi-k2-thinking-turbo".pricing."moonshotai"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 0.00000115
output_cost_per_token = 0.000008
[models."kimi-k2-thinking-turbo".pricing."moonshotai-cn"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 0.00000115
output_cost_per_token = 0.000008

[models."kimi-k2-turbo-preview"]
display_name = "Kimi K2 Turbo"
model_family = "kimi"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 0.00000115
output_cost_per_token = 0.000008
cache_read_input_token_cost = 1.5e-7
litellm_provider = "moonshot"
providers = ["moonshot", "abacus", "moonshotai", "moonshotai-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-09-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://platform.moonshot.ai/docs/pricing/chat#generation-model-kimi-k2"
supports_tool_choice = true
supports_web_search = true

[models."kimi-k2-turbo-preview".pricing."abacus"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.000008
[models."kimi-k2-turbo-preview".pricing."moonshot"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 0.00000115
output_cost_per_token = 0.000008
[models."kimi-k2-turbo-preview".pricing."moonshotai"]
cache_read_input_token_cost = 6e-7
input_cost_per_token = 0.0000024
output_cost_per_token = 0.00001
[models."kimi-k2-turbo-preview".pricing."moonshotai-cn"]
cache_read_input_token_cost = 6e-7
input_cost_per_token = 0.0000024
output_cost_per_token = 0.00001

[models."kimi-k2.5"]
display_name = "Kimi K2.5"
model_family = "kimi"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
litellm_provider = "azure_ai"
providers = ["azure_ai", "alibaba-cn", "moonshot", "moonshotai", "moonshotai-cn"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2026-01"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/kimi-k2-5-now-in-microsoft-foundry/4492321"
supports_tool_choice = true
supports_video_input = true

[models."kimi-k2.5".pricing."alibaba-cn"]
input_cost_per_token = 5.739999999999999e-7
output_cost_per_token = 0.000002411
[models."kimi-k2.5".pricing."azure_ai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
[models."kimi-k2.5".pricing."moonshot"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
[models."kimi-k2.5".pricing."moonshotai"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
[models."kimi-k2.5".pricing."moonshotai-cn"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003

[models."kimi-k2p5"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
cache_read_input_token_cost = 1e-7
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
supports_function_calling = true
source = "https://fireworks.ai/pricing"
supports_response_schema = true
supports_tool_choice = true

[models."kimi-k2p5".pricing."fireworks_ai"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003

[models."kimi-latest"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005
cache_read_input_token_cost = 1.5e-7
litellm_provider = "moonshot"
providers = ["moonshot"]
supports_function_calling = true
supports_vision = true
source = "https://platform.moonshot.ai/docs/pricing"
supports_tool_choice = true

[models."kimi-latest".pricing."moonshot"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005

[models."kimi-latest-128k"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005
cache_read_input_token_cost = 1.5e-7
litellm_provider = "moonshot"
providers = ["moonshot"]
supports_function_calling = true
supports_vision = true
source = "https://platform.moonshot.ai/docs/pricing"
supports_tool_choice = true

[models."kimi-latest-128k".pricing."moonshot"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005

[models."kimi-latest-32k"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
cache_read_input_token_cost = 1.5e-7
litellm_provider = "moonshot"
providers = ["moonshot"]
supports_function_calling = true
supports_vision = true
source = "https://platform.moonshot.ai/docs/pricing"
supports_tool_choice = true

[models."kimi-latest-32k".pricing."moonshot"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003

[models."kimi-latest-8k"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 0.000002
cache_read_input_token_cost = 1.5e-7
litellm_provider = "moonshot"
providers = ["moonshot"]
supports_function_calling = true
supports_vision = true
source = "https://platform.moonshot.ai/docs/pricing"
supports_tool_choice = true

[models."kimi-latest-8k".pricing."moonshot"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 2e-7
output_cost_per_token = 0.000002

[models."kimi-thinking-preview"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
cache_read_input_token_cost = 1.5e-7
litellm_provider = "moonshot"
providers = ["moonshot"]
supports_vision = true
source = "https://platform.moonshot.ai/docs/pricing/chat#generation-model-kimi-k2"

[models."kimi-thinking-preview".pricing."moonshot"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025

[models."kuaishou/kat-coder-pro-v1"]
display_name = "KAT-Coder-Pro-V1"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 64000
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
cache_read_input_token_cost = 6e-8
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2025-10-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."kuaishou/kat-coder-pro-v1".pricing."zenmux"]
cache_read_input_token_cost = 6e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."kuaishou/kat-coder-pro-v1-free"]
display_name = "KAT-Coder-Pro-V1 Free"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 64000
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2025-10-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."kwaipilot/kat-coder"]
display_name = "KAT-Coder-Pro V1(Free)"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 32000
litellm_provider = "novita-ai"
providers = ["novita-ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-09-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."kwaipilot/kat-coder-pro"]
display_name = "Kat Coder Pro"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
cache_read_input_token_cost = 6e-8
litellm_provider = "novita"
providers = ["novita", "kilo", "novita-ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2026-01-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
input_cost_per_token_cache_hit = 6e-8
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."kwaipilot/kat-coder-pro".pricing."kilo"]
cache_read_input_token_cost = 4.14e-8
input_cost_per_token = 2.07e-7
output_cost_per_token = 8.28e-7
[models."kwaipilot/kat-coder-pro".pricing."novita"]
cache_read_input_token_cost = 6e-8
input_cost_per_token = 3e-7
input_cost_per_token_cache_hit = 6e-8
output_cost_per_token = 0.0000012
[models."kwaipilot/kat-coder-pro".pricing."novita-ai"]
cache_read_input_token_cost = 6e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."kwaipilot/kat-coder-pro:free"]
display_name = "Kat Coder Pro (free)"
model_family = "kat-coder"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 65536
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-11"
release_date = "2025-11-10"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."labs-devstral-small-2512"]
display_name = "Devstral Small 2"
model_family = "devstral"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-12"
release_date = "2025-12-09"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://docs.mistral.ai/models/devstral-small-2-25-12"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."labs-devstral-small-2512".pricing."mistral"]
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7

[models."learnlm-1.5-pro-experimental"]
mode = "chat"
max_input_tokens = 32767
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "gemini"
providers = ["gemini"]
supports_function_calling = true
supports_vision = true
source = "https://aistudio.google.com"
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token_above_128k_tokens = 0
supports_audio_output = false
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."learnlm-1.5-pro-experimental".pricing."gemini"]
input_cost_per_audio_per_second = 0
input_cost_per_audio_per_second_above_128k_tokens = 0
input_cost_per_character = 0
input_cost_per_character_above_128k_tokens = 0
input_cost_per_image = 0
input_cost_per_image_above_128k_tokens = 0
input_cost_per_token = 0
input_cost_per_token_above_128k_tokens = 0
input_cost_per_video_per_second = 0
input_cost_per_video_per_second_above_128k_tokens = 0
output_cost_per_character = 0
output_cost_per_character_above_128k_tokens = 0
output_cost_per_token = 0
output_cost_per_token_above_128k_tokens = 0

[models."lfm-40b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1e-7
output_cost_per_token = 2e-7
litellm_provider = "lambda_ai"
providers = ["lambda_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lfm-40b".pricing."lambda_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 2e-7

[models."lfm-7b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2.5e-8
output_cost_per_token = 4e-8
litellm_provider = "lambda_ai"
providers = ["lambda_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."lfm-7b".pricing."lambda_ai"]
input_cost_per_token = 2.5e-8
output_cost_per_token = 4e-8

[models."liquid/lfm-2-5-1-2b-instruct:free"]
display_name = "LFM2.5-1.2B-Instruct (free)"
model_family = "liquid"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2026-01-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."liquid/lfm-2-5-1-2b-thinking:free"]
display_name = "LFM2.5-1.2B-Thinking (free)"
model_family = "liquid"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2026-01-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."llama-2-70b-chat"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000028
litellm_provider = "perplexity"
providers = ["perplexity"]

[models."llama-2-70b-chat".pricing."perplexity"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000028

[models."llama-3-1-405b-instruct"]
display_name = "Llama 3.1 405B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
litellm_provider = "cortecs"
providers = ["cortecs"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."llama-3-1-8b-instruct-turbo"]
display_name = "Meta Llama 3.1 8B Instruct Turbo"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
input_cost_per_token = 2e-8
output_cost_per_token = 3e-8
litellm_provider = "helicone"
providers = ["helicone"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."llama-3-1-8b-instruct-turbo".pricing."helicone"]
input_cost_per_token = 2e-8
output_cost_per_token = 3e-8

[models."llama-3.1-70b-instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001
litellm_provider = "perplexity"
providers = ["perplexity"]

[models."llama-3.1-70b-instruct".pricing."perplexity"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001

[models."llama-3.1-8b"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3e-8
output_cost_per_token = 5e-8
litellm_provider = "llamagate"
providers = ["llamagate"]
supports_function_calling = true
supports_response_schema = true

[models."llama-3.1-8b".pricing."llamagate"]
input_cost_per_token = 3e-8
output_cost_per_token = 5e-8

[models."llama-3.1-8b-instant"]
display_name = "Llama 3.1 8B Instant"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5e-8
output_cost_per_token = 8e-8
litellm_provider = "groq"
providers = ["groq", "helicone"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = false
supports_tool_choice = true

[models."llama-3.1-8b-instant".pricing."groq"]
input_cost_per_token = 5e-8
output_cost_per_token = 8e-8
[models."llama-3.1-8b-instant".pricing."helicone"]
input_cost_per_token = 5e-8
output_cost_per_token = 8e-8

[models."llama-3.1-8b-instruct"]
display_name = "Meta Llama 3.1 8B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "perplexity"
providers = ["perplexity", "helicone", "ovhcloud", "scaleway"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."llama-3.1-8b-instruct".pricing."helicone"]
input_cost_per_token = 2e-8
output_cost_per_token = 5e-8
[models."llama-3.1-8b-instruct".pricing."ovhcloud"]
input_cost_per_token = 1.1e-7
output_cost_per_token = 1.1e-7
[models."llama-3.1-8b-instruct".pricing."perplexity"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
[models."llama-3.1-8b-instruct".pricing."scaleway"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7

[models."llama-3.1-sonar-huge-128k-online"]
mode = "chat"
max_input_tokens = 127072
max_output_tokens = 127072
max_tokens = 127072
input_cost_per_token = 0.000005
output_cost_per_token = 0.000005
litellm_provider = "perplexity"
providers = ["perplexity"]
deprecation_date = "2025-02-22"

[models."llama-3.1-sonar-huge-128k-online".pricing."perplexity"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000005

[models."llama-3.1-sonar-large-128k-chat"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001
litellm_provider = "perplexity"
providers = ["perplexity"]
deprecation_date = "2025-02-22"

[models."llama-3.1-sonar-large-128k-chat".pricing."perplexity"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001

[models."llama-3.1-sonar-large-128k-online"]
mode = "chat"
max_input_tokens = 127072
max_output_tokens = 127072
max_tokens = 127072
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001
litellm_provider = "perplexity"
providers = ["perplexity"]
deprecation_date = "2025-02-22"

[models."llama-3.1-sonar-large-128k-online".pricing."perplexity"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001

[models."llama-3.1-sonar-small-128k-chat"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "perplexity"
providers = ["perplexity"]
deprecation_date = "2025-02-22"

[models."llama-3.1-sonar-small-128k-chat".pricing."perplexity"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."llama-3.1-sonar-small-128k-online"]
mode = "chat"
max_input_tokens = 127072
max_output_tokens = 127072
max_tokens = 127072
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "perplexity"
providers = ["perplexity"]
deprecation_date = "2025-02-22"

[models."llama-3.1-sonar-small-128k-online".pricing."perplexity"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."llama-3.2-3b"]
display_name = "Llama 3.2 3B"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 4e-8
output_cost_per_token = 8e-8
litellm_provider = "llamagate"
providers = ["llamagate", "venice"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-10-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true

[models."llama-3.2-3b".pricing."llamagate"]
input_cost_per_token = 4e-8
output_cost_per_token = 8e-8
[models."llama-3.2-3b".pricing."venice"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."llama-3.3-70b"]
display_name = "Llama 3.3 70B"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 8.5e-7
output_cost_per_token = 0.0000012
litellm_provider = "cerebras"
providers = ["cerebras", "venice"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2025-04-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."llama-3.3-70b".pricing."cerebras"]
input_cost_per_token = 8.5e-7
output_cost_per_token = 0.0000012
[models."llama-3.3-70b".pricing."venice"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000028

[models."llama-3.3-70b-versatile"]
display_name = "Llama 3.3 70B Versatile"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 5.9e-7
output_cost_per_token = 7.9e-7
litellm_provider = "groq"
providers = ["groq", "abacus", "helicone"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-12-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = false
supports_tool_choice = true

[models."llama-3.3-70b-versatile".pricing."abacus"]
input_cost_per_token = 5.9e-7
output_cost_per_token = 7.900000000000001e-7
[models."llama-3.3-70b-versatile".pricing."groq"]
input_cost_per_token = 5.9e-7
output_cost_per_token = 7.9e-7
[models."llama-3.3-70b-versatile".pricing."helicone"]
input_cost_per_token = 5.9e-7
output_cost_per_token = 7.9e-7

[models."llama-4-maverick"]
display_name = "Meta Llama 4 Maverick 17B 128E"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "helicone"
providers = ["helicone"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-01-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."llama-4-maverick".pricing."helicone"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."llama-4-maverick-17b-128e-instruct-fp8"]
display_name = "Llama 4 Maverick 17B 128E Instruct FP8"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5e-8
output_cost_per_token = 1e-7
litellm_provider = "lambda_ai"
providers = ["lambda_ai", "azure", "azure-cognitive-services", "llama"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-04-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."llama-4-maverick-17b-128e-instruct-fp8".pricing."azure"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001
[models."llama-4-maverick-17b-128e-instruct-fp8".pricing."azure-cognitive-services"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001
[models."llama-4-maverick-17b-128e-instruct-fp8".pricing."lambda_ai"]
input_cost_per_token = 5e-8
output_cost_per_token = 1e-7

[models."llama-4-scout"]
display_name = "Meta Llama 4 Scout 17B 16E"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 8e-8
output_cost_per_token = 3e-7
litellm_provider = "helicone"
providers = ["helicone"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-01-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."llama-4-scout".pricing."helicone"]
input_cost_per_token = 8e-8
output_cost_per_token = 3e-7

[models."llama-4-scout-17b-16e-instruct"]
display_name = "Llama 4 Scout 17B 16E Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5e-8
output_cost_per_token = 1e-7
litellm_provider = "lambda_ai"
providers = ["lambda_ai", "azure", "azure-cognitive-services"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-04-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."llama-4-scout-17b-16e-instruct".pricing."azure"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 7.8e-7
[models."llama-4-scout-17b-16e-instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 7.8e-7
[models."llama-4-scout-17b-16e-instruct".pricing."lambda_ai"]
input_cost_per_token = 5e-8
output_cost_per_token = 1e-7

[models."llama-guard-3-8b"]
display_name = "Llama Guard 3 8B"
model_family = "llama"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7
litellm_provider = "groq"
providers = ["groq"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."llama-guard-3-8b".pricing."groq"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7

[models."llama-guard-4"]
display_name = "Meta Llama Guard 4 12B"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 1024
input_cost_per_token = 2.1e-7
output_cost_per_token = 2.1e-7
litellm_provider = "helicone"
providers = ["helicone"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-01-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."llama-guard-4".pricing."helicone"]
input_cost_per_token = 2.1e-7
output_cost_per_token = 2.1e-7

[models."llama-prompt-guard-2-22m"]
display_name = "Meta Llama Prompt Guard 2 22M"
model_family = "llama"
mode = "chat"
max_input_tokens = 512
max_output_tokens = 2
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8
litellm_provider = "helicone"
providers = ["helicone"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-10-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."llama-prompt-guard-2-22m".pricing."helicone"]
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8

[models."llama-prompt-guard-2-86m"]
display_name = "Meta Llama Prompt Guard 2 86M"
model_family = "llama"
mode = "chat"
max_input_tokens = 512
max_output_tokens = 2
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8
litellm_provider = "helicone"
providers = ["helicone"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-10-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."llama-prompt-guard-2-86m".pricing."helicone"]
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8

[models."llama2"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]

[models."llama2".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."llama2-70b-chat"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake"]

[models."llama2-uncensored"]
mode = "completion"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]

[models."llama2-uncensored".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."llama2:13b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]

[models."llama2:13b".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."llama2:70b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]

[models."llama2:70b".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."llama2:7b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]

[models."llama2:7b".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."llama3"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]

[models."llama3".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."llama3-70b"]
mode = "chat"
max_input_tokens = 8000
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake"]

[models."llama3-70b-8192"]
display_name = "Llama 3 70B"
model_family = "llama"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
input_cost_per_token = 5.9e-7
output_cost_per_token = 7.900000000000001e-7
litellm_provider = "groq"
providers = ["groq"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-03"
release_date = "2024-04-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."llama3-70b-8192".pricing."groq"]
input_cost_per_token = 5.9e-7
output_cost_per_token = 7.900000000000001e-7

[models."llama3-8b"]
mode = "chat"
max_input_tokens = 8000
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake"]

[models."llama3-8b-8192"]
display_name = "Llama 3 8B"
model_family = "llama"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 8e-8
litellm_provider = "groq"
providers = ["groq"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-03"
release_date = "2024-04-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."llama3-8b-8192".pricing."groq"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 8e-8

[models."llama3-8b-instruct"]
mode = "chat"
max_tokens = 512
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "gradient_ai"
providers = ["gradient_ai"]
supported_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions"]
supports_tool_choice = false

[models."llama3-8b-instruct".pricing."gradient_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."llama3.1"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = true

[models."llama3.1".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."llama3.1-405b"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake"]

[models."llama3.1-405b-instruct-fp8"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 8e-7
output_cost_per_token = 8e-7
litellm_provider = "lambda_ai"
providers = ["lambda_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."llama3.1-405b-instruct-fp8".pricing."lambda_ai"]
input_cost_per_token = 8e-7
output_cost_per_token = 8e-7

[models."llama3.1-70b"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 6e-7
output_cost_per_token = 6e-7
litellm_provider = "cerebras"
providers = ["cerebras", "snowflake"]
supports_function_calling = true
supports_tool_choice = true

[models."llama3.1-70b".pricing."cerebras"]
input_cost_per_token = 6e-7
output_cost_per_token = 6e-7

[models."llama3.1-70b-instruct-fp8"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7
litellm_provider = "lambda_ai"
providers = ["lambda_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."llama3.1-70b-instruct-fp8".pricing."lambda_ai"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7

[models."llama3.1-8b"]
display_name = "Llama 3.1 8B"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "cerebras"
providers = ["cerebras", "snowflake"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."llama3.1-8b".pricing."cerebras"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."llama3.1-8b-instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2.5e-8
output_cost_per_token = 4e-8
litellm_provider = "lambda_ai"
providers = ["lambda_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."llama3.1-8b-instruct".pricing."lambda_ai"]
input_cost_per_token = 2.5e-8
output_cost_per_token = 4e-8

[models."llama3.1-nemotron-70b-instruct-fp8"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7
litellm_provider = "lambda_ai"
providers = ["lambda_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."llama3.1-nemotron-70b-instruct-fp8".pricing."lambda_ai"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7

[models."llama3.2-11b-vision-instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.5e-8
output_cost_per_token = 2.5e-8
litellm_provider = "lambda_ai"
providers = ["lambda_ai"]
supports_function_calling = true
supports_vision = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."llama3.2-11b-vision-instruct".pricing."lambda_ai"]
input_cost_per_token = 1.5e-8
output_cost_per_token = 2.5e-8

[models."llama3.2-1b"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake"]

[models."llama3.2-3b"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake"]

[models."llama3.2-3b-instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.5e-8
output_cost_per_token = 2.5e-8
litellm_provider = "lambda_ai"
providers = ["lambda_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."llama3.2-3b-instruct".pricing."lambda_ai"]
input_cost_per_token = 1.5e-8
output_cost_per_token = 2.5e-8

[models."llama3.3-70b"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake"]

[models."llama3.3-70b-instruct"]
mode = "chat"
max_tokens = 2048
input_cost_per_token = 6.5e-7
output_cost_per_token = 6.5e-7
litellm_provider = "gradient_ai"
providers = ["gradient_ai"]
supported_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions"]
supports_tool_choice = false

[models."llama3.3-70b-instruct".pricing."gradient_ai"]
input_cost_per_token = 6.5e-7
output_cost_per_token = 6.5e-7

[models."llama3.3-70b-instruct-fp8"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7
litellm_provider = "lambda_ai"
providers = ["lambda_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."llama3.3-70b-instruct-fp8".pricing."lambda_ai"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7

[models."llama3:70b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]

[models."llama3:70b".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."llama3:8b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]

[models."llama3:8b".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."llava-7b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 1e-7
output_cost_per_token = 2e-7
litellm_provider = "llamagate"
providers = ["llamagate"]
supports_vision = true
supports_response_schema = true

[models."llava-7b".pricing."llamagate"]
input_cost_per_token = 1e-7
output_cost_per_token = 2e-7

[models."llava-v1.6-mistral-7b-hf"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 2.9e-7
output_cost_per_token = 2.9e-7
litellm_provider = "ovhcloud"
providers = ["ovhcloud"]
supports_function_calling = false
supports_vision = true
source = "https://endpoints.ai.cloud.ovh.net/models/llava-next-mistral-7b"
supports_response_schema = true
supports_tool_choice = false

[models."llava-v1.6-mistral-7b-hf".pricing."ovhcloud"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 2.9e-7

[models."long-form"]
mode = "audio_speech"
litellm_provider = "aws_polly"
providers = ["aws_polly"]
source = "https://aws.amazon.com/polly/pricing/"
input_cost_per_character = 0.0001
supported_endpoints = ["/v1/audio/speech"]

[models."long-form".pricing."aws_polly"]
input_cost_per_character = 0.0001

[models."low/1024-x-1024/gpt-image-1"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 1.0490417e-8
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."low/1024-x-1024/gpt-image-1".pricing."azure"]
input_cost_per_pixel = 1.0490417e-8
output_cost_per_pixel = 0
[models."low/1024-x-1024/gpt-image-1".pricing."openai"]
input_cost_per_image = 0.011
input_cost_per_pixel = 1.0490417e-8
output_cost_per_pixel = 0

[models."low/1024-x-1024/gpt-image-1-mini"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 2.0751953125e-9
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."low/1024-x-1024/gpt-image-1-mini".pricing."azure"]
input_cost_per_pixel = 2.0751953125e-9
output_cost_per_pixel = 0
[models."low/1024-x-1024/gpt-image-1-mini".pricing."openai"]
input_cost_per_image = 0.005

[models."low/1024-x-1024/gpt-image-1.5"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.009
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."low/1024-x-1024/gpt-image-1.5".pricing."openai"]
input_cost_per_image = 0.009

[models."low/1024-x-1024/gpt-image-1.5-2025-12-16"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.009
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."low/1024-x-1024/gpt-image-1.5-2025-12-16".pricing."openai"]
input_cost_per_image = 0.009

[models."low/1024-x-1536/gpt-image-1"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 1.0172526e-8
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."low/1024-x-1536/gpt-image-1".pricing."azure"]
input_cost_per_pixel = 1.0172526e-8
output_cost_per_pixel = 0
[models."low/1024-x-1536/gpt-image-1".pricing."openai"]
input_cost_per_image = 0.016
input_cost_per_pixel = 1.0172526e-8
output_cost_per_pixel = 0

[models."low/1024-x-1536/gpt-image-1-mini"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 2.0751953125e-9
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."low/1024-x-1536/gpt-image-1-mini".pricing."azure"]
input_cost_per_pixel = 2.0751953125e-9
output_cost_per_pixel = 0
[models."low/1024-x-1536/gpt-image-1-mini".pricing."openai"]
input_cost_per_image = 0.006

[models."low/1024-x-1536/gpt-image-1.5"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.013
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."low/1024-x-1536/gpt-image-1.5".pricing."openai"]
input_cost_per_image = 0.013

[models."low/1024-x-1536/gpt-image-1.5-2025-12-16"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.013
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."low/1024-x-1536/gpt-image-1.5-2025-12-16".pricing."openai"]
input_cost_per_image = 0.013

[models."low/1536-x-1024/gpt-image-1"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 1.0172526e-8
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."low/1536-x-1024/gpt-image-1".pricing."azure"]
input_cost_per_pixel = 1.0172526e-8
output_cost_per_pixel = 0
[models."low/1536-x-1024/gpt-image-1".pricing."openai"]
input_cost_per_image = 0.016
input_cost_per_pixel = 1.0172526e-8
output_cost_per_pixel = 0

[models."low/1536-x-1024/gpt-image-1-mini"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 2.0345052083e-9
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."low/1536-x-1024/gpt-image-1-mini".pricing."azure"]
input_cost_per_pixel = 2.0345052083e-9
output_cost_per_pixel = 0
[models."low/1536-x-1024/gpt-image-1-mini".pricing."openai"]
input_cost_per_image = 0.006

[models."low/1536-x-1024/gpt-image-1.5"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.013
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."low/1536-x-1024/gpt-image-1.5".pricing."openai"]
input_cost_per_image = 0.013

[models."low/1536-x-1024/gpt-image-1.5-2025-12-16"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.013
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."low/1536-x-1024/gpt-image-1.5-2025-12-16".pricing."openai"]
input_cost_per_image = 0.013

[models."lucidnova-rf1-100b"]
display_name = "LucidNova RF1 100B"
model_family = "nova"
mode = "chat"
max_input_tokens = 120000
max_output_tokens = 8000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005
litellm_provider = "lucidquery"
providers = ["lucidquery"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-09-16"
release_date = "2024-12-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."lucidnova-rf1-100b".pricing."lucidquery"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005

[models."lucidquery-nexus-coder"]
display_name = "LucidQuery Nexus Coder"
model_family = "lucid"
mode = "chat"
max_input_tokens = 250000
max_output_tokens = 60000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005
litellm_provider = "lucidquery"
providers = ["lucidquery"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-08-01"
release_date = "2025-09-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."lucidquery-nexus-coder".pricing."lucidquery"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005

[models."luminous-base"]
mode = "completion"
max_tokens = 2048
input_cost_per_token = 0.00003
output_cost_per_token = 0.000033
litellm_provider = "aleph_alpha"
providers = ["aleph_alpha"]

[models."luminous-base".pricing."aleph_alpha"]
input_cost_per_token = 0.00003
output_cost_per_token = 0.000033

[models."luminous-base-control"]
mode = "chat"
max_tokens = 2048
input_cost_per_token = 0.0000375
output_cost_per_token = 0.00004125
litellm_provider = "aleph_alpha"
providers = ["aleph_alpha"]

[models."luminous-base-control".pricing."aleph_alpha"]
input_cost_per_token = 0.0000375
output_cost_per_token = 0.00004125

[models."luminous-extended"]
mode = "completion"
max_tokens = 2048
input_cost_per_token = 0.000045
output_cost_per_token = 0.0000495
litellm_provider = "aleph_alpha"
providers = ["aleph_alpha"]

[models."luminous-extended".pricing."aleph_alpha"]
input_cost_per_token = 0.000045
output_cost_per_token = 0.0000495

[models."luminous-extended-control"]
mode = "chat"
max_tokens = 2048
input_cost_per_token = 0.00005625
output_cost_per_token = 0.000061875
litellm_provider = "aleph_alpha"
providers = ["aleph_alpha"]

[models."luminous-extended-control".pricing."aleph_alpha"]
input_cost_per_token = 0.00005625
output_cost_per_token = 0.000061875

[models."luminous-supreme"]
mode = "completion"
max_tokens = 2048
input_cost_per_token = 0.000175
output_cost_per_token = 0.0001925
litellm_provider = "aleph_alpha"
providers = ["aleph_alpha"]

[models."luminous-supreme".pricing."aleph_alpha"]
input_cost_per_token = 0.000175
output_cost_per_token = 0.0001925

[models."luminous-supreme-control"]
mode = "chat"
max_tokens = 2048
input_cost_per_token = 0.00021875
output_cost_per_token = 0.000240625
litellm_provider = "aleph_alpha"
providers = ["aleph_alpha"]

[models."luminous-supreme-control".pricing."aleph_alpha"]
input_cost_per_token = 0.00021875
output_cost_per_token = 0.000240625

[models."magistral-medium"]
display_name = "Magistral Medium"
model_family = "magistral-medium"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2025-03-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_tool_choice = true

[models."magistral-medium".pricing."vercel"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005
[models."magistral-medium".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005

[models."magistral-medium-2506"]
mode = "chat"
max_input_tokens = 40000
max_output_tokens = 40000
max_tokens = 40000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = true
source = "https://mistral.ai/news/magistral"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."magistral-medium-2506".pricing."mistral"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005

[models."magistral-medium-2509"]
mode = "chat"
max_input_tokens = 40000
max_output_tokens = 40000
max_tokens = 40000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = true
source = "https://mistral.ai/news/magistral"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."magistral-medium-2509".pricing."mistral"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005

[models."magistral-medium-latest"]
display_name = "Magistral Medium"
model_family = "magistral-medium"
mode = "chat"
max_input_tokens = 40000
max_output_tokens = 40000
max_tokens = 40000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2025-03-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://mistral.ai/news/magistral"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."magistral-medium-latest".pricing."mistral"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005

[models."magistral-small"]
display_name = "Magistral Small"
model_family = "magistral-small"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "mistral", "vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2025-03-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."magistral-small".pricing."mistral"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
[models."magistral-small".pricing."vercel"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
[models."magistral-small".pricing."vercel_ai_gateway"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."magistral-small-2506"]
mode = "chat"
max_input_tokens = 40000
max_output_tokens = 40000
max_tokens = 40000
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = true
source = "https://mistral.ai/pricing#api-pricing"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."magistral-small-2506".pricing."mistral"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."magistral-small-latest"]
mode = "chat"
max_input_tokens = 40000
max_output_tokens = 40000
max_tokens = 40000
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = true
source = "https://mistral.ai/pricing#api-pricing"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."magistral-small-latest".pricing."mistral"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."mamba-codestral-7B-v0.1"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 1.9e-7
output_cost_per_token = 1.9e-7
litellm_provider = "ovhcloud"
providers = ["ovhcloud"]
supports_function_calling = false
source = "https://endpoints.ai.cloud.ovh.net/models/mamba-codestral-7b-v0-1"
supports_response_schema = true
supports_tool_choice = false

[models."mamba-codestral-7B-v0.1".pricing."ovhcloud"]
input_cost_per_token = 1.9e-7
output_cost_per_token = 1.9e-7

[models."mancer/weaver"]
mode = "chat"
max_tokens = 8000
input_cost_per_token = 0.000005625
output_cost_per_token = 0.000005625
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_tool_choice = true

[models."mancer/weaver".pricing."openrouter"]
input_cost_per_token = 0.000005625
output_cost_per_token = 0.000005625

[models."max-x-max/50-steps/stability.stable-diffusion-xl-v0"]
mode = "image_generation"
max_input_tokens = 77
max_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.036

[models."max-x-max/50-steps/stability.stable-diffusion-xl-v0".pricing."bedrock"]
output_cost_per_image = 0.036

[models."max-x-max/max-steps/stability.stable-diffusion-xl-v0"]
mode = "image_generation"
max_input_tokens = 77
max_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.072

[models."max-x-max/max-steps/stability.stable-diffusion-xl-v0".pricing."bedrock"]
output_cost_per_image = 0.072

[models."medium/1024-x-1024/gpt-image-1"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 4.0054321e-8
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."medium/1024-x-1024/gpt-image-1".pricing."azure"]
input_cost_per_pixel = 4.0054321e-8
output_cost_per_pixel = 0
[models."medium/1024-x-1024/gpt-image-1".pricing."openai"]
input_cost_per_image = 0.042
input_cost_per_pixel = 4.0054321e-8
output_cost_per_pixel = 0

[models."medium/1024-x-1024/gpt-image-1-mini"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 8.056640625e-9
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."medium/1024-x-1024/gpt-image-1-mini".pricing."azure"]
input_cost_per_pixel = 8.056640625e-9
output_cost_per_pixel = 0
[models."medium/1024-x-1024/gpt-image-1-mini".pricing."openai"]
input_cost_per_image = 0.011

[models."medium/1024-x-1024/gpt-image-1.5"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.034
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."medium/1024-x-1024/gpt-image-1.5".pricing."openai"]
input_cost_per_image = 0.034

[models."medium/1024-x-1024/gpt-image-1.5-2025-12-16"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.034
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."medium/1024-x-1024/gpt-image-1.5-2025-12-16".pricing."openai"]
input_cost_per_image = 0.034

[models."medium/1024-x-1536/gpt-image-1"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 4.0054321e-8
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."medium/1024-x-1536/gpt-image-1".pricing."azure"]
input_cost_per_pixel = 4.0054321e-8
output_cost_per_pixel = 0
[models."medium/1024-x-1536/gpt-image-1".pricing."openai"]
input_cost_per_image = 0.063
input_cost_per_pixel = 4.0054321e-8
output_cost_per_pixel = 0

[models."medium/1024-x-1536/gpt-image-1-mini"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 8.056640625e-9
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."medium/1024-x-1536/gpt-image-1-mini".pricing."azure"]
input_cost_per_pixel = 8.056640625e-9
output_cost_per_pixel = 0
[models."medium/1024-x-1536/gpt-image-1-mini".pricing."openai"]
input_cost_per_image = 0.015

[models."medium/1024-x-1536/gpt-image-1.5"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.05
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."medium/1024-x-1536/gpt-image-1.5".pricing."openai"]
input_cost_per_image = 0.05

[models."medium/1024-x-1536/gpt-image-1.5-2025-12-16"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.05
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."medium/1024-x-1536/gpt-image-1.5-2025-12-16".pricing."openai"]
input_cost_per_image = 0.05

[models."medium/1536-x-1024/gpt-image-1"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 4.0054321e-8
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."medium/1536-x-1024/gpt-image-1".pricing."azure"]
input_cost_per_pixel = 4.0054321e-8
output_cost_per_pixel = 0
[models."medium/1536-x-1024/gpt-image-1".pricing."openai"]
input_cost_per_image = 0.063
input_cost_per_pixel = 4.0054321e-8
output_cost_per_pixel = 0

[models."medium/1536-x-1024/gpt-image-1-mini"]
mode = "image_generation"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 7.9752604167e-9
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."medium/1536-x-1024/gpt-image-1-mini".pricing."azure"]
input_cost_per_pixel = 7.9752604167e-9
output_cost_per_pixel = 0
[models."medium/1536-x-1024/gpt-image-1-mini".pricing."openai"]
input_cost_per_image = 0.015

[models."medium/1536-x-1024/gpt-image-1.5"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.05
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."medium/1536-x-1024/gpt-image-1.5".pricing."openai"]
input_cost_per_image = 0.05

[models."medium/1536-x-1024/gpt-image-1.5-2025-12-16"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.05
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."medium/1536-x-1024/gpt-image-1.5-2025-12-16".pricing."openai"]
input_cost_per_image = 0.05

[models."medlm-large"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 0.000005
output_cost_per_character = 0.000015
supports_tool_choice = true

[models."medlm-large".pricing."vertex_ai-language-models"]
input_cost_per_character = 0.000005
output_cost_per_character = 0.000015

[models."medlm-medium"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "vertex_ai-language-models"
providers = ["vertex_ai-language-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 5e-7
output_cost_per_character = 0.000001
supports_tool_choice = true

[models."medlm-medium".pricing."vertex_ai-language-models"]
input_cost_per_character = 5e-7
output_cost_per_character = 0.000001

[models."meituan/longcat-flash-chat"]
display_name = "Meituan: LongCat Flash Chat"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7
cache_read_input_token_cost = 2.0000000000000002e-7
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-08-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meituan/longcat-flash-chat".pricing."kilo"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7

[models."meituan/longcat-flash-thinking"]
display_name = "LongCat Flash Thinking"
model_family = "longcat"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000015
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-09-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meituan/longcat-flash-thinking".pricing."vercel"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000015

[models."mercury"]
display_name = "Mercury"
model_family = "mercury"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001
cache_read_input_token_cost = 2.5e-7
litellm_provider = "inception"
providers = ["inception"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-10"
release_date = "2025-06-26"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mercury".pricing."inception"]
cache_read_input_token_cost = 2.5e-7
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001

[models."mercury-coder"]
display_name = "Mercury Coder"
model_family = "mercury"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001
cache_read_input_token_cost = 2.5e-7
litellm_provider = "inception"
providers = ["inception"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-10"
release_date = "2025-02-26"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mercury-coder".pricing."inception"]
cache_read_input_token_cost = 2.5e-7
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001

[models."meta-llama-3-8b-instruct"]
display_name = "Meta-Llama-3-8B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 2048
input_cost_per_token = 3e-7
output_cost_per_token = 6.1e-7
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-04-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta-llama-3-8b-instruct".pricing."azure"]
input_cost_per_token = 3e-7
output_cost_per_token = 6.1e-7
[models."meta-llama-3-8b-instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 3e-7
output_cost_per_token = 6.1e-7

[models."meta-llama-3.1-70b-instruct"]
display_name = "Meta-Llama-3.1-70B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-7
output_cost_per_token = 6e-7
litellm_provider = "friendliai"
providers = ["friendliai", "azure", "azure-cognitive-services"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."meta-llama-3.1-70b-instruct".pricing."azure"]
input_cost_per_token = 0.00000268
output_cost_per_token = 0.00000354
[models."meta-llama-3.1-70b-instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 0.00000268
output_cost_per_token = 0.00000354
[models."meta-llama-3.1-70b-instruct".pricing."friendliai"]
input_cost_per_token = 6e-7
output_cost_per_token = 6e-7

[models."meta-llama-3.1-8b-instruct"]
display_name = "Meta-Llama-3.1-8B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "friendliai"
providers = ["friendliai", "azure", "azure-cognitive-services"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."meta-llama-3.1-8b-instruct".pricing."azure"]
input_cost_per_token = 3e-7
output_cost_per_token = 6.1e-7
[models."meta-llama-3.1-8b-instruct".pricing."azure-cognitive-services"]
input_cost_per_token = 3e-7
output_cost_per_token = 6.1e-7
[models."meta-llama-3.1-8b-instruct".pricing."friendliai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."meta-llama/Llama-2-13b-chat-hf"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2.5e-7
output_cost_per_token = 2.5e-7
litellm_provider = "anyscale"
providers = ["anyscale"]

[models."meta-llama/Llama-2-13b-chat-hf".pricing."anyscale"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 2.5e-7

[models."meta-llama/Llama-2-70b-chat-hf"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001
litellm_provider = "anyscale"
providers = ["anyscale"]

[models."meta-llama/Llama-2-70b-chat-hf".pricing."anyscale"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001

[models."meta-llama/Llama-2-7b-chat-hf"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "anyscale"
providers = ["anyscale"]

[models."meta-llama/Llama-2-7b-chat-hf".pricing."anyscale"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7

[models."meta-llama/Llama-3.1-8B-Instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 3e-8
output_cost_per_token = 3e-8
litellm_provider = "nscale"
providers = ["nscale", "wandb"]
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."meta-llama/Llama-3.1-8B-Instruct".metadata]
notes = "Pricing listed as $0.06/1M tokens total. Assumed 50/50 split for input/output."

[models."meta-llama/Llama-3.1-8B-Instruct".pricing."nscale"]
input_cost_per_token = 3e-8
output_cost_per_token = 3e-8
[models."meta-llama/Llama-3.1-8B-Instruct".pricing."wandb"]
input_cost_per_token = 0.022
output_cost_per_token = 0.022

[models."meta-llama/Llama-3.2-11B-Vision-Instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 4.9e-8
output_cost_per_token = 4.9e-8
litellm_provider = "deepinfra"
providers = ["deepinfra"]
supports_tool_choice = false

[models."meta-llama/Llama-3.2-11B-Vision-Instruct".pricing."deepinfra"]
input_cost_per_token = 4.9e-8
output_cost_per_token = 4.9e-8

[models."meta-llama/Llama-3.2-3B-Instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-8
output_cost_per_token = 2e-8
litellm_provider = "deepinfra"
providers = ["deepinfra", "hyperbolic"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."meta-llama/Llama-3.2-3B-Instruct".pricing."deepinfra"]
input_cost_per_token = 2e-8
output_cost_per_token = 2e-8
[models."meta-llama/Llama-3.2-3B-Instruct".pricing."hyperbolic"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7

[models."meta-llama/Llama-3.2-3B-Instruct-Turbo"]
mode = "chat"
litellm_provider = "together_ai"
providers = ["together_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."meta-llama/Llama-3.3-70B-Instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2.3e-7
output_cost_per_token = 4e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "hyperbolic", "nscale", "wandb"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."meta-llama/Llama-3.3-70B-Instruct".pricing."deepinfra"]
input_cost_per_token = 2.3e-7
output_cost_per_token = 4e-7
[models."meta-llama/Llama-3.3-70B-Instruct".pricing."hyperbolic"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7
[models."meta-llama/Llama-3.3-70B-Instruct".pricing."nscale"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
[models."meta-llama/Llama-3.3-70B-Instruct".pricing."wandb"]
input_cost_per_token = 0.071
output_cost_per_token = 0.071

[models."meta-llama/Llama-3.3-70B-Instruct-Turbo"]
display_name = "Llama 3.3 70B"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.3e-7
output_cost_per_token = 3.9e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "together_ai", "togetherai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-12-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."meta-llama/Llama-3.3-70B-Instruct-Turbo".pricing."deepinfra"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 3.9e-7
[models."meta-llama/Llama-3.3-70B-Instruct-Turbo".pricing."together_ai"]
input_cost_per_token = 8.8e-7
output_cost_per_token = 8.8e-7
[models."meta-llama/Llama-3.3-70B-Instruct-Turbo".pricing."togetherai"]
input_cost_per_token = 8.8e-7
output_cost_per_token = 8.8e-7

[models."meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"]
mode = "chat"
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "together_ai"
providers = ["together_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."meta-llama/Llama-3.3-70B-Instruct-Turbo-Free".pricing."together_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"]
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 1048576
max_tokens = 1048576
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "together_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8".pricing."deepinfra"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8".pricing."together_ai"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 8.5e-7

[models."meta-llama/Llama-4-Scout-17B-16E-Instruct"]
mode = "chat"
max_input_tokens = 327680
max_output_tokens = 327680
max_tokens = 327680
input_cost_per_token = 8e-8
output_cost_per_token = 3e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "nscale", "together_ai", "wandb"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."meta-llama/Llama-4-Scout-17B-16E-Instruct".pricing."deepinfra"]
input_cost_per_token = 8e-8
output_cost_per_token = 3e-7
[models."meta-llama/Llama-4-Scout-17B-16E-Instruct".pricing."nscale"]
input_cost_per_token = 9e-8
output_cost_per_token = 2.9e-7
[models."meta-llama/Llama-4-Scout-17B-16E-Instruct".pricing."together_ai"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 5.9e-7
[models."meta-llama/Llama-4-Scout-17B-16E-Instruct".pricing."wandb"]
input_cost_per_token = 0.017
output_cost_per_token = 0.066

[models."meta-llama/Llama-Guard-3-8B"]
display_name = "Llama-Guard-3-8B"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 5.5e-8
output_cost_per_token = 5.5e-8
litellm_provider = "deepinfra"
providers = ["deepinfra", "kilo", "nebius"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-04-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = false

[models."meta-llama/Llama-Guard-3-8B".pricing."deepinfra"]
input_cost_per_token = 5.5e-8
output_cost_per_token = 5.5e-8
[models."meta-llama/Llama-Guard-3-8B".pricing."kilo"]
input_cost_per_token = 2e-8
output_cost_per_token = 6e-8
[models."meta-llama/Llama-Guard-3-8B".pricing."nebius"]
cache_read_input_token_cost = 2e-9
input_cost_per_token = 2e-8
output_cost_per_token = 6e-8

[models."meta-llama/Llama-Guard-4-12B"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 1.8e-7
output_cost_per_token = 1.8e-7
litellm_provider = "deepinfra"
providers = ["deepinfra"]
supports_tool_choice = false

[models."meta-llama/Llama-Guard-4-12B".pricing."deepinfra"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 1.8e-7

[models."meta-llama/Meta-Llama-3-70B-Instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001
litellm_provider = "anyscale"
providers = ["anyscale", "hyperbolic"]
supports_function_calling = true
source = "https://docs.anyscale.com/preview/endpoints/text-generation/supported-models/meta-llama-Meta-Llama-3-70B-Instruct"
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."meta-llama/Meta-Llama-3-70B-Instruct".pricing."anyscale"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001
[models."meta-llama/Meta-Llama-3-70B-Instruct".pricing."hyperbolic"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7

[models."meta-llama/Meta-Llama-3-8B-Instruct"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "anyscale"
providers = ["anyscale", "deepinfra"]
source = "https://docs.anyscale.com/preview/endpoints/text-generation/supported-models/meta-llama-Meta-Llama-3-8B-Instruct"
supports_tool_choice = true

[models."meta-llama/Meta-Llama-3-8B-Instruct".pricing."anyscale"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."meta-llama/Meta-Llama-3-8B-Instruct".pricing."deepinfra"]
input_cost_per_token = 3e-8
output_cost_per_token = 6e-8

[models."meta-llama/Meta-Llama-3.1-405B-Instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7
litellm_provider = "hyperbolic"
providers = ["hyperbolic"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."meta-llama/Meta-Llama-3.1-405B-Instruct".pricing."hyperbolic"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7

[models."meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo"]
display_name = "Llama 3.1 405B Instruct Turbo"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
input_cost_per_token = 0.0000035
output_cost_per_token = 0.0000035
litellm_provider = "together_ai"
providers = ["together_ai", "abacus"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo".pricing."abacus"]
input_cost_per_token = 0.0000035
output_cost_per_token = 0.0000035
[models."meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo".pricing."together_ai"]
input_cost_per_token = 0.0000035
output_cost_per_token = 0.0000035

[models."meta-llama/Meta-Llama-3.1-70B-Instruct"]
display_name = "Llama 3.1 70B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 4e-7
output_cost_per_token = 4e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "abacus", "hyperbolic"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."meta-llama/Meta-Llama-3.1-70B-Instruct".pricing."abacus"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 4.0000000000000003e-7
[models."meta-llama/Meta-Llama-3.1-70B-Instruct".pricing."deepinfra"]
input_cost_per_token = 4e-7
output_cost_per_token = 4e-7
[models."meta-llama/Meta-Llama-3.1-70B-Instruct".pricing."hyperbolic"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7

[models."meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1e-7
output_cost_per_token = 2.8e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "together_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo".pricing."deepinfra"]
input_cost_per_token = 1e-7
output_cost_per_token = 2.8e-7
[models."meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo".pricing."together_ai"]
input_cost_per_token = 8.8e-7
output_cost_per_token = 8.8e-7

[models."meta-llama/Meta-Llama-3.1-8B-Instruct"]
display_name = "Llama 3.1 8B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 3e-8
output_cost_per_token = 5e-8
litellm_provider = "deepinfra"
providers = ["deepinfra", "abacus", "hyperbolic", "nebius", "siliconflow"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."meta-llama/Meta-Llama-3.1-8B-Instruct".pricing."abacus"]
input_cost_per_token = 2e-8
output_cost_per_token = 5.0000000000000004e-8
[models."meta-llama/Meta-Llama-3.1-8B-Instruct".pricing."deepinfra"]
input_cost_per_token = 3e-8
output_cost_per_token = 5e-8
[models."meta-llama/Meta-Llama-3.1-8B-Instruct".pricing."hyperbolic"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 3e-7
[models."meta-llama/Meta-Llama-3.1-8B-Instruct".pricing."nebius"]
cache_read_input_token_cost = 2e-9
input_cost_per_token = 2e-8
output_cost_per_token = 6e-8
[models."meta-llama/Meta-Llama-3.1-8B-Instruct".pricing."siliconflow"]
input_cost_per_token = 6e-8
output_cost_per_token = 6e-8

[models."meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-8
output_cost_per_token = 3e-8
litellm_provider = "deepinfra"
providers = ["deepinfra", "together_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo".pricing."deepinfra"]
input_cost_per_token = 2e-8
output_cost_per_token = 3e-8
[models."meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo".pricing."together_ai"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 1.8e-7

[models."meta-llama/llama-3-1-405b-instruct"]
display_name = "Meta: Llama 3.1 405B Instruct"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 26200
input_cost_per_token = 0.000004
output_cost_per_token = 0.000004
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-07-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta-llama/llama-3-1-405b-instruct".pricing."kilo"]
input_cost_per_token = 0.000004
output_cost_per_token = 0.000004

[models."meta-llama/llama-3-1-405b-instruct:free"]
display_name = "Llama 3.1 405B Instruct (free)"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta-llama/llama-3-1-70b-instruct"]
display_name = "Meta: Llama 3.1 70B Instruct"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 26215
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 4.0000000000000003e-7
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-07-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta-llama/llama-3-1-70b-instruct".pricing."kilo"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 4.0000000000000003e-7

[models."meta-llama/llama-3-2-11b-vision-instruct"]
display_name = "Llama 3.2 11B Vision Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 3.5e-7
output_cost_per_token = 3.5e-7
litellm_provider = "watsonx"
providers = ["watsonx", "kilo", "openrouter"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-09-25"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true

[models."meta-llama/llama-3-2-11b-vision-instruct".pricing."kilo"]
input_cost_per_token = 4.9e-8
output_cost_per_token = 4.9e-8
[models."meta-llama/llama-3-2-11b-vision-instruct".pricing."watsonx"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 3.5e-7

[models."meta-llama/llama-3-2-1b-instruct"]
display_name = "Meta: Llama 3.2 1B Instruct"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "watsonx"
providers = ["watsonx", "kilo"]
supports_function_calling = true
supports_vision = false
supports_reasoning = false
open_weights = true
release_date = "2024-09-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true

[models."meta-llama/llama-3-2-1b-instruct".pricing."kilo"]
input_cost_per_token = 2.7e-8
output_cost_per_token = 2.0000000000000002e-7
[models."meta-llama/llama-3-2-1b-instruct".pricing."watsonx"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."meta-llama/llama-3-2-3b-instruct"]
display_name = "Meta: Llama 3.2 3B Instruct"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "watsonx"
providers = ["watsonx", "kilo"]
supports_function_calling = true
supports_vision = false
supports_reasoning = false
open_weights = true
release_date = "2024-09-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true

[models."meta-llama/llama-3-2-3b-instruct".pricing."kilo"]
input_cost_per_token = 2e-8
output_cost_per_token = 2e-8
[models."meta-llama/llama-3-2-3b-instruct".pricing."watsonx"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7

[models."meta-llama/llama-3-2-3b-instruct:free"]
display_name = "Llama 3.2 3B Instruct (free)"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-09-25"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta-llama/llama-3-2-90b-vision-instruct"]
display_name = "Llama 3.2 90B Vision Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002
litellm_provider = "watsonx"
providers = ["watsonx", "io-net"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-09-25"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true

[models."meta-llama/llama-3-2-90b-vision-instruct".pricing."io-net"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 3.5e-7
output_cost_per_token = 4.0000000000000003e-7
[models."meta-llama/llama-3-2-90b-vision-instruct".pricing."watsonx"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002

[models."meta-llama/llama-3-3-70b-instruct"]
display_name = "Llama 3.3 70B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 7.1e-7
output_cost_per_token = 7.1e-7
litellm_provider = "watsonx"
providers = ["watsonx", "berget", "cloudferro-sherlock", "friendli", "io-net", "kilo", "meganova", "nano-gpt", "nebius", "novita-ai", "wandb"]
supports_function_calling = true
supports_vision = false
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-10-09"
release_date = "2024-12-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true

[models."meta-llama/llama-3-3-70b-instruct".pricing."berget"]
input_cost_per_token = 9.000000000000001e-7
output_cost_per_token = 9.000000000000001e-7
[models."meta-llama/llama-3-3-70b-instruct".pricing."cloudferro-sherlock"]
input_cost_per_token = 0.00000292
output_cost_per_token = 0.00000292
[models."meta-llama/llama-3-3-70b-instruct".pricing."friendli"]
input_cost_per_token = 6e-7
output_cost_per_token = 6e-7
[models."meta-llama/llama-3-3-70b-instruct".pricing."io-net"]
cache_read_input_token_cost = 6.5e-8
input_cost_per_token = 1.3e-7
output_cost_per_token = 3.8e-7
[models."meta-llama/llama-3-3-70b-instruct".pricing."kilo"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3.2e-7
[models."meta-llama/llama-3-3-70b-instruct".pricing."meganova"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
[models."meta-llama/llama-3-3-70b-instruct".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
[models."meta-llama/llama-3-3-70b-instruct".pricing."nebius"]
cache_read_input_token_cost = 1.2999999999999999e-8
input_cost_per_token = 1.3e-7
output_cost_per_token = 4.0000000000000003e-7
[models."meta-llama/llama-3-3-70b-instruct".pricing."novita-ai"]
input_cost_per_token = 1.35e-7
output_cost_per_token = 4.0000000000000003e-7
[models."meta-llama/llama-3-3-70b-instruct".pricing."wandb"]
input_cost_per_token = 7.1e-7
output_cost_per_token = 7.1e-7
[models."meta-llama/llama-3-3-70b-instruct".pricing."watsonx"]
input_cost_per_token = 7.1e-7
output_cost_per_token = 7.1e-7

[models."meta-llama/llama-3-3-70b-instruct-fast"]
display_name = "Llama-3.3-70B-Instruct (Fast)"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 2.5e-7
output_cost_per_token = 7.5e-7
cache_read_input_token_cost = 2.5000000000000002e-8
litellm_provider = "nebius"
providers = ["nebius"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-08"
release_date = "2025-12-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta-llama/llama-3-3-70b-instruct-fast".pricing."nebius"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 7.5e-7

[models."meta-llama/llama-3-3-70b-instruct:free"]
display_name = "Llama 3.3 70B Instruct (free)"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2024-12-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta-llama/llama-3-70b-instruct"]
display_name = "Llama3 70B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8000
max_tokens = 8192
input_cost_per_token = 5.9e-7
output_cost_per_token = 7.9e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "novita", "novita-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2024-04-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."meta-llama/llama-3-70b-instruct".pricing."kilo"]
input_cost_per_token = 5.1e-7
output_cost_per_token = 7.4e-7
[models."meta-llama/llama-3-70b-instruct".pricing."novita"]
input_cost_per_token = 5.1e-7
output_cost_per_token = 7.4e-7
[models."meta-llama/llama-3-70b-instruct".pricing."novita-ai"]
input_cost_per_token = 5.1e-7
output_cost_per_token = 7.4e-7
[models."meta-llama/llama-3-70b-instruct".pricing."openrouter"]
input_cost_per_token = 5.9e-7
output_cost_per_token = 7.9e-7

[models."meta-llama/llama-3-8b-instruct"]
display_name = "Llama 3 8B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 4e-8
output_cost_per_token = 4e-8
litellm_provider = "novita"
providers = ["novita", "kilo", "novita-ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-04-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."meta-llama/llama-3-8b-instruct".pricing."kilo"]
input_cost_per_token = 3e-8
output_cost_per_token = 4e-8
[models."meta-llama/llama-3-8b-instruct".pricing."novita"]
input_cost_per_token = 4e-8
output_cost_per_token = 4e-8
[models."meta-llama/llama-3-8b-instruct".pricing."novita-ai"]
input_cost_per_token = 4e-8
output_cost_per_token = 4e-8

[models."meta-llama/llama-3.1-8b-instruct"]
display_name = "Llama 3.1 8B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2e-8
output_cost_per_token = 5e-8
litellm_provider = "novita"
providers = ["novita", "friendli", "kilo", "novita-ai", "wandb"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-07-24"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."meta-llama/llama-3.1-8b-instruct".pricing."friendli"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 1.0000000000000001e-7
[models."meta-llama/llama-3.1-8b-instruct".pricing."kilo"]
input_cost_per_token = 2e-8
output_cost_per_token = 5.0000000000000004e-8
[models."meta-llama/llama-3.1-8b-instruct".pricing."novita"]
input_cost_per_token = 2e-8
output_cost_per_token = 5e-8
[models."meta-llama/llama-3.1-8b-instruct".pricing."novita-ai"]
input_cost_per_token = 2e-8
output_cost_per_token = 5.0000000000000004e-8
[models."meta-llama/llama-3.1-8b-instruct".pricing."wandb"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 2.2e-7

[models."meta-llama/llama-3.2-3b-instruct"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 3e-8
output_cost_per_token = 5e-8
litellm_provider = "novita"
providers = ["novita"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."meta-llama/llama-3.2-3b-instruct".pricing."novita"]
input_cost_per_token = 3e-8
output_cost_per_token = 5e-8

[models."meta-llama/llama-3.3-70b-instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 120000
max_tokens = 120000
input_cost_per_token = 1.35e-7
output_cost_per_token = 4e-7
litellm_provider = "novita"
providers = ["novita"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."meta-llama/llama-3.3-70b-instruct".pricing."novita"]
input_cost_per_token = 1.35e-7
output_cost_per_token = 4e-7

[models."meta-llama/llama-4-maverick"]
display_name = "Meta: Llama 4 Maverick"
model_family = "llama"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 16384
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "kilo"
providers = ["kilo", "nano-gpt"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2025-04-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta-llama/llama-4-maverick".pricing."kilo"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."meta-llama/llama-4-maverick".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002

[models."meta-llama/llama-4-maverick-17b"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.0000014
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = true
supports_vision = false
supports_parallel_function_calling = true

[models."meta-llama/llama-4-maverick-17b".pricing."watsonx"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.0000014

[models."meta-llama/llama-4-maverick-17b-128e-instruct"]
display_name = "Llama 4 Maverick 17B"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7
litellm_provider = "groq"
providers = ["groq"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-04-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_tool_choice = true

[models."meta-llama/llama-4-maverick-17b-128e-instruct".pricing."groq"]
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7

[models."meta-llama/llama-4-maverick-17b-128e-instruct-fp8"]
display_name = "Llama 4 Maverick 17B 128E Instruct FP8"
model_family = "llama"
mode = "chat"
max_input_tokens = 1048576
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2.7e-7
output_cost_per_token = 8.5e-7
litellm_provider = "novita"
providers = ["novita", "abacus", "io-net", "novita-ai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-04-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."meta-llama/llama-4-maverick-17b-128e-instruct-fp8".pricing."abacus"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.9e-7
[models."meta-llama/llama-4-maverick-17b-128e-instruct-fp8".pricing."io-net"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."meta-llama/llama-4-maverick-17b-128e-instruct-fp8".pricing."novita"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 8.5e-7
[models."meta-llama/llama-4-maverick-17b-128e-instruct-fp8".pricing."novita-ai"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 8.5e-7

[models."meta-llama/llama-4-scout"]
display_name = "Meta: Llama 4 Scout"
mode = "chat"
max_input_tokens = 327680
max_output_tokens = 16384
input_cost_per_token = 8e-8
output_cost_per_token = 3e-7
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-04-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta-llama/llama-4-scout".pricing."kilo"]
input_cost_per_token = 8e-8
output_cost_per_token = 3e-7

[models."meta-llama/llama-4-scout-17b-16e-instruct"]
display_name = "Llama 4 Scout 17B"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.1e-7
output_cost_per_token = 3.4e-7
litellm_provider = "groq"
providers = ["groq", "novita", "novita-ai", "wandb"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-04-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."meta-llama/llama-4-scout-17b-16e-instruct".pricing."groq"]
input_cost_per_token = 1.1e-7
output_cost_per_token = 3.4e-7
[models."meta-llama/llama-4-scout-17b-16e-instruct".pricing."novita"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 5.9e-7
[models."meta-llama/llama-4-scout-17b-16e-instruct".pricing."novita-ai"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 5.9e-7
[models."meta-llama/llama-4-scout-17b-16e-instruct".pricing."wandb"]
input_cost_per_token = 1.7000000000000001e-7
output_cost_per_token = 6.6e-7

[models."meta-llama/llama-4-scout:free"]
display_name = "Llama 4 Scout (free)"
model_family = "llama"
mode = "chat"
max_input_tokens = 64000
max_output_tokens = 64000
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-04-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta-llama/llama-guard-3-11b-vision"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 3.5e-7
output_cost_per_token = 3.5e-7
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = false
supports_vision = true
supports_parallel_function_calling = false

[models."meta-llama/llama-guard-3-11b-vision".pricing."watsonx"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 3.5e-7

[models."meta-llama/llama-guard-4-12b"]
display_name = "Llama Guard 4 12B"
model_family = "llama"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "groq"
providers = ["groq", "kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-04-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]

[models."meta-llama/llama-guard-4-12b".pricing."groq"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
[models."meta-llama/llama-guard-4-12b".pricing."kilo"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 1.8e-7

[models."meta-llama/meta-llama-3-1-8b-instruct-fast"]
display_name = "Meta-Llama-3.1-8B-Instruct (Fast)"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
input_cost_per_token = 3e-8
output_cost_per_token = 9e-8
cache_read_input_token_cost = 3e-9
litellm_provider = "nebius"
providers = ["nebius"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta-llama/meta-llama-3-1-8b-instruct-fast".pricing."nebius"]
cache_read_input_token_cost = 3e-9
input_cost_per_token = 3e-8
output_cost_per_token = 9e-8

[models."meta-textgeneration-llama-2-13b"]
mode = "completion"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "sagemaker"
providers = ["sagemaker"]

[models."meta-textgeneration-llama-2-13b".pricing."sagemaker"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."meta-textgeneration-llama-2-13b-f"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "sagemaker"
providers = ["sagemaker"]

[models."meta-textgeneration-llama-2-13b-f".pricing."sagemaker"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."meta-textgeneration-llama-2-70b"]
mode = "completion"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "sagemaker"
providers = ["sagemaker"]

[models."meta-textgeneration-llama-2-70b".pricing."sagemaker"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."meta-textgeneration-llama-2-70b-b-f"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "sagemaker"
providers = ["sagemaker"]

[models."meta-textgeneration-llama-2-70b-b-f".pricing."sagemaker"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."meta-textgeneration-llama-2-7b"]
mode = "completion"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "sagemaker"
providers = ["sagemaker"]

[models."meta-textgeneration-llama-2-7b".pricing."sagemaker"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."meta-textgeneration-llama-2-7b-f"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "sagemaker"
providers = ["sagemaker"]

[models."meta-textgeneration-llama-2-7b-f".pricing."sagemaker"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."meta.llama-3.1-405b-instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 4000
input_cost_per_token = 0.00001068
output_cost_per_token = 0.00001068
litellm_provider = "oci"
providers = ["oci"]
supports_function_calling = true
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_response_schema = false

[models."meta.llama-3.1-405b-instruct".pricing."oci"]
input_cost_per_token = 0.00001068
output_cost_per_token = 0.00001068

[models."meta.llama-3.2-90b-vision-instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 4000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002
litellm_provider = "oci"
providers = ["oci"]
supports_function_calling = true
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_response_schema = false

[models."meta.llama-3.2-90b-vision-instruct".pricing."oci"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002

[models."meta.llama-3.3-70b-instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 4000
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7
litellm_provider = "oci"
providers = ["oci"]
supports_function_calling = true
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_response_schema = false

[models."meta.llama-3.3-70b-instruct".pricing."oci"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7

[models."meta.llama-4-maverick-17b-128e-instruct-fp8"]
mode = "chat"
max_input_tokens = 512000
max_output_tokens = 4000
max_tokens = 4000
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7
litellm_provider = "oci"
providers = ["oci"]
supports_function_calling = true
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_response_schema = false

[models."meta.llama-4-maverick-17b-128e-instruct-fp8".pricing."oci"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7

[models."meta.llama-4-scout-17b-16e-instruct"]
mode = "chat"
max_input_tokens = 192000
max_output_tokens = 4000
max_tokens = 4000
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7
litellm_provider = "oci"
providers = ["oci"]
supports_function_calling = true
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_response_schema = false

[models."meta.llama-4-scout-17b-16e-instruct".pricing."oci"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7

[models."meta.llama2-13b-chat-v1"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 7.5e-7
output_cost_per_token = 0.000001
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."meta.llama2-13b-chat-v1".pricing."bedrock"]
input_cost_per_token = 7.5e-7
output_cost_per_token = 0.000001

[models."meta.llama2-70b-chat-v1"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00000195
output_cost_per_token = 0.00000256
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."meta.llama2-70b-chat-v1".pricing."bedrock"]
input_cost_per_token = 0.00000195
output_cost_per_token = 0.00000256

[models."meta.llama3-1-405b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00000532
output_cost_per_token = 0.000016
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_tool_choice = false

[models."meta.llama3-1-405b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 0.00000532
output_cost_per_token = 0.000016

[models."meta.llama3-1-70b-instruct-v1:0"]
display_name = "Llama 3.1 70B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 9.9e-7
output_cost_per_token = 9.9e-7
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = false

[models."meta.llama3-1-70b-instruct-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7
[models."meta.llama3-1-70b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 9.9e-7
output_cost_per_token = 9.9e-7

[models."meta.llama3-1-8b-instruct-v1:0"]
display_name = "Llama 3.1 8B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 2.2e-7
output_cost_per_token = 2.2e-7
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = false

[models."meta.llama3-1-8b-instruct-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 2.2e-7
[models."meta.llama3-1-8b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 2.2e-7

[models."meta.llama3-2-11b-instruct-v1:0"]
display_name = "Llama 3.2 11B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 3.5e-7
output_cost_per_token = 3.5e-7
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-09-25"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = false

[models."meta.llama3-2-11b-instruct-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 1.6e-7
output_cost_per_token = 1.6e-7
[models."meta.llama3-2-11b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 3.5e-7

[models."meta.llama3-2-1b-instruct-v1:0"]
display_name = "Llama 3.2 1B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-09-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = false

[models."meta.llama3-2-1b-instruct-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 1.0000000000000001e-7
[models."meta.llama3-2-1b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."meta.llama3-2-3b-instruct-v1:0"]
display_name = "Llama 3.2 3B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-09-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = false

[models."meta.llama3-2-3b-instruct-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."meta.llama3-2-3b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7

[models."meta.llama3-2-90b-instruct-v1:0"]
display_name = "Llama 3.2 90B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-09-25"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = false

[models."meta.llama3-2-90b-instruct-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7
[models."meta.llama3-2-90b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002

[models."meta.llama3-3-70b-instruct-v1:0"]
display_name = "Llama 3.3 70B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-12-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = false

[models."meta.llama3-3-70b-instruct-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7
[models."meta.llama3-3-70b-instruct-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7

[models."meta.llama3-70b-instruct-v1:0"]
display_name = "Llama 3 70B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000286
output_cost_per_token = 0.00000378
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."meta.llama3-70b-instruct-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 0.00000265
output_cost_per_token = 0.0000035
[models."meta.llama3-70b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 0.00000265
output_cost_per_token = 0.0000035
[models."meta.llama3-70b-instruct-v1:0".pricing."bedrock/eu-west-1"]
input_cost_per_token = 0.00000286
output_cost_per_token = 0.00000378
[models."meta.llama3-70b-instruct-v1:0".pricing."bedrock/us-east-1"]
input_cost_per_token = 0.00000265
output_cost_per_token = 0.0000035

[models."meta.llama3-8b-instruct-v1:0"]
display_name = "Llama 3 8B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3.2e-7
output_cost_per_token = 6.5e-7
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-03"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."meta.llama3-8b-instruct-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 3e-7
output_cost_per_token = 6e-7
[models."meta.llama3-8b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 3e-7
output_cost_per_token = 6e-7
[models."meta.llama3-8b-instruct-v1:0".pricing."bedrock/eu-west-1"]
input_cost_per_token = 3.2e-7
output_cost_per_token = 6.5e-7
[models."meta.llama3-8b-instruct-v1:0".pricing."bedrock/us-east-1"]
input_cost_per_token = 3e-7
output_cost_per_token = 6e-7

[models."meta.llama4-maverick-17b-instruct-v1:0"]
display_name = "Llama 4 Maverick 17B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2.4e-7
output_cost_per_token = 9.7e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-04-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "code"]
input_cost_per_token_batches = 1.2e-7
output_cost_per_token_batches = 4.85e-7
supports_tool_choice = false

[models."meta.llama4-maverick-17b-instruct-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 2.4e-7
output_cost_per_token = 9.7e-7
[models."meta.llama4-maverick-17b-instruct-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 2.4e-7
input_cost_per_token_batches = 1.2e-7
output_cost_per_token = 9.7e-7
output_cost_per_token_batches = 4.85e-7

[models."meta.llama4-scout-17b-instruct-v1:0"]
display_name = "Llama 4 Scout 17B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.7e-7
output_cost_per_token = 6.6e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-04-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "code"]
input_cost_per_token_batches = 8.5e-8
output_cost_per_token_batches = 3.3e-7
supports_tool_choice = false

[models."meta.llama4-scout-17b-instruct-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 1.7000000000000001e-7
output_cost_per_token = 6.6e-7
[models."meta.llama4-scout-17b-instruct-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 1.7e-7
input_cost_per_token_batches = 8.5e-8
output_cost_per_token = 6.6e-7
output_cost_per_token_batches = 3.3e-7

[models."meta/codellama-70b"]
display_name = "Codellama 70b"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2024-01-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/llama-2-13b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-7
output_cost_per_token = 5e-7
litellm_provider = "replicate"
providers = ["replicate"]
supports_tool_choice = true

[models."meta/llama-2-13b".pricing."replicate"]
input_cost_per_token = 1e-7
output_cost_per_token = 5e-7

[models."meta/llama-2-13b-chat"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-7
output_cost_per_token = 5e-7
litellm_provider = "replicate"
providers = ["replicate"]
supports_tool_choice = true

[models."meta/llama-2-13b-chat".pricing."replicate"]
input_cost_per_token = 1e-7
output_cost_per_token = 5e-7

[models."meta/llama-2-70b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 6.5e-7
output_cost_per_token = 0.00000275
litellm_provider = "replicate"
providers = ["replicate"]
supports_tool_choice = true

[models."meta/llama-2-70b".pricing."replicate"]
input_cost_per_token = 6.5e-7
output_cost_per_token = 0.00000275

[models."meta/llama-2-70b-chat"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 6.5e-7
output_cost_per_token = 0.00000275
litellm_provider = "replicate"
providers = ["replicate"]
supports_tool_choice = true

[models."meta/llama-2-70b-chat".pricing."replicate"]
input_cost_per_token = 6.5e-7
output_cost_per_token = 0.00000275

[models."meta/llama-2-7b"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 5e-8
output_cost_per_token = 2.5e-7
litellm_provider = "replicate"
providers = ["replicate"]
supports_tool_choice = true

[models."meta/llama-2-7b".pricing."replicate"]
input_cost_per_token = 5e-8
output_cost_per_token = 2.5e-7

[models."meta/llama-2-7b-chat"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 5e-8
output_cost_per_token = 2.5e-7
litellm_provider = "replicate"
providers = ["replicate"]
supports_tool_choice = true

[models."meta/llama-2-7b-chat".pricing."replicate"]
input_cost_per_token = 5e-8
output_cost_per_token = 2.5e-7

[models."meta/llama-3-1-405b-instruct"]
display_name = "Llama 3.1 405b Instruct"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-07-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/llama-3-1-70b-instruct"]
display_name = "Llama 3.1 70b Instruct"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-07-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/llama-3-1-8b-instruct"]
display_name = "Llama 3.1 8B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 16000
max_output_tokens = 4096
input_cost_per_token = 2.5000000000000002e-8
output_cost_per_token = 2.5000000000000002e-8
litellm_provider = "inference"
providers = ["inference"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/llama-3-1-8b-instruct".pricing."inference"]
input_cost_per_token = 2.5000000000000002e-8
output_cost_per_token = 2.5000000000000002e-8

[models."meta/llama-3-2-11b-vision-instruct"]
display_name = "Llama 3.2 11b Vision Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia", "github-models", "inference"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-09-18"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/llama-3-2-11b-vision-instruct".pricing."inference"]
input_cost_per_token = 5.5e-8
output_cost_per_token = 5.5e-8

[models."meta/llama-3-2-1b-instruct"]
display_name = "Llama 3.2 1b Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia", "inference"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-09-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/llama-3-2-1b-instruct".pricing."inference"]
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8

[models."meta/llama-3-2-3b-instruct"]
display_name = "Llama 3.2 3B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 16000
max_output_tokens = 4096
input_cost_per_token = 2e-8
output_cost_per_token = 2e-8
litellm_provider = "inference"
providers = ["inference"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/llama-3-2-3b-instruct".pricing."inference"]
input_cost_per_token = 2e-8
output_cost_per_token = 2e-8

[models."meta/llama-3-2-90b-vision-instruct"]
display_name = "Llama-3.2-90B-Vision-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-09-25"
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/llama-3-3-70b-instruct"]
display_name = "Llama 3.3 70b Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia", "github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-11-26"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/llama-3-3-70b-instruct-maas"]
display_name = "Llama 3.3 70B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7
litellm_provider = "google-vertex"
providers = ["google-vertex"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2025-04-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/llama-3-3-70b-instruct-maas".pricing."google-vertex"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7

[models."meta/llama-3-70b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6.5e-7
output_cost_per_token = 0.00000275
litellm_provider = "replicate"
providers = ["replicate", "vercel_ai_gateway"]
supports_tool_choice = true

[models."meta/llama-3-70b".pricing."replicate"]
input_cost_per_token = 6.5e-7
output_cost_per_token = 0.00000275
[models."meta/llama-3-70b".pricing."vercel_ai_gateway"]
input_cost_per_token = 5.9e-7
output_cost_per_token = 7.9e-7

[models."meta/llama-3-70b-instruct"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6.5e-7
output_cost_per_token = 0.00000275
litellm_provider = "replicate"
providers = ["replicate"]
supports_tool_choice = true

[models."meta/llama-3-70b-instruct".pricing."replicate"]
input_cost_per_token = 6.5e-7
output_cost_per_token = 0.00000275

[models."meta/llama-3-8b"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5e-8
output_cost_per_token = 2.5e-7
litellm_provider = "replicate"
providers = ["replicate", "vercel_ai_gateway"]
supports_tool_choice = true

[models."meta/llama-3-8b".pricing."replicate"]
input_cost_per_token = 5e-8
output_cost_per_token = 2.5e-7
[models."meta/llama-3-8b".pricing."vercel_ai_gateway"]
input_cost_per_token = 5e-8
output_cost_per_token = 8e-8

[models."meta/llama-3-8b-instruct"]
mode = "chat"
max_input_tokens = 8086
max_output_tokens = 8086
max_tokens = 8086
input_cost_per_token = 5e-8
output_cost_per_token = 2.5e-7
litellm_provider = "replicate"
providers = ["replicate"]
supports_tool_choice = true

[models."meta/llama-3-8b-instruct".pricing."replicate"]
input_cost_per_token = 5e-8
output_cost_per_token = 2.5e-7

[models."meta/llama-3.1-405b-instruct-maas"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 0.000005
output_cost_per_token = 0.000016
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_vision = true
source = "https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3.2-90b-vision-instruct-maas"
supports_system_messages = true
supports_tool_choice = true

[models."meta/llama-3.1-405b-instruct-maas".pricing."vertex_ai"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000016

[models."meta/llama-3.1-70b"]
display_name = "Llama 3.1 70B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."meta/llama-3.1-70b".pricing."vercel"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 4.0000000000000003e-7
[models."meta/llama-3.1-70b".pricing."vercel_ai_gateway"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7

[models."meta/llama-3.1-70b-instruct-maas"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_vision = true
source = "https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3.2-90b-vision-instruct-maas"
supports_system_messages = true
supports_tool_choice = true

[models."meta/llama-3.1-70b-instruct-maas".pricing."vertex_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."meta/llama-3.1-8b"]
display_name = "Llama 3.1 8B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 5e-8
output_cost_per_token = 8e-8
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true

[models."meta/llama-3.1-8b".pricing."vercel"]
input_cost_per_token = 3e-8
output_cost_per_token = 5.0000000000000004e-8
[models."meta/llama-3.1-8b".pricing."vercel_ai_gateway"]
input_cost_per_token = 5e-8
output_cost_per_token = 8e-8

[models."meta/llama-3.1-8b-instruct-maas"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_vision = true
source = "https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3.2-90b-vision-instruct-maas"
supports_system_messages = true
supports_tool_choice = true

[models."meta/llama-3.1-8b-instruct-maas".metadata]
notes = "VertexAI states that The Llama 3.1 API service for llama-3.1-70b-instruct-maas and llama-3.1-8b-instruct-maas are in public preview and at no cost."

[models."meta/llama-3.1-8b-instruct-maas".pricing."vertex_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."meta/llama-3.2-11b"]
display_name = "Llama 3.2 11B Vision Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.6e-7
output_cost_per_token = 1.6e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-12"
release_date = "2024-09-25"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."meta/llama-3.2-11b".pricing."vercel"]
input_cost_per_token = 1.6e-7
output_cost_per_token = 1.6e-7
[models."meta/llama-3.2-11b".pricing."vercel_ai_gateway"]
input_cost_per_token = 1.6e-7
output_cost_per_token = 1.6e-7

[models."meta/llama-3.2-1b"]
display_name = "Llama 3.2 1B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-12"
release_date = "2024-09-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."meta/llama-3.2-1b".pricing."vercel"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 1.0000000000000001e-7
[models."meta/llama-3.2-1b".pricing."vercel_ai_gateway"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."meta/llama-3.2-3b"]
display_name = "Llama 3.2 3B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-12"
release_date = "2024-09-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true

[models."meta/llama-3.2-3b".pricing."vercel"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."meta/llama-3.2-3b".pricing."vercel_ai_gateway"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7

[models."meta/llama-3.2-90b"]
display_name = "Llama 3.2 90B Vision Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2023-12"
release_date = "2024-09-25"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."meta/llama-3.2-90b".pricing."vercel"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7
[models."meta/llama-3.2-90b".pricing."vercel_ai_gateway"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7

[models."meta/llama-3.2-90b-vision-instruct-maas"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_vision = true
source = "https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3.2-90b-vision-instruct-maas"
supports_system_messages = true
supports_tool_choice = true

[models."meta/llama-3.2-90b-vision-instruct-maas".metadata]
notes = "VertexAI states that The Llama 3.2 API service is at no cost during public preview, and will be priced as per dollar-per-1M-tokens at GA."

[models."meta/llama-3.2-90b-vision-instruct-maas".pricing."vertex_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."meta/llama-3.3-70b"]
display_name = "Llama-3.3-70B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-12-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."meta/llama-3.3-70b".pricing."vercel_ai_gateway"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7

[models."meta/llama-4-maverick"]
display_name = "Llama-4-Maverick-17B-128E-Instruct-FP8"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-04-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."meta/llama-4-maverick".pricing."vercel_ai_gateway"]
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7

[models."meta/llama-4-maverick-17b-128e-instruct"]
display_name = "Llama 4 Maverick 17b 128e Instruct"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-02"
release_date = "2025-04-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/llama-4-maverick-17b-128e-instruct-fp8"]
display_name = "Llama 4 Maverick 17B 128E Instruct FP8"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2025-01-31"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/llama-4-maverick-17b-128e-instruct-maas"]
display_name = "Llama 4 Maverick 17B 128E Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 1000000
max_tokens = 1000000
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000115
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-04-29"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "code"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_tool_choice = true

[models."meta/llama-4-maverick-17b-128e-instruct-maas".pricing."google-vertex"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000115
[models."meta/llama-4-maverick-17b-128e-instruct-maas".pricing."vertex_ai"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000115

[models."meta/llama-4-maverick-17b-16e-instruct-maas"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 1000000
max_tokens = 1000000
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000115
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "code"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_tool_choice = true

[models."meta/llama-4-maverick-17b-16e-instruct-maas".pricing."vertex_ai"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000115

[models."meta/llama-4-scout"]
display_name = "Llama-4-Scout-17B-16E-Instruct-FP8"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-08"
release_date = "2025-04-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."meta/llama-4-scout".pricing."vercel_ai_gateway"]
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7

[models."meta/llama-4-scout-17b-128e-instruct-maas"]
mode = "chat"
max_input_tokens = 10000000
max_output_tokens = 10000000
max_tokens = 10000000
input_cost_per_token = 2.5e-7
output_cost_per_token = 7e-7
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "code"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_tool_choice = true

[models."meta/llama-4-scout-17b-128e-instruct-maas".pricing."vertex_ai"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 7e-7

[models."meta/llama-4-scout-17b-16e-instruct"]
display_name = "Llama 4 Scout 17b 16e Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia", "github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-02"
release_date = "2025-04-02"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/llama-4-scout-17b-16e-instruct-maas"]
mode = "chat"
max_input_tokens = 10000000
max_output_tokens = 10000000
max_tokens = 10000000
input_cost_per_token = 2.5e-7
output_cost_per_token = 7e-7
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "code"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_tool_choice = true

[models."meta/llama-4-scout-17b-16e-instruct-maas".pricing."vertex_ai"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 7e-7

[models."meta/llama3-405b-instruct-maas"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_tool_choice = true

[models."meta/llama3-405b-instruct-maas".pricing."vertex_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."meta/llama3-70b-instruct"]
display_name = "Llama3 70b Instruct"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-04-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/llama3-70b-instruct-maas"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_tool_choice = true

[models."meta/llama3-70b-instruct-maas".pricing."vertex_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."meta/llama3-8b-instruct"]
display_name = "Llama3 8b Instruct"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-04-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/llama3-8b-instruct-maas"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_tool_choice = true

[models."meta/llama3-8b-instruct-maas".pricing."vertex_ai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."meta/meta-llama-3-1-405b-instruct"]
display_name = "Meta-Llama-3.1-405B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/meta-llama-3-1-70b-instruct"]
display_name = "Meta-Llama-3.1-70B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/meta-llama-3-1-8b-instruct"]
display_name = "Meta-Llama-3.1-8B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/meta-llama-3-70b-instruct"]
display_name = "Meta-Llama-3-70B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 2048
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-04-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."meta/meta-llama-3-8b-instruct"]
display_name = "Meta-Llama-3-8B-Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 2048
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-04-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."microsoft/Phi-4-mini-instruct"]
display_name = "Phi-4-Mini"
model_family = "phi"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.008
output_cost_per_token = 0.035
litellm_provider = "wandb"
providers = ["wandb", "github-models", "nvidia"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-12"
release_date = "2024-12-01"
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text"]

[models."microsoft/Phi-4-mini-instruct".pricing."wandb"]
input_cost_per_token = 0.008
output_cost_per_token = 0.035

[models."microsoft/Phi-4-multimodal-instruct"]
display_name = "Phi-4 15B"
model_family = "phi"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 32000
input_cost_per_token = 2.4e-7
output_cost_per_token = 4.6999999999999995e-7
litellm_provider = "evroc"
providers = ["evroc", "github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text", "image"]
source = "modelsdev"

[models."microsoft/Phi-4-multimodal-instruct".pricing."evroc"]
input_cost_per_token = 2.4e-7
output_cost_per_token = 4.6999999999999995e-7

[models."microsoft/WizardLM-2-8x22B"]
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 4.8e-7
output_cost_per_token = 4.8e-7
litellm_provider = "deepinfra"
providers = ["deepinfra"]
supports_tool_choice = false

[models."microsoft/WizardLM-2-8x22B".pricing."deepinfra"]
input_cost_per_token = 4.8e-7
output_cost_per_token = 4.8e-7

[models."microsoft/mai-ds-r1"]
display_name = "MAI-DS-R1"
model_family = "mai"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 8192
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-06"
release_date = "2025-01-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."microsoft/mai-ds-r1:free"]
display_name = "MAI DS R1 (free)"
model_family = "mai"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04-21"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."microsoft/phi-3-5-mini-instruct"]
display_name = "Phi-3.5-mini instruct (128k)"
model_family = "phi"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-08-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."microsoft/phi-3-5-moe-instruct"]
display_name = "Phi 3.5 Moe Instruct"
model_family = "phi"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia", "github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-08-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."microsoft/phi-3-5-vision-instruct"]
display_name = "Phi 3.5 Vision Instruct"
model_family = "phi"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia", "github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-08-16"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."microsoft/phi-3-medium-128k-instruct"]
display_name = "Phi 3 Medium 128k Instruct"
model_family = "phi"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia", "github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-05-07"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."microsoft/phi-3-medium-4k-instruct"]
display_name = "Phi 3 Medium 4k Instruct"
model_family = "phi"
mode = "chat"
max_input_tokens = 4000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia", "github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-05-07"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."microsoft/phi-3-mini-128k-instruct"]
display_name = "Phi-3-mini instruct (128k)"
model_family = "phi"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-04-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."microsoft/phi-3-mini-4k-instruct"]
display_name = "Phi-3-mini instruct (4k)"
model_family = "phi"
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 1024
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-04-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."microsoft/phi-3-small-128k-instruct"]
display_name = "Phi 3 Small 128k Instruct"
model_family = "phi"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia", "github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-05-07"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."microsoft/phi-3-small-8k-instruct"]
display_name = "Phi 3 Small 8k Instruct"
model_family = "phi"
mode = "chat"
max_input_tokens = 8000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia", "github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-05-07"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."microsoft/phi-3-vision-128k-instruct"]
display_name = "Phi 3 Vision 128k Instruct"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-05-19"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."microsoft/phi-4"]
display_name = "Phi-4"
model_family = "phi"
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 7e-8
output_cost_per_token = 1.4e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "github-models", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-12-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."microsoft/phi-4".pricing."deepinfra"]
input_cost_per_token = 7e-8
output_cost_per_token = 1.4e-7
[models."microsoft/phi-4".pricing."kilo"]
input_cost_per_token = 6e-8
output_cost_per_token = 1.4e-7

[models."microsoft/phi-4-mini-reasoning"]
display_name = "Phi-4-mini-reasoning"
model_family = "phi"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-12-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."microsoft/phi-4-reasoning"]
display_name = "Phi-4-Reasoning"
model_family = "phi"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-12-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."microsoft/wizardlm-2-8x22b"]
display_name = "Wizardlm 2 8x22B"
mode = "chat"
max_input_tokens = 65535
max_output_tokens = 8000
max_tokens = 8000
input_cost_per_token = 6.2e-7
output_cost_per_token = 6.2e-7
litellm_provider = "novita"
providers = ["novita", "kilo", "novita-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2024-04-24"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."microsoft/wizardlm-2-8x22b".pricing."kilo"]
input_cost_per_token = 6.2e-7
output_cost_per_token = 6.2e-7
[models."microsoft/wizardlm-2-8x22b".pricing."novita"]
input_cost_per_token = 6.2e-7
output_cost_per_token = 6.2e-7
[models."microsoft/wizardlm-2-8x22b".pricing."novita-ai"]
input_cost_per_token = 6.2e-7
output_cost_per_token = 6.2e-7

[models."minimax-m2-1-free"]
display_name = "MiniMax M2.1 Free"
model_family = "minimax-free"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
litellm_provider = "opencode"
providers = ["opencode"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2025-12-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."minimax-m2-5-free"]
display_name = "MiniMax M2.5 Free"
model_family = "minimax-free"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
litellm_provider = "opencode"
providers = ["opencode"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2026-02-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."minimax-m21"]
display_name = "MiniMax M2.1"
model_family = "minimax"
mode = "chat"
max_input_tokens = 198000
max_output_tokens = 49500
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000016000000000000001
cache_read_input_token_cost = 4e-8
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."minimax-m21".pricing."venice"]
cache_read_input_token_cost = 4e-8
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000016000000000000001

[models."minimax-m25"]
display_name = "MiniMax M2.5"
model_family = "minimax"
mode = "chat"
max_input_tokens = 198000
max_output_tokens = 32000
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000016000000000000001
cache_read_input_token_cost = 4e-8
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-02-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."minimax-m25".pricing."venice"]
cache_read_input_token_cost = 4e-8
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000016000000000000001

[models."minimax-m2p1"]
display_name = "MiniMax-M2.1"
model_family = "minimax"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 204800
max_tokens = 204800
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
cache_read_input_token_cost = 3e-8
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai", "cortecs"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://fireworks.ai/models/fireworks/minimax-m2p1"
supports_response_schema = true
supports_tool_choice = true

[models."minimax-m2p1".pricing."cortecs"]
input_cost_per_token = 3.4000000000000003e-7
output_cost_per_token = 0.00000134
[models."minimax-m2p1".pricing."fireworks_ai"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."minimax.minimax-m2"]
display_name = "MiniMax M2"
model_family = "minimax"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-10-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."minimax.minimax-m2".pricing."amazon-bedrock"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."minimax.minimax-m2".pricing."bedrock_converse"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."minimax.minimax-m2.1"]
display_name = "MiniMax M2.1"
model_family = "minimax"
mode = "chat"
max_input_tokens = 196000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.00000144
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock", "bedrock_converse"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."minimax.minimax-m2.1".pricing."amazon-bedrock"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."minimax.minimax-m2.1".pricing."bedrock/eu-west-1"]
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.00000144
[models."minimax.minimax-m2.1".pricing."bedrock/us-east-1"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."minimax.minimax-m2.1".pricing."bedrock/us-west-2"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."minimax.minimax-m2.1".pricing."bedrock_converse"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."minimax/minimax-01"]
display_name = "MiniMax-01"
model_family = "minimax"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 1000000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000011
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-01-15"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."minimax/minimax-01".pricing."kilo"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000011
[models."minimax/minimax-01".pricing."openrouter"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000011

[models."minimax/minimax-m1"]
display_name = "MiniMax M1"
model_family = "minimax"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 40000
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000022
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-06-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."minimax/minimax-m1".pricing."kilo"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000022
[models."minimax/minimax-m1".pricing."openrouter"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000022

[models."minimax/minimax-m2"]
display_name = "MiniMax M2"
model_family = "minimax"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 204800
max_tokens = 204800
input_cost_per_token = 2.55e-7
output_cost_per_token = 0.00000102
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "novita", "novita-ai", "vercel", "zenmux"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-10-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."minimax/minimax-m2".pricing."kilo"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.55e-7
output_cost_per_token = 0.000001
[models."minimax/minimax-m2".pricing."novita"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
input_cost_per_token_cache_hit = 3e-8
output_cost_per_token = 0.0000012
[models."minimax/minimax-m2".pricing."novita-ai"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."minimax/minimax-m2".pricing."openrouter"]
input_cost_per_token = 2.55e-7
output_cost_per_token = 0.00000102
[models."minimax/minimax-m2".pricing."vercel"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.00000115
[models."minimax/minimax-m2".pricing."zenmux"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."minimax/minimax-m2-1-lightning"]
display_name = "MiniMax M2.1 Lightning"
model_family = "minimax"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000024
cache_read_input_token_cost = 3e-8
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-10-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."minimax/minimax-m2-1-lightning".pricing."vercel"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000024

[models."minimax/minimax-m2-5-lightning"]
display_name = "MiniMax: MiniMax M2.5 highspeed"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000048
cache_read_input_token_cost = 6e-8
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2026-02-13"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."minimax/minimax-m2-5-lightning".pricing."zenmux"]
cache_read_input_token_cost = 6e-8
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000048

[models."minimax/minimax-m2-5-official"]
display_name = "MiniMax M2.5"
model_family = "minimax"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
litellm_provider = "nano-gpt"
providers = ["nano-gpt"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2026-02-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."minimax/minimax-m2-5-official".pricing."nano-gpt"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."minimax/minimax-m2-5:free"]
display_name = "MiniMax: MiniMax M2.5 (free)"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-02-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."minimax/minimax-m2.1"]
display_name = "Minimax M2.1"
model_family = "minimax"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.0000012
cache_read_input_token_cost = 0
cache_creation_input_token_cost = 0
litellm_provider = "openrouter"
providers = ["openrouter", "jiekou", "kilo", "nano-gpt", "novita", "novita-ai", "vercel", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_computer_use = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."minimax/minimax-m2.1".pricing."jiekou"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."minimax/minimax-m2.1".pricing."kilo"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 2.7e-7
output_cost_per_token = 9.499999999999999e-7
[models."minimax/minimax-m2.1".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
[models."minimax/minimax-m2.1".pricing."novita"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
input_cost_per_token_cache_hit = 3e-8
output_cost_per_token = 0.0000012
[models."minimax/minimax-m2.1".pricing."novita-ai"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."minimax/minimax-m2.1".pricing."openrouter"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 0
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.0000012
[models."minimax/minimax-m2.1".pricing."vercel"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."minimax/minimax-m2.1".pricing."zenmux"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."minimax/minimax-m2.5"]
display_name = "MiniMax M2.5"
model_family = "minimax"
mode = "chat"
max_input_tokens = 196608
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000011
cache_read_input_token_cost = 1.5e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "nano-gpt", "novita-ai", "vercel", "zenmux"]
supports_function_calling = true
supports_vision = false
supports_reasoning = true
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2026-02-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://openrouter.ai/minimax/minimax-m2.5"
supports_computer_use = false
supports_tool_choice = true

[models."minimax/minimax-m2.5".pricing."kilo"]
cache_read_input_token_cost = 2.9e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."minimax/minimax-m2.5".pricing."nano-gpt"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."minimax/minimax-m2.5".pricing."novita-ai"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."minimax/minimax-m2.5".pricing."openrouter"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000011
[models."minimax/minimax-m2.5".pricing."vercel"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."minimax/minimax-m2.5".pricing."zenmux"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."minimaxai/minimax-m1-80k"]
display_name = "MiniMax M1"
model_family = "minimax"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 40000
max_tokens = 40000
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000022
litellm_provider = "novita"
providers = ["novita", "jiekou", "novita-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."minimaxai/minimax-m1-80k".pricing."jiekou"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000022
[models."minimaxai/minimax-m1-80k".pricing."novita"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000022
[models."minimaxai/minimax-m1-80k".pricing."novita-ai"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000022

[models."minimaxai/minimax-m2"]
display_name = "MiniMax-M2"
model_family = "minimax"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
litellm_provider = "nvidia"
providers = ["nvidia", "deepinfra"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2025-10-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."minimaxai/minimax-m2".pricing."deepinfra"]
input_cost_per_token = 2.54e-7
output_cost_per_token = 0.00000102

[models."minimaxai/minimax-m2-maas"]
mode = "chat"
max_input_tokens = 196608
max_output_tokens = 196608
max_tokens = 196608
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_tool_choice = true

[models."minimaxai/minimax-m2-maas".pricing."vertex_ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."ministral-14b"]
display_name = "Ministral 14B"
model_family = "ministral"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-12-01"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."ministral-14b".pricing."vercel"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7

[models."ministral-14b-2512"]
display_name = "ministral-14b-2512"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
input_cost_per_token = 3.3e-7
output_cost_per_token = 3.3e-7
litellm_provider = "302ai"
providers = ["302ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-12"
release_date = "2025-12-16"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."ministral-14b-2512".pricing."302ai"]
input_cost_per_token = 3.3e-7
output_cost_per_token = 3.3e-7

[models."ministral-3b"]
display_name = "Ministral 3B"
model_family = "ministral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 4e-8
output_cost_per_token = 4e-8
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2024-10-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azuremarketplace.microsoft.com/en/marketplace/apps/000-000.ministral-3b-2410-offer?tab=Overview"
supports_tool_choice = true

[models."ministral-3b".pricing."azure"]
input_cost_per_token = 4e-8
output_cost_per_token = 4e-8
[models."ministral-3b".pricing."azure-cognitive-services"]
input_cost_per_token = 4e-8
output_cost_per_token = 4e-8
[models."ministral-3b".pricing."azure_ai"]
input_cost_per_token = 4e-8
output_cost_per_token = 4e-8
[models."ministral-3b".pricing."vercel"]
input_cost_per_token = 4e-8
output_cost_per_token = 4e-8
[models."ministral-3b".pricing."vercel_ai_gateway"]
input_cost_per_token = 4e-8
output_cost_per_token = 4e-8

[models."ministral-3b-latest"]
display_name = "Ministral 3B"
model_family = "ministral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
input_cost_per_token = 4e-8
output_cost_per_token = 4e-8
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2024-10-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."ministral-3b-latest".pricing."mistral"]
input_cost_per_token = 4e-8
output_cost_per_token = 4e-8

[models."ministral-8b"]
display_name = "Ministral 8B"
model_family = "ministral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 4000
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2024-10-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."ministral-8b".pricing."vercel"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 1.0000000000000001e-7
[models."ministral-8b".pricing."vercel_ai_gateway"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."ministral-8b-latest"]
display_name = "Ministral 8B"
model_family = "ministral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 1.0000000000000001e-7
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2024-10-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."ministral-8b-latest".pricing."mistral"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 1.0000000000000001e-7

[models."miromind-ai/MiroThinker-v1-5-235B"]
display_name = "MiroThinker V1.5 235B"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 8192
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
cache_read_input_token_cost = 1.5e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2026-01-10"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."miromind-ai/MiroThinker-v1-5-235B".pricing."chutes"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."mistral"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = true

[models."mistral".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."mistral-31-24b"]
display_name = "Venice Medium"
model_family = "mistral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2025-03-18"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistral-31-24b".pricing."venice"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002

[models."mistral-7B-Instruct-v0.1"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = true

[models."mistral-7B-Instruct-v0.1".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."mistral-7B-Instruct-v0.2"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = true

[models."mistral-7B-Instruct-v0.2".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."mistral-7b"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake"]

[models."mistral-7b-instruct"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
litellm_provider = "perplexity"
providers = ["perplexity"]

[models."mistral-7b-instruct".pricing."perplexity"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7

[models."mistral-7b-v0.3"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1e-7
output_cost_per_token = 1.5e-7
litellm_provider = "llamagate"
providers = ["llamagate"]
supports_function_calling = true
supports_response_schema = true

[models."mistral-7b-v0.3".pricing."llamagate"]
input_cost_per_token = 1e-7
output_cost_per_token = 1.5e-7

[models."mistral-ai/codestral-2501"]
display_name = "Codestral 25.01"
model_family = "codestral"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8192
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-03"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistral-ai/ministral-3b"]
display_name = "Ministral 3B"
model_family = "ministral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-03"
release_date = "2024-10-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistral-ai/mistral-large-2411"]
display_name = "Mistral Large 24.11"
model_family = "mistral-large"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-09"
release_date = "2024-11-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistral-ai/mistral-medium-2505"]
display_name = "Mistral Medium 3 (25.05)"
model_family = "mistral-medium"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-09"
release_date = "2025-05-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistral-ai/mistral-nemo"]
display_name = "Mistral Nemo"
model_family = "mistral-nemo"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-03"
release_date = "2024-07-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistral-ai/mistral-small-2503"]
display_name = "Mistral Small 3.1"
model_family = "mistral-small"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
litellm_provider = "github-models"
providers = ["github-models"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-09"
release_date = "2025-03-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistral-document-ai-2505"]
mode = "ocr"
litellm_provider = "azure_ai"
providers = ["azure_ai"]
source = "https://devblogs.microsoft.com/foundry/whats-new-in-azure-ai-foundry-august-2025/#mistral-document-ai-(ocr)-%E2%80%94-serverless-in-foundry"
ocr_cost_per_page = 0.003
supported_endpoints = ["/v1/ocr"]

[models."mistral-document-ai-2505".pricing."azure_ai"]
ocr_cost_per_page = 0.003

[models."mistral-embed"]
display_name = "Mistral Embed"
model_family = "mistral-embed"
mode = "embedding"
max_input_tokens = 8192
max_output_tokens = 0
max_tokens = 8192
input_cost_per_token = 1e-7
litellm_provider = "mistral"
providers = ["mistral", "vercel", "vercel_ai_gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2023-12-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."mistral-embed".pricing."mistral"]
input_cost_per_token = 1e-7
[models."mistral-embed".pricing."vercel"]
input_cost_per_token = 1.0000000000000001e-7
[models."mistral-embed".pricing."vercel_ai_gateway"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."mistral-large"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000004
output_cost_per_token = 0.000012
litellm_provider = "azure_ai"
providers = ["azure_ai", "snowflake", "vercel_ai_gateway"]
supports_function_calling = true
supports_tool_choice = true

[models."mistral-large".pricing."azure_ai"]
input_cost_per_token = 0.000004
output_cost_per_token = 0.000012
[models."mistral-large".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006

[models."mistral-large-2402"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 32000
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
litellm_provider = "azure"
providers = ["azure", "mistral"]
supports_function_calling = true
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral-large-2402".pricing."azure"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
[models."mistral-large-2402".pricing."mistral"]
input_cost_per_token = 0.000004
output_cost_per_token = 0.000012

[models."mistral-large-2407"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
litellm_provider = "azure_ai"
providers = ["azure_ai", "mistral"]
supports_function_calling = true
source = "https://azuremarketplace.microsoft.com/en/marketplace/apps/000-000.mistral-ai-large-2407-offer?tab=Overview"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral-large-2407".pricing."azure_ai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
[models."mistral-large-2407".pricing."mistral"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000009

[models."mistral-large-2411"]
display_name = "Mistral Large 2.1"
model_family = "mistral-large"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
litellm_provider = "mistral"
providers = ["mistral", "azure", "azure-cognitive-services", "helicone", "vertex_ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-11"
release_date = "2024-11-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral-large-2411".pricing."azure"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
[models."mistral-large-2411".pricing."azure-cognitive-services"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
[models."mistral-large-2411".pricing."helicone"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
[models."mistral-large-2411".pricing."mistral"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
[models."mistral-large-2411".pricing."vertex_ai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006

[models."mistral-large-2512"]
display_name = "mistral-large-2512"
model_family = "mistral-large"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 262144
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000032999999999999997
litellm_provider = "302ai"
providers = ["302ai", "mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-12"
release_date = "2025-12-16"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistral-large-2512".pricing."302ai"]
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000032999999999999997
[models."mistral-large-2512".pricing."mistral"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."mistral-large-3"]
display_name = "Mistral Large 3"
model_family = "mistral-large"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "azure_ai"
providers = ["azure_ai", "mistral", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-12-02"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://azure.microsoft.com/en-us/blog/introducing-mistral-large-3-in-microsoft-foundry-open-capable-and-ready-for-production-workloads/"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral-large-3".pricing."azure_ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
[models."mistral-large-3".pricing."mistral"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
[models."mistral-large-3".pricing."vercel"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."mistral-large-instruct-2407"]
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = true

[models."mistral-large-instruct-2407".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."mistral-large-latest"]
display_name = "Mistral Large"
model_family = "mistral-large"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
litellm_provider = "azure"
providers = ["azure", "azure_ai", "mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-11"
release_date = "2024-11-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral-large-latest".pricing."azure"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
[models."mistral-large-latest".pricing."azure_ai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
[models."mistral-large-latest".pricing."mistral"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006

[models."mistral-large2"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake"]

[models."mistral-large@2407"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_tool_choice = true

[models."mistral-large@2407".pricing."vertex_ai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006

[models."mistral-large@2411-001"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_tool_choice = true

[models."mistral-large@2411-001".pricing."vertex_ai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006

[models."mistral-large@latest"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_tool_choice = true

[models."mistral-large@latest".pricing."vertex_ai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006

[models."mistral-medium"]
display_name = "Mistral Medium 3.1"
model_family = "mistral-medium"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.0000027
output_cost_per_token = 0.0000081
litellm_provider = "mistral"
providers = ["mistral", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-05-07"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral-medium".pricing."mistral"]
input_cost_per_token = 0.0000027
output_cost_per_token = 0.0000081
[models."mistral-medium".pricing."vercel"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002

[models."mistral-medium-2312"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.0000027
output_cost_per_token = 0.0000081
litellm_provider = "mistral"
providers = ["mistral"]
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral-medium-2312".pricing."mistral"]
input_cost_per_token = 0.0000027
output_cost_per_token = 0.0000081

[models."mistral-medium-2505"]
display_name = "Mistral Medium 3"
model_family = "mistral-medium"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services", "mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-05"
release_date = "2025-05-07"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral-medium-2505".pricing."azure"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002
[models."mistral-medium-2505".pricing."azure-cognitive-services"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002
[models."mistral-medium-2505".pricing."azure_ai"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002
[models."mistral-medium-2505".pricing."mistral"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002

[models."mistral-medium-2508"]
display_name = "Mistral Medium 3.1"
model_family = "mistral-medium"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-05"
release_date = "2025-08-12"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistral-medium-2508".pricing."mistral"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002

[models."mistral-medium-3"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_tool_choice = true

[models."mistral-medium-3".pricing."vertex_ai"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002

[models."mistral-medium-3@001"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_tool_choice = true

[models."mistral-medium-3@001".pricing."vertex_ai"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002

[models."mistral-medium-latest"]
display_name = "Mistral Medium"
model_family = "mistral-medium"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-05"
release_date = "2025-05-07"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral-medium-latest".pricing."mistral"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002

[models."mistral-nemo"]
display_name = "Mistral Nemo"
model_family = "mistral-nemo"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services", "helicone", "mistral", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2024-07-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://azuremarketplace.microsoft.com/en/marketplace/apps/000-000.mistral-nemo-12b-2407?tab=PlansAndPrice"

[models."mistral-nemo".pricing."azure"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."mistral-nemo".pricing."azure-cognitive-services"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."mistral-nemo".pricing."azure_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."mistral-nemo".pricing."helicone"]
input_cost_per_token = 0.00002
output_cost_per_token = 0.00004
[models."mistral-nemo".pricing."mistral"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."mistral-nemo".pricing."vercel"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.7000000000000001e-7

[models."mistral-nemo-12b-instruct"]
display_name = "Mistral Nemo 12B Instruct"
model_family = "mistral-nemo"
mode = "chat"
max_input_tokens = 16000
max_output_tokens = 4096
input_cost_per_token = 3.7999999999999996e-8
output_cost_per_token = 1.0000000000000001e-7
litellm_provider = "inference"
providers = ["inference"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistral-nemo-12b-instruct".pricing."inference"]
input_cost_per_token = 3.7999999999999996e-8
output_cost_per_token = 1.0000000000000001e-7

[models."mistral-nemo-instruct-2407"]
mode = "chat"
max_tokens = 512
input_cost_per_token = 3e-7
output_cost_per_token = 3e-7
litellm_provider = "gradient_ai"
providers = ["gradient_ai"]
supported_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions"]
supports_tool_choice = false

[models."mistral-nemo-instruct-2407".pricing."gradient_ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 3e-7

[models."mistral-nemo@2407"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000003
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_tool_choice = true

[models."mistral-nemo@2407".pricing."vertex_ai"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000003

[models."mistral-nemo@latest"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_tool_choice = true

[models."mistral-nemo@latest".pricing."vertex_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7

[models."mistral-ocr-2505"]
mode = "ocr"
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
source = "https://cloud.google.com/generative-ai-app-builder/pricing"
ocr_cost_per_page = 0.0005
supported_endpoints = ["/v1/ocr"]

[models."mistral-ocr-2505".pricing."vertex_ai"]
ocr_cost_per_page = 0.0005

[models."mistral-ocr-2505-completion"]
mode = "ocr"
litellm_provider = "mistral"
providers = ["mistral"]
source = "https://mistral.ai/pricing#api-pricing"
annotation_cost_per_page = 0.003
ocr_cost_per_page = 0.001
supported_endpoints = ["/v1/ocr"]

[models."mistral-ocr-2505-completion".pricing."mistral"]
annotation_cost_per_page = 0.003
ocr_cost_per_page = 0.001

[models."mistral-ocr-latest"]
mode = "ocr"
litellm_provider = "mistral"
providers = ["mistral"]
source = "https://mistral.ai/pricing#api-pricing"
annotation_cost_per_page = 0.003
ocr_cost_per_page = 0.001
supported_endpoints = ["/v1/ocr"]

[models."mistral-ocr-latest".pricing."mistral"]
annotation_cost_per_page = 0.003
ocr_cost_per_page = 0.001

[models."mistral-saba-24b"]
display_name = "Mistral Saba 24B"
model_family = "mistral"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 7.9e-7
output_cost_per_token = 7.9e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "groq"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-08"
release_date = "2025-02-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."mistral-saba-24b".pricing."groq"]
input_cost_per_token = 7.900000000000001e-7
output_cost_per_token = 7.900000000000001e-7
[models."mistral-saba-24b".pricing."vercel_ai_gateway"]
input_cost_per_token = 7.9e-7
output_cost_per_token = 7.9e-7

[models."mistral-small"]
display_name = "Mistral Small"
model_family = "mistral-small"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
litellm_provider = "azure_ai"
providers = ["azure_ai", "helicone", "mistral", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-03"
release_date = "2024-09-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral-small".pricing."azure_ai"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
[models."mistral-small".pricing."helicone"]
input_cost_per_token = 0.000075
output_cost_per_token = 0.0002
[models."mistral-small".pricing."mistral"]
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7
[models."mistral-small".pricing."vercel"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
[models."mistral-small".pricing."vercel_ai_gateway"]
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7

[models."mistral-small-2503"]
display_name = "Mistral Small 3.1"
model_family = "mistral-small"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
litellm_provider = "azure_ai"
providers = ["azure_ai", "azure", "azure-cognitive-services", "vertex_ai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-09"
release_date = "2025-03-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."mistral-small-2503".pricing."azure"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
[models."mistral-small-2503".pricing."azure-cognitive-services"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
[models."mistral-small-2503".pricing."azure_ai"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
[models."mistral-small-2503".pricing."vertex_ai"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003

[models."mistral-small-2503@001"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_tool_choice = true

[models."mistral-small-2503@001".pricing."vertex_ai"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003

[models."mistral-small-2506"]
display_name = "Mistral Small 3.2"
model_family = "mistral-small"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-03"
release_date = "2025-06-20"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistral-small-2506".pricing."mistral"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7

[models."mistral-small-latest"]
display_name = "Mistral Small"
model_family = "mistral-small"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-03"
release_date = "2024-09-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral-small-latest".pricing."mistral"]
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7

[models."mistral-tiny"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 2.5e-7
output_cost_per_token = 2.5e-7
litellm_provider = "mistral"
providers = ["mistral"]
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."mistral-tiny".pricing."mistral"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 2.5e-7

[models."mistral.magistral-small-2509"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_reasoning = true
supports_system_messages = true

[models."mistral.magistral-small-2509".pricing."bedrock_converse"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."mistral.ministral-3-14b-instruct"]
display_name = "Ministral 14B 3.0"
model_family = "ministral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."mistral.ministral-3-14b-instruct".pricing."amazon-bedrock"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7
[models."mistral.ministral-3-14b-instruct".pricing."bedrock_converse"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."mistral.ministral-3-3b-instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_system_messages = true

[models."mistral.ministral-3-3b-instruct".pricing."bedrock_converse"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."mistral.ministral-3-8b-instruct"]
display_name = "Ministral 3 8B"
model_family = "ministral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."mistral.ministral-3-8b-instruct".pricing."amazon-bedrock"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."mistral.ministral-3-8b-instruct".pricing."bedrock_converse"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7

[models."mistral.mistral-7b-instruct-v0:2"]
display_name = "Mistral-7B-Instruct-v0.3"
model_family = "mistral"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 1.5e-7
output_cost_per_token = 2e-7
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-04-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."mistral.mistral-7b-instruct-v0:2".pricing."amazon-bedrock"]
input_cost_per_token = 1.1e-7
output_cost_per_token = 1.1e-7
[models."mistral.mistral-7b-instruct-v0:2".pricing."bedrock"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 2e-7
[models."mistral.mistral-7b-instruct-v0:2".pricing."bedrock/us-east-1"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 2e-7
[models."mistral.mistral-7b-instruct-v0:2".pricing."bedrock/us-west-2"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 2e-7

[models."mistral.mistral-large-2402-v1:0"]
display_name = "Mistral Large (24.02)"
model_family = "mistral-large"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."mistral.mistral-large-2402-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
[models."mistral.mistral-large-2402-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
[models."mistral.mistral-large-2402-v1:0".pricing."bedrock/us-east-1"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
[models."mistral.mistral-large-2402-v1:0".pricing."bedrock/us-west-2"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024

[models."mistral.mistral-large-2407-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.000003
output_cost_per_token = 0.000009
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_tool_choice = true

[models."mistral.mistral-large-2407-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000009

[models."mistral.mistral-large-3-675b-instruct"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_system_messages = true

[models."mistral.mistral-large-3-675b-instruct".pricing."bedrock_converse"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."mistral.mistral-small-2402-v1:0"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true

[models."mistral.mistral-small-2402-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003

[models."mistral.mixtral-8x7b-instruct-v0:1"]
display_name = "Mixtral-8x7B-Instruct-v0.1"
model_family = "mixtral"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 4.5e-7
output_cost_per_token = 7e-7
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-04-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."mistral.mixtral-8x7b-instruct-v0:1".pricing."amazon-bedrock"]
input_cost_per_token = 7e-7
output_cost_per_token = 7e-7
[models."mistral.mixtral-8x7b-instruct-v0:1".pricing."bedrock"]
input_cost_per_token = 4.5e-7
output_cost_per_token = 7e-7
[models."mistral.mixtral-8x7b-instruct-v0:1".pricing."bedrock/us-east-1"]
input_cost_per_token = 4.5e-7
output_cost_per_token = 7e-7
[models."mistral.mixtral-8x7b-instruct-v0:1".pricing."bedrock/us-west-2"]
input_cost_per_token = 4.5e-7
output_cost_per_token = 7e-7

[models."mistral.voxtral-mini-3b-2507"]
display_name = "Voxtral Mini 3B 2507"
model_family = "mistral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 4e-8
output_cost_per_token = 4e-8
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-12-01"
supported_modalities = ["audio", "text"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_system_messages = true

[models."mistral.voxtral-mini-3b-2507".pricing."amazon-bedrock"]
input_cost_per_token = 4e-8
output_cost_per_token = 4e-8
[models."mistral.voxtral-mini-3b-2507".pricing."bedrock_converse"]
input_cost_per_token = 4e-8
output_cost_per_token = 4e-8

[models."mistral.voxtral-small-24b-2507"]
display_name = "Voxtral Small 24B 2507"
model_family = "mistral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-07-01"
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_system_messages = true

[models."mistral.voxtral-small-24b-2507".pricing."amazon-bedrock"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 3.5e-7
[models."mistral.voxtral-small-24b-2507".pricing."bedrock_converse"]
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7

[models."mistralai/Devstral-2-123B-Instruct-2512-TEE"]
display_name = "Devstral 2 123B Instruct 2512 TEE"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65536
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.2e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2026-01-10"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/Devstral-2-123B-Instruct-2512-TEE".pricing."chutes"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.2e-7

[models."mistralai/Magistral-Small-2506"]
display_name = "Magistral Small 2506"
model_family = "magistral-small"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
cache_read_input_token_cost = 2.5e-7
litellm_provider = "io-net"
providers = ["io-net"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-06-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/Magistral-Small-2506".pricing."io-net"]
cache_read_input_token_cost = 2.5e-7
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."mistralai/Magistral-Small-2509"]
display_name = "Magistral Small 1.2 24B"
model_family = "magistral-small"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 5.9e-7
output_cost_per_token = 0.00000236
litellm_provider = "evroc"
providers = ["evroc"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-06-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/Magistral-Small-2509".pricing."evroc"]
input_cost_per_token = 5.9e-7
output_cost_per_token = 0.00000236

[models."mistralai/Mistral-7B-Instruct-v0.1"]
display_name = "Mistral: Mistral 7B Instruct v0.1"
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "anyscale"
providers = ["anyscale", "kilo", "together_ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://docs.anyscale.com/preview/endpoints/text-generation/supported-models/mistralai-Mistral-7B-Instruct-v0.1"
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistralai/Mistral-7B-Instruct-v0.1".pricing."anyscale"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."mistralai/Mistral-7B-Instruct-v0.1".pricing."kilo"]
input_cost_per_token = 1.1e-7
output_cost_per_token = 1.9e-7

[models."mistralai/Mistral-Large-Instruct-2411"]
display_name = "Mistral Large Instruct 2411"
model_family = "mistral-large"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
cache_read_input_token_cost = 0.000001
litellm_provider = "io-net"
providers = ["io-net"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-11-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/Mistral-Large-Instruct-2411".pricing."io-net"]
cache_read_input_token_cost = 0.000001
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006

[models."mistralai/Mistral-Nemo-Instruct-2407"]
display_name = "Mistral Nemo Instruct 2407"
model_family = "mistral"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2e-8
output_cost_per_token = 4e-8
litellm_provider = "deepinfra"
providers = ["deepinfra", "io-net", "meganova"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-05"
release_date = "2024-07-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."mistralai/Mistral-Nemo-Instruct-2407".pricing."deepinfra"]
input_cost_per_token = 2e-8
output_cost_per_token = 4e-8
[models."mistralai/Mistral-Nemo-Instruct-2407".pricing."io-net"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 2e-8
output_cost_per_token = 4e-8
[models."mistralai/Mistral-Nemo-Instruct-2407".pricing."meganova"]
input_cost_per_token = 2e-8
output_cost_per_token = 4e-8

[models."mistralai/Mistral-Small-24B-Instruct-2501"]
display_name = "Mistral: Mistral Small 3"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 5e-8
output_cost_per_token = 8e-8
litellm_provider = "deepinfra"
providers = ["deepinfra", "kilo", "together_ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_tool_choice = true

[models."mistralai/Mistral-Small-24B-Instruct-2501".pricing."deepinfra"]
input_cost_per_token = 5e-8
output_cost_per_token = 8e-8
[models."mistralai/Mistral-Small-24B-Instruct-2501".pricing."kilo"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 8e-8

[models."mistralai/Mistral-Small-3.2-24B-Instruct-2506"]
display_name = "Mistral Small 3.2 24B Instruct"
model_family = "mistral-small"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 7.5e-8
output_cost_per_token = 2e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "berget", "meganova"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-06-20"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."mistralai/Mistral-Small-3.2-24B-Instruct-2506".pricing."berget"]
input_cost_per_token = 3e-7
output_cost_per_token = 3e-7
[models."mistralai/Mistral-Small-3.2-24B-Instruct-2506".pricing."deepinfra"]
input_cost_per_token = 7.5e-8
output_cost_per_token = 2e-7

[models."mistralai/Mixtral-8x22B-Instruct-v0.1"]
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "anyscale"
providers = ["anyscale"]
supports_function_calling = true
source = "https://docs.anyscale.com/preview/endpoints/text-generation/supported-models/mistralai-Mixtral-8x22B-Instruct-v0.1"

[models."mistralai/Mixtral-8x22B-Instruct-v0.1".pricing."anyscale"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."mistralai/Mixtral-8x7B-Instruct-v0.1"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "anyscale"
providers = ["anyscale", "deepinfra", "together_ai"]
supports_function_calling = true
source = "https://docs.anyscale.com/preview/endpoints/text-generation/supported-models/mistralai-Mixtral-8x7B-Instruct-v0.1"
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."mistralai/Mixtral-8x7B-Instruct-v0.1".pricing."anyscale"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."mistralai/Mixtral-8x7B-Instruct-v0.1".pricing."deepinfra"]
input_cost_per_token = 4e-7
output_cost_per_token = 4e-7
[models."mistralai/Mixtral-8x7B-Instruct-v0.1".pricing."together_ai"]
input_cost_per_token = 6e-7
output_cost_per_token = 6e-7

[models."mistralai/Voxtral-Small-24B-2507"]
display_name = "Voxtral Small 24B"
model_family = "voxtral"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 32000
input_cost_per_token = 2.36e-9
output_cost_per_token = 2.36e-9
litellm_provider = "evroc"
providers = ["evroc", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-03-01"
supported_modalities = ["audio", "text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/Voxtral-Small-24B-2507".pricing."evroc"]
input_cost_per_token = 2.36e-9
output_cost_per_token = 2.36e-9
[models."mistralai/Voxtral-Small-24B-2507".pricing."kilo"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7

[models."mistralai/codestral-2"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 3e-7
output_cost_per_token = 9e-7
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_tool_choice = true

[models."mistralai/codestral-2".pricing."vertex_ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 9e-7

[models."mistralai/codestral-22b-instruct-v0-1"]
display_name = "Codestral 22b Instruct V0.1"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-05-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/codestral-2508"]
display_name = "Codestral 2508"
model_family = "codestral"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
input_cost_per_token = 3e-7
output_cost_per_token = 9.000000000000001e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-05"
release_date = "2025-08-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/codestral-2508".pricing."kilo"]
input_cost_per_token = 3e-7
output_cost_per_token = 9.000000000000001e-7
[models."mistralai/codestral-2508".pricing."openrouter"]
input_cost_per_token = 3e-7
output_cost_per_token = 9.000000000000001e-7

[models."mistralai/codestral-2@001"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 3e-7
output_cost_per_token = 9e-7
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_tool_choice = true

[models."mistralai/codestral-2@001".pricing."vertex_ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 9e-7

[models."mistralai/devstral-2-123b-instruct-2512"]
display_name = "Devstral-2-123B-Instruct-2512"
model_family = "devstral"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
litellm_provider = "nvidia"
providers = ["nvidia", "nano-gpt"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-12"
release_date = "2025-12-08"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/devstral-2-123b-instruct-2512".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002

[models."mistralai/devstral-2512"]
display_name = "Devstral 2 2512"
model_family = "devstral"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_vision = false
supports_reasoning = false
supports_prompt_caching = false
open_weights = true
knowledge_cutoff = "2025-12"
release_date = "2025-09-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
input_cost_per_image = 0
supports_tool_choice = true

[models."mistralai/devstral-2512".pricing."kilo"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.2e-7
[models."mistralai/devstral-2512".pricing."openrouter"]
input_cost_per_image = 0
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."mistralai/devstral-2512:free"]
display_name = "Devstral 2 2512 (free)"
model_family = "devstral"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-12"
release_date = "2025-09-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/devstral-medium"]
display_name = "Mistral: Devstral Medium"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 26215
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-07-10"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/devstral-medium".pricing."kilo"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002

[models."mistralai/devstral-medium-2507"]
display_name = "Devstral Medium"
model_family = "devstral"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-05"
release_date = "2025-07-10"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/devstral-medium-2507".pricing."openrouter"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002

[models."mistralai/devstral-small"]
display_name = "Mistral: Devstral Small 1.1"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 26215
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-05-07"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/devstral-small".pricing."kilo"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7

[models."mistralai/devstral-small-2-24b-instruct-2512"]
display_name = "Devstral Small 2 24B Instruct 2512"
model_family = "devstral"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
input_cost_per_token = 1.2e-7
output_cost_per_token = 4.6999999999999995e-7
litellm_provider = "evroc"
providers = ["evroc"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/devstral-small-2-24b-instruct-2512".pricing."evroc"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 4.6999999999999995e-7

[models."mistralai/devstral-small-2505"]
display_name = "Devstral Small"
model_family = "devstral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
input_cost_per_token = 6e-8
output_cost_per_token = 1.2e-7
litellm_provider = "openrouter"
providers = ["openrouter", "io-net"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-05"
release_date = "2025-05-07"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/devstral-small-2505".pricing."io-net"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.2e-7
[models."mistralai/devstral-small-2505".pricing."openrouter"]
input_cost_per_token = 6e-8
output_cost_per_token = 1.2e-7

[models."mistralai/devstral-small-2505:free"]
display_name = "Devstral Small 2505 (free)"
model_family = "devstral"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-05"
release_date = "2025-05-21"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/devstral-small-2507"]
display_name = "Devstral Small 1.1"
model_family = "devstral"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-05"
release_date = "2025-07-10"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/devstral-small-2507".pricing."openrouter"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7

[models."mistralai/mamba-codestral-7b-v0-1"]
display_name = "Mamba Codestral 7b V0.1"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2024-07-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/ministral-14b-2512"]
display_name = "Mistral: Ministral 3 14B 2512"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = false
open_weights = false
release_date = "2025-12-16"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_image = 0
supports_tool_choice = true

[models."mistralai/ministral-14b-2512".pricing."kilo"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7
[models."mistralai/ministral-14b-2512".pricing."openrouter"]
input_cost_per_image = 0
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."mistralai/ministral-14b-instruct-2512"]
display_name = "Ministral 3 14B Instruct 2512"
model_family = "ministral"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
litellm_provider = "nvidia"
providers = ["nvidia", "nano-gpt"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-12"
release_date = "2025-12-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/ministral-14b-instruct-2512".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002

[models."mistralai/ministral-3b-2512"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = false
input_cost_per_image = 0
supports_tool_choice = true

[models."mistralai/ministral-3b-2512".pricing."openrouter"]
input_cost_per_image = 0
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."mistralai/ministral-8b-2512"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = false
input_cost_per_image = 0
supports_tool_choice = true

[models."mistralai/ministral-8b-2512".pricing."openrouter"]
input_cost_per_image = 0
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7

[models."mistralai/mistral-7b-instruct"]
display_name = "Mistral: Mistral 7B Instruct"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 8192
input_cost_per_token = 1.3e-7
output_cost_per_token = 1.3e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2024-05-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."mistralai/mistral-7b-instruct".pricing."kilo"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7
[models."mistralai/mistral-7b-instruct".pricing."openrouter"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 1.3e-7

[models."mistralai/mistral-7b-instruct-v0-3"]
display_name = "Mistral: Mistral 7B Instruct v0.3"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 4096
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-04-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/mistral-7b-instruct-v0-3".pricing."kilo"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7

[models."mistralai/mistral-7b-instruct-v0.2"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 5e-8
output_cost_per_token = 2.5e-7
litellm_provider = "replicate"
providers = ["replicate"]
supports_tool_choice = true

[models."mistralai/mistral-7b-instruct-v0.2".pricing."replicate"]
input_cost_per_token = 5e-8
output_cost_per_token = 2.5e-7

[models."mistralai/mistral-7b-instruct:free"]
display_name = "Mistral 7B Instruct (free)"
model_family = "mistral"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-05"
release_date = "2024-05-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/mistral-7b-v0.1"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 5e-8
output_cost_per_token = 2.5e-7
litellm_provider = "replicate"
providers = ["replicate"]
supports_tool_choice = true

[models."mistralai/mistral-7b-v0.1".pricing."replicate"]
input_cost_per_token = 5e-8
output_cost_per_token = 2.5e-7

[models."mistralai/mistral-large"]
display_name = "Mistral Large"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 16384
max_tokens = 32000
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "watsonx"]
supports_function_calling = true
supports_reasoning = false
supports_prompt_caching = true
open_weights = true
release_date = "2024-07-24"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."mistralai/mistral-large".pricing."kilo"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
[models."mistralai/mistral-large".pricing."openrouter"]
input_cost_per_token = 0.000008
output_cost_per_token = 0.000024
[models."mistralai/mistral-large".pricing."watsonx"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.00001

[models."mistralai/mistral-large-2-instruct"]
display_name = "Mistral Large 2 Instruct"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-07-24"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/mistral-large-2411"]
display_name = "Mistral Large 2411"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 26215
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-07-24"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/mistral-large-2411".pricing."kilo"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006

[models."mistralai/mistral-large-2512"]
display_name = "Mistral: Mistral Large 3 2512"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = false
open_weights = true
release_date = "2024-11-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_image = 0
supports_tool_choice = true

[models."mistralai/mistral-large-2512".pricing."kilo"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
[models."mistralai/mistral-large-2512".pricing."openrouter"]
input_cost_per_image = 0
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."mistralai/mistral-large-3-675b-instruct-2512"]
display_name = "Mistral Large 3 675B Instruct 2512"
model_family = "mistral-large"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
litellm_provider = "nvidia"
providers = ["nvidia", "nano-gpt"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2025-12-02"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/mistral-large-3-675b-instruct-2512".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002

[models."mistralai/mistral-medium-2505"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000003
output_cost_per_token = 0.00001
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = true
supports_vision = false
supports_parallel_function_calling = true

[models."mistralai/mistral-medium-2505".pricing."watsonx"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.00001

[models."mistralai/mistral-medium-3"]
display_name = "Mistral Medium 3"
model_family = "mistral-medium"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "kilo", "openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-05"
release_date = "2025-05-07"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."mistralai/mistral-medium-3".pricing."kilo"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002
[models."mistralai/mistral-medium-3".pricing."openrouter"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002
[models."mistralai/mistral-medium-3".pricing."vertex_ai"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002

[models."mistralai/mistral-medium-3-1"]
display_name = "Mistral Medium 3.1"
model_family = "mistral-medium"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-05"
release_date = "2025-08-12"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/mistral-medium-3-1".pricing."kilo"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002
[models."mistralai/mistral-medium-3-1".pricing."openrouter"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002

[models."mistralai/mistral-medium-3@001"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
supports_tool_choice = true

[models."mistralai/mistral-medium-3@001".pricing."vertex_ai"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002

[models."mistralai/mistral-nemo"]
display_name = "Mistral Nemo"
model_family = "mistral-nemo"
mode = "chat"
max_input_tokens = 60288
max_output_tokens = 16000
max_tokens = 16000
input_cost_per_token = 4e-8
output_cost_per_token = 1.7e-7
litellm_provider = "novita"
providers = ["novita", "kilo", "novita-ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-07-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."mistralai/mistral-nemo".pricing."kilo"]
input_cost_per_token = 2e-8
output_cost_per_token = 4e-8
[models."mistralai/mistral-nemo".pricing."novita"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.7e-7
[models."mistralai/mistral-nemo".pricing."novita-ai"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.7000000000000001e-7

[models."mistralai/mistral-nemo:free"]
display_name = "Mistral Nemo (free)"
model_family = "mistral-nemo"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2024-07-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/mistral-small-2503"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = true
supports_vision = false
supports_parallel_function_calling = true

[models."mistralai/mistral-small-2503".pricing."watsonx"]
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7

[models."mistralai/mistral-small-3-1-24b-instruct-2503"]
display_name = "Mistral Small 3.1 24b Instruct 2503"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7
litellm_provider = "watsonx"
providers = ["watsonx", "nvidia"]
supports_function_calling = true
supports_vision = false
supports_reasoning = false
open_weights = true
release_date = "2025-03-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true

[models."mistralai/mistral-small-3-1-24b-instruct-2503".pricing."watsonx"]
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7

[models."mistralai/mistral-small-3-1-24b-instruct:free"]
display_name = "Mistral: Mistral Small 3.1 24B (free)"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 25600
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-03-17"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/mistral-small-3-2-24b-instruct:free"]
display_name = "Mistral Small 3.2 24B (free)"
model_family = "mistral-small"
mode = "chat"
max_input_tokens = 96000
max_output_tokens = 96000
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2025-06-20"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."mistralai/mistral-small-3.1-24b-instruct"]
display_name = "Mistral Small 3.1 24B Instruct"
model_family = "mistral-small"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 32000
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-03-17"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."mistralai/mistral-small-3.1-24b-instruct".pricing."kilo"]
cache_read_input_token_cost = 1.5e-8
input_cost_per_token = 3e-8
output_cost_per_token = 1.1e-7
[models."mistralai/mistral-small-3.1-24b-instruct".pricing."openrouter"]
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7

[models."mistralai/mistral-small-3.2-24b-instruct"]
display_name = "Mistral Small 3.2 24B Instruct"
model_family = "mistral-small"
mode = "chat"
max_input_tokens = 96000
max_output_tokens = 8192
max_tokens = 32000
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-06-20"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."mistralai/mistral-small-3.2-24b-instruct".pricing."kilo"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 6e-8
output_cost_per_token = 1.8e-7
[models."mistralai/mistral-small-3.2-24b-instruct".pricing."openrouter"]
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7

[models."mistralai/mixtral-8x22b-instruct"]
display_name = "Mistral: Mixtral 8x22B Instruct"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 13108
max_tokens = 65536
input_cost_per_token = 6.5e-7
output_cost_per_token = 6.5e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-04-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."mistralai/mixtral-8x22b-instruct".pricing."kilo"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
[models."mistralai/mixtral-8x22b-instruct".pricing."openrouter"]
input_cost_per_token = 6.5e-7
output_cost_per_token = 6.5e-7

[models."mistralai/mixtral-8x22b-instruct-v0.1"]
mode = "chat"
input_cost_per_token = 6e-7
output_cost_per_token = 6e-7
litellm_provider = "nscale"
providers = ["nscale"]
source = "https://docs.nscale.com/docs/inference/serverless-models/current#chat-models"

[models."mistralai/mixtral-8x22b-instruct-v0.1".metadata]
notes = "Pricing listed as $1.20/1M tokens total. Assumed 50/50 split for input/output."

[models."mistralai/mixtral-8x22b-instruct-v0.1".pricing."nscale"]
input_cost_per_token = 6e-7
output_cost_per_token = 6e-7

[models."mistralai/mixtral-8x7b-instruct-v0.1"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 3e-7
output_cost_per_token = 0.000001
litellm_provider = "replicate"
providers = ["replicate"]
supports_tool_choice = true

[models."mistralai/mixtral-8x7b-instruct-v0.1".pricing."replicate"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.000001

[models."mistralai/pixtral-12b-2409"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 3.5e-7
output_cost_per_token = 3.5e-7
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = false
supports_vision = true
supports_parallel_function_calling = false

[models."mistralai/pixtral-12b-2409".pricing."watsonx"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 3.5e-7

[models."mixtral-8x22B-Instruct-v0.1"]
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = true

[models."mixtral-8x22B-Instruct-v0.1".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."mixtral-8x22b-instruct"]
display_name = "Mixtral 8x22B"
model_family = "mixtral"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-04-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."mixtral-8x22b-instruct".pricing."vercel"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
[models."mixtral-8x22b-instruct".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."mixtral-8x7B-Instruct-v0.1"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = true

[models."mixtral-8x7B-Instruct-v0.1".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."mixtral-8x7b"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake"]

[models."mixtral-8x7b-instruct"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
litellm_provider = "perplexity"
providers = ["perplexity"]

[models."mixtral-8x7b-instruct".pricing."perplexity"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7

[models."model-router"]
display_name = "Model Router"
model_family = "model-router"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.4e-7
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-05-19"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."model-router".pricing."azure"]
input_cost_per_token = 1.4e-7
[models."model-router".pricing."azure-cognitive-services"]
input_cost_per_token = 1.4e-7

[models."model_router"]
mode = "chat"
input_cost_per_token = 1.4e-7
output_cost_per_token = 0
litellm_provider = "azure_ai"
providers = ["azure_ai"]
source = "https://azure.microsoft.com/en-us/pricing/details/ai-services/"
comment = "Flat cost of $0.14 per M input tokens for Azure AI Foundry Model Router infrastructure. Use pattern: azure_ai/model_router/<deployment-name> where deployment-name is your Azure deployment (e.g., azure-model-router)"

[models."model_router".pricing."azure_ai"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 0

[models."moonshot-kimi-k2-instruct"]
display_name = "Moonshot Kimi K2 Instruct"
model_family = "kimi"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 5.739999999999999e-7
output_cost_per_token = 0.000002294
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."moonshot-kimi-k2-instruct".pricing."alibaba-cn"]
input_cost_per_token = 5.739999999999999e-7
output_cost_per_token = 0.000002294

[models."moonshot-v1-128k"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005
litellm_provider = "moonshot"
providers = ["moonshot"]
supports_function_calling = true
source = "https://platform.moonshot.ai/docs/pricing"
supports_tool_choice = true

[models."moonshot-v1-128k".pricing."moonshot"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005

[models."moonshot-v1-128k-0430"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005
litellm_provider = "moonshot"
providers = ["moonshot"]
supports_function_calling = true
source = "https://platform.moonshot.ai/docs/pricing"
supports_tool_choice = true

[models."moonshot-v1-128k-0430".pricing."moonshot"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005

[models."moonshot-v1-128k-vision-preview"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005
litellm_provider = "moonshot"
providers = ["moonshot"]
supports_function_calling = true
supports_vision = true
source = "https://platform.moonshot.ai/docs/pricing"
supports_tool_choice = true

[models."moonshot-v1-128k-vision-preview".pricing."moonshot"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005

[models."moonshot-v1-32k"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
litellm_provider = "moonshot"
providers = ["moonshot"]
supports_function_calling = true
source = "https://platform.moonshot.ai/docs/pricing"
supports_tool_choice = true

[models."moonshot-v1-32k".pricing."moonshot"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003

[models."moonshot-v1-32k-0430"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
litellm_provider = "moonshot"
providers = ["moonshot"]
supports_function_calling = true
source = "https://platform.moonshot.ai/docs/pricing"
supports_tool_choice = true

[models."moonshot-v1-32k-0430".pricing."moonshot"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003

[models."moonshot-v1-32k-vision-preview"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
litellm_provider = "moonshot"
providers = ["moonshot"]
supports_function_calling = true
supports_vision = true
source = "https://platform.moonshot.ai/docs/pricing"
supports_tool_choice = true

[models."moonshot-v1-32k-vision-preview".pricing."moonshot"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003

[models."moonshot-v1-8k"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 0.000002
litellm_provider = "moonshot"
providers = ["moonshot"]
supports_function_calling = true
source = "https://platform.moonshot.ai/docs/pricing"
supports_tool_choice = true

[models."moonshot-v1-8k".pricing."moonshot"]
input_cost_per_token = 2e-7
output_cost_per_token = 0.000002

[models."moonshot-v1-8k-0430"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 0.000002
litellm_provider = "moonshot"
providers = ["moonshot"]
supports_function_calling = true
source = "https://platform.moonshot.ai/docs/pricing"
supports_tool_choice = true

[models."moonshot-v1-8k-0430".pricing."moonshot"]
input_cost_per_token = 2e-7
output_cost_per_token = 0.000002

[models."moonshot-v1-8k-vision-preview"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 0.000002
litellm_provider = "moonshot"
providers = ["moonshot"]
supports_function_calling = true
supports_vision = true
source = "https://platform.moonshot.ai/docs/pricing"
supports_tool_choice = true

[models."moonshot-v1-8k-vision-preview".pricing."moonshot"]
input_cost_per_token = 2e-7
output_cost_per_token = 0.000002

[models."moonshot-v1-auto"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005
litellm_provider = "moonshot"
providers = ["moonshot"]
supports_function_calling = true
source = "https://platform.moonshot.ai/docs/pricing"
supports_tool_choice = true

[models."moonshot-v1-auto".pricing."moonshot"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000005

[models."moonshot.kimi-k2-thinking"]
display_name = "Kimi K2 Thinking"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-02"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."moonshot.kimi-k2-thinking".pricing."amazon-bedrock"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."moonshot.kimi-k2-thinking".pricing."bedrock_converse"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025

[models."moonshotai.kimi-k2-thinking"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 7.3e-7
output_cost_per_token = 0.00000303
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_reasoning = true

[models."moonshotai.kimi-k2-thinking".pricing."bedrock"]
input_cost_per_token = 7.3e-7
output_cost_per_token = 0.00000303
[models."moonshotai.kimi-k2-thinking".pricing."bedrock/us-east-1"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."moonshotai.kimi-k2-thinking".pricing."bedrock/us-west-2"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025

[models."moonshotai.kimi-k2.5"]
display_name = "Kimi K2.5"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 6e-7
output_cost_per_token = 0.00000303
litellm_provider = "bedrock"
providers = ["bedrock", "amazon-bedrock", "bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = true
release_date = "2026-02-06"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "https://platform.moonshot.ai/docs/guide/kimi-k2-5-quickstart"
supports_system_messages = true
supports_tool_choice = true
supports_video_input = true

[models."moonshotai.kimi-k2.5".pricing."amazon-bedrock"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
[models."moonshotai.kimi-k2.5".pricing."bedrock"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.00000303
[models."moonshotai.kimi-k2.5".pricing."bedrock/us-east-1"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
[models."moonshotai.kimi-k2.5".pricing."bedrock/us-west-2"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
[models."moonshotai.kimi-k2.5".pricing."bedrock_converse"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003

[models."moonshotai/Kimi-K2-5-TEE"]
display_name = "Kimi K2.5 TEE"
model_family = "kimi"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65535
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2026-01-27"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."moonshotai/Kimi-K2-5-TEE".pricing."chutes"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003

[models."moonshotai/Kimi-K2-Instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002
litellm_provider = "deepinfra"
providers = ["deepinfra", "hyperbolic", "together_ai", "wandb"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."moonshotai/Kimi-K2-Instruct".pricing."deepinfra"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002
[models."moonshotai/Kimi-K2-Instruct".pricing."hyperbolic"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002
[models."moonshotai/Kimi-K2-Instruct".pricing."together_ai"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
[models."moonshotai/Kimi-K2-Instruct".pricing."wandb"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025

[models."moonshotai/Kimi-K2-Instruct-0905"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002
cache_read_input_token_cost = 4e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "together_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."moonshotai/Kimi-K2-Instruct-0905".pricing."deepinfra"]
cache_read_input_token_cost = 4e-7
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002
[models."moonshotai/Kimi-K2-Instruct-0905".pricing."together_ai"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003

[models."moonshotai/Kimi-K2-Thinking"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000012
litellm_provider = "gmi"
providers = ["gmi"]

[models."moonshotai/Kimi-K2-Thinking".pricing."gmi"]
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000012

[models."moonshotai/Kimi-K2-Thinking-TEE"]
display_name = "Kimi K2 Thinking TEE"
model_family = "kimi-thinking"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65535
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.00000175
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."moonshotai/Kimi-K2-Thinking-TEE".pricing."chutes"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.00000175

[models."moonshotai/Kimi-K2.5"]
display_name = "Kimi K2.5"
model_family = "kimi"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000028
litellm_provider = "together_ai"
providers = ["together_ai", "baseten", "deepinfra", "evroc", "huggingface", "jiekou", "kilo", "meganova", "nano-gpt", "nebius", "novita-ai", "nvidia", "openrouter", "siliconflow", "togetherai", "vercel", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2026-01-27"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "https://www.together.ai/models/kimi-k2-5"
reasoning_cost_per_token = 0.0000025
supports_tool_choice = true

[models."moonshotai/Kimi-K2.5".pricing."baseten"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
[models."moonshotai/Kimi-K2.5".pricing."deepinfra"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000028
[models."moonshotai/Kimi-K2.5".pricing."evroc"]
input_cost_per_token = 0.00000147
output_cost_per_token = 0.0000059
[models."moonshotai/Kimi-K2.5".pricing."huggingface"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
[models."moonshotai/Kimi-K2.5".pricing."jiekou"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
[models."moonshotai/Kimi-K2.5".pricing."kilo"]
input_cost_per_token = 2.3000000000000002e-7
output_cost_per_token = 0.000003
[models."moonshotai/Kimi-K2.5".pricing."meganova"]
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.0000028
[models."moonshotai/Kimi-K2.5".pricing."nano-gpt"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000018999999999999998
[models."moonshotai/Kimi-K2.5".pricing."nebius"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000025
reasoning_cost_per_token = 0.0000025
[models."moonshotai/Kimi-K2.5".pricing."novita-ai"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
[models."moonshotai/Kimi-K2.5".pricing."openrouter"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
[models."moonshotai/Kimi-K2.5".pricing."siliconflow"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.000003
[models."moonshotai/Kimi-K2.5".pricing."together_ai"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000028
[models."moonshotai/Kimi-K2.5".pricing."togetherai"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000028
[models."moonshotai/Kimi-K2.5".pricing."vercel"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000012
[models."moonshotai/Kimi-K2.5".pricing."zenmux"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 5.8e-7
output_cost_per_token = 0.00000302

[models."moonshotai/kimi-dev-72b:free"]
display_name = "Kimi Dev 72b (free)"
model_family = "kimi"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2025-06-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."moonshotai/kimi-k2"]
display_name = "Kimi K2 Instruct"
model_family = "kimi"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000022
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "fastrouter", "kilo", "openrouter", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-07-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."moonshotai/kimi-k2".pricing."fastrouter"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000022
[models."moonshotai/kimi-k2".pricing."kilo"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000024
[models."moonshotai/kimi-k2".pricing."openrouter"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000022
[models."moonshotai/kimi-k2".pricing."vercel"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
[models."moonshotai/kimi-k2".pricing."vercel_ai_gateway"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000022

[models."moonshotai/kimi-k2-0905"]
display_name = "Kimi K2 0905"
model_family = "kimi"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
litellm_provider = "novita"
providers = ["novita", "jiekou", "kilo", "novita-ai", "openrouter", "vercel", "zenmux"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."moonshotai/kimi-k2-0905".pricing."jiekou"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."moonshotai/kimi-k2-0905".pricing."kilo"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002
[models."moonshotai/kimi-k2-0905".pricing."novita"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."moonshotai/kimi-k2-0905".pricing."novita-ai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."moonshotai/kimi-k2-0905".pricing."openrouter"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."moonshotai/kimi-k2-0905".pricing."vercel"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."moonshotai/kimi-k2-0905".pricing."zenmux"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025

[models."moonshotai/kimi-k2-0905:exacto"]
display_name = "Kimi K2 Instruct 0905 (exacto)"
model_family = "kimi"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 16384
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-09-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."moonshotai/kimi-k2-0905:exacto".pricing."kilo"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."moonshotai/kimi-k2-0905:exacto".pricing."openrouter"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025

[models."moonshotai/kimi-k2-5-thinking"]
display_name = "Kimi K2.5 Thinking"
model_family = "kimi"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 65536
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000018999999999999998
litellm_provider = "nano-gpt"
providers = ["nano-gpt"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-01-26"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."moonshotai/kimi-k2-5-thinking".pricing."nano-gpt"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000018999999999999998

[models."moonshotai/kimi-k2-instruct"]
display_name = "Kimi K2 Instruct"
model_family = "kimi"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 5.7e-7
output_cost_per_token = 0.0000023
litellm_provider = "novita"
providers = ["novita", "deepinfra", "groq", "huggingface", "jiekou", "nano-gpt", "nebius", "novita-ai", "nvidia", "siliconflow", "togetherai", "wandb"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-01"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."moonshotai/kimi-k2-instruct".pricing."deepinfra"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002
[models."moonshotai/kimi-k2-instruct".pricing."groq"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
[models."moonshotai/kimi-k2-instruct".pricing."huggingface"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
[models."moonshotai/kimi-k2-instruct".pricing."jiekou"]
input_cost_per_token = 5.699999999999999e-7
output_cost_per_token = 0.0000023
[models."moonshotai/kimi-k2-instruct".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
[models."moonshotai/kimi-k2-instruct".pricing."nebius"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000024
[models."moonshotai/kimi-k2-instruct".pricing."novita"]
input_cost_per_token = 5.7e-7
output_cost_per_token = 0.0000023
[models."moonshotai/kimi-k2-instruct".pricing."novita-ai"]
input_cost_per_token = 5.699999999999999e-7
output_cost_per_token = 0.0000023
[models."moonshotai/kimi-k2-instruct".pricing."siliconflow"]
input_cost_per_token = 5.8e-7
output_cost_per_token = 0.00000229
[models."moonshotai/kimi-k2-instruct".pricing."togetherai"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
[models."moonshotai/kimi-k2-instruct".pricing."wandb"]
input_cost_per_token = 0.00000135
output_cost_per_token = 0.000004

[models."moonshotai/kimi-k2-instruct-0905"]
display_name = "Kimi K2 0905"
model_family = "kimi"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
cache_read_input_token_cost = 5e-7
litellm_provider = "groq"
providers = ["groq", "baseten", "chutes", "huggingface", "io-net", "nvidia", "siliconflow", "siliconflow-cn", "togetherai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-09-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_tool_choice = true

[models."moonshotai/kimi-k2-instruct-0905".pricing."baseten"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."moonshotai/kimi-k2-instruct-0905".pricing."chutes"]
cache_read_input_token_cost = 1.95e-7
input_cost_per_token = 3.9e-7
output_cost_per_token = 0.0000018999999999999998
[models."moonshotai/kimi-k2-instruct-0905".pricing."groq"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
[models."moonshotai/kimi-k2-instruct-0905".pricing."huggingface"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003
[models."moonshotai/kimi-k2-instruct-0905".pricing."io-net"]
cache_read_input_token_cost = 1.95e-7
input_cost_per_token = 3.9e-7
output_cost_per_token = 0.0000018999999999999998
[models."moonshotai/kimi-k2-instruct-0905".pricing."siliconflow"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002
[models."moonshotai/kimi-k2-instruct-0905".pricing."siliconflow-cn"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002
[models."moonshotai/kimi-k2-instruct-0905".pricing."togetherai"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000003

[models."moonshotai/kimi-k2-thinking"]
display_name = "Kimi K2 Thinking"
model_family = "kimi-thinking"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
litellm_provider = "novita"
providers = ["novita", "baseten", "deepinfra", "huggingface", "io-net", "kilo", "meganova", "nano-gpt", "nebius", "novita-ai", "nvidia", "openrouter", "siliconflow", "siliconflow-cn", "togetherai", "vercel", "zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2025-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
reasoning_cost_per_token = 0.0000025
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."moonshotai/kimi-k2-thinking".pricing."baseten"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."moonshotai/kimi-k2-thinking".pricing."deepinfra"]
input_cost_per_token = 4.6999999999999995e-7
output_cost_per_token = 0.000002
[models."moonshotai/kimi-k2-thinking".pricing."huggingface"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."moonshotai/kimi-k2-thinking".pricing."io-net"]
cache_read_input_token_cost = 2.75e-7
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.00000225
[models."moonshotai/kimi-k2-thinking".pricing."kilo"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.00000175
[models."moonshotai/kimi-k2-thinking".pricing."meganova"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000026
[models."moonshotai/kimi-k2-thinking".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
[models."moonshotai/kimi-k2-thinking".pricing."nebius"]
cache_read_input_token_cost = 6e-8
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
reasoning_cost_per_token = 0.0000025
[models."moonshotai/kimi-k2-thinking".pricing."novita"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."moonshotai/kimi-k2-thinking".pricing."novita-ai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."moonshotai/kimi-k2-thinking".pricing."openrouter"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
[models."moonshotai/kimi-k2-thinking".pricing."siliconflow"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000025
[models."moonshotai/kimi-k2-thinking".pricing."siliconflow-cn"]
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000025
[models."moonshotai/kimi-k2-thinking".pricing."togetherai"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.000004
[models."moonshotai/kimi-k2-thinking".pricing."vercel"]
cache_read_input_token_cost = 1.4e-7
input_cost_per_token = 4.6999999999999995e-7
output_cost_per_token = 0.000002
[models."moonshotai/kimi-k2-thinking".pricing."zenmux"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025

[models."moonshotai/kimi-k2-thinking-maas"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_tool_choice = true
supports_web_search = true

[models."moonshotai/kimi-k2-thinking-maas".pricing."vertex_ai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025

[models."moonshotai/kimi-k2-thinking-turbo"]
display_name = "Kimi K2 Thinking Turbo"
model_family = "kimi-thinking"
mode = "chat"
max_input_tokens = 262114
max_output_tokens = 262114
input_cost_per_token = 0.00000115
output_cost_per_token = 0.000008
cache_read_input_token_cost = 1.5e-7
litellm_provider = "vercel"
providers = ["vercel", "zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-08"
release_date = "2025-11-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."moonshotai/kimi-k2-thinking-turbo".pricing."vercel"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 0.00000115
output_cost_per_token = 0.000008
[models."moonshotai/kimi-k2-thinking-turbo".pricing."zenmux"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 0.00000115
output_cost_per_token = 0.000008

[models."moonshotai/kimi-k2-turbo"]
display_name = "Kimi K2 Turbo"
model_family = "kimi"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 16384
input_cost_per_token = 0.0000024
output_cost_per_token = 0.00001
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-08"
release_date = "2025-09-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."moonshotai/kimi-k2-turbo".pricing."vercel"]
input_cost_per_token = 0.0000024
output_cost_per_token = 0.00001

[models."moonshotai/kimi-k2.5"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
cache_read_input_token_cost = 1e-7
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_vision = true
source = "https://openrouter.ai/moonshotai/kimi-k2.5"
supports_tool_choice = true
supports_video_input = true

[models."moonshotai/kimi-k2.5".pricing."openrouter"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003

[models."moonshotai/kimi-k2:free"]
display_name = "Kimi K2 (free)"
model_family = "kimi"
mode = "chat"
max_input_tokens = 32800
max_output_tokens = 32800
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."morph-v3-fast"]
display_name = "Morph v3 Fast"
model_family = "morph"
mode = "chat"
max_input_tokens = 16000
max_output_tokens = 16000
max_tokens = 16000
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000012
litellm_provider = "morph"
providers = ["morph"]
supports_function_calling = false
supports_vision = false
supports_reasoning = false
open_weights = false
release_date = "2024-08-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = false
supports_system_messages = true
supports_tool_choice = false

[models."morph-v3-fast".pricing."morph"]
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000012

[models."morph-v3-large"]
display_name = "Morph v3 Large"
model_family = "morph"
mode = "chat"
max_input_tokens = 16000
max_output_tokens = 16000
max_tokens = 16000
input_cost_per_token = 9e-7
output_cost_per_token = 0.0000019
litellm_provider = "morph"
providers = ["morph"]
supports_function_calling = false
supports_vision = false
supports_reasoning = false
open_weights = false
release_date = "2024-08-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = false
supports_system_messages = true
supports_tool_choice = false

[models."morph-v3-large".pricing."morph"]
input_cost_per_token = 9e-7
output_cost_per_token = 0.0000019

[models."morph/morph-v3-fast"]
display_name = "Morph v3 Fast"
model_family = "morph"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000012
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "kilo", "vercel"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2024-08-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."morph/morph-v3-fast".pricing."kilo"]
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.0000012
[models."morph/morph-v3-fast".pricing."vercel"]
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.0000012
[models."morph/morph-v3-fast".pricing."vercel_ai_gateway"]
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000012

[models."morph/morph-v3-large"]
display_name = "Morph v3 Large"
model_family = "morph"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 9e-7
output_cost_per_token = 0.0000019
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "kilo", "vercel"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2024-08-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."morph/morph-v3-large".pricing."kilo"]
input_cost_per_token = 9.000000000000001e-7
output_cost_per_token = 0.0000018999999999999998
[models."morph/morph-v3-large".pricing."vercel"]
input_cost_per_token = 9.000000000000001e-7
output_cost_per_token = 0.0000018999999999999998
[models."morph/morph-v3-large".pricing."vercel_ai_gateway"]
input_cost_per_token = 9e-7
output_cost_per_token = 0.0000019

[models."multimodalembedding"]
mode = "embedding"
max_input_tokens = 2048
max_tokens = 2048
input_cost_per_token = 8e-7
output_cost_per_token = 0
litellm_provider = "vertex_ai-embedding-models"
providers = ["vertex_ai-embedding-models"]
supported_modalities = ["text", "image", "video"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"
input_cost_per_character = 2e-7
input_cost_per_image = 0.0001
input_cost_per_video_per_second = 0.0005
input_cost_per_video_per_second_above_15s_interval = 0.002
input_cost_per_video_per_second_above_8s_interval = 0.001
output_vector_size = 768
supported_endpoints = ["/v1/embeddings"]

[models."multimodalembedding".pricing."vertex_ai-embedding-models"]
input_cost_per_character = 2e-7
input_cost_per_image = 0.0001
input_cost_per_token = 8e-7
input_cost_per_video_per_second = 0.0005
input_cost_per_video_per_second_above_15s_interval = 0.002
input_cost_per_video_per_second_above_8s_interval = 0.001
output_cost_per_token = 0

[models."multimodalembedding@001"]
mode = "embedding"
max_input_tokens = 2048
max_tokens = 2048
input_cost_per_token = 8e-7
output_cost_per_token = 0
litellm_provider = "vertex_ai-embedding-models"
providers = ["vertex_ai-embedding-models"]
supported_modalities = ["text", "image", "video"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"
input_cost_per_character = 2e-7
input_cost_per_image = 0.0001
input_cost_per_video_per_second = 0.0005
input_cost_per_video_per_second_above_15s_interval = 0.002
input_cost_per_video_per_second_above_8s_interval = 0.001
output_vector_size = 768
supported_endpoints = ["/v1/embeddings"]

[models."multimodalembedding@001".pricing."vertex_ai-embedding-models"]
input_cost_per_character = 2e-7
input_cost_per_image = 0.0001
input_cost_per_token = 8e-7
input_cost_per_video_per_second = 0.0005
input_cost_per_video_per_second_above_15s_interval = 0.002
input_cost_per_video_per_second_above_8s_interval = 0.001
output_cost_per_token = 0

[models."nano"]
mode = "audio_transcription"
litellm_provider = "assemblyai"
providers = ["assemblyai"]
input_cost_per_second = 0.00010278
output_cost_per_second = 0

[models."nano".pricing."assemblyai"]
input_cost_per_second = 0.00010278
output_cost_per_second = 0

[models."nano-banana"]
display_name = "Nano-Banana"
model_family = "nano-banana"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 0
input_cost_per_token = 2.1e-7
output_cost_per_token = 0.0000018000000000000001
cache_read_input_token_cost = 2.1000000000000003e-8
litellm_provider = "poe"
providers = ["poe"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-08-21"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "image"]
source = "modelsdev"

[models."nano-banana".pricing."poe"]
cache_read_input_token_cost = 2.1000000000000003e-8
input_cost_per_token = 2.1e-7
output_cost_per_token = 0.0000018000000000000001

[models."nano-banana-pro"]
display_name = "Nano-Banana-Pro"
model_family = "nano-banana"
mode = "image_generation"
max_input_tokens = 65536
max_output_tokens = 0
litellm_provider = "aiml"
providers = ["aiml", "poe"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-11-19"
supported_modalities = ["text", "image"]
supported_output_modalities = ["image"]
source = "https://docs.aimlapi.com/api-references/image-models/google/gemini-3-pro-image-preview"
output_cost_per_image = 0.1575
supported_endpoints = ["/v1/images/generations"]

[models."nano-banana-pro".metadata]
notes = "Gemini 3 Pro Image (Nano Banana Pro) - Advanced text-to-image generation with reasoning and 4K resolution support"

[models."nano-banana-pro".pricing."aiml"]
output_cost_per_image = 0.1575
[models."nano-banana-pro".pricing."poe"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000012

[models."neural"]
mode = "audio_speech"
litellm_provider = "aws_polly"
providers = ["aws_polly"]
source = "https://aws.amazon.com/polly/pricing/"
input_cost_per_character = 0.000016
supported_endpoints = ["/v1/audio/speech"]

[models."neural".pricing."aws_polly"]
input_cost_per_character = 0.000016

[models."neuralmagic/Meta-Llama-3-1-8B-Instruct-FP8"]
display_name = "Llama 3.1 8B"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 1.6e-7
output_cost_per_token = 2.7e-7
litellm_provider = "stackit"
providers = ["stackit"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."neuralmagic/Meta-Llama-3-1-8B-Instruct-FP8".pricing."stackit"]
input_cost_per_token = 1.6e-7
output_cost_per_token = 2.7e-7

[models."neuralmagic/Mistral-Nemo-Instruct-2407-FP8"]
display_name = "Mistral Nemo"
model_family = "mistral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 4.9e-7
output_cost_per_token = 7.1e-7
litellm_provider = "stackit"
providers = ["stackit"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-07-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."neuralmagic/Mistral-Nemo-Instruct-2407-FP8".pricing."stackit"]
input_cost_per_token = 4.9e-7
output_cost_per_token = 7.1e-7

[models."nex-agi/DeepSeek-V3-1-Nex-N1"]
display_name = "nex-agi/DeepSeek-V3.1-Nex-N1"
model_family = "deepseek"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 131000
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002
litellm_provider = "siliconflow"
providers = ["siliconflow", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nex-agi/DeepSeek-V3-1-Nex-N1".pricing."kilo"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.000001
[models."nex-agi/DeepSeek-V3-1-Nex-N1".pricing."siliconflow"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002

[models."nomic-ai/nomic-embed-text-v1"]
mode = "embedding"
max_input_tokens = 8192
max_tokens = 8192
input_cost_per_token = 8e-9
output_cost_per_token = 0
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
source = "https://fireworks.ai/pricing"

[models."nomic-ai/nomic-embed-text-v1".pricing."fireworks_ai"]
input_cost_per_token = 8e-9
output_cost_per_token = 0

[models."nomic-ai/nomic-embed-text-v1.5"]
mode = "embedding"
max_input_tokens = 8192
max_tokens = 8192
input_cost_per_token = 8e-9
output_cost_per_token = 0
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
source = "https://fireworks.ai/pricing"

[models."nomic-ai/nomic-embed-text-v1.5".pricing."fireworks_ai"]
input_cost_per_token = 8e-9
output_cost_per_token = 0

[models."nomic-embed-text"]
mode = "embedding"
max_input_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-8
output_cost_per_token = 0
litellm_provider = "llamagate"
providers = ["llamagate"]

[models."nomic-embed-text".pricing."llamagate"]
input_cost_per_token = 2e-8
output_cost_per_token = 0

[models."nousresearch/deephermes-3-llama-3-8b-preview"]
display_name = "DeepHermes 3 Llama 3 8B Preview"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2025-02-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nousresearch/hermes-2-pro-llama-3-8b"]
display_name = "Hermes 2 Pro Llama 3 8B"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.4e-7
output_cost_per_token = 1.4e-7
litellm_provider = "novita"
providers = ["novita", "kilo", "novita-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2024-06-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."nousresearch/hermes-2-pro-llama-3-8b".pricing."kilo"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 1.4e-7
[models."nousresearch/hermes-2-pro-llama-3-8b".pricing."novita"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 1.4e-7
[models."nousresearch/hermes-2-pro-llama-3-8b".pricing."novita-ai"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 1.4e-7

[models."nousresearch/hermes-3-llama-3-1-405b:free"]
display_name = "Hermes 3 405B Instruct (free)"
model_family = "hermes"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2024-08-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nousresearch/hermes-4-405b:thinking"]
display_name = "Hermes 4 405b Thinking"
model_family = "hermes"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
litellm_provider = "nano-gpt"
providers = ["nano-gpt"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2024-08-13"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nousresearch/hermes-4-405b:thinking".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002

[models."nova"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00007167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."nova".pricing."deepgram"]
input_cost_per_second = 0.00007167
output_cost_per_second = 0

[models."nova-2"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00007167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova-2".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."nova-2".pricing."deepgram"]
input_cost_per_second = 0.00007167
output_cost_per_second = 0

[models."nova-2-atc"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00007167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova-2-atc".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."nova-2-atc".pricing."deepgram"]
input_cost_per_second = 0.00007167
output_cost_per_second = 0

[models."nova-2-automotive"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00007167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova-2-automotive".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."nova-2-automotive".pricing."deepgram"]
input_cost_per_second = 0.00007167
output_cost_per_second = 0

[models."nova-2-conversationalai"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00007167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova-2-conversationalai".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."nova-2-conversationalai".pricing."deepgram"]
input_cost_per_second = 0.00007167
output_cost_per_second = 0

[models."nova-2-drivethru"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00007167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova-2-drivethru".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."nova-2-drivethru".pricing."deepgram"]
input_cost_per_second = 0.00007167
output_cost_per_second = 0

[models."nova-2-finance"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00007167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova-2-finance".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."nova-2-finance".pricing."deepgram"]
input_cost_per_second = 0.00007167
output_cost_per_second = 0

[models."nova-2-general"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00007167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova-2-general".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."nova-2-general".pricing."deepgram"]
input_cost_per_second = 0.00007167
output_cost_per_second = 0

[models."nova-2-lite"]
display_name = "Nova 2 Lite"
model_family = "nova"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 1000000
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = false
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-12-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nova-2-lite".pricing."vercel"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025

[models."nova-2-lite-v1"]
display_name = "Nova 2 Lite"
model_family = "nova-lite"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
litellm_provider = "nova"
providers = ["nova", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-12-01"
supported_modalities = ["text", "image", "video", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nova-2-lite-v1".pricing."kilo"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025

[models."nova-2-meeting"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00007167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova-2-meeting".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."nova-2-meeting".pricing."deepgram"]
input_cost_per_second = 0.00007167
output_cost_per_second = 0

[models."nova-2-phonecall"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00007167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova-2-phonecall".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."nova-2-phonecall".pricing."deepgram"]
input_cost_per_second = 0.00007167
output_cost_per_second = 0

[models."nova-2-pro-v1"]
display_name = "Nova 2 Pro"
model_family = "nova-pro"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
litellm_provider = "nova"
providers = ["nova"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-12-03"
supported_modalities = ["text", "image", "video", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nova-2-video"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00007167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova-2-video".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."nova-2-video".pricing."deepgram"]
input_cost_per_second = 0.00007167
output_cost_per_second = 0

[models."nova-2-voicemail"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00007167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova-2-voicemail".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."nova-2-voicemail".pricing."deepgram"]
input_cost_per_second = 0.00007167
output_cost_per_second = 0

[models."nova-3"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00007167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova-3".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."nova-3".pricing."deepgram"]
input_cost_per_second = 0.00007167
output_cost_per_second = 0

[models."nova-3-general"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00007167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova-3-general".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."nova-3-general".pricing."deepgram"]
input_cost_per_second = 0.00007167
output_cost_per_second = 0

[models."nova-3-medical"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00008667
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova-3-medical".metadata]
calculation = "$0.0052/60 seconds = $0.00008667 per second (multilingual)"
original_pricing_per_minute = 0.0052

[models."nova-3-medical".pricing."deepgram"]
input_cost_per_second = 0.00008667
output_cost_per_second = 0

[models."nova-general"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00007167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova-general".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."nova-general".pricing."deepgram"]
input_cost_per_second = 0.00007167
output_cost_per_second = 0

[models."nova-lite"]
display_name = "Nova Lite"
model_family = "nova-lite"
mode = "chat"
max_input_tokens = 300000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-12-03"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
supports_response_schema = true

[models."nova-lite".pricing."vercel"]
cache_read_input_token_cost = 1.5e-8
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7
[models."nova-lite".pricing."vercel_ai_gateway"]
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7

[models."nova-lite-v1"]
mode = "chat"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7
litellm_provider = "amazon-nova"
providers = ["amazon-nova"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supports_response_schema = true

[models."nova-lite-v1".pricing."amazon-nova"]
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7

[models."nova-micro"]
display_name = "Nova Micro"
model_family = "nova-micro"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3.5e-8
output_cost_per_token = 1.4e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-12-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true

[models."nova-micro".pricing."vercel"]
cache_read_input_token_cost = 8.75e-9
input_cost_per_token = 3.5e-8
output_cost_per_token = 1.4e-7
[models."nova-micro".pricing."vercel_ai_gateway"]
input_cost_per_token = 3.5e-8
output_cost_per_token = 1.4e-7

[models."nova-micro-v1"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 3.5e-8
output_cost_per_token = 1.4e-7
litellm_provider = "amazon-nova"
providers = ["amazon-nova"]
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true

[models."nova-micro-v1".pricing."amazon-nova"]
input_cost_per_token = 3.5e-8
output_cost_per_token = 1.4e-7

[models."nova-phonecall"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.00007167
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."nova-phonecall".metadata]
calculation = "$0.0043/60 seconds = $0.00007167 per second"
original_pricing_per_minute = 0.0043

[models."nova-phonecall".pricing."deepgram"]
input_cost_per_second = 0.00007167
output_cost_per_second = 0

[models."nova-premier-v1"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 0.0000025
output_cost_per_token = 0.0000125
litellm_provider = "amazon-nova"
providers = ["amazon-nova"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = false
supports_pdf_input = true
supports_response_schema = true

[models."nova-premier-v1".pricing."amazon-nova"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.0000125

[models."nova-pro"]
display_name = "Nova Pro"
model_family = "nova-pro"
mode = "chat"
max_input_tokens = 300000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000032
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-12-03"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
supports_response_schema = true

[models."nova-pro".pricing."vercel"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.0000032000000000000003
[models."nova-pro".pricing."vercel_ai_gateway"]
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000032

[models."nova-pro-v1"]
display_name = "Nova Pro 1.0"
model_family = "nova-pro"
mode = "chat"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000032
litellm_provider = "amazon-nova"
providers = ["amazon-nova", "cortecs", "kilo"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-12-03"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_response_schema = true

[models."nova-pro-v1".pricing."amazon-nova"]
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000032
[models."nova-pro-v1".pricing."cortecs"]
input_cost_per_token = 0.000001016
output_cost_per_token = 0.000004061
[models."nova-pro-v1".pricing."kilo"]
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.0000032000000000000003

[models."nvidia.nemotron-nano-12b-v2"]
display_name = "NVIDIA Nemotron Nano 12B v2 VL BF16"
model_family = "nemotron"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
release_date = "2024-12-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."nvidia.nemotron-nano-12b-v2".pricing."amazon-bedrock"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7
[models."nvidia.nemotron-nano-12b-v2".pricing."bedrock_converse"]
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7

[models."nvidia.nemotron-nano-3-30b"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."nvidia.nemotron-nano-3-30b".pricing."bedrock_converse"]
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7

[models."nvidia.nemotron-nano-9b-v2"]
display_name = "NVIDIA Nemotron Nano 9B v2"
model_family = "nemotron"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-8
output_cost_per_token = 2.3e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."nvidia.nemotron-nano-9b-v2".pricing."amazon-bedrock"]
input_cost_per_token = 6e-8
output_cost_per_token = 2.3000000000000002e-7
[models."nvidia.nemotron-nano-9b-v2".pricing."bedrock_converse"]
input_cost_per_token = 6e-8
output_cost_per_token = 2.3e-7

[models."nvidia/Llama-3-3-70B-Instruct-FP8"]
display_name = "Llama 3.3 70B"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
input_cost_per_token = 0.00000118
output_cost_per_token = 0.00000118
litellm_provider = "evroc"
providers = ["evroc"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/Llama-3-3-70B-Instruct-FP8".pricing."evroc"]
input_cost_per_token = 0.00000118
output_cost_per_token = 0.00000118

[models."nvidia/Llama-3.1-Nemotron-70B-Instruct"]
display_name = "Llama 3.1 Nemotron 70b Instruct"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 6e-7
output_cost_per_token = 6e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "kilo", "nvidia"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-10-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."nvidia/Llama-3.1-Nemotron-70B-Instruct".pricing."deepinfra"]
input_cost_per_token = 6e-7
output_cost_per_token = 6e-7
[models."nvidia/Llama-3.1-Nemotron-70B-Instruct".pricing."kilo"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000012

[models."nvidia/Llama-3.3-Nemotron-Super-49B-v1.5"]
display_name = "Llama 3.3 Nemotron Super 49b V1.5"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "kilo", "nvidia"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-03-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."nvidia/Llama-3.3-Nemotron-Super-49B-v1.5".pricing."deepinfra"]
input_cost_per_token = 1e-7
output_cost_per_token = 4e-7
[models."nvidia/Llama-3.3-Nemotron-Super-49B-v1.5".pricing."kilo"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7

[models."nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16"]
display_name = "NVIDIA Nemotron 3 Nano 30B A3B BF16"
model_family = "nemotron"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16".pricing."chutes"]
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7

[models."nvidia/NVIDIA-Nemotron-Nano-9B-v2"]
display_name = "nvidia-nemotron-nano-9b-v2"
model_family = "nemotron"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 4e-8
output_cost_per_token = 1.6e-7
litellm_provider = "deepinfra"
providers = ["deepinfra", "nvidia"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-09"
release_date = "2025-08-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."nvidia/NVIDIA-Nemotron-Nano-9B-v2".pricing."deepinfra"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.6e-7

[models."nvidia/cosmos-nemotron-34b"]
display_name = "Cosmos Nemotron 34B"
model_family = "nemotron"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = false
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-01"
release_date = "2024-01-01"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/llama-3-1-nemotron-51b-instruct"]
display_name = "Llama 3.1 Nemotron 51b Instruct"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-09-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/llama-3-1-nemotron-ultra-253b-v1"]
display_name = "Llama-3.1-Nemotron-Ultra-253B-v1"
model_family = "llama"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
litellm_provider = "nvidia"
providers = ["nvidia", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-07"
release_date = "2024-07-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/llama-3-1-nemotron-ultra-253b-v1".pricing."kilo"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000018000000000000001

[models."nvidia/llama-3-3-nemotron-super-49b-v1"]
display_name = "Llama 3.3 Nemotron Super 49b V1"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-03-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/llama-3_1-nemotron-ultra-253b-v1"]
display_name = "Llama-3.1-Nemotron-Ultra-253B-v1"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000018000000000000001
cache_read_input_token_cost = 6e-8
litellm_provider = "nebius"
providers = ["nebius"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2025-01-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/llama-3_1-nemotron-ultra-253b-v1".pricing."nebius"]
cache_read_input_token_cost = 6e-8
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000018000000000000001

[models."nvidia/llama-3_2-nv-rerankqa-1b-v2"]
mode = "rerank"
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "nvidia_nim"
providers = ["nvidia_nim"]
input_cost_per_query = 0

[models."nvidia/llama-3_2-nv-rerankqa-1b-v2".pricing."nvidia_nim"]
input_cost_per_query = 0
input_cost_per_token = 0
output_cost_per_token = 0

[models."nvidia/llama-3_3-nemotron-super-49b-v1_5"]
display_name = "Llama 3 3 Nemotron Super 49B V1 5"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
litellm_provider = "nano-gpt"
providers = ["nano-gpt"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-08-08"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/llama-3_3-nemotron-super-49b-v1_5".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002

[models."nvidia/llama-embed-nemotron-8b"]
display_name = "Llama Embed Nemotron 8B"
model_family = "llama"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 2048
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-03"
release_date = "2025-03-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/llama3-chatqa-1-5-70b"]
display_name = "Llama3 Chatqa 1.5 70b"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-04-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/nemoretriever-ocr-v1"]
display_name = "NeMo Retriever OCR v1"
model_family = "nemoretriever"
mode = "chat"
max_input_tokens = 0
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-01"
release_date = "2024-01-01"
supported_modalities = ["image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/nemotron-3-nano-30b-a3b"]
display_name = "nemotron-3-nano-30b-a3b"
model_family = "nemotron"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
litellm_provider = "nvidia"
providers = ["nvidia", "kilo", "vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-09"
release_date = "2024-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/nemotron-3-nano-30b-a3b".pricing."kilo"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.0000000000000002e-7
[models."nvidia/nemotron-3-nano-30b-a3b".pricing."vercel"]
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7

[models."nvidia/nemotron-3-nano-30b-a3b:free"]
display_name = "Nemotron 3 Nano 30B A3B (free)"
model_family = "nemotron"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-11"
release_date = "2025-12-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/nemotron-4-340b-instruct"]
display_name = "Nemotron 4 340b Instruct"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-06-13"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/nemotron-nano-12b-v2-vl"]
display_name = "Nvidia Nemotron Nano 12B V2 VL"
model_family = "nemotron"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7
litellm_provider = "vercel"
providers = ["vercel", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-12"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/nemotron-nano-12b-v2-vl".pricing."kilo"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7
[models."nvidia/nemotron-nano-12b-v2-vl".pricing."vercel"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7

[models."nvidia/nemotron-nano-12b-v2-vl:free"]
display_name = "Nemotron Nano 12B 2 VL (free)"
model_family = "nemotron"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-11"
release_date = "2025-10-28"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/nemotron-nano-9b-v2"]
display_name = "Nvidia Nemotron Nano 9B V2"
model_family = "nemotron"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 4e-8
output_cost_per_token = 1.6e-7
litellm_provider = "vercel"
providers = ["vercel", "kilo", "openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-08-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/nemotron-nano-9b-v2".pricing."kilo"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.6e-7
[models."nvidia/nemotron-nano-9b-v2".pricing."openrouter"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.6e-7
[models."nvidia/nemotron-nano-9b-v2".pricing."vercel"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.6e-7

[models."nvidia/nemotron-nano-9b-v2:free"]
display_name = "Nemotron Nano 9B V2 (free)"
model_family = "nemotron"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-09"
release_date = "2025-09-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/nemotron-nano-v2-12b"]
display_name = "Nemotron-Nano-V2-12b"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 4096
input_cost_per_token = 7e-8
output_cost_per_token = 2.0000000000000002e-7
cache_read_input_token_cost = 7e-9
litellm_provider = "nebius"
providers = ["nebius"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2025-03-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/nemotron-nano-v2-12b".pricing."nebius"]
cache_read_input_token_cost = 7e-9
input_cost_per_token = 7e-8
output_cost_per_token = 2.0000000000000002e-7

[models."nvidia/nv-rerankqa-mistral-4b-v3"]
mode = "rerank"
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "nvidia_nim"
providers = ["nvidia_nim"]
input_cost_per_query = 0

[models."nvidia/nv-rerankqa-mistral-4b-v3".pricing."nvidia_nim"]
input_cost_per_query = 0
input_cost_per_token = 0
output_cost_per_token = 0

[models."nvidia/nvidia-nemotron-3-nano-30b-a3b"]
display_name = "Nemotron-3-Nano-30B-A3B"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 4096
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7
cache_read_input_token_cost = 6e-9
litellm_provider = "nebius"
providers = ["nebius"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-05"
release_date = "2025-08-10"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."nvidia/nvidia-nemotron-3-nano-30b-a3b".pricing."nebius"]
cache_read_input_token_cost = 6e-9
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7

[models."nvidia/parakeet-tdt-0-6b-v2"]
display_name = "Parakeet TDT 0.6B v2"
model_family = "parakeet"
mode = "chat"
max_input_tokens = 0
max_output_tokens = 4096
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-01"
release_date = "2024-01-01"
supported_modalities = ["audio"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."o1"]
display_name = "o1"
model_family = "o"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006
cache_read_input_token_cost = 0.0000075
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services", "cloudflare-ai-gateway", "github-models", "helicone", "kilo", "openai", "openrouter", "poe", "replicate", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2023-09"
release_date = "2024-12-05"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."o1".pricing."azure"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006
[models."o1".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006
[models."o1".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006
[models."o1".pricing."helicone"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006
[models."o1".pricing."kilo"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006
[models."o1".pricing."openai"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006
[models."o1".pricing."openrouter"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006
[models."o1".pricing."poe"]
input_cost_per_token = 0.000014
output_cost_per_token = 0.000054
[models."o1".pricing."replicate"]
input_cost_per_token = 0.000015
output_cost_per_reasoning_token = 0.00006
output_cost_per_token = 0.00006
[models."o1".pricing."vercel"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006
[models."o1".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006

[models."o1-2024-12-17"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.0000165
output_cost_per_token = 0.000066
cache_read_input_token_cost = 0.00000825
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."o1-2024-12-17".pricing."azure"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006
[models."o1-2024-12-17".pricing."azure/eu"]
cache_read_input_token_cost = 0.00000825
input_cost_per_token = 0.0000165
output_cost_per_token = 0.000066
[models."o1-2024-12-17".pricing."azure/us"]
cache_read_input_token_cost = 0.00000825
input_cost_per_token = 0.0000165
output_cost_per_token = 0.000066
[models."o1-2024-12-17".pricing."openai"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006

[models."o1-mini"]
display_name = "OpenAI o1-mini"
model_family = "o-mini"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 0.00000121
output_cost_per_token = 0.00000484
cache_read_input_token_cost = 6.05e-7
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services", "github-models", "helicone", "openai", "replicate"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2023-10"
release_date = "2024-09-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true

[models."o1-mini".pricing."azure"]
cache_read_input_token_cost = 6.05e-7
input_cost_per_token = 0.00000121
output_cost_per_token = 0.00000484
[models."o1-mini".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o1-mini".pricing."helicone"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o1-mini".pricing."openai"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o1-mini".pricing."replicate"]
input_cost_per_token = 0.0000011
output_cost_per_reasoning_token = 0.0000044
output_cost_per_token = 0.0000044

[models."o1-mini-2024-09-12"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 0.00000121
output_cost_per_token = 0.00000484
cache_read_input_token_cost = 6.05e-7
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
input_cost_per_token_batches = 6.05e-7
output_cost_per_token_batches = 0.00000242
supports_parallel_function_calling = true

[models."o1-mini-2024-09-12".pricing."azure"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o1-mini-2024-09-12".pricing."azure/eu"]
cache_read_input_token_cost = 6.05e-7
input_cost_per_token = 0.00000121
input_cost_per_token_batches = 6.05e-7
output_cost_per_token = 0.00000484
output_cost_per_token_batches = 0.00000242
[models."o1-mini-2024-09-12".pricing."azure/us"]
cache_read_input_token_cost = 6.05e-7
input_cost_per_token = 0.00000121
input_cost_per_token_batches = 6.05e-7
output_cost_per_token = 0.00000484
output_cost_per_token_batches = 0.00000242
[models."o1-mini-2024-09-12".pricing."openai"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000003
output_cost_per_token = 0.000012

[models."o1-preview"]
display_name = "OpenAI o1-preview"
model_family = "o"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006
cache_read_input_token_cost = 0.0000075
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services", "github-models", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2023-10"
release_date = "2024-09-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true

[models."o1-preview".pricing."azure"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006
[models."o1-preview".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 0.00000825
input_cost_per_token = 0.0000165
output_cost_per_token = 0.000066
[models."o1-preview".pricing."openai"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006

[models."o1-preview-2024-09-12"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.0000165
output_cost_per_token = 0.000066
cache_read_input_token_cost = 0.00000825
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_parallel_function_calling = true

[models."o1-preview-2024-09-12".pricing."azure"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006
[models."o1-preview-2024-09-12".pricing."azure/eu"]
cache_read_input_token_cost = 0.00000825
input_cost_per_token = 0.0000165
output_cost_per_token = 0.000066
[models."o1-preview-2024-09-12".pricing."azure/us"]
cache_read_input_token_cost = 0.00000825
input_cost_per_token = 0.0000165
output_cost_per_token = 0.000066
[models."o1-preview-2024-09-12".pricing."openai"]
cache_read_input_token_cost = 0.0000075
input_cost_per_token = 0.000015
output_cost_per_token = 0.00006

[models."o1-pro"]
display_name = "o1-pro"
model_family = "o-pro"
mode = "responses"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.00015
output_cost_per_token = 0.0006
litellm_provider = "openai"
providers = ["openai", "kilo", "poe"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2023-09"
release_date = "2025-03-19"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_token_batches = 0.000075
output_cost_per_token_batches = 0.0003
supported_endpoints = ["/v1/responses", "/v1/batch"]
supports_native_streaming = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."o1-pro".pricing."kilo"]
input_cost_per_token = 0.00015
output_cost_per_token = 0.0006
[models."o1-pro".pricing."openai"]
input_cost_per_token = 0.00015
input_cost_per_token_batches = 0.000075
output_cost_per_token = 0.0006
output_cost_per_token_batches = 0.0003
[models."o1-pro".pricing."poe"]
input_cost_per_token = 0.00014
output_cost_per_token = 0.00054

[models."o1-pro-2025-03-19"]
mode = "responses"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.00015
output_cost_per_token = 0.0006
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_token_batches = 0.000075
output_cost_per_token_batches = 0.0003
supported_endpoints = ["/v1/responses", "/v1/batch"]
supports_native_streaming = false
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."o1-pro-2025-03-19".pricing."openai"]
input_cost_per_token = 0.00015
input_cost_per_token_batches = 0.000075
output_cost_per_token = 0.0006
output_cost_per_token_batches = 0.0003

[models."o3"]
display_name = "o3"
model_family = "o"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
cache_read_input_token_cost = 5e-7
litellm_provider = "azure"
providers = ["azure", "abacus", "azure-cognitive-services", "cloudflare-ai-gateway", "github-models", "helicone", "jiekou", "kilo", "openai", "poe", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-05"
release_date = "2026-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses", "/v1/completions"]
supports_parallel_function_calling = false
supports_response_schema = true
supports_service_tier = true
supports_tool_choice = true

[models."o3".pricing."abacus"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."o3".pricing."azure"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."o3".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."o3".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."o3".pricing."helicone"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."o3".pricing."jiekou"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00004
[models."o3".pricing."kilo"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."o3".pricing."openai"]
cache_read_input_token_cost = 5e-7
cache_read_input_token_cost_flex = 2.5e-7
cache_read_input_token_cost_priority = 8.75e-7
input_cost_per_token = 0.000002
input_cost_per_token_flex = 0.000001
input_cost_per_token_priority = 0.0000035
output_cost_per_token = 0.000008
output_cost_per_token_flex = 0.000004
output_cost_per_token_priority = 0.000014
[models."o3".pricing."poe"]
cache_read_input_token_cost = 4.5000000000000003e-7
input_cost_per_token = 0.0000018000000000000001
output_cost_per_token = 0.0000072000000000000005
[models."o3".pricing."vercel"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."o3".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008

[models."o3-2025-04-16"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
cache_read_input_token_cost = 5e-7
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
deprecation_date = "2026-04-16"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses", "/v1/completions"]
supports_parallel_function_calling = false
supports_response_schema = true
supports_service_tier = true
supports_tool_choice = true

[models."o3-2025-04-16".pricing."azure"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."o3-2025-04-16".pricing."azure/us"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000022
output_cost_per_token = 0.0000088
[models."o3-2025-04-16".pricing."openai"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008

[models."o3-deep-research"]
display_name = "o3-deep-research"
model_family = "o"
mode = "responses"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.00001
output_cost_per_token = 0.00004
cache_read_input_token_cost = 0.0000025
litellm_provider = "azure"
providers = ["azure", "kilo", "openai", "poe", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-06-26"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true
supports_web_search = true

[models."o3-deep-research".pricing."azure"]
cache_read_input_token_cost = 0.0000025
input_cost_per_token = 0.00001
output_cost_per_token = 0.00004
[models."o3-deep-research".pricing."kilo"]
cache_read_input_token_cost = 0.0000025
input_cost_per_token = 0.00001
output_cost_per_token = 0.00004
[models."o3-deep-research".pricing."openai"]
cache_read_input_token_cost = 0.0000025
input_cost_per_token = 0.00001
input_cost_per_token_batches = 0.000005
output_cost_per_token = 0.00004
output_cost_per_token_batches = 0.00002
[models."o3-deep-research".pricing."poe"]
cache_read_input_token_cost = 0.0000022
input_cost_per_token = 0.000009
output_cost_per_token = 0.000036
[models."o3-deep-research".pricing."vercel"]
cache_read_input_token_cost = 0.0000025
input_cost_per_token = 0.00001
output_cost_per_token = 0.00004

[models."o3-deep-research-2025-06-26"]
mode = "responses"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.00001
output_cost_per_token = 0.00004
cache_read_input_token_cost = 0.0000025
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_token_batches = 0.000005
output_cost_per_token_batches = 0.00002
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."o3-deep-research-2025-06-26".pricing."openai"]
cache_read_input_token_cost = 0.0000025
input_cost_per_token = 0.00001
input_cost_per_token_batches = 0.000005
output_cost_per_token = 0.00004
output_cost_per_token_batches = 0.00002

[models."o3-mini"]
display_name = "o3-mini"
model_family = "o"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
cache_read_input_token_cost = 5.5e-7
litellm_provider = "azure"
providers = ["azure", "abacus", "azure-cognitive-services", "cloudflare-ai-gateway", "github-models", "helicone", "jiekou", "kilo", "openai", "openrouter", "poe", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = false
supports_reasoning = true
supports_prompt_caching = true
open_weights = false
knowledge_cutoff = "2024-05"
release_date = "2026-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."o3-mini".pricing."abacus"]
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o3-mini".pricing."azure"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o3-mini".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o3-mini".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o3-mini".pricing."helicone"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o3-mini".pricing."jiekou"]
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o3-mini".pricing."kilo"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o3-mini".pricing."openai"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o3-mini".pricing."openrouter"]
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o3-mini".pricing."poe"]
input_cost_per_token = 9.9e-7
output_cost_per_token = 0.000004
[models."o3-mini".pricing."vercel"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o3-mini".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044

[models."o3-mini-2025-01-31"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.00000121
output_cost_per_token = 0.00000484
cache_read_input_token_cost = 6.05e-7
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = false
supports_reasoning = true
supports_prompt_caching = true
input_cost_per_token_batches = 6.05e-7
output_cost_per_token_batches = 0.00000242
supports_response_schema = true
supports_tool_choice = true

[models."o3-mini-2025-01-31".pricing."azure"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o3-mini-2025-01-31".pricing."azure/eu"]
cache_read_input_token_cost = 6.05e-7
input_cost_per_token = 0.00000121
input_cost_per_token_batches = 6.05e-7
output_cost_per_token = 0.00000484
output_cost_per_token_batches = 0.00000242
[models."o3-mini-2025-01-31".pricing."azure/us"]
cache_read_input_token_cost = 6.05e-7
input_cost_per_token = 0.00000121
input_cost_per_token_batches = 6.05e-7
output_cost_per_token = 0.00000484
output_cost_per_token_batches = 0.00000242
[models."o3-mini-2025-01-31".pricing."openai"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044

[models."o3-mini-high"]
display_name = "OpenAI: o3 Mini High"
model_family = "o-mini"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "poe"]
supports_function_calling = true
supports_vision = false
supports_reasoning = true
open_weights = false
release_date = "2025-01-31"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_tool_choice = true

[models."o3-mini-high".pricing."kilo"]
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o3-mini-high".pricing."openrouter"]
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o3-mini-high".pricing."poe"]
input_cost_per_token = 9.9e-7
output_cost_per_token = 0.000004

[models."o3-pro"]
display_name = "o3-pro"
model_family = "o-pro"
mode = "responses"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.00002
output_cost_per_token = 0.00008
litellm_provider = "azure"
providers = ["azure", "abacus", "cloudflare-ai-gateway", "helicone", "kilo", "openai", "poe", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-05"
release_date = "2025-06-10"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_token_batches = 0.00001
output_cost_per_token_batches = 0.00004
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_parallel_function_calling = false
supports_response_schema = true
supports_tool_choice = true

[models."o3-pro".pricing."abacus"]
input_cost_per_token = 0.00002
output_cost_per_token = 0.00008
[models."o3-pro".pricing."azure"]
input_cost_per_token = 0.00002
input_cost_per_token_batches = 0.00001
output_cost_per_token = 0.00008
output_cost_per_token_batches = 0.00004
[models."o3-pro".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 0.00002
output_cost_per_token = 0.00008
[models."o3-pro".pricing."helicone"]
input_cost_per_token = 0.00002
output_cost_per_token = 0.00008
[models."o3-pro".pricing."kilo"]
input_cost_per_token = 0.00002
output_cost_per_token = 0.00008
[models."o3-pro".pricing."openai"]
input_cost_per_token = 0.00002
input_cost_per_token_batches = 0.00001
output_cost_per_token = 0.00008
output_cost_per_token_batches = 0.00004
[models."o3-pro".pricing."poe"]
input_cost_per_token = 0.000018
output_cost_per_token = 0.000072
[models."o3-pro".pricing."vercel"]
input_cost_per_token = 0.00002
output_cost_per_token = 0.00008

[models."o3-pro-2025-06-10"]
mode = "responses"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.00002
output_cost_per_token = 0.00008
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_token_batches = 0.00001
output_cost_per_token_batches = 0.00004
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_parallel_function_calling = false
supports_response_schema = true
supports_tool_choice = true

[models."o3-pro-2025-06-10".pricing."azure"]
input_cost_per_token = 0.00002
input_cost_per_token_batches = 0.00001
output_cost_per_token = 0.00008
output_cost_per_token_batches = 0.00004
[models."o3-pro-2025-06-10".pricing."openai"]
input_cost_per_token = 0.00002
input_cost_per_token_batches = 0.00001
output_cost_per_token = 0.00008
output_cost_per_token_batches = 0.00004

[models."o4-mini"]
display_name = "o4-mini"
model_family = "o"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
cache_read_input_token_cost = 2.75e-7
litellm_provider = "azure"
providers = ["azure", "abacus", "aihubmix", "azure-cognitive-services", "cloudflare-ai-gateway", "github-models", "helicone", "jiekou", "kilo", "openai", "openrouter", "poe", "replicate", "requesty", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-05"
release_date = "2026-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_parallel_function_calling = false
supports_response_schema = true
supports_service_tier = true
supports_system_messages = true
supports_tool_choice = true

[models."o4-mini".pricing."abacus"]
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o4-mini".pricing."aihubmix"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.0000015
output_cost_per_token = 0.000006
[models."o4-mini".pricing."azure"]
cache_read_input_token_cost = 2.75e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o4-mini".pricing."azure-cognitive-services"]
cache_read_input_token_cost = 2.8e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o4-mini".pricing."cloudflare-ai-gateway"]
cache_read_input_token_cost = 2.8e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o4-mini".pricing."helicone"]
cache_read_input_token_cost = 2.75e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o4-mini".pricing."jiekou"]
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o4-mini".pricing."kilo"]
cache_read_input_token_cost = 2.75e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o4-mini".pricing."openai"]
cache_read_input_token_cost = 2.75e-7
cache_read_input_token_cost_flex = 1.375e-7
cache_read_input_token_cost_priority = 5e-7
input_cost_per_token = 0.0000011
input_cost_per_token_flex = 5.5e-7
input_cost_per_token_priority = 0.000002
output_cost_per_token = 0.0000044
output_cost_per_token_flex = 0.0000022
output_cost_per_token_priority = 0.000008
[models."o4-mini".pricing."openrouter"]
cache_read_input_token_cost = 2.8e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o4-mini".pricing."poe"]
cache_read_input_token_cost = 2.5e-7
input_cost_per_token = 9.9e-7
output_cost_per_token = 0.000004
[models."o4-mini".pricing."replicate"]
input_cost_per_token = 0.000001
output_cost_per_reasoning_token = 0.000004
output_cost_per_token = 0.000004
[models."o4-mini".pricing."requesty"]
cache_read_input_token_cost = 2.8e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o4-mini".pricing."vercel"]
cache_read_input_token_cost = 2.8e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o4-mini".pricing."vercel_ai_gateway"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 2.75e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044

[models."o4-mini-2025-04-16"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
cache_read_input_token_cost = 2.75e-7
litellm_provider = "azure"
providers = ["azure", "openai"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_parallel_function_calling = false
supports_response_schema = true
supports_service_tier = true
supports_tool_choice = true

[models."o4-mini-2025-04-16".pricing."azure"]
cache_read_input_token_cost = 2.75e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
[models."o4-mini-2025-04-16".pricing."azure/us"]
cache_read_input_token_cost = 3.1e-7
input_cost_per_token = 0.00000121
output_cost_per_token = 0.00000484
[models."o4-mini-2025-04-16".pricing."openai"]
cache_read_input_token_cost = 2.75e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044

[models."o4-mini-deep-research"]
display_name = "o4-mini-deep-research"
model_family = "o-mini"
mode = "responses"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
cache_read_input_token_cost = 5e-7
litellm_provider = "openai"
providers = ["openai", "kilo", "poe"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-05"
release_date = "2024-06-26"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_token_batches = 0.000001
output_cost_per_token_batches = 0.000004
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."o4-mini-deep-research".pricing."kilo"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."o4-mini-deep-research".pricing."openai"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
output_cost_per_token = 0.000008
output_cost_per_token_batches = 0.000004
[models."o4-mini-deep-research".pricing."poe"]
cache_read_input_token_cost = 4.5000000000000003e-7
input_cost_per_token = 0.0000018000000000000001
output_cost_per_token = 0.0000072000000000000005

[models."o4-mini-deep-research-2025-06-26"]
mode = "responses"
max_input_tokens = 200000
max_output_tokens = 100000
max_tokens = 100000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
cache_read_input_token_cost = 5e-7
litellm_provider = "openai"
providers = ["openai"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
input_cost_per_token_batches = 0.000001
output_cost_per_token_batches = 0.000004
supported_endpoints = ["/v1/chat/completions", "/v1/batch", "/v1/responses"]
supports_native_streaming = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."o4-mini-deep-research-2025-06-26".pricing."openai"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000002
input_cost_per_token_batches = 0.000001
output_cost_per_token = 0.000008
output_cost_per_token_batches = 0.000004

[models."olafangensan-glm-4-7-flash-heretic"]
display_name = "GLM 4.7 Flash Heretic"
model_family = "glm-flash"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
input_cost_per_token = 1.4e-7
output_cost_per_token = 8.000000000000001e-7
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-02-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."olafangensan-glm-4-7-flash-heretic".pricing."venice"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 8.000000000000001e-7

[models."omni-moderation-2024-09-26"]
mode = "moderation"
max_input_tokens = 32768
max_output_tokens = 0
max_tokens = 0
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "openai"
providers = ["openai"]

[models."omni-moderation-2024-09-26".pricing."openai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."omni-moderation-latest"]
mode = "moderation"
max_input_tokens = 32768
max_output_tokens = 0
max_tokens = 0
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "openai"
providers = ["openai"]

[models."omni-moderation-latest".pricing."openai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."omni-moderation-latest-intents"]
mode = "moderation"
max_input_tokens = 32768
max_output_tokens = 0
max_tokens = 0
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "openai"
providers = ["openai"]

[models."omni-moderation-latest-intents".pricing."openai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."open-codestral-mamba"]
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 2.5e-7
output_cost_per_token = 2.5e-7
litellm_provider = "mistral"
providers = ["mistral"]
source = "https://mistral.ai/technology/"
supports_assistant_prefill = true
supports_tool_choice = true

[models."open-codestral-mamba".pricing."mistral"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 2.5e-7

[models."open-mistral-7b"]
display_name = "Mistral 7B"
model_family = "mistral"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 2.5e-7
output_cost_per_token = 2.5e-7
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-12"
release_date = "2023-09-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."open-mistral-7b".pricing."mistral"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 2.5e-7

[models."open-mistral-nemo"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 3e-7
output_cost_per_token = 3e-7
litellm_provider = "mistral"
providers = ["mistral"]
source = "https://mistral.ai/technology/"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."open-mistral-nemo".pricing."mistral"]
input_cost_per_token = 3e-7
output_cost_per_token = 3e-7

[models."open-mistral-nemo-2407"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 3e-7
output_cost_per_token = 3e-7
litellm_provider = "mistral"
providers = ["mistral"]
source = "https://mistral.ai/technology/"
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."open-mistral-nemo-2407".pricing."mistral"]
input_cost_per_token = 3e-7
output_cost_per_token = 3e-7

[models."open-mixtral-8x22b"]
display_name = "Mixtral 8x22B"
model_family = "mixtral"
mode = "chat"
max_input_tokens = 65336
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-04-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."open-mixtral-8x22b".pricing."mistral"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006

[models."open-mixtral-8x7b"]
display_name = "Mixtral 8x7B"
model_family = "mixtral"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8191
max_tokens = 8191
input_cost_per_token = 7e-7
output_cost_per_token = 7e-7
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-01"
release_date = "2023-12-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."open-mixtral-8x7b".pricing."mistral"]
input_cost_per_token = 7e-7
output_cost_per_token = 7e-7

[models."openai-gpt-4o"]
mode = "chat"
max_tokens = 16384
litellm_provider = "gradient_ai"
providers = ["gradient_ai"]
supported_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions"]
supports_tool_choice = false

[models."openai-gpt-4o-mini"]
mode = "chat"
max_tokens = 16384
litellm_provider = "gradient_ai"
providers = ["gradient_ai"]
supported_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions"]
supports_tool_choice = false

[models."openai-gpt-52"]
display_name = "GPT-5.2"
model_family = "gpt"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 64000
input_cost_per_token = 0.0000021899999999999998
output_cost_per_token = 0.0000175
cache_read_input_token_cost = 2.19e-7
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-08-31"
release_date = "2025-12-13"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."openai-gpt-52".pricing."venice"]
cache_read_input_token_cost = 2.19e-7
input_cost_per_token = 0.0000021899999999999998
output_cost_per_token = 0.0000175

[models."openai-gpt-52-codex"]
display_name = "GPT-5.2 Codex"
model_family = "gpt-codex"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 64000
input_cost_per_token = 0.0000021899999999999998
output_cost_per_token = 0.0000175
cache_read_input_token_cost = 2.19e-7
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-08"
release_date = "2025-01-15"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."openai-gpt-52-codex".pricing."venice"]
cache_read_input_token_cost = 2.19e-7
input_cost_per_token = 0.0000021899999999999998
output_cost_per_token = 0.0000175

[models."openai-gpt-oss-120b"]
display_name = "OpenAI GPT OSS 120B"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
input_cost_per_token = 7e-8
output_cost_per_token = 3e-7
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2025-11-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."openai-gpt-oss-120b".pricing."venice"]
input_cost_per_token = 7e-8
output_cost_per_token = 3e-7

[models."openai-o3"]
mode = "chat"
max_tokens = 100000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
litellm_provider = "gradient_ai"
providers = ["gradient_ai"]
supported_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions"]
supports_tool_choice = false

[models."openai-o3".pricing."gradient_ai"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008

[models."openai-o3-mini"]
mode = "chat"
max_tokens = 100000
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044
litellm_provider = "gradient_ai"
providers = ["gradient_ai"]
supported_modalities = ["text"]
supported_endpoints = ["/v1/chat/completions"]
supports_tool_choice = false

[models."openai-o3-mini".pricing."gradient_ai"]
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000044

[models."openai.gpt-oss-120b-1:0"]
display_name = "gpt-oss-120b"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_tool_choice = true

[models."openai.gpt-oss-120b-1:0".pricing."amazon-bedrock"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."openai.gpt-oss-120b-1:0".pricing."bedrock_converse"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."openai.gpt-oss-20b-1:0"]
display_name = "gpt-oss-20b"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 7e-8
output_cost_per_token = 3e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_tool_choice = true

[models."openai.gpt-oss-20b-1:0".pricing."amazon-bedrock"]
input_cost_per_token = 7e-8
output_cost_per_token = 3e-7
[models."openai.gpt-oss-20b-1:0".pricing."bedrock_converse"]
input_cost_per_token = 7e-8
output_cost_per_token = 3e-7

[models."openai.gpt-oss-safeguard-120b"]
display_name = "GPT OSS Safeguard 120B"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."openai.gpt-oss-safeguard-120b".pricing."amazon-bedrock"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."openai.gpt-oss-safeguard-120b".pricing."bedrock_converse"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."openai.gpt-oss-safeguard-20b"]
display_name = "GPT OSS Safeguard 20B"
model_family = "gpt-oss"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 7e-8
output_cost_per_token = 2e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."openai.gpt-oss-safeguard-20b".pricing."amazon-bedrock"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.0000000000000002e-7
[models."openai.gpt-oss-safeguard-20b".pricing."bedrock_converse"]
input_cost_per_token = 7e-8
output_cost_per_token = 2e-7

[models."openthinker-7b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 8e-8
output_cost_per_token = 1.5e-7
litellm_provider = "llamagate"
providers = ["llamagate"]
supports_function_calling = true
supports_reasoning = true
supports_response_schema = true

[models."openthinker-7b".pricing."llamagate"]
input_cost_per_token = 8e-8
output_cost_per_token = 1.5e-7

[models."orca-mini"]
mode = "completion"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]

[models."orca-mini".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."osmosis/osmosis-structure-0-6b"]
display_name = "Osmosis Structure 0.6B"
model_family = "osmosis"
mode = "chat"
max_input_tokens = 4000
max_output_tokens = 2048
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 5e-7
litellm_provider = "inference"
providers = ["inference"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."osmosis/osmosis-structure-0-6b".pricing."inference"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 5e-7

[models."outpaint"]
mode = "image_edit"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.004
supported_endpoints = ["/v1/images/edits"]

[models."outpaint".pricing."stability"]
output_cost_per_image = 0.004

[models."paddlepaddle/paddleocr-vl"]
display_name = "PaddleOCR-VL"
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2e-8
output_cost_per_token = 2e-8
litellm_provider = "novita"
providers = ["novita", "novita-ai", "siliconflow-cn"]
supports_function_calling = false
supports_vision = true
supports_reasoning = false
open_weights = true
release_date = "2025-10-22"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."paddlepaddle/paddleocr-vl".pricing."novita"]
input_cost_per_token = 2e-8
output_cost_per_token = 2e-8
[models."paddlepaddle/paddleocr-vl".pricing."novita-ai"]
input_cost_per_token = 2e-8
output_cost_per_token = 2e-8

[models."phi-4-mini"]
display_name = "Phi-4-mini"
model_family = "phi"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-12-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."phi-4-mini".pricing."azure"]
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7
[models."phi-4-mini".pricing."azure-cognitive-services"]
input_cost_per_token = 7.5e-8
output_cost_per_token = 3e-7

[models."phi-4-multimodal"]
display_name = "Phi-4-multimodal"
model_family = "phi"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
input_cost_per_token = 8e-8
output_cost_per_token = 3.2e-7
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-12-11"
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."phi-4-multimodal".pricing."azure"]
input_cost_per_token = 8e-8
output_cost_per_token = 3.2e-7
[models."phi-4-multimodal".pricing."azure-cognitive-services"]
input_cost_per_token = 8e-8
output_cost_per_token = 3.2e-7

[models."phi-4-reasoning-plus"]
display_name = "Phi-4-reasoning-plus"
model_family = "phi"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 4096
input_cost_per_token = 1.25e-7
output_cost_per_token = 5e-7
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2024-12-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."phi-4-reasoning-plus".pricing."azure"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 5e-7
[models."phi-4-reasoning-plus".pricing."azure-cognitive-services"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 5e-7

[models."pixtral-12b"]
display_name = "Pixtral 12B"
model_family = "pixtral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 4000
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "mistral", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-09"
release_date = "2024-09-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_tool_choice = true

[models."pixtral-12b".pricing."mistral"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."pixtral-12b".pricing."vercel"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."pixtral-12b".pricing."vercel_ai_gateway"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7

[models."pixtral-12b-2409"]
display_name = "Pixtral 12B 2409"
model_family = "pixtral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "mistral"
providers = ["mistral", "scaleway"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
release_date = "2024-09-25"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."pixtral-12b-2409".pricing."mistral"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
[models."pixtral-12b-2409".pricing."scaleway"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7

[models."pixtral-large"]
display_name = "Pixtral Large"
model_family = "pixtral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4000
max_tokens = 4000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-11"
release_date = "2024-11-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_tool_choice = true

[models."pixtral-large".pricing."vercel"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
[models."pixtral-large".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006

[models."pixtral-large-2411"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_vision = true
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."pixtral-large-2411".pricing."mistral"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006

[models."pixtral-large-latest"]
display_name = "Pixtral Large"
model_family = "pixtral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
litellm_provider = "mistral"
providers = ["mistral"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-11"
release_date = "2024-11-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."pixtral-large-latest".pricing."mistral"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006

[models."playai-tts"]
mode = "audio_speech"
max_input_tokens = 10000
max_output_tokens = 10000
max_tokens = 10000
litellm_provider = "groq"
providers = ["groq"]
input_cost_per_character = 0.00005

[models."playai-tts".pricing."groq"]
input_cost_per_character = 0.00005

[models."pplx-70b-chat"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000028
litellm_provider = "perplexity"
providers = ["perplexity"]

[models."pplx-70b-chat".pricing."perplexity"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000028

[models."pplx-70b-online"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0.0000028
litellm_provider = "perplexity"
providers = ["perplexity"]
input_cost_per_request = 0.005

[models."pplx-70b-online".pricing."perplexity"]
input_cost_per_request = 0.005
input_cost_per_token = 0
output_cost_per_token = 0.0000028

[models."pplx-7b-chat"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
litellm_provider = "perplexity"
providers = ["perplexity"]

[models."pplx-7b-chat".pricing."perplexity"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7

[models."pplx-7b-online"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 2.8e-7
litellm_provider = "perplexity"
providers = ["perplexity"]
input_cost_per_request = 0.005

[models."pplx-7b-online".pricing."perplexity"]
input_cost_per_request = 0.005
input_cost_per_token = 0
output_cost_per_token = 2.8e-7

[models."preset/advanced-deep-research"]
mode = "responses"
litellm_provider = "perplexity"
providers = ["perplexity"]
supports_function_calling = true
supports_preset = true
supports_web_search = true

[models."preset/deep-research"]
mode = "responses"
litellm_provider = "perplexity"
providers = ["perplexity"]
supports_function_calling = true
supports_preset = true
supports_web_search = true

[models."preset/fast-search"]
mode = "responses"
litellm_provider = "perplexity"
providers = ["perplexity"]
supports_function_calling = true
supports_preset = true
supports_web_search = true

[models."preset/pro-search"]
mode = "responses"
litellm_provider = "perplexity"
providers = ["perplexity"]
supports_function_calling = true
supports_preset = true
supports_web_search = true

[models."prime-intellect/intellect-3"]
display_name = "INTELLECT 3"
model_family = "intellect"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000011
litellm_provider = "vercel"
providers = ["vercel", "kilo", "openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-11-26"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."prime-intellect/intellect-3".pricing."kilo"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000011
[models."prime-intellect/intellect-3".pricing."openrouter"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000011
[models."prime-intellect/intellect-3".pricing."vercel"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000011

[models."qvq-max"]
display_name = "QVQ Max"
model_family = "qvq"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000048
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-03-25"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qvq-max".pricing."alibaba"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.0000048
[models."qvq-max".pricing."alibaba-cn"]
input_cost_per_token = 0.000001147
output_cost_per_token = 0.000004588

[models."qwen-2-5-coder-32b"]
display_name = "Qwen 2.5 Coder 32B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 7.900000000000001e-7
output_cost_per_token = 7.900000000000001e-7
litellm_provider = "abacus"
providers = ["abacus"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-11-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen-2-5-coder-32b".pricing."abacus"]
input_cost_per_token = 7.900000000000001e-7
output_cost_per_token = 7.900000000000001e-7

[models."qwen-3-235b-a22b-instruct-2507"]
display_name = "Qwen 3 235B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 32000
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000012
litellm_provider = "cerebras"
providers = ["cerebras"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen-3-235b-a22b-instruct-2507".pricing."cerebras"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000012

[models."qwen-3-32b"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 4e-7
output_cost_per_token = 8e-7
litellm_provider = "cerebras"
providers = ["cerebras"]
supports_function_calling = true
supports_reasoning = true
source = "https://inference-docs.cerebras.ai/support/pricing"
supports_tool_choice = true

[models."qwen-3-32b".pricing."cerebras"]
input_cost_per_token = 4e-7
output_cost_per_token = 8e-7

[models."qwen-coder"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015
litellm_provider = "dashscope"
providers = ["dashscope"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true

[models."qwen-coder".pricing."dashscope"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015

[models."qwen-deep-research"]
display_name = "Qwen Deep Research"
model_family = "qwen"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 32768
input_cost_per_token = 0.000007742
output_cost_per_token = 0.000023367000000000002
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen-deep-research".pricing."alibaba-cn"]
input_cost_per_token = 0.000007742
output_cost_per_token = 0.000023367000000000002

[models."qwen-doc-turbo"]
display_name = "Qwen Doc Turbo"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 8.7e-8
output_cost_per_token = 1.44e-7
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen-doc-turbo".pricing."alibaba-cn"]
input_cost_per_token = 8.7e-8
output_cost_per_token = 1.44e-7

[models."qwen-flash"]
display_name = "Qwen-Flash"
model_family = "qwen"
mode = "chat"
max_input_tokens = 997952
max_output_tokens = 32768
max_tokens = 32768
litellm_provider = "dashscope"
providers = ["dashscope", "302ai", "alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-07-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 5e-8, output_cost_per_token = 4e-7, range = [0, 256000] }, { input_cost_per_token = 2.5e-7, output_cost_per_token = 0.000002, range = [256000, 1000000] }]

[models."qwen-flash".pricing."302ai"]
input_cost_per_token = 2.2e-8
output_cost_per_token = 2.2e-7
[models."qwen-flash".pricing."alibaba"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 4.0000000000000003e-7
[models."qwen-flash".pricing."alibaba-cn"]
input_cost_per_token = 2.2e-8
output_cost_per_token = 2.16e-7

[models."qwen-flash-2025-07-28"]
mode = "chat"
max_input_tokens = 997952
max_output_tokens = 32768
max_tokens = 32768
litellm_provider = "dashscope"
providers = ["dashscope"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 5e-8, output_cost_per_token = 4e-7, range = [0, 256000] }, { input_cost_per_token = 2.5e-7, output_cost_per_token = 0.000002, range = [256000, 1000000] }]

[models."qwen-long"]
display_name = "Qwen Long"
model_family = "qwen"
mode = "chat"
max_input_tokens = 10000000
max_output_tokens = 8192
input_cost_per_token = 7.2e-8
output_cost_per_token = 2.8699999999999996e-7
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-01-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen-long".pricing."alibaba-cn"]
input_cost_per_token = 7.2e-8
output_cost_per_token = 2.8699999999999996e-7

[models."qwen-math-plus"]
display_name = "Qwen Math Plus"
model_family = "qwen"
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 3072
input_cost_per_token = 5.739999999999999e-7
output_cost_per_token = 0.0000017210000000000001
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-08-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen-math-plus".pricing."alibaba-cn"]
input_cost_per_token = 5.739999999999999e-7
output_cost_per_token = 0.0000017210000000000001

[models."qwen-math-turbo"]
display_name = "Qwen Math Turbo"
model_family = "qwen"
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 3072
input_cost_per_token = 2.8699999999999996e-7
output_cost_per_token = 8.61e-7
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-09-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen-math-turbo".pricing."alibaba-cn"]
input_cost_per_token = 2.8699999999999996e-7
output_cost_per_token = 8.61e-7

[models."qwen-max"]
display_name = "Qwen Max"
model_family = "qwen"
mode = "chat"
max_input_tokens = 30720
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.0000016
output_cost_per_token = 0.0000064
litellm_provider = "dashscope"
providers = ["dashscope", "alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true

[models."qwen-max".pricing."alibaba"]
input_cost_per_token = 0.0000016000000000000001
output_cost_per_token = 0.0000064000000000000006
[models."qwen-max".pricing."alibaba-cn"]
input_cost_per_token = 3.45e-7
output_cost_per_token = 0.000001377
[models."qwen-max".pricing."dashscope"]
input_cost_per_token = 0.0000016
output_cost_per_token = 0.0000064

[models."qwen-max-latest"]
display_name = "Qwen-Max-Latest"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 3.4300000000000004e-7
output_cost_per_token = 0.0000013720000000000002
litellm_provider = "302ai"
providers = ["302ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2024-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen-max-latest".pricing."302ai"]
input_cost_per_token = 3.4300000000000004e-7
output_cost_per_token = 0.0000013720000000000002

[models."qwen-mt-plus"]
display_name = "Qwen-MT Plus"
model_family = "qwen"
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 8192
input_cost_per_token = 0.00000246
output_cost_per_token = 0.00000737
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen-mt-plus".pricing."alibaba"]
input_cost_per_token = 0.00000246
output_cost_per_token = 0.00000737
[models."qwen-mt-plus".pricing."alibaba-cn"]
input_cost_per_token = 2.5900000000000003e-7
output_cost_per_token = 7.75e-7

[models."qwen-mt-turbo"]
display_name = "Qwen-MT Turbo"
model_family = "qwen"
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 8192
input_cost_per_token = 1.6e-7
output_cost_per_token = 4.9e-7
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen-mt-turbo".pricing."alibaba"]
input_cost_per_token = 1.6e-7
output_cost_per_token = 4.9e-7
[models."qwen-mt-turbo".pricing."alibaba-cn"]
input_cost_per_token = 1.01e-7
output_cost_per_token = 2.8e-7

[models."qwen-omni-turbo"]
display_name = "Qwen-Omni Turbo"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 2048
input_cost_per_token = 7e-8
output_cost_per_token = 2.7e-7
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-01-19"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
source = "modelsdev"

[models."qwen-omni-turbo".pricing."alibaba"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.7e-7
[models."qwen-omni-turbo".pricing."alibaba-cn"]
input_cost_per_token = 5.8e-8
output_cost_per_token = 2.3000000000000002e-7

[models."qwen-omni-turbo-realtime"]
display_name = "Qwen-Omni Turbo Realtime"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 2048
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.0000010700000000000001
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-05-08"
supported_modalities = ["text", "image", "audio"]
supported_output_modalities = ["text", "audio"]
source = "modelsdev"

[models."qwen-omni-turbo-realtime".pricing."alibaba"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 0.0000010700000000000001
[models."qwen-omni-turbo-realtime".pricing."alibaba-cn"]
input_cost_per_token = 2.3000000000000002e-7
output_cost_per_token = 9.18e-7

[models."qwen-plus"]
display_name = "Qwen-Plus"
model_family = "qwen"
mode = "chat"
max_input_tokens = 129024
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000012
litellm_provider = "dashscope"
providers = ["dashscope", "302ai", "alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://www.alibabacloud.com/help/en/model-studio/models"
reasoning_cost_per_token = 0.000004
supports_tool_choice = true

[models."qwen-plus".pricing."302ai"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 0.0000012
[models."qwen-plus".pricing."alibaba"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000012
reasoning_cost_per_token = 0.000004
[models."qwen-plus".pricing."alibaba-cn"]
input_cost_per_token = 1.1500000000000001e-7
output_cost_per_token = 2.8699999999999996e-7
reasoning_cost_per_token = 0.000001147
[models."qwen-plus".pricing."dashscope"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000012

[models."qwen-plus-2025-01-25"]
mode = "chat"
max_input_tokens = 129024
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000012
litellm_provider = "dashscope"
providers = ["dashscope"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true

[models."qwen-plus-2025-01-25".pricing."dashscope"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000012

[models."qwen-plus-2025-04-28"]
mode = "chat"
max_input_tokens = 129024
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000012
litellm_provider = "dashscope"
providers = ["dashscope"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.alibabacloud.com/help/en/model-studio/models"
output_cost_per_reasoning_token = 0.000004
supports_tool_choice = true

[models."qwen-plus-2025-04-28".pricing."dashscope"]
input_cost_per_token = 4e-7
output_cost_per_reasoning_token = 0.000004
output_cost_per_token = 0.0000012

[models."qwen-plus-2025-07-14"]
mode = "chat"
max_input_tokens = 129024
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000012
litellm_provider = "dashscope"
providers = ["dashscope"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.alibabacloud.com/help/en/model-studio/models"
output_cost_per_reasoning_token = 0.000004
supports_tool_choice = true

[models."qwen-plus-2025-07-14".pricing."dashscope"]
input_cost_per_token = 4e-7
output_cost_per_reasoning_token = 0.000004
output_cost_per_token = 0.0000012

[models."qwen-plus-2025-07-28"]
mode = "chat"
max_input_tokens = 997952
max_output_tokens = 32768
max_tokens = 32768
litellm_provider = "dashscope"
providers = ["dashscope"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 4e-7, output_cost_per_reasoning_token = 0.000004, output_cost_per_token = 0.0000012, range = [0, 256000] }, { input_cost_per_token = 0.0000012, output_cost_per_reasoning_token = 0.000012, output_cost_per_token = 0.0000036, range = [256000, 1000000] }]

[models."qwen-plus-2025-09-11"]
mode = "chat"
max_input_tokens = 997952
max_output_tokens = 32768
max_tokens = 32768
litellm_provider = "dashscope"
providers = ["dashscope"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 4e-7, output_cost_per_reasoning_token = 0.000004, output_cost_per_token = 0.0000012, range = [0, 256000] }, { input_cost_per_token = 0.0000012, output_cost_per_reasoning_token = 0.000012, output_cost_per_token = 0.0000036, range = [256000, 1000000] }]

[models."qwen-plus-character"]
display_name = "Qwen Plus Character"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 4096
input_cost_per_token = 1.1500000000000001e-7
output_cost_per_token = 2.8699999999999996e-7
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen-plus-character".pricing."alibaba-cn"]
input_cost_per_token = 1.1500000000000001e-7
output_cost_per_token = 2.8699999999999996e-7

[models."qwen-plus-character-ja"]
display_name = "Qwen Plus Character (Japanese)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 512
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000014
litellm_provider = "alibaba"
providers = ["alibaba"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen-plus-character-ja".pricing."alibaba"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000014

[models."qwen-plus-latest"]
mode = "chat"
max_input_tokens = 997952
max_output_tokens = 32768
max_tokens = 32768
litellm_provider = "dashscope"
providers = ["dashscope"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 4e-7, output_cost_per_reasoning_token = 0.000004, output_cost_per_token = 0.0000012, range = [0, 256000] }, { input_cost_per_token = 0.0000012, output_cost_per_reasoning_token = 0.000012, output_cost_per_token = 0.0000036, range = [256000, 1000000] }]

[models."qwen-qwq-32b"]
display_name = "Qwen QwQ 32B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 16384
input_cost_per_token = 2.9e-7
output_cost_per_token = 3.9e-7
litellm_provider = "groq"
providers = ["groq"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-09"
release_date = "2024-11-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen-qwq-32b".pricing."groq"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 3.9e-7

[models."qwen-turbo"]
display_name = "Qwen Turbo"
model_family = "qwen"
mode = "chat"
max_input_tokens = 129024
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 5e-8
output_cost_per_token = 2e-7
litellm_provider = "dashscope"
providers = ["dashscope", "alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-11-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://www.alibabacloud.com/help/en/model-studio/models"
output_cost_per_reasoning_token = 5e-7
reasoning_cost_per_token = 5e-7
supports_tool_choice = true

[models."qwen-turbo".pricing."alibaba"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.0000000000000002e-7
reasoning_cost_per_token = 5e-7
[models."qwen-turbo".pricing."alibaba-cn"]
input_cost_per_token = 4.4e-8
output_cost_per_token = 8.7e-8
reasoning_cost_per_token = 4.31e-7
[models."qwen-turbo".pricing."dashscope"]
input_cost_per_token = 5e-8
output_cost_per_reasoning_token = 5e-7
output_cost_per_token = 2e-7

[models."qwen-turbo-2024-11-01"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5e-8
output_cost_per_token = 2e-7
litellm_provider = "dashscope"
providers = ["dashscope"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true

[models."qwen-turbo-2024-11-01".pricing."dashscope"]
input_cost_per_token = 5e-8
output_cost_per_token = 2e-7

[models."qwen-turbo-2025-04-28"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 5e-8
output_cost_per_token = 2e-7
litellm_provider = "dashscope"
providers = ["dashscope"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.alibabacloud.com/help/en/model-studio/models"
output_cost_per_reasoning_token = 5e-7
supports_tool_choice = true

[models."qwen-turbo-2025-04-28".pricing."dashscope"]
input_cost_per_token = 5e-8
output_cost_per_reasoning_token = 5e-7
output_cost_per_token = 2e-7

[models."qwen-turbo-latest"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 5e-8
output_cost_per_token = 2e-7
litellm_provider = "dashscope"
providers = ["dashscope"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.alibabacloud.com/help/en/model-studio/models"
output_cost_per_reasoning_token = 5e-7
supports_tool_choice = true

[models."qwen-turbo-latest".pricing."dashscope"]
input_cost_per_token = 5e-8
output_cost_per_reasoning_token = 5e-7
output_cost_per_token = 2e-7

[models."qwen-vl-max"]
display_name = "Qwen-VL Max"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.0000032000000000000003
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-04-08"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen-vl-max".pricing."alibaba"]
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.0000032000000000000003
[models."qwen-vl-max".pricing."alibaba-cn"]
input_cost_per_token = 2.3000000000000002e-7
output_cost_per_token = 5.739999999999999e-7

[models."qwen-vl-ocr"]
display_name = "Qwen-VL OCR"
model_family = "qwen"
mode = "chat"
max_input_tokens = 34096
max_output_tokens = 4096
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-10-28"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen-vl-ocr".pricing."alibaba"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7
[models."qwen-vl-ocr".pricing."alibaba-cn"]
input_cost_per_token = 7.17e-7
output_cost_per_token = 7.17e-7

[models."qwen-vl-plus"]
display_name = "Qwen-VL Plus"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 2.1e-7
output_cost_per_token = 6.3e-7
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-01-25"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen-vl-plus".pricing."alibaba"]
input_cost_per_token = 2.1e-7
output_cost_per_token = 6.3e-7
[models."qwen-vl-plus".pricing."alibaba-cn"]
input_cost_per_token = 1.1500000000000001e-7
output_cost_per_token = 2.8699999999999996e-7

[models."qwen.qwen3-235b-a22b-2507-v1:0"]
display_name = "Qwen3 235B A22B 2507"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2025-09-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."qwen.qwen3-235b-a22b-2507-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7
[models."qwen.qwen3-235b-a22b-2507-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7

[models."qwen.qwen3-32b-v1:0"]
display_name = "Qwen3 32B (dense)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2025-09-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."qwen.qwen3-32b-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."qwen.qwen3-32b-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."qwen.qwen3-coder-30b-a3b-v1:0"]
display_name = "Qwen3 Coder 30B A3B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-09-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."qwen.qwen3-coder-30b-a3b-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."qwen.qwen3-coder-30b-a3b-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7

[models."qwen.qwen3-coder-480b-a35b-v1:0"]
display_name = "Qwen3 Coder 480B A35B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262000
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 2.2e-7
output_cost_per_token = 0.0000018
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2025-09-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."qwen.qwen3-coder-480b-a35b-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 0.0000018000000000000001
[models."qwen.qwen3-coder-480b-a35b-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 0.0000018

[models."qwen.qwen3-coder-next"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-7
output_cost_per_token = 0.00000144
litellm_provider = "bedrock"
providers = ["bedrock", "bedrock_converse"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."qwen.qwen3-coder-next".pricing."bedrock/eu-west-1"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.00000144
[models."qwen.qwen3-coder-next".pricing."bedrock/us-east-1"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000012
[models."qwen.qwen3-coder-next".pricing."bedrock/us-west-2"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000012
[models."qwen.qwen3-coder-next".pricing."bedrock_converse"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000012

[models."qwen.qwen3-next-80b-a3b"]
display_name = "Qwen/Qwen3-Next-80B-A3B-Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000012
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-09-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."qwen.qwen3-next-80b-a3b".pricing."amazon-bedrock"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 0.0000014
[models."qwen.qwen3-next-80b-a3b".pricing."bedrock_converse"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000012

[models."qwen.qwen3-vl-235b-a22b"]
display_name = "Qwen/Qwen3-VL-235B-A22B-Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5.3e-7
output_cost_per_token = 0.00000266
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
release_date = "2025-10-04"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."qwen.qwen3-vl-235b-a22b".pricing."amazon-bedrock"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015
[models."qwen.qwen3-vl-235b-a22b".pricing."bedrock_converse"]
input_cost_per_token = 5.3e-7
output_cost_per_token = 0.00000266

[models."qwen/qwen-2-5-7b-instruct"]
display_name = "Qwen: Qwen2.5 7B Instruct"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 6554
input_cost_per_token = 4e-8
output_cost_per_token = 1.0000000000000001e-7
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen-2-5-7b-instruct".pricing."kilo"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.0000000000000001e-7

[models."qwen/qwen-2-5-7b-vision-instruct"]
display_name = "Qwen 2.5 7B Vision Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 125000
max_output_tokens = 4096
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7
litellm_provider = "inference"
providers = ["inference"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2025-01-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen-2-5-7b-vision-instruct".pricing."inference"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7

[models."qwen/qwen-2-5-vl-7b-instruct"]
display_name = "Qwen: Qwen2.5-VL 7B Instruct"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 6554
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2024-08-28"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen-2-5-vl-7b-instruct".pricing."kilo"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 2.0000000000000002e-7

[models."qwen/qwen-2-5-vl-7b-instruct:free"]
display_name = "Qwen2.5-VL 7B Instruct (free)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-02"
release_date = "2024-08-28"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen-2.5-72b-instruct"]
display_name = "Qwen 2.5 72B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3.8e-7
output_cost_per_token = 4e-7
litellm_provider = "novita"
providers = ["novita", "kilo", "novita-ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-10-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen/qwen-2.5-72b-instruct".pricing."kilo"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 3.9e-7
[models."qwen/qwen-2.5-72b-instruct".pricing."novita"]
input_cost_per_token = 3.8e-7
output_cost_per_token = 4e-7
[models."qwen/qwen-2.5-72b-instruct".pricing."novita-ai"]
input_cost_per_token = 3.8e-7
output_cost_per_token = 4.0000000000000003e-7

[models."qwen/qwen-2.5-coder-32b-instruct"]
display_name = "Qwen2.5 Coder 32B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 33792
max_output_tokens = 33792
max_tokens = 33792
input_cost_per_token = 1.8e-7
output_cost_per_token = 1.8e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2024-11-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."qwen/qwen-2.5-coder-32b-instruct".pricing."kilo"]
cache_read_input_token_cost = 1.5e-8
input_cost_per_token = 3e-8
output_cost_per_token = 1.1e-7
[models."qwen/qwen-2.5-coder-32b-instruct".pricing."openrouter"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 1.8e-7

[models."qwen/qwen-max"]
display_name = "Qwen: Qwen-Max "
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
input_cost_per_token = 0.0000016000000000000001
output_cost_per_token = 0.0000064000000000000006
cache_read_input_token_cost = 3.2e-7
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen-max".pricing."kilo"]
cache_read_input_token_cost = 3.2e-7
input_cost_per_token = 0.0000016000000000000001
output_cost_per_token = 0.0000064000000000000006

[models."qwen/qwen-mt-plus"]
display_name = "Qwen MT Plus"
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2.5e-7
output_cost_per_token = 7.5e-7
litellm_provider = "novita"
providers = ["novita", "novita-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-09-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."qwen/qwen-mt-plus".pricing."novita"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 7.5e-7
[models."qwen/qwen-mt-plus".pricing."novita-ai"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 7.5e-7

[models."qwen/qwen-plus"]
display_name = "Qwen: Qwen-Plus"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 32768
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000012
cache_read_input_token_cost = 8e-8
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-01-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen-plus".pricing."kilo"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000012

[models."qwen/qwen-turbo"]
display_name = "Qwen: Qwen-Turbo"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.0000000000000002e-7
cache_read_input_token_cost = 1e-8
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-11-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen-turbo".pricing."kilo"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.0000000000000002e-7

[models."qwen/qwen-vl-max"]
display_name = "Qwen: Qwen VL Max"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.0000032000000000000003
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2024-04-08"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen-vl-max".pricing."kilo"]
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.0000032000000000000003

[models."qwen/qwen-vl-plus"]
display_name = "Qwen: Qwen VL Plus"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 2.1e-7
output_cost_per_token = 6.3e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = false
supports_vision = true
supports_reasoning = false
open_weights = false
release_date = "2024-01-25"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."qwen/qwen-vl-plus".pricing."kilo"]
cache_read_input_token_cost = 4.2000000000000006e-8
input_cost_per_token = 2.1e-7
output_cost_per_token = 6.3e-7
[models."qwen/qwen-vl-plus".pricing."openrouter"]
input_cost_per_token = 2.1e-7
output_cost_per_token = 6.3e-7

[models."qwen/qwen2-5-coder-7b-fast"]
display_name = "Qwen2.5-Coder-7B (Fast)"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 3e-8
output_cost_per_token = 9e-8
cache_read_input_token_cost = 3e-9
litellm_provider = "nebius"
providers = ["nebius"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-09"
release_date = "2024-09-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen2-5-coder-7b-fast".pricing."nebius"]
cache_read_input_token_cost = 3e-9
input_cost_per_token = 3e-8
output_cost_per_token = 9e-8

[models."qwen/qwen2-5-vl-32b-instruct:free"]
display_name = "Qwen2.5 VL 32B Instruct (free)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-03"
release_date = "2025-03-24"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen2-5-vl-72b-instruct:free"]
display_name = "Qwen2.5 VL 72B Instruct (free)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-02"
release_date = "2025-02-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen2.5-7b-instruct"]
display_name = "Qwen2.5 7B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 7e-8
output_cost_per_token = 7e-8
litellm_provider = "novita"
providers = ["novita", "novita-ai", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-04-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen/qwen2.5-7b-instruct".pricing."novita"]
input_cost_per_token = 7e-8
output_cost_per_token = 7e-8
[models."qwen/qwen2.5-7b-instruct".pricing."novita-ai"]
input_cost_per_token = 7e-8
output_cost_per_token = 7e-8
[models."qwen/qwen2.5-7b-instruct".pricing."siliconflow"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 5.0000000000000004e-8
[models."qwen/qwen2.5-7b-instruct".pricing."siliconflow-cn"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 5.0000000000000004e-8

[models."qwen/qwen2.5-vl-72b-instruct"]
display_name = "Qwen2.5-VL-72B-Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 8e-7
output_cost_per_token = 8e-7
litellm_provider = "novita"
providers = ["novita", "kilo", "nebius", "novita-ai", "openrouter", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2025-01-20"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."qwen/qwen2.5-vl-72b-instruct".pricing."kilo"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 6e-7
[models."qwen/qwen2.5-vl-72b-instruct".pricing."nebius"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 2.5e-7
output_cost_per_token = 7.5e-7
[models."qwen/qwen2.5-vl-72b-instruct".pricing."novita"]
input_cost_per_token = 8e-7
output_cost_per_token = 8e-7
[models."qwen/qwen2.5-vl-72b-instruct".pricing."novita-ai"]
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 8.000000000000001e-7
[models."qwen/qwen2.5-vl-72b-instruct".pricing."siliconflow"]
input_cost_per_token = 5.9e-7
output_cost_per_token = 5.9e-7
[models."qwen/qwen2.5-vl-72b-instruct".pricing."siliconflow-cn"]
input_cost_per_token = 5.9e-7
output_cost_per_token = 5.9e-7

[models."qwen/qwen3-14b:free"]
display_name = "Qwen3 14B (free)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 40960
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-235b-a22b-07-25"]
display_name = "Qwen3 235B A22B Instruct 2507"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 131072
input_cost_per_token = 1.5e-7
output_cost_per_token = 8.5e-7
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-235b-a22b-07-25".pricing."openrouter"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 8.5e-7

[models."qwen/qwen3-235b-a22b-07-25:free"]
display_name = "Qwen3 235B A22B Instruct 2507 (free)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 131072
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-235b-a22b-2507"]
display_name = "Qwen: Qwen3 235B A22B Instruct 2507"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 7.1e-8
output_cost_per_token = 1e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://openrouter.ai/qwen/qwen3-235b-a22b-2507"
supports_tool_choice = true

[models."qwen/qwen3-235b-a22b-2507".pricing."kilo"]
input_cost_per_token = 7.099999999999999e-8
output_cost_per_token = 1.0000000000000001e-7
[models."qwen/qwen3-235b-a22b-2507".pricing."openrouter"]
input_cost_per_token = 7.1e-8
output_cost_per_token = 1e-7

[models."qwen/qwen3-235b-a22b-fp8"]
display_name = "Qwen3 235B A22B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 20000
max_tokens = 20000
input_cost_per_token = 2e-7
output_cost_per_token = 8e-7
litellm_provider = "novita"
providers = ["novita", "jiekou", "novita-ai"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."qwen/qwen3-235b-a22b-fp8".pricing."jiekou"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7
[models."qwen/qwen3-235b-a22b-fp8".pricing."novita"]
input_cost_per_token = 2e-7
output_cost_per_token = 8e-7
[models."qwen/qwen3-235b-a22b-fp8".pricing."novita-ai"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7

[models."qwen/qwen3-235b-a22b-instruct-2507"]
display_name = "Qwen3 235B A22B Instruct 2507"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2.64e-7
output_cost_per_token = 0.00000106
litellm_provider = "replicate"
providers = ["replicate", "abacus", "friendli", "jiekou", "meganova", "modelscope", "nebius", "novita", "novita-ai", "siliconflow", "siliconflow-cn", "submodel", "wandb"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen/qwen3-235b-a22b-instruct-2507".pricing."abacus"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 6e-7
[models."qwen/qwen3-235b-a22b-instruct-2507".pricing."friendli"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7
[models."qwen/qwen3-235b-a22b-instruct-2507".pricing."jiekou"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 8.000000000000001e-7
[models."qwen/qwen3-235b-a22b-instruct-2507".pricing."meganova"]
input_cost_per_token = 9e-8
output_cost_per_token = 6e-7
[models."qwen/qwen3-235b-a22b-instruct-2507".pricing."nebius"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7
[models."qwen/qwen3-235b-a22b-instruct-2507".pricing."novita"]
input_cost_per_token = 9e-8
output_cost_per_token = 5.8e-7
[models."qwen/qwen3-235b-a22b-instruct-2507".pricing."novita-ai"]
input_cost_per_token = 9e-8
output_cost_per_token = 5.8e-7
[models."qwen/qwen3-235b-a22b-instruct-2507".pricing."replicate"]
input_cost_per_token = 2.64e-7
output_cost_per_token = 0.00000106
[models."qwen/qwen3-235b-a22b-instruct-2507".pricing."siliconflow"]
input_cost_per_token = 9e-8
output_cost_per_token = 6e-7
[models."qwen/qwen3-235b-a22b-instruct-2507".pricing."siliconflow-cn"]
input_cost_per_token = 9e-8
output_cost_per_token = 6e-7
[models."qwen/qwen3-235b-a22b-instruct-2507".pricing."submodel"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 3e-7
[models."qwen/qwen3-235b-a22b-instruct-2507".pricing."wandb"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 1.0000000000000001e-7

[models."qwen/qwen3-235b-a22b-instruct-2507-maas"]
display_name = "Qwen3 235B A22B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-08-13"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_regions = ["global"]
supports_tool_choice = true

[models."qwen/qwen3-235b-a22b-instruct-2507-maas".pricing."google-vertex"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 8.8e-7
[models."qwen/qwen3-235b-a22b-instruct-2507-maas".pricing."vertex_ai"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001

[models."qwen/qwen3-235b-a22b-thinking-2507"]
display_name = "Qwen3 235B A22b Thinking 2507"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 1.1e-7
output_cost_per_token = 6e-7
litellm_provider = "openrouter"
providers = ["openrouter", "chutes", "huggingface", "io-net", "jiekou", "kilo", "modelscope", "nano-gpt", "nebius", "novita", "novita-ai", "siliconflow", "siliconflow-cn", "submodel", "wandb"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://openrouter.ai/qwen/qwen3-235b-a22b-thinking-2507"
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen/qwen3-235b-a22b-thinking-2507".pricing."chutes"]
input_cost_per_token = 1.1e-7
output_cost_per_token = 6e-7
[models."qwen/qwen3-235b-a22b-thinking-2507".pricing."huggingface"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.000003
[models."qwen/qwen3-235b-a22b-thinking-2507".pricing."io-net"]
cache_read_input_token_cost = 5.5e-8
input_cost_per_token = 1.1e-7
output_cost_per_token = 6e-7
[models."qwen/qwen3-235b-a22b-thinking-2507".pricing."jiekou"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.000003
[models."qwen/qwen3-235b-a22b-thinking-2507".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
[models."qwen/qwen3-235b-a22b-thinking-2507".pricing."nebius"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7
[models."qwen/qwen3-235b-a22b-thinking-2507".pricing."novita"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.000003
[models."qwen/qwen3-235b-a22b-thinking-2507".pricing."novita-ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.000003
[models."qwen/qwen3-235b-a22b-thinking-2507".pricing."openrouter"]
input_cost_per_token = 1.1e-7
output_cost_per_token = 6e-7
[models."qwen/qwen3-235b-a22b-thinking-2507".pricing."siliconflow"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 6e-7
[models."qwen/qwen3-235b-a22b-thinking-2507".pricing."siliconflow-cn"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 6e-7
[models."qwen/qwen3-235b-a22b-thinking-2507".pricing."submodel"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7
[models."qwen/qwen3-235b-a22b-thinking-2507".pricing."wandb"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 1.0000000000000001e-7

[models."qwen/qwen3-235b-a22b:free"]
display_name = "Qwen3 235B A22B (free)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-30b-a3b-2507"]
display_name = "Qwen3 30B A3B 2507"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 16384
litellm_provider = "lmstudio"
providers = ["lmstudio"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-30b-a3b-fp8"]
display_name = "Qwen3 30B A3B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 20000
max_tokens = 20000
input_cost_per_token = 9e-8
output_cost_per_token = 4.5e-7
litellm_provider = "novita"
providers = ["novita", "jiekou", "novita-ai"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."qwen/qwen3-30b-a3b-fp8".pricing."jiekou"]
input_cost_per_token = 9e-8
output_cost_per_token = 4.5000000000000003e-7
[models."qwen/qwen3-30b-a3b-fp8".pricing."novita"]
input_cost_per_token = 9e-8
output_cost_per_token = 4.5e-7
[models."qwen/qwen3-30b-a3b-fp8".pricing."novita-ai"]
input_cost_per_token = 9e-8
output_cost_per_token = 4.5000000000000003e-7

[models."qwen/qwen3-30b-a3b-instruct-2507"]
display_name = "Qwen3-30B-A3B-Instruct-2507"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
cache_read_input_token_cost = 1e-8
litellm_provider = "nebius"
providers = ["nebius", "chutes", "kilo", "modelscope", "openrouter", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-12"
release_date = "2026-01-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-30b-a3b-instruct-2507".pricing."chutes"]
input_cost_per_token = 8e-8
output_cost_per_token = 3.3e-7
[models."qwen/qwen3-30b-a3b-instruct-2507".pricing."kilo"]
cache_read_input_token_cost = 4e-8
input_cost_per_token = 8e-8
output_cost_per_token = 3.3e-7
[models."qwen/qwen3-30b-a3b-instruct-2507".pricing."nebius"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
[models."qwen/qwen3-30b-a3b-instruct-2507".pricing."openrouter"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7
[models."qwen/qwen3-30b-a3b-instruct-2507".pricing."siliconflow"]
input_cost_per_token = 9e-8
output_cost_per_token = 3e-7
[models."qwen/qwen3-30b-a3b-instruct-2507".pricing."siliconflow-cn"]
input_cost_per_token = 9e-8
output_cost_per_token = 3e-7

[models."qwen/qwen3-30b-a3b-thinking-2507"]
display_name = "Qwen3-30B-A3B-Thinking-2507"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
cache_read_input_token_cost = 1e-8
litellm_provider = "nebius"
providers = ["nebius", "kilo", "modelscope", "openrouter", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-12"
release_date = "2026-01-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"
reasoning_cost_per_token = 3e-7

[models."qwen/qwen3-30b-a3b-thinking-2507".pricing."kilo"]
input_cost_per_token = 5.0999999999999993e-8
output_cost_per_token = 3.4000000000000003e-7
[models."qwen/qwen3-30b-a3b-thinking-2507".pricing."nebius"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
reasoning_cost_per_token = 3e-7
[models."qwen/qwen3-30b-a3b-thinking-2507".pricing."openrouter"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7
[models."qwen/qwen3-30b-a3b-thinking-2507".pricing."siliconflow"]
input_cost_per_token = 9e-8
output_cost_per_token = 3e-7
[models."qwen/qwen3-30b-a3b-thinking-2507".pricing."siliconflow-cn"]
input_cost_per_token = 9e-8
output_cost_per_token = 3e-7

[models."qwen/qwen3-30b-a3b:free"]
display_name = "Qwen3 30B A3B (free)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 40960
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-32b"]
display_name = "Qwen3 32B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 131000
max_tokens = 131000
input_cost_per_token = 2.9e-7
output_cost_per_token = 5.9e-7
litellm_provider = "groq"
providers = ["groq", "abacus", "chutes", "kilo", "nebius", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-11-08"
release_date = "2024-12-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = false
supports_tool_choice = true

[models."qwen/qwen3-32b".pricing."abacus"]
input_cost_per_token = 9e-8
output_cost_per_token = 2.9e-7
[models."qwen/qwen3-32b".pricing."chutes"]
cache_read_input_token_cost = 4e-8
input_cost_per_token = 8e-8
output_cost_per_token = 2.4e-7
[models."qwen/qwen3-32b".pricing."groq"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 5.9e-7
[models."qwen/qwen3-32b".pricing."kilo"]
cache_read_input_token_cost = 4e-8
input_cost_per_token = 8e-8
output_cost_per_token = 2.4e-7
[models."qwen/qwen3-32b".pricing."nebius"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
[models."qwen/qwen3-32b".pricing."siliconflow"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7
[models."qwen/qwen3-32b".pricing."siliconflow-cn"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7

[models."qwen/qwen3-32b-fast"]
display_name = "Qwen3-32B (Fast)"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7
cache_read_input_token_cost = 2e-8
litellm_provider = "nebius"
providers = ["nebius"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-12"
release_date = "2026-01-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-32b-fast".pricing."nebius"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7

[models."qwen/qwen3-32b-fp8"]
display_name = "Qwen3 32B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 20000
max_tokens = 20000
input_cost_per_token = 1e-7
output_cost_per_token = 4.5e-7
litellm_provider = "novita"
providers = ["novita", "jiekou", "novita-ai"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."qwen/qwen3-32b-fp8".pricing."jiekou"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.5000000000000003e-7
[models."qwen/qwen3-32b-fp8".pricing."novita"]
input_cost_per_token = 1e-7
output_cost_per_token = 4.5e-7
[models."qwen/qwen3-32b-fp8".pricing."novita-ai"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.5000000000000003e-7

[models."qwen/qwen3-32b:free"]
display_name = "Qwen3 32B (free)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 40960
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-4b"]
display_name = "Qwen: Qwen3 4B"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 7.15e-8
output_cost_per_token = 2.73e-7
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-04-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-4b".pricing."kilo"]
input_cost_per_token = 7.15e-8
output_cost_per_token = 2.73e-7

[models."qwen/qwen3-4b-fp8"]
display_name = "Qwen3 4B"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 20000
max_tokens = 20000
input_cost_per_token = 3e-8
output_cost_per_token = 3e-8
litellm_provider = "novita"
providers = ["novita", "novita-ai"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
release_date = "2025-04-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."qwen/qwen3-4b-fp8".pricing."novita"]
input_cost_per_token = 3e-8
output_cost_per_token = 3e-8
[models."qwen/qwen3-4b-fp8".pricing."novita-ai"]
input_cost_per_token = 3e-8
output_cost_per_token = 3e-8

[models."qwen/qwen3-4b:free"]
display_name = "Qwen3 4B (free)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 40960
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-5-397b-a17b"]
display_name = "Qwen3.5-397B-A17B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 64000
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000036000000000000003
litellm_provider = "novita-ai"
providers = ["novita-ai", "huggingface", "kilo", "nano-gpt", "openrouter", "togetherai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2026-02-17"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-5-397b-a17b".pricing."huggingface"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000036000000000000003
[models."qwen/qwen3-5-397b-a17b".pricing."kilo"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000036000000000000003
[models."qwen/qwen3-5-397b-a17b".pricing."nano-gpt"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000036000000000000003
[models."qwen/qwen3-5-397b-a17b".pricing."novita-ai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000036000000000000003
[models."qwen/qwen3-5-397b-a17b".pricing."openrouter"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000036000000000000003
[models."qwen/qwen3-5-397b-a17b".pricing."togetherai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000036000000000000003

[models."qwen/qwen3-5-397b-a17b-thinking"]
display_name = "Qwen3.5 397B A17B Thinking"
model_family = "qwen"
mode = "chat"
max_input_tokens = 258000
max_output_tokens = 8192
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000036000000000000003
litellm_provider = "nano-gpt"
providers = ["nano-gpt"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-02-16"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-5-397b-a17b-thinking".pricing."nano-gpt"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000036000000000000003

[models."qwen/qwen3-5-plus-02-15"]
display_name = "Qwen3.5 Plus 2026-02-15"
model_family = "qwen"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 65536
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000024
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2026-02-16"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-5-plus-02-15".pricing."kilo"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000024
[models."qwen/qwen3-5-plus-02-15".pricing."openrouter"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000024

[models."qwen/qwen3-5-plus-thinking"]
display_name = "Qwen3.5 Plus Thinking"
model_family = "qwen"
mode = "chat"
max_input_tokens = 983600
max_output_tokens = 8192
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000024
litellm_provider = "nano-gpt"
providers = ["nano-gpt"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2026-02-16"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-5-plus-thinking".pricing."nano-gpt"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000024

[models."qwen/qwen3-8b-fp8"]
display_name = "Qwen3 8B"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 20000
max_tokens = 20000
input_cost_per_token = 3.5e-8
output_cost_per_token = 1.38e-7
litellm_provider = "novita"
providers = ["novita", "novita-ai"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
release_date = "2025-04-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."qwen/qwen3-8b-fp8".pricing."novita"]
input_cost_per_token = 3.5e-8
output_cost_per_token = 1.38e-7
[models."qwen/qwen3-8b-fp8".pricing."novita-ai"]
input_cost_per_token = 3.5e-8
output_cost_per_token = 1.3800000000000002e-7

[models."qwen/qwen3-8b:free"]
display_name = "Qwen3 8B (free)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 40960
max_output_tokens = 40960
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-coder"]
display_name = "Qwen3 Coder"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262100
max_output_tokens = 262100
max_tokens = 262100
input_cost_per_token = 2.2e-7
output_cost_per_token = 9.5e-7
litellm_provider = "openrouter"
providers = ["openrouter", "fastrouter", "kilo", "nano-gpt"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://openrouter.ai/qwen/qwen3-coder"
supports_tool_choice = true

[models."qwen/qwen3-coder".pricing."fastrouter"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."qwen/qwen3-coder".pricing."kilo"]
cache_read_input_token_cost = 2.2e-8
input_cost_per_token = 2.2e-7
output_cost_per_token = 0.000001
[models."qwen/qwen3-coder".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
[models."qwen/qwen3-coder".pricing."openrouter"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 9.5e-7

[models."qwen/qwen3-coder-30b"]
display_name = "Qwen3 Coder 30B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65536
litellm_provider = "lmstudio"
providers = ["lmstudio"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-coder-30b-a3b-instruct"]
display_name = "Qwen3-Coder-30B-A3B-Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 160000
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 7e-8
output_cost_per_token = 2.7e-7
litellm_provider = "novita"
providers = ["novita", "kilo", "modelscope", "nebius", "novita-ai", "openrouter", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-12"
release_date = "2026-01-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen/qwen3-coder-30b-a3b-instruct".pricing."kilo"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.7e-7
[models."qwen/qwen3-coder-30b-a3b-instruct".pricing."nebius"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
[models."qwen/qwen3-coder-30b-a3b-instruct".pricing."novita"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.7e-7
[models."qwen/qwen3-coder-30b-a3b-instruct".pricing."novita-ai"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.7e-7
[models."qwen/qwen3-coder-30b-a3b-instruct".pricing."openrouter"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.7e-7
[models."qwen/qwen3-coder-30b-a3b-instruct".pricing."siliconflow"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
[models."qwen/qwen3-coder-30b-a3b-instruct".pricing."siliconflow-cn"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7

[models."qwen/qwen3-coder-480b-a35b-instruct"]
display_name = "Qwen3 Coder 480B A35B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000013
litellm_provider = "novita"
providers = ["novita", "abacus", "baseten", "deepinfra", "huggingface", "jiekou", "nebius", "novita-ai", "nvidia", "siliconflow", "siliconflow-cn", "wandb"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen/qwen3-coder-480b-a35b-instruct".pricing."abacus"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.0000012
[models."qwen/qwen3-coder-480b-a35b-instruct".pricing."baseten"]
input_cost_per_token = 3.8e-7
output_cost_per_token = 0.00000153
[models."qwen/qwen3-coder-480b-a35b-instruct".pricing."deepinfra"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000016000000000000001
[models."qwen/qwen3-coder-480b-a35b-instruct".pricing."huggingface"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002
[models."qwen/qwen3-coder-480b-a35b-instruct".pricing."jiekou"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.0000012
[models."qwen/qwen3-coder-480b-a35b-instruct".pricing."nebius"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000018000000000000001
[models."qwen/qwen3-coder-480b-a35b-instruct".pricing."novita"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000013
[models."qwen/qwen3-coder-480b-a35b-instruct".pricing."novita-ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000013
[models."qwen/qwen3-coder-480b-a35b-instruct".pricing."siliconflow"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001
[models."qwen/qwen3-coder-480b-a35b-instruct".pricing."siliconflow-cn"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001
[models."qwen/qwen3-coder-480b-a35b-instruct".pricing."wandb"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000015

[models."qwen/qwen3-coder-480b-a35b-instruct-maas"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000001
output_cost_per_token = 0.000004
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_regions = ["global"]
supports_tool_choice = true

[models."qwen/qwen3-coder-480b-a35b-instruct-maas".pricing."vertex_ai"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000004

[models."qwen/qwen3-coder-flash"]
display_name = "Qwen3 Coder Flash"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 66536
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-coder-flash".pricing."kilo"]
cache_read_input_token_cost = 6e-8
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015
[models."qwen/qwen3-coder-flash".pricing."openrouter"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015

[models."qwen/qwen3-coder-next"]
display_name = "qwen/qwen3-coder-next"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65536
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015
litellm_provider = "jiekou"
providers = ["jiekou", "chutes", "huggingface", "kilo", "novita-ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2026-02"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-coder-next".pricing."chutes"]
input_cost_per_token = 7e-8
output_cost_per_token = 3e-7
[models."qwen/qwen3-coder-next".pricing."huggingface"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015
[models."qwen/qwen3-coder-next".pricing."jiekou"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015
[models."qwen/qwen3-coder-next".pricing."kilo"]
cache_read_input_token_cost = 3.5e-8
input_cost_per_token = 7e-8
output_cost_per_token = 3e-7
[models."qwen/qwen3-coder-next".pricing."novita-ai"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015

[models."qwen/qwen3-coder-plus"]
display_name = "Qwen3-Coder-Plus"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
cache_read_input_token_cost = 1.0000000000000001e-7
litellm_provider = "zenmux"
providers = ["zenmux", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2025-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-coder-plus".pricing."kilo"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."qwen/qwen3-coder-plus".pricing."zenmux"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005

[models."qwen/qwen3-coder:exacto"]
display_name = "Qwen3 Coder (exacto)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
input_cost_per_token = 3.8e-7
output_cost_per_token = 0.00000153
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-coder:exacto".pricing."kilo"]
cache_read_input_token_cost = 2.2e-8
input_cost_per_token = 2.2e-7
output_cost_per_token = 0.0000018000000000000001
[models."qwen/qwen3-coder:exacto".pricing."openrouter"]
input_cost_per_token = 3.8e-7
output_cost_per_token = 0.00000153

[models."qwen/qwen3-coder:free"]
display_name = "Qwen3 Coder 480B A35B Instruct (free)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 66536
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-embedding-0.6b"]
mode = "embedding"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 7e-8
output_cost_per_token = 0
litellm_provider = "novita"
providers = ["novita"]

[models."qwen/qwen3-embedding-0.6b".pricing."novita"]
input_cost_per_token = 7e-8
output_cost_per_token = 0

[models."qwen/qwen3-embedding-8b"]
display_name = "Qwen3 Embedding 8B"
model_family = "text-embedding"
mode = "embedding"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 7e-8
output_cost_per_token = 0
litellm_provider = "novita"
providers = ["novita", "evroc", "huggingface", "nebius"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-10"
release_date = "2025-07-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."qwen/qwen3-embedding-8b".pricing."evroc"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 1.2e-7
[models."qwen/qwen3-embedding-8b".pricing."huggingface"]
input_cost_per_token = 1e-8
[models."qwen/qwen3-embedding-8b".pricing."nebius"]
input_cost_per_token = 1e-8
[models."qwen/qwen3-embedding-8b".pricing."novita"]
input_cost_per_token = 7e-8
output_cost_per_token = 0

[models."qwen/qwen3-max"]
display_name = "Qwen3 Max"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 0.00000211
output_cost_per_token = 0.00000845
litellm_provider = "novita"
providers = ["novita", "kilo", "novita-ai", "openrouter", "zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-09-24"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen/qwen3-max".pricing."kilo"]
cache_read_input_token_cost = 2.4e-7
input_cost_per_token = 0.0000012
output_cost_per_token = 0.000006
[models."qwen/qwen3-max".pricing."novita"]
input_cost_per_token = 0.00000211
output_cost_per_token = 0.00000845
[models."qwen/qwen3-max".pricing."novita-ai"]
input_cost_per_token = 0.0000021099999999999997
output_cost_per_token = 0.000008449999999999999
[models."qwen/qwen3-max".pricing."openrouter"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.000006
[models."qwen/qwen3-max".pricing."zenmux"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.000006

[models."qwen/qwen3-max-thinking"]
display_name = "Qwen: Qwen3 Max Thinking"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65536
input_cost_per_token = 0.0000012
output_cost_per_token = 0.000006
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2026-01-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-max-thinking".pricing."kilo"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.000006

[models."qwen/qwen3-next-80b-a3b-instruct"]
display_name = "Qwen3-Next-80B-A3B-Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000015
litellm_provider = "novita"
providers = ["novita", "chutes", "huggingface", "io-net", "jiekou", "kilo", "novita-ai", "nvidia", "openrouter", "siliconflow", "siliconflow-cn", "togetherai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-12"
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen/qwen3-next-80b-a3b-instruct".pricing."chutes"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 8.000000000000001e-7
[models."qwen/qwen3-next-80b-a3b-instruct".pricing."huggingface"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.000001
[models."qwen/qwen3-next-80b-a3b-instruct".pricing."io-net"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 8.000000000000001e-7
[models."qwen/qwen3-next-80b-a3b-instruct".pricing."jiekou"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000015
[models."qwen/qwen3-next-80b-a3b-instruct".pricing."kilo"]
input_cost_per_token = 9e-8
output_cost_per_token = 0.0000011
[models."qwen/qwen3-next-80b-a3b-instruct".pricing."novita"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000015
[models."qwen/qwen3-next-80b-a3b-instruct".pricing."novita-ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000015
[models."qwen/qwen3-next-80b-a3b-instruct".pricing."openrouter"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 0.0000014
[models."qwen/qwen3-next-80b-a3b-instruct".pricing."siliconflow"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 0.0000014
[models."qwen/qwen3-next-80b-a3b-instruct".pricing."siliconflow-cn"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 0.0000014
[models."qwen/qwen3-next-80b-a3b-instruct".pricing."togetherai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000015

[models."qwen/qwen3-next-80b-a3b-instruct-maas"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000012
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_regions = ["global"]
supports_tool_choice = true

[models."qwen/qwen3-next-80b-a3b-instruct-maas".pricing."vertex_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000012

[models."qwen/qwen3-next-80b-a3b-instruct:free"]
display_name = "Qwen3 Next 80B A3B Instruct (free)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-09-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen/qwen3-next-80b-a3b-thinking"]
display_name = "Qwen3-Next-80B-A3B-Thinking"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000015
litellm_provider = "novita"
providers = ["novita", "huggingface", "jiekou", "kilo", "nebius", "novita-ai", "nvidia", "openrouter", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
reasoning_cost_per_token = 0.0000012
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen/qwen3-next-80b-a3b-thinking".pricing."huggingface"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.000002
[models."qwen/qwen3-next-80b-a3b-thinking".pricing."jiekou"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000015
[models."qwen/qwen3-next-80b-a3b-thinking".pricing."kilo"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000012
[models."qwen/qwen3-next-80b-a3b-thinking".pricing."nebius"]
cache_read_input_token_cost = 1.5e-8
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000012
reasoning_cost_per_token = 0.0000012
[models."qwen/qwen3-next-80b-a3b-thinking".pricing."novita"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000015
[models."qwen/qwen3-next-80b-a3b-thinking".pricing."novita-ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000015
[models."qwen/qwen3-next-80b-a3b-thinking".pricing."openrouter"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 0.0000014
[models."qwen/qwen3-next-80b-a3b-thinking".pricing."siliconflow"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7
[models."qwen/qwen3-next-80b-a3b-thinking".pricing."siliconflow-cn"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7

[models."qwen/qwen3-next-80b-a3b-thinking-maas"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000012
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supports_function_calling = true
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
supported_regions = ["global"]
supports_tool_choice = true

[models."qwen/qwen3-next-80b-a3b-thinking-maas".pricing."vertex_ai"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 0.0000012

[models."qwen/qwen3-omni-30b-a3b-instruct"]
display_name = "Qwen3 Omni 30B A3B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2.5e-7
output_cost_per_token = 9.7e-7
litellm_provider = "novita"
providers = ["novita", "novita-ai", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2025-09-24"
supported_modalities = ["text", "video", "audio", "image"]
supported_output_modalities = ["text", "audio"]
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen/qwen3-omni-30b-a3b-instruct".pricing."novita"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 9.7e-7
[models."qwen/qwen3-omni-30b-a3b-instruct".pricing."novita-ai"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 9.7e-7
[models."qwen/qwen3-omni-30b-a3b-instruct".pricing."siliconflow"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."qwen/qwen3-omni-30b-a3b-instruct".pricing."siliconflow-cn"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7

[models."qwen/qwen3-omni-30b-a3b-thinking"]
display_name = "Qwen3 Omni 30B A3B Thinking"
model_family = "qwen"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 2.5e-7
output_cost_per_token = 9.7e-7
litellm_provider = "novita"
providers = ["novita", "novita-ai", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = true
release_date = "2025-09-24"
supported_modalities = ["text", "audio", "video", "image"]
supported_output_modalities = ["text"]
supports_audio_input = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen/qwen3-omni-30b-a3b-thinking".pricing."novita"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 9.7e-7
[models."qwen/qwen3-omni-30b-a3b-thinking".pricing."novita-ai"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 9.7e-7
[models."qwen/qwen3-omni-30b-a3b-thinking".pricing."siliconflow"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."qwen/qwen3-omni-30b-a3b-thinking".pricing."siliconflow-cn"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7

[models."qwen/qwen3-reranker-8b"]
mode = "rerank"
max_input_tokens = 32768
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 5e-8
output_cost_per_token = 5e-8
litellm_provider = "novita"
providers = ["novita"]

[models."qwen/qwen3-reranker-8b".pricing."novita"]
input_cost_per_token = 5e-8
output_cost_per_token = 5e-8

[models."qwen/qwen3-vl-235b-a22b-instruct"]
display_name = "Qwen3 VL 235B A22B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015
litellm_provider = "novita"
providers = ["novita", "chutes", "kilo", "novita-ai", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
release_date = "2025-09-24"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen/qwen3-vl-235b-a22b-instruct".pricing."chutes"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."qwen/qwen3-vl-235b-a22b-instruct".pricing."kilo"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.8e-7
[models."qwen/qwen3-vl-235b-a22b-instruct".pricing."novita"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015
[models."qwen/qwen3-vl-235b-a22b-instruct".pricing."novita-ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015
[models."qwen/qwen3-vl-235b-a22b-instruct".pricing."siliconflow"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015
[models."qwen/qwen3-vl-235b-a22b-instruct".pricing."siliconflow-cn"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015

[models."qwen/qwen3-vl-235b-a22b-thinking"]
display_name = "Qwen3 VL 235B A22B Thinking"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 9.8e-7
output_cost_per_token = 0.00000395
litellm_provider = "novita"
providers = ["novita", "kilo", "novita-ai", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = true
release_date = "2025-09-24"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."qwen/qwen3-vl-235b-a22b-thinking".pricing."novita"]
input_cost_per_token = 9.8e-7
output_cost_per_token = 0.00000395
[models."qwen/qwen3-vl-235b-a22b-thinking".pricing."novita-ai"]
input_cost_per_token = 9.8e-7
output_cost_per_token = 0.00000395
[models."qwen/qwen3-vl-235b-a22b-thinking".pricing."siliconflow"]
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.0000035
[models."qwen/qwen3-vl-235b-a22b-thinking".pricing."siliconflow-cn"]
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.0000035

[models."qwen/qwen3-vl-30b-a3b-instruct"]
display_name = "Qwen3 VL 30B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 7e-7
litellm_provider = "novita"
providers = ["novita", "evroc", "kilo", "novita-ai", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
release_date = "2025-07-30"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen/qwen3-vl-30b-a3b-instruct".pricing."evroc"]
input_cost_per_token = 2.4e-7
output_cost_per_token = 9.399999999999999e-7
[models."qwen/qwen3-vl-30b-a3b-instruct".pricing."kilo"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 5.2e-7
[models."qwen/qwen3-vl-30b-a3b-instruct".pricing."novita"]
input_cost_per_token = 2e-7
output_cost_per_token = 7e-7
[models."qwen/qwen3-vl-30b-a3b-instruct".pricing."novita-ai"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 7e-7
[models."qwen/qwen3-vl-30b-a3b-instruct".pricing."siliconflow"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.000001
[models."qwen/qwen3-vl-30b-a3b-instruct".pricing."siliconflow-cn"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.000001

[models."qwen/qwen3-vl-30b-a3b-thinking"]
display_name = "qwen/qwen3-vl-30b-a3b-thinking"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 2e-7
output_cost_per_token = 0.000001
litellm_provider = "novita"
providers = ["novita", "kilo", "novita-ai", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = true
release_date = "2025-10-11"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen/qwen3-vl-30b-a3b-thinking".pricing."novita"]
input_cost_per_token = 2e-7
output_cost_per_token = 0.000001
[models."qwen/qwen3-vl-30b-a3b-thinking".pricing."novita-ai"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.000001
[models."qwen/qwen3-vl-30b-a3b-thinking".pricing."siliconflow"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.000001
[models."qwen/qwen3-vl-30b-a3b-thinking".pricing."siliconflow-cn"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.000001

[models."qwen/qwen3-vl-8b-instruct"]
display_name = "qwen/qwen3-vl-8b-instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 8e-8
output_cost_per_token = 5e-7
litellm_provider = "novita"
providers = ["novita", "kilo", "novita-ai", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = true
release_date = "2025-10-17"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen/qwen3-vl-8b-instruct".pricing."kilo"]
input_cost_per_token = 8e-8
output_cost_per_token = 5e-7
[models."qwen/qwen3-vl-8b-instruct".pricing."novita"]
input_cost_per_token = 8e-8
output_cost_per_token = 5e-7
[models."qwen/qwen3-vl-8b-instruct".pricing."novita-ai"]
input_cost_per_token = 8e-8
output_cost_per_token = 5e-7
[models."qwen/qwen3-vl-8b-instruct".pricing."siliconflow"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 6.800000000000001e-7
[models."qwen/qwen3-vl-8b-instruct".pricing."siliconflow-cn"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 6.800000000000001e-7

[models."qwen/qwq-32b:free"]
display_name = "QwQ 32B (free)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-03"
release_date = "2025-03-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen2-5-14b-instruct"]
display_name = "Qwen2.5 14B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.0000014
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen2-5-14b-instruct".pricing."alibaba"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.0000014
[models."qwen2-5-14b-instruct".pricing."alibaba-cn"]
input_cost_per_token = 1.44e-7
output_cost_per_token = 4.31e-7

[models."qwen2-5-32b-instruct"]
display_name = "Qwen2.5 32B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000028
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen2-5-32b-instruct".pricing."alibaba"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000028
[models."qwen2-5-32b-instruct".pricing."alibaba-cn"]
input_cost_per_token = 2.8699999999999996e-7
output_cost_per_token = 8.61e-7

[models."qwen2-5-72b-instruct"]
display_name = "Qwen2.5 72B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 0.0000014
output_cost_per_token = 0.0000056
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen2-5-72b-instruct".pricing."alibaba"]
input_cost_per_token = 0.0000014
output_cost_per_token = 0.0000056
[models."qwen2-5-72b-instruct".pricing."alibaba-cn"]
input_cost_per_token = 5.739999999999999e-7
output_cost_per_token = 0.0000017210000000000001

[models."qwen2-5-7b-instruct"]
display_name = "Qwen2.5 7B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 1.75e-7
output_cost_per_token = 7e-7
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen2-5-7b-instruct".pricing."alibaba"]
input_cost_per_token = 1.75e-7
output_cost_per_token = 7e-7
[models."qwen2-5-7b-instruct".pricing."alibaba-cn"]
input_cost_per_token = 7.2e-8
output_cost_per_token = 1.44e-7

[models."qwen2-5-coder-32b-instruct"]
display_name = "Qwen2.5-Coder 32B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 2.8699999999999996e-7
output_cost_per_token = 8.61e-7
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn", "ovhcloud"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen2-5-coder-32b-instruct".pricing."alibaba-cn"]
input_cost_per_token = 2.8699999999999996e-7
output_cost_per_token = 8.61e-7
[models."qwen2-5-coder-32b-instruct".pricing."ovhcloud"]
input_cost_per_token = 9.6e-7
output_cost_per_token = 9.6e-7

[models."qwen2-5-coder-7b-fast"]
display_name = "Qwen2.5 Coder 7B fast"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8192
input_cost_per_token = 3e-8
output_cost_per_token = 9e-8
litellm_provider = "helicone"
providers = ["helicone"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-09"
release_date = "2024-09-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen2-5-coder-7b-fast".pricing."helicone"]
input_cost_per_token = 3e-8
output_cost_per_token = 9e-8

[models."qwen2-5-coder-7b-instruct"]
display_name = "Qwen2.5-Coder 7B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 1.44e-7
output_cost_per_token = 2.8699999999999996e-7
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen2-5-coder-7b-instruct".pricing."alibaba-cn"]
input_cost_per_token = 1.44e-7
output_cost_per_token = 2.8699999999999996e-7

[models."qwen2-5-math-72b-instruct"]
display_name = "Qwen2.5-Math 72B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 3072
input_cost_per_token = 5.739999999999999e-7
output_cost_per_token = 0.0000017210000000000001
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen2-5-math-72b-instruct".pricing."alibaba-cn"]
input_cost_per_token = 5.739999999999999e-7
output_cost_per_token = 0.0000017210000000000001

[models."qwen2-5-math-7b-instruct"]
display_name = "Qwen2.5-Math 7B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 3072
input_cost_per_token = 1.44e-7
output_cost_per_token = 2.8699999999999996e-7
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen2-5-math-7b-instruct".pricing."alibaba-cn"]
input_cost_per_token = 1.44e-7
output_cost_per_token = 2.8699999999999996e-7

[models."qwen2-5-omni-7b"]
display_name = "Qwen2.5-Omni 7B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 2048
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-12"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
source = "modelsdev"

[models."qwen2-5-omni-7b".pricing."alibaba"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 4.0000000000000003e-7
[models."qwen2-5-omni-7b".pricing."alibaba-cn"]
input_cost_per_token = 8.7e-8
output_cost_per_token = 3.45e-7

[models."qwen2-5-vl-72b-instruct"]
display_name = "Qwen2.5-VL 72B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 0.0000028
output_cost_per_token = 0.000008400000000000001
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn", "ovhcloud"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-09"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen2-5-vl-72b-instruct".pricing."alibaba"]
input_cost_per_token = 0.0000028
output_cost_per_token = 0.000008400000000000001
[models."qwen2-5-vl-72b-instruct".pricing."alibaba-cn"]
input_cost_per_token = 0.000002294
output_cost_per_token = 0.000006881
[models."qwen2-5-vl-72b-instruct".pricing."ovhcloud"]
input_cost_per_token = 0.00000101
output_cost_per_token = 0.00000101

[models."qwen2-5-vl-7b-instruct"]
display_name = "Qwen2.5-VL 7B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.0000010500000000000001
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2024-04"
release_date = "2024-09"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen2-5-vl-7b-instruct".pricing."alibaba"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.0000010500000000000001
[models."qwen2-5-vl-7b-instruct".pricing."alibaba-cn"]
input_cost_per_token = 2.8699999999999996e-7
output_cost_per_token = 7.17e-7

[models."qwen2.5-coder-7b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-8
output_cost_per_token = 1.2e-7
litellm_provider = "llamagate"
providers = ["llamagate"]
supports_function_calling = true
supports_response_schema = true

[models."qwen2.5-coder-7b".pricing."llamagate"]
input_cost_per_token = 6e-8
output_cost_per_token = 1.2e-7

[models."qwen25-coder-32b-instruct"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 5e-8
output_cost_per_token = 1e-7
litellm_provider = "lambda_ai"
providers = ["lambda_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen25-coder-32b-instruct".pricing."lambda_ai"]
input_cost_per_token = 5e-8
output_cost_per_token = 1e-7

[models."qwen3-14b"]
display_name = "Qwen3 14B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.0000014
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"
reasoning_cost_per_token = 0.0000042000000000000004

[models."qwen3-14b".pricing."alibaba"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.0000014
reasoning_cost_per_token = 0.0000042000000000000004
[models."qwen3-14b".pricing."alibaba-cn"]
input_cost_per_token = 1.44e-7
output_cost_per_token = 5.739999999999999e-7
reasoning_cost_per_token = 0.000001434

[models."qwen3-235b"]
display_name = "Qwen3-235B-A22B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
litellm_provider = "iflowcn"
providers = ["iflowcn"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2024-12-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-235b-a22b"]
display_name = "Qwen3-235B-A22B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.0000028599999999999997
litellm_provider = "302ai"
providers = ["302ai", "alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-04-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"
reasoning_cost_per_token = 0.000008400000000000001

[models."qwen3-235b-a22b".pricing."302ai"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.0000028599999999999997
[models."qwen3-235b-a22b".pricing."alibaba"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000028
reasoning_cost_per_token = 0.000008400000000000001
[models."qwen3-235b-a22b".pricing."alibaba-cn"]
input_cost_per_token = 2.8699999999999996e-7
output_cost_per_token = 0.000001147
reasoning_cost_per_token = 0.000002868

[models."qwen3-235b-a22b-instruct"]
display_name = "Qwen3-235B-A22B-Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 64000
litellm_provider = "iflowcn"
providers = ["iflowcn"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-235b-a22b-instruct-2507"]
display_name = "qwen3-235b-a22b-instruct-2507"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 65536
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.0000011430000000000001
litellm_provider = "302ai"
providers = ["302ai", "aihubmix", "scaleway", "venice"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-07-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-235b-a22b-instruct-2507".pricing."302ai"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.0000011430000000000001
[models."qwen3-235b-a22b-instruct-2507".pricing."aihubmix"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 0.00000112
[models."qwen3-235b-a22b-instruct-2507".pricing."scaleway"]
input_cost_per_token = 7.5e-7
output_cost_per_token = 0.00000225
[models."qwen3-235b-a22b-instruct-2507".pricing."venice"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 7.5e-7

[models."qwen3-235b-a22b-thinking"]
display_name = "Qwen3 235B A22B Thinking"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 81920
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000029
litellm_provider = "helicone"
providers = ["helicone"]
supports_function_calling = false
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-07"
release_date = "2025-07-25"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-235b-a22b-thinking".pricing."helicone"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000029

[models."qwen3-235b-a22b-thinking-2507"]
display_name = "Qwen 3 235B A22B Thinking 2507"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.0000035
litellm_provider = "venice"
providers = ["venice", "aihubmix", "iflowcn"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2025-04-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-235b-a22b-thinking-2507".pricing."aihubmix"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 0.0000028
[models."qwen3-235b-a22b-thinking-2507".pricing."venice"]
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.0000035

[models."qwen3-30b-a3b"]
display_name = "Qwen3-30B-A3B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 129024
max_output_tokens = 16384
max_tokens = 16384
litellm_provider = "dashscope"
providers = ["dashscope", "302ai", "helicone"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-04-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true

[models."qwen3-30b-a3b".pricing."302ai"]
input_cost_per_token = 1.1e-7
output_cost_per_token = 0.00000108
[models."qwen3-30b-a3b".pricing."helicone"]
input_cost_per_token = 8e-8
output_cost_per_token = 2.9e-7

[models."qwen3-32b-fp8"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 5e-8
output_cost_per_token = 1e-7
litellm_provider = "lambda_ai"
providers = ["lambda_ai"]
supports_function_calling = true
supports_reasoning = true
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."qwen3-32b-fp8".pricing."lambda_ai"]
input_cost_per_token = 5e-8
output_cost_per_token = 1e-7

[models."qwen3-4b"]
display_name = "Venice Small"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8000
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 1.5e-7
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-07"
release_date = "2025-04-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-4b".pricing."venice"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 1.5e-7

[models."qwen3-5-397b-a17b"]
display_name = "Qwen3.5 397B-A17B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65536
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000036000000000000003
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2026-02-16"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"
reasoning_cost_per_token = 0.0000036000000000000003

[models."qwen3-5-397b-a17b".pricing."alibaba"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000036000000000000003
reasoning_cost_per_token = 0.0000036000000000000003
[models."qwen3-5-397b-a17b".pricing."alibaba-cn"]
input_cost_per_token = 4.3e-7
output_cost_per_token = 0.00000258
reasoning_cost_per_token = 0.00000258

[models."qwen3-5-plus"]
display_name = "Qwen3.5 Plus"
model_family = "qwen"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 65536
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000024
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2026-02-16"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"
reasoning_cost_per_token = 0.0000024

[models."qwen3-5-plus".pricing."alibaba"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000024
reasoning_cost_per_token = 0.0000024
[models."qwen3-5-plus".pricing."alibaba-cn"]
input_cost_per_token = 5.73e-7
output_cost_per_token = 0.00000344
reasoning_cost_per_token = 0.00000344

[models."qwen3-8b"]
display_name = "Qwen3 8B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 4e-8
output_cost_per_token = 1.4e-7
litellm_provider = "llamagate"
providers = ["llamagate", "alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
reasoning_cost_per_token = 0.0000021000000000000002
supports_response_schema = true

[models."qwen3-8b".pricing."alibaba"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 7e-7
reasoning_cost_per_token = 0.0000021000000000000002
[models."qwen3-8b".pricing."alibaba-cn"]
input_cost_per_token = 7.2e-8
output_cost_per_token = 2.8699999999999996e-7
reasoning_cost_per_token = 7.17e-7
[models."qwen3-8b".pricing."llamagate"]
input_cost_per_token = 4e-8
output_cost_per_token = 1.4e-7

[models."qwen3-asr-flash"]
display_name = "Qwen3-ASR Flash"
model_family = "qwen"
mode = "chat"
max_input_tokens = 53248
max_output_tokens = 4096
input_cost_per_token = 3.5e-8
output_cost_per_token = 3.5e-8
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-09-08"
supported_modalities = ["audio"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-asr-flash".pricing."alibaba"]
input_cost_per_token = 3.5e-8
output_cost_per_token = 3.5e-8
[models."qwen3-asr-flash".pricing."alibaba-cn"]
input_cost_per_token = 3.2e-8
output_cost_per_token = 3.2e-8

[models."qwen3-coder"]
display_name = "Qwen3 Coder 480B A35B Instruct Turbo"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 16384
input_cost_per_token = 2.2e-7
output_cost_per_token = 9.499999999999999e-7
litellm_provider = "helicone"
providers = ["helicone", "opencode"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-07"
release_date = "2025-07-23"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-coder".pricing."helicone"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 9.499999999999999e-7
[models."qwen3-coder".pricing."opencode"]
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.0000018000000000000001

[models."qwen3-coder-30b-a3b"]
display_name = "Qwen3-Coder 30B-A3B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
litellm_provider = "privatemode-ai"
providers = ["privatemode-ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-coder-30b-a3b-instruct"]
display_name = "Qwen3-Coder 30B-A3B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65536
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.00000225
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn", "helicone", "ovhcloud", "scaleway"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-coder-30b-a3b-instruct".pricing."alibaba"]
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.00000225
[models."qwen3-coder-30b-a3b-instruct".pricing."alibaba-cn"]
input_cost_per_token = 2.16e-7
output_cost_per_token = 8.61e-7
[models."qwen3-coder-30b-a3b-instruct".pricing."helicone"]
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7
[models."qwen3-coder-30b-a3b-instruct".pricing."ovhcloud"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.6e-7
[models."qwen3-coder-30b-a3b-instruct".pricing."scaleway"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7

[models."qwen3-coder-480b-a35b-instruct"]
display_name = "qwen3-coder-480b-a35b-instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65536
input_cost_per_token = 8.6e-7
output_cost_per_token = 0.00000343
litellm_provider = "302ai"
providers = ["302ai", "aihubmix", "alibaba", "alibaba-cn", "cortecs", "venice"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-coder-480b-a35b-instruct".pricing."302ai"]
input_cost_per_token = 8.6e-7
output_cost_per_token = 0.00000343
[models."qwen3-coder-480b-a35b-instruct".pricing."aihubmix"]
input_cost_per_token = 8.2e-7
output_cost_per_token = 0.00000329
[models."qwen3-coder-480b-a35b-instruct".pricing."alibaba"]
input_cost_per_token = 0.0000015
output_cost_per_token = 0.0000075
[models."qwen3-coder-480b-a35b-instruct".pricing."alibaba-cn"]
input_cost_per_token = 8.61e-7
output_cost_per_token = 0.0000034409999999999998
[models."qwen3-coder-480b-a35b-instruct".pricing."cortecs"]
input_cost_per_token = 4.41e-7
output_cost_per_token = 0.000001984
[models."qwen3-coder-480b-a35b-instruct".pricing."venice"]
input_cost_per_token = 7.5e-7
output_cost_per_token = 0.000003

[models."qwen3-coder-flash"]
display_name = "Qwen3 Coder Flash"
model_family = "qwen"
mode = "chat"
max_input_tokens = 997952
max_output_tokens = 65536
max_tokens = 65536
litellm_provider = "dashscope"
providers = ["dashscope", "alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-07-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true
tiered_pricing = [{ cache_read_input_token_cost = 8e-8, input_cost_per_token = 3e-7, output_cost_per_token = 0.0000015, range = [0, 32000] }, { cache_read_input_token_cost = 1.2e-7, input_cost_per_token = 5e-7, output_cost_per_token = 0.0000025, range = [32000, 128000] }, { cache_read_input_token_cost = 2e-7, input_cost_per_token = 8e-7, output_cost_per_token = 0.000004, range = [128000, 256000] }, { cache_read_input_token_cost = 4e-7, input_cost_per_token = 0.0000016, output_cost_per_token = 0.0000096, range = [256000, 1000000] }]

[models."qwen3-coder-flash".pricing."alibaba"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015
[models."qwen3-coder-flash".pricing."alibaba-cn"]
input_cost_per_token = 1.44e-7
output_cost_per_token = 5.739999999999999e-7

[models."qwen3-coder-flash-2025-07-28"]
mode = "chat"
max_input_tokens = 997952
max_output_tokens = 65536
max_tokens = 65536
litellm_provider = "dashscope"
providers = ["dashscope"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 3e-7, output_cost_per_token = 0.0000015, range = [0, 32000] }, { input_cost_per_token = 5e-7, output_cost_per_token = 0.0000025, range = [32000, 128000] }, { input_cost_per_token = 8e-7, output_cost_per_token = 0.000004, range = [128000, 256000] }, { input_cost_per_token = 0.0000016, output_cost_per_token = 0.0000096, range = [256000, 1000000] }]

[models."qwen3-coder-plus"]
display_name = "Qwen3 Coder Plus"
model_family = "qwen"
mode = "chat"
max_input_tokens = 997952
max_output_tokens = 65536
max_tokens = 65536
litellm_provider = "dashscope"
providers = ["dashscope", "alibaba", "alibaba-cn", "iflowcn"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true
tiered_pricing = [{ cache_read_input_token_cost = 1e-7, input_cost_per_token = 0.000001, output_cost_per_token = 0.000005, range = [0, 32000] }, { cache_read_input_token_cost = 1.8e-7, input_cost_per_token = 0.0000018, output_cost_per_token = 0.000009, range = [32000, 128000] }, { cache_read_input_token_cost = 3e-7, input_cost_per_token = 0.000003, output_cost_per_token = 0.000015, range = [128000, 256000] }, { cache_read_input_token_cost = 6e-7, input_cost_per_token = 0.000006, output_cost_per_token = 0.00006, range = [256000, 1000000] }]

[models."qwen3-coder-plus".pricing."alibaba"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."qwen3-coder-plus".pricing."alibaba-cn"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005

[models."qwen3-coder-plus-2025-07-22"]
mode = "chat"
max_input_tokens = 997952
max_output_tokens = 65536
max_tokens = 65536
litellm_provider = "dashscope"
providers = ["dashscope"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 0.000001, output_cost_per_token = 0.000005, range = [0, 32000] }, { input_cost_per_token = 0.0000018, output_cost_per_token = 0.000009, range = [32000, 128000] }, { input_cost_per_token = 0.000003, output_cost_per_token = 0.000015, range = [128000, 256000] }, { input_cost_per_token = 0.000006, output_cost_per_token = 0.00006, range = [256000, 1000000] }]

[models."qwen3-coder:480b-cloud"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]
supports_function_calling = true

[models."qwen3-coder:480b-cloud".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."qwen3-embedding-4b"]
display_name = "Qwen3-Embedding 4B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 2560
litellm_provider = "privatemode-ai"
providers = ["privatemode-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2025-06-06"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-embedding-8b"]
mode = "embedding"
max_input_tokens = 40960
max_tokens = 40960
input_cost_per_token = 2e-8
output_cost_per_token = 0
litellm_provider = "llamagate"
providers = ["llamagate"]

[models."qwen3-embedding-8b".pricing."llamagate"]
input_cost_per_token = 2e-8
output_cost_per_token = 0

[models."qwen3-livetranslate-flash-realtime"]
display_name = "Qwen3-LiveTranslate Flash Realtime"
model_family = "qwen"
mode = "chat"
max_input_tokens = 53248
max_output_tokens = 4096
input_cost_per_token = 0.00001
output_cost_per_token = 0.00001
litellm_provider = "alibaba"
providers = ["alibaba"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-09-22"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
source = "modelsdev"

[models."qwen3-livetranslate-flash-realtime".pricing."alibaba"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.00001

[models."qwen3-max"]
display_name = "Qwen3 Max"
model_family = "qwen"
mode = "chat"
max_input_tokens = 258048
max_output_tokens = 65536
max_tokens = 65536
litellm_provider = "dashscope"
providers = ["dashscope", "abacus", "alibaba", "alibaba-cn", "iflowcn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-09-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 0.0000012, output_cost_per_token = 0.000006, range = [0, 32000] }, { input_cost_per_token = 0.0000024, output_cost_per_token = 0.000012, range = [32000, 128000] }, { input_cost_per_token = 0.000003, output_cost_per_token = 0.000015, range = [128000, 252000] }]

[models."qwen3-max".pricing."abacus"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.000006
[models."qwen3-max".pricing."alibaba"]
input_cost_per_token = 0.0000012
output_cost_per_token = 0.000006
[models."qwen3-max".pricing."alibaba-cn"]
input_cost_per_token = 8.61e-7
output_cost_per_token = 0.0000034409999999999998

[models."qwen3-max-2025-09-23"]
display_name = "qwen3-max-2025-09-23"
mode = "chat"
max_input_tokens = 258048
max_output_tokens = 65536
input_cost_per_token = 8.6e-7
output_cost_per_token = 0.00000343
litellm_provider = "302ai"
providers = ["302ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-09-24"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-max-2025-09-23".pricing."302ai"]
input_cost_per_token = 8.6e-7
output_cost_per_token = 0.00000343

[models."qwen3-max-2026-01-23"]
display_name = "Qwen3 Max"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65536
input_cost_per_token = 3.4000000000000003e-7
output_cost_per_token = 0.00000137
litellm_provider = "aihubmix"
providers = ["aihubmix"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-09-23"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-max-2026-01-23".pricing."aihubmix"]
input_cost_per_token = 3.4000000000000003e-7
output_cost_per_token = 0.00000137

[models."qwen3-max-preview"]
display_name = "Qwen3-Max-Preview"
model_family = "qwen"
mode = "chat"
max_input_tokens = 258048
max_output_tokens = 65536
max_tokens = 65536
litellm_provider = "dashscope"
providers = ["dashscope", "iflowcn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-12"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true
tiered_pricing = [{ input_cost_per_token = 0.0000012, output_cost_per_token = 0.000006, range = [0, 32000] }, { input_cost_per_token = 0.0000024, output_cost_per_token = 0.000012, range = [32000, 128000] }, { input_cost_per_token = 0.000003, output_cost_per_token = 0.000015, range = [128000, 252000] }]

[models."qwen3-next-80b"]
display_name = "Qwen 3 Next 80b"
model_family = "qwen"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 64000
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.0000018999999999999998
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2025-04-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-next-80b".pricing."venice"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.0000018999999999999998

[models."qwen3-next-80b-a3b-instruct"]
display_name = "Qwen3-Next 80B-A3B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn", "helicone"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-next-80b-a3b-instruct".pricing."alibaba"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.000002
[models."qwen3-next-80b-a3b-instruct".pricing."alibaba-cn"]
input_cost_per_token = 1.44e-7
output_cost_per_token = 5.739999999999999e-7
[models."qwen3-next-80b-a3b-instruct".pricing."helicone"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 0.0000014

[models."qwen3-next-80b-a3b-thinking"]
display_name = "Qwen3-Next 80B-A3B (Thinking)"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
input_cost_per_token = 5e-7
output_cost_per_token = 0.000006
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn", "cortecs"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-next-80b-a3b-thinking".pricing."alibaba"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.000006
[models."qwen3-next-80b-a3b-thinking".pricing."alibaba-cn"]
input_cost_per_token = 1.44e-7
output_cost_per_token = 0.000001434
[models."qwen3-next-80b-a3b-thinking".pricing."cortecs"]
input_cost_per_token = 1.64e-7
output_cost_per_token = 0.000001311

[models."qwen3-omni-flash"]
display_name = "Qwen3-Omni Flash"
model_family = "qwen"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 16384
input_cost_per_token = 4.3e-7
output_cost_per_token = 0.00000166
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-09-15"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
source = "modelsdev"

[models."qwen3-omni-flash".pricing."alibaba"]
input_cost_per_token = 4.3e-7
output_cost_per_token = 0.00000166
[models."qwen3-omni-flash".pricing."alibaba-cn"]
input_cost_per_token = 5.8e-8
output_cost_per_token = 2.3000000000000002e-7

[models."qwen3-omni-flash-realtime"]
display_name = "Qwen3-Omni Flash Realtime"
model_family = "qwen"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 16384
input_cost_per_token = 5.2e-7
output_cost_per_token = 0.00000199
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-09-15"
supported_modalities = ["text", "image", "audio", "video"]
supported_output_modalities = ["text", "audio"]
source = "modelsdev"

[models."qwen3-omni-flash-realtime".pricing."alibaba"]
input_cost_per_token = 5.2e-7
output_cost_per_token = 0.00000199
[models."qwen3-omni-flash-realtime".pricing."alibaba-cn"]
input_cost_per_token = 2.3000000000000002e-7
output_cost_per_token = 9.18e-7

[models."qwen3-vl-235b-a22b"]
display_name = "Qwen3-VL 235B-A22B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000028
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn", "venice"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"
reasoning_cost_per_token = 0.000008400000000000001

[models."qwen3-vl-235b-a22b".pricing."alibaba"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000028
reasoning_cost_per_token = 0.000008400000000000001
[models."qwen3-vl-235b-a22b".pricing."alibaba-cn"]
input_cost_per_token = 2.8670499999999997e-7
output_cost_per_token = 0.0000011468199999999999
reasoning_cost_per_token = 0.000002867051
[models."qwen3-vl-235b-a22b".pricing."venice"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.0000015

[models."qwen3-vl-235b-a22b-instruct"]
display_name = "Qwen3 VL 235B A22B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 16384
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015
litellm_provider = "helicone"
providers = ["helicone"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-09"
release_date = "2025-09-23"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."qwen3-vl-235b-a22b-instruct".pricing."helicone"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015

[models."qwen3-vl-30b-a3b"]
display_name = "Qwen3-VL 30B-A3B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"
reasoning_cost_per_token = 0.0000024

[models."qwen3-vl-30b-a3b".pricing."alibaba"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7
reasoning_cost_per_token = 0.0000024
[models."qwen3-vl-30b-a3b".pricing."alibaba-cn"]
input_cost_per_token = 1.08e-7
output_cost_per_token = 4.31e-7
reasoning_cost_per_token = 0.0000010760000000000002

[models."qwen3-vl-8b"]
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1.5e-7
output_cost_per_token = 5.5e-7
litellm_provider = "llamagate"
providers = ["llamagate"]
supports_function_calling = true
supports_vision = true
supports_response_schema = true

[models."qwen3-vl-8b".pricing."llamagate"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 5.5e-7

[models."qwen3-vl-plus"]
display_name = "Qwen3-VL Plus"
model_family = "qwen"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 32768
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000016000000000000001
litellm_provider = "alibaba"
providers = ["alibaba", "alibaba-cn", "iflowcn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-04"
release_date = "2025-09-23"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"
reasoning_cost_per_token = 0.0000048

[models."qwen3-vl-plus".pricing."alibaba"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000016000000000000001
reasoning_cost_per_token = 0.0000048
[models."qwen3-vl-plus".pricing."alibaba-cn"]
input_cost_per_token = 1.43353e-7
output_cost_per_token = 0.000001433525
reasoning_cost_per_token = 0.000004300576

[models."qwq-plus"]
display_name = "QwQ Plus"
model_family = "qwen"
mode = "chat"
max_input_tokens = 98304
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000024
litellm_provider = "dashscope"
providers = ["dashscope", "alibaba", "alibaba-cn"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-03-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://www.alibabacloud.com/help/en/model-studio/models"
supports_tool_choice = true

[models."qwq-plus".pricing."alibaba"]
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.0000024
[models."qwq-plus".pricing."alibaba-cn"]
input_cost_per_token = 2.3000000000000002e-7
output_cost_per_token = 5.739999999999999e-7
[models."qwq-plus".pricing."dashscope"]
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000024

[models."ranking/nvidia/llama-3.2-nv-rerankqa-1b-v2"]
mode = "rerank"
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "nvidia_nim"
providers = ["nvidia_nim"]
input_cost_per_query = 0

[models."ranking/nvidia/llama-3.2-nv-rerankqa-1b-v2".pricing."nvidia_nim"]
input_cost_per_query = 0
input_cost_per_token = 0
output_cost_per_token = 0

[models."recraftv2"]
mode = "image_generation"
litellm_provider = "recraft"
providers = ["recraft"]
source = "https://www.recraft.ai/docs#pricing"
output_cost_per_image = 0.022
supported_endpoints = ["/v1/images/generations"]

[models."recraftv2".pricing."recraft"]
output_cost_per_image = 0.022

[models."recraftv3"]
mode = "image_generation"
litellm_provider = "recraft"
providers = ["recraft"]
source = "https://www.recraft.ai/docs#pricing"
output_cost_per_image = 0.04
supported_endpoints = ["/v1/images/generations"]

[models."recraftv3".pricing."recraft"]
output_cost_per_image = 0.04

[models."rednote-hilab/dots.ocr"]
display_name = "dots.ocr"
model_family = "rednote"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8
cache_read_input_token_cost = 5e-9
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."rednote-hilab/dots.ocr".pricing."chutes"]
cache_read_input_token_cost = 5e-9
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8

[models."reka-core"]
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake"]

[models."reka-flash"]
mode = "chat"
max_input_tokens = 100000
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake"]

[models."rekaai/reka-flash-3"]
display_name = "Reka Flash 3"
model_family = "reka"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-10"
release_date = "2025-03-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."remove-background"]
mode = "image_edit"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.005
supported_endpoints = ["/v1/images/edits"]

[models."remove-background".pricing."stability"]
output_cost_per_image = 0.005

[models."replace-background-and-relight"]
mode = "image_edit"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.008
supported_endpoints = ["/v1/images/edits"]

[models."replace-background-and-relight".pricing."stability"]
output_cost_per_image = 0.008

[models."replicateopenai/gpt-oss-20b"]
mode = "chat"
input_cost_per_token = 9e-8
output_cost_per_token = 3.6e-7
litellm_provider = "replicate"
providers = ["replicate"]
supports_function_calling = true
supports_system_messages = true

[models."replicateopenai/gpt-oss-20b".pricing."replicate"]
input_cost_per_token = 9e-8
output_cost_per_token = 3.6e-7

[models."rerank-2"]
mode = "rerank"
max_input_tokens = 16000
max_output_tokens = 16000
max_tokens = 16000
input_cost_per_token = 5e-8
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage"]
max_query_tokens = 16000

[models."rerank-2".pricing."voyage"]
input_cost_per_token = 5e-8
output_cost_per_token = 0

[models."rerank-2-lite"]
mode = "rerank"
max_input_tokens = 8000
max_output_tokens = 8000
max_tokens = 8000
input_cost_per_token = 2e-8
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage"]
max_query_tokens = 8000

[models."rerank-2-lite".pricing."voyage"]
input_cost_per_token = 2e-8
output_cost_per_token = 0

[models."rerank-2.5"]
mode = "rerank"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 5e-8
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage"]
max_query_tokens = 32000

[models."rerank-2.5".pricing."voyage"]
input_cost_per_token = 5e-8
output_cost_per_token = 0

[models."rerank-2.5-lite"]
mode = "rerank"
max_input_tokens = 32000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 2e-8
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage"]
max_query_tokens = 32000

[models."rerank-2.5-lite".pricing."voyage"]
input_cost_per_token = 2e-8
output_cost_per_token = 0

[models."rerank-english-v2.0"]
mode = "rerank"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "cohere"
providers = ["cohere"]
input_cost_per_query = 0.002
max_query_tokens = 2048

[models."rerank-english-v2.0".pricing."cohere"]
input_cost_per_query = 0.002
input_cost_per_token = 0
output_cost_per_token = 0

[models."rerank-english-v3.0"]
mode = "rerank"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "cohere"
providers = ["cohere"]
input_cost_per_query = 0.002
max_query_tokens = 2048

[models."rerank-english-v3.0".pricing."cohere"]
input_cost_per_query = 0.002
input_cost_per_token = 0
output_cost_per_token = 0

[models."rerank-multilingual-v2.0"]
mode = "rerank"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "cohere"
providers = ["cohere"]
input_cost_per_query = 0.002
max_query_tokens = 2048

[models."rerank-multilingual-v2.0".pricing."cohere"]
input_cost_per_query = 0.002
input_cost_per_token = 0
output_cost_per_token = 0

[models."rerank-multilingual-v3.0"]
mode = "rerank"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "cohere"
providers = ["cohere"]
input_cost_per_query = 0.002
max_query_tokens = 2048

[models."rerank-multilingual-v3.0".pricing."cohere"]
input_cost_per_query = 0.002
input_cost_per_token = 0
output_cost_per_token = 0

[models."rerank-v3.5"]
mode = "rerank"
max_input_tokens = 4096
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "cohere"
providers = ["cohere"]
input_cost_per_query = 0.002
max_query_tokens = 2048

[models."rerank-v3.5".pricing."cohere"]
input_cost_per_query = 0.002
input_cost_per_token = 0
output_cost_per_token = 0

[models."route-llm"]
display_name = "Route LLM"
model_family = "gpt"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "abacus"
providers = ["abacus"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2024-01-01"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."route-llm".pricing."abacus"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."sa-east-1/deepseek.v3.2"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 7.4e-7
output_cost_per_token = 0.00000222
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_reasoning = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_tool_choice = true

[models."sa-east-1/deepseek.v3.2".pricing."bedrock"]
input_cost_per_token = 7.4e-7
output_cost_per_token = 0.00000222

[models."sa-east-1/meta.llama3-70b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000445
output_cost_per_token = 0.00000588
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."sa-east-1/meta.llama3-70b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 0.00000445
output_cost_per_token = 0.00000588

[models."sa-east-1/meta.llama3-8b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5e-7
output_cost_per_token = 0.00000101
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."sa-east-1/meta.llama3-8b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.00000101

[models."sa-east-1/minimax.minimax-m2.1"]
mode = "chat"
max_input_tokens = 196000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.00000144
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."sa-east-1/minimax.minimax-m2.1".pricing."bedrock"]
input_cost_per_token = 3.6e-7
output_cost_per_token = 0.00000144

[models."sa-east-1/moonshotai.kimi-k2-thinking"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 7.3e-7
output_cost_per_token = 0.00000303
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_reasoning = true

[models."sa-east-1/moonshotai.kimi-k2-thinking".pricing."bedrock"]
input_cost_per_token = 7.3e-7
output_cost_per_token = 0.00000303

[models."sa-east-1/moonshotai.kimi-k2.5"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 7.2e-7
output_cost_per_token = 0.0000036
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."sa-east-1/moonshotai.kimi-k2.5".pricing."bedrock"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 0.0000036

[models."sa-east-1/qwen.qwen3-coder-next"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-7
output_cost_per_token = 0.00000144
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."sa-east-1/qwen.qwen3-coder-next".pricing."bedrock"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.00000144

[models."sample_spec"]
mode = "one of: chat, embedding, completion, image_generation, audio_transcription, audio_speech, image_generation, moderation, rerank, search"
max_input_tokens = "max input tokens, if the provider specifies it. if not default to max_tokens"
max_output_tokens = "max output tokens, if the provider specifies it. if not default to max_tokens"
max_tokens = "LEGACY parameter. set to max_output_tokens if provider specifies it. IF not set to max_input_tokens, if provider specifies it."
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "one of https://docs.litellm.ai/docs/providers"
providers = ["one of https://docs.litellm.ai/docs/providers", "one of https:"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
deprecation_date = "date when the model becomes deprecated in the format YYYY-MM-DD"
code_interpreter_cost_per_session = 0
computer_use_input_cost_per_1k_tokens = 0
computer_use_output_cost_per_1k_tokens = 0
file_search_cost_per_1k_calls = 0
file_search_cost_per_gb_per_day = 0
input_cost_per_audio_token = 0
output_cost_per_reasoning_token = 0
supported_regions = ["global", "us-west-2", "eu-west-1", "ap-southeast-1", "ap-northeast-1"]
supports_audio_input = true
supports_audio_output = true
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_web_search = true
vector_store_cost_per_gb_per_day = 0

[models."sample_spec".search_context_cost_per_query]
search_context_size_high = 0
search_context_size_low = 0
search_context_size_medium = 0

[models."sample_spec".pricing."one of https://docs.litellm.ai/docs/providers"]
code_interpreter_cost_per_session = 0
computer_use_input_cost_per_1k_tokens = 0
computer_use_output_cost_per_1k_tokens = 0
file_search_cost_per_1k_calls = 0
file_search_cost_per_gb_per_day = 0
input_cost_per_audio_token = 0
input_cost_per_token = 0
output_cost_per_reasoning_token = 0
output_cost_per_token = 0
vector_store_cost_per_gb_per_day = 0

[models."sao10k/l3-70b-euryale-v2.1"]
display_name = "L3 70B Euryale V2.1	"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000148
output_cost_per_token = 0.00000148
litellm_provider = "novita"
providers = ["novita", "novita-ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-06-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."sao10k/l3-70b-euryale-v2.1".pricing."novita"]
input_cost_per_token = 0.00000148
output_cost_per_token = 0.00000148
[models."sao10k/l3-70b-euryale-v2.1".pricing."novita-ai"]
input_cost_per_token = 0.00000148
output_cost_per_token = 0.00000148

[models."sao10k/l3-8b-lunaris"]
display_name = "Sao10k L3 8B Lunaris	"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5e-8
output_cost_per_token = 5e-8
litellm_provider = "novita"
providers = ["novita", "novita-ai"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2024-11-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."sao10k/l3-8b-lunaris".pricing."novita"]
input_cost_per_token = 5e-8
output_cost_per_token = 5e-8
[models."sao10k/l3-8b-lunaris".pricing."novita-ai"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 5.0000000000000004e-8

[models."sao10k/l31-70b-euryale-v2.2"]
display_name = "L31 70B Euryale V2.2"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000148
output_cost_per_token = 0.00000148
litellm_provider = "novita"
providers = ["novita", "novita-ai"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2024-09-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."sao10k/l31-70b-euryale-v2.2".pricing."novita"]
input_cost_per_token = 0.00000148
output_cost_per_token = 0.00000148
[models."sao10k/l31-70b-euryale-v2.2".pricing."novita-ai"]
input_cost_per_token = 0.00000148
output_cost_per_token = 0.00000148

[models."sarvam-m"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0
output_cost_per_token = 0
cache_read_input_token_cost = 0
cache_creation_input_token_cost = 0
litellm_provider = "sarvam"
providers = ["sarvam"]
supports_reasoning = true
cache_creation_input_token_cost_above_1hr = 0

[models."sarvam-m".pricing."sarvam"]
cache_creation_input_token_cost = 0
cache_creation_input_token_cost_above_1hr = 0
cache_read_input_token_cost = 0
input_cost_per_token = 0
output_cost_per_token = 0

[models."sarvamai/sarvam-m:free"]
display_name = "Sarvam-M (free)"
model_family = "sarvam"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-05"
release_date = "2025-05-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."scribe_v1"]
mode = "audio_transcription"
litellm_provider = "elevenlabs"
providers = ["elevenlabs"]
source = "https://elevenlabs.io/pricing"
input_cost_per_second = 0.0000611
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."scribe_v1".metadata]
calculation = "$0.22/hour = $0.00366/minute = $0.0000611 per second (enterprise pricing)"
notes = "ElevenLabs Scribe v1 - state-of-the-art speech recognition model with 99 language support"
original_pricing_per_hour = 0.22

[models."scribe_v1".pricing."elevenlabs"]
input_cost_per_second = 0.0000611
output_cost_per_second = 0

[models."scribe_v1_experimental"]
mode = "audio_transcription"
litellm_provider = "elevenlabs"
providers = ["elevenlabs"]
source = "https://elevenlabs.io/pricing"
input_cost_per_second = 0.0000611
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."scribe_v1_experimental".metadata]
calculation = "$0.22/hour = $0.00366/minute = $0.0000611 per second (enterprise pricing)"
notes = "ElevenLabs Scribe v1 experimental - enhanced version of the main Scribe model"
original_pricing_per_hour = 0.22

[models."scribe_v1_experimental".pricing."elevenlabs"]
input_cost_per_second = 0.0000611
output_cost_per_second = 0

[models."sd3"]
mode = "image_generation"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.065
supported_endpoints = ["/v1/images/generations"]

[models."sd3".pricing."stability"]
output_cost_per_image = 0.065

[models."sd3-large"]
mode = "image_generation"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.065
supported_endpoints = ["/v1/images/generations"]

[models."sd3-large".pricing."stability"]
output_cost_per_image = 0.065

[models."sd3-large-turbo"]
mode = "image_generation"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.04
supported_endpoints = ["/v1/images/generations"]

[models."sd3-large-turbo".pricing."stability"]
output_cost_per_image = 0.04

[models."sd3-medium"]
mode = "image_generation"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.035
supported_endpoints = ["/v1/images/generations"]

[models."sd3-medium".pricing."stability"]
output_cost_per_image = 0.035

[models."sd3.5-large"]
mode = "image_generation"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.065
supported_endpoints = ["/v1/images/generations"]

[models."sd3.5-large".pricing."stability"]
output_cost_per_image = 0.065

[models."sd3.5-large-turbo"]
mode = "image_generation"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.04
supported_endpoints = ["/v1/images/generations"]

[models."sd3.5-large-turbo".pricing."stability"]
output_cost_per_image = 0.04

[models."sd3.5-medium"]
mode = "image_generation"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.035
supported_endpoints = ["/v1/images/generations"]

[models."sd3.5-medium".pricing."stability"]
output_cost_per_image = 0.035

[models."sdaia/allam-1-13b-instruct"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.0000018
output_cost_per_token = 0.0000018
litellm_provider = "watsonx"
providers = ["watsonx"]
supports_function_calling = false
supports_vision = false
supports_parallel_function_calling = false

[models."sdaia/allam-1-13b-instruct".pricing."watsonx"]
input_cost_per_token = 0.0000018
output_cost_per_token = 0.0000018

[models."search"]
mode = "search"
litellm_provider = "dataforseo"
providers = ["dataforseo", "duckduckgo", "exa_ai", "firecrawl", "google_pse", "linkup", "parallel_ai", "perplexity", "searxng", "tavily"]
input_cost_per_query = 0.003

[models."search".pricing."dataforseo"]
input_cost_per_query = 0.003
[models."search".pricing."duckduckgo"]
input_cost_per_query = 0
[models."search".pricing."google_pse"]
input_cost_per_query = 0.005
[models."search".pricing."linkup"]
input_cost_per_query = 0.00587
[models."search".pricing."parallel_ai"]
input_cost_per_query = 0.004
[models."search".pricing."perplexity"]
input_cost_per_query = 0.005
[models."search".pricing."searxng"]
input_cost_per_query = 0
[models."search".pricing."tavily"]
input_cost_per_query = 0.008

[models."search-advanced"]
mode = "search"
litellm_provider = "tavily"
providers = ["tavily"]
input_cost_per_query = 0.016

[models."search-advanced".pricing."tavily"]
input_cost_per_query = 0.016

[models."search-and-recolor"]
mode = "image_edit"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.005
supported_endpoints = ["/v1/images/edits"]

[models."search-and-recolor".pricing."stability"]
output_cost_per_image = 0.005

[models."search-and-replace"]
mode = "image_edit"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.005
supported_endpoints = ["/v1/images/edits"]

[models."search-and-replace".pricing."stability"]
output_cost_per_image = 0.005

[models."search-deep"]
mode = "search"
litellm_provider = "linkup"
providers = ["linkup"]
input_cost_per_query = 0.05867

[models."search-deep".pricing."linkup"]
input_cost_per_query = 0.05867

[models."search-pro"]
mode = "search"
litellm_provider = "parallel_ai"
providers = ["parallel_ai"]
input_cost_per_query = 0.009

[models."search-pro".pricing."parallel_ai"]
input_cost_per_query = 0.009

[models."search_api"]
mode = "vector_store"
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
input_cost_per_query = 0.0015

[models."search_api".pricing."vertex_ai"]
input_cost_per_query = 0.0015

[models."sherlock-dash-alpha"]
display_name = "Sherlock Dash Alpha"
model_family = "sherlock"
mode = "chat"
max_input_tokens = 1840000
max_output_tokens = 0
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-11"
release_date = "2025-11-15"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."sherlock-think-alpha"]
display_name = "Sherlock Think Alpha"
model_family = "sherlock"
mode = "chat"
max_input_tokens = 1840000
max_output_tokens = 0
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-11"
release_date = "2025-11-15"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."sketch"]
mode = "image_edit"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.005
supported_endpoints = ["/v1/images/edits"]

[models."sketch".pricing."stability"]
output_cost_per_image = 0.005

[models."skywork/r1v4-lite"]
display_name = "Skywork R1V4-Lite"
model_family = "skywork"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7
litellm_provider = "novita"
providers = ["novita", "novita-ai"]
supports_function_calling = false
supports_vision = true
supports_reasoning = false
open_weights = true
release_date = "2025-11-18"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."skywork/r1v4-lite".pricing."novita"]
input_cost_per_token = 2e-7
output_cost_per_token = 6e-7
[models."skywork/r1v4-lite".pricing."novita-ai"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 6e-7

[models."snowflake-arctic"]
mode = "chat"
max_input_tokens = 4096
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake"]

[models."snowflake-llama-3.1-405b"]
mode = "chat"
max_input_tokens = 8000
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake"]

[models."snowflake-llama-3.3-70b"]
mode = "chat"
max_input_tokens = 8000
max_output_tokens = 8192
max_tokens = 8192
litellm_provider = "snowflake"
providers = ["snowflake"]

[models."solar-mini"]
display_name = "solar-mini"
model_family = "solar-mini"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 4096
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "upstage"
providers = ["upstage"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-09"
release_date = "2024-06-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."solar-mini".pricing."upstage"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7

[models."solar-pro2"]
display_name = "solar-pro2"
model_family = "solar-pro"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 8192
input_cost_per_token = 2.5e-7
output_cost_per_token = 2.5e-7
litellm_provider = "upstage"
providers = ["upstage"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-03"
release_date = "2025-05-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."solar-pro2".pricing."upstage"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 2.5e-7

[models."solar-pro3"]
display_name = "solar-pro3"
model_family = "solar-pro"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 2.5e-7
output_cost_per_token = 2.5e-7
litellm_provider = "upstage"
providers = ["upstage"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-03"
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."solar-pro3".pricing."upstage"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 2.5e-7

[models."sonar"]
display_name = "Sonar"
model_family = "sonar"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8000
max_tokens = 128000
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001
litellm_provider = "perplexity"
providers = ["perplexity", "helicone", "kilo", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-02"
release_date = "2025-02-19"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_web_search = true

[models."sonar".search_context_cost_per_query]
search_context_size_high = 0.012
search_context_size_low = 0.005
search_context_size_medium = 0.008

[models."sonar".pricing."helicone"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001
[models."sonar".pricing."kilo"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001
[models."sonar".pricing."vercel"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001
[models."sonar".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000001

[models."sonar-deep-research"]
display_name = "Perplexity Sonar Deep Research"
model_family = "sonar-deep-research"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 128000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
litellm_provider = "perplexity"
providers = ["perplexity", "helicone", "kilo"]
supports_function_calling = false
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-01-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
citation_cost_per_token = 0.000002
output_cost_per_reasoning_token = 0.000003
reasoning_cost_per_token = 0.000003
supports_web_search = true

[models."sonar-deep-research".search_context_cost_per_query]
search_context_size_high = 0.005
search_context_size_low = 0.005
search_context_size_medium = 0.005

[models."sonar-deep-research".pricing."helicone"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."sonar-deep-research".pricing."kilo"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."sonar-deep-research".pricing."perplexity"]
citation_cost_per_token = 0.000002
input_cost_per_token = 0.000002
output_cost_per_reasoning_token = 0.000003
output_cost_per_token = 0.000008

[models."sonar-medium-chat"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000018
litellm_provider = "perplexity"
providers = ["perplexity"]

[models."sonar-medium-chat".pricing."perplexity"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000018

[models."sonar-medium-online"]
mode = "chat"
max_input_tokens = 12000
max_output_tokens = 12000
max_tokens = 12000
input_cost_per_token = 0
output_cost_per_token = 0.0000018
litellm_provider = "perplexity"
providers = ["perplexity"]
input_cost_per_request = 0.005

[models."sonar-medium-online".pricing."perplexity"]
input_cost_per_request = 0.005
input_cost_per_token = 0
output_cost_per_token = 0.0000018

[models."sonar-pro"]
display_name = "Sonar Pro"
model_family = "sonar-pro"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8000
max_tokens = 8000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "perplexity"
providers = ["perplexity", "helicone", "kilo", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-09"
release_date = "2025-02-19"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_web_search = true

[models."sonar-pro".search_context_cost_per_query]
search_context_size_high = 0.014
search_context_size_low = 0.006
search_context_size_medium = 0.01

[models."sonar-pro".pricing."helicone"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."sonar-pro".pricing."kilo"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."sonar-pro".pricing."perplexity"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."sonar-pro".pricing."vercel"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."sonar-pro".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."sonar-reasoning"]
display_name = "Sonar Reasoning"
model_family = "sonar-reasoning"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8000
max_tokens = 128000
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
litellm_provider = "perplexity"
providers = ["perplexity", "helicone", "vercel", "vercel_ai_gateway"]
supports_function_calling = false
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-09"
release_date = "2025-02-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_web_search = true

[models."sonar-reasoning".search_context_cost_per_query]
search_context_size_high = 0.014
search_context_size_low = 0.005
search_context_size_medium = 0.008

[models."sonar-reasoning".pricing."helicone"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."sonar-reasoning".pricing."perplexity"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."sonar-reasoning".pricing."vercel"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."sonar-reasoning".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005

[models."sonar-reasoning-pro"]
display_name = "Sonar Reasoning Pro"
model_family = "sonar-reasoning"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8000
max_tokens = 128000
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
litellm_provider = "perplexity"
providers = ["perplexity", "helicone", "kilo", "vercel", "vercel_ai_gateway"]
supports_function_calling = false
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-09"
release_date = "2025-02-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_web_search = true

[models."sonar-reasoning-pro".search_context_cost_per_query]
search_context_size_high = 0.014
search_context_size_low = 0.006
search_context_size_medium = 0.01

[models."sonar-reasoning-pro".pricing."helicone"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."sonar-reasoning-pro".pricing."kilo"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."sonar-reasoning-pro".pricing."perplexity"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."sonar-reasoning-pro".pricing."vercel"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008
[models."sonar-reasoning-pro".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000008

[models."sonar-small-chat"]
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7
litellm_provider = "perplexity"
providers = ["perplexity"]

[models."sonar-small-chat".pricing."perplexity"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.8e-7

[models."sonar-small-online"]
mode = "chat"
max_input_tokens = 12000
max_output_tokens = 12000
max_tokens = 12000
input_cost_per_token = 0
output_cost_per_token = 2.8e-7
litellm_provider = "perplexity"
providers = ["perplexity"]
input_cost_per_request = 0.005

[models."sonar-small-online".pricing."perplexity"]
input_cost_per_request = 0.005
input_cost_per_token = 0
output_cost_per_token = 2.8e-7

[models."sora-2"]
display_name = "Sora-2"
model_family = "sora"
mode = "video_generation"
max_input_tokens = 0
max_output_tokens = 0
litellm_provider = "openai"
providers = ["openai", "azure"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-10-06"
supported_modalities = ["text", "image"]
supported_output_modalities = ["video"]
source = "https://platform.openai.com/docs/api-reference/videos"
output_cost_per_video_per_second = 0.1
supported_resolutions = ["720x1280", "1280x720"]

[models."sora-2".pricing."azure"]
output_cost_per_video_per_second = 0.1
[models."sora-2".pricing."openai"]
output_cost_per_video_per_second = 0.1

[models."sora-2-pro"]
display_name = "Sora-2-Pro"
model_family = "sora"
mode = "video_generation"
max_input_tokens = 0
max_output_tokens = 0
litellm_provider = "openai"
providers = ["openai", "azure"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-10-06"
supported_modalities = ["text", "image"]
supported_output_modalities = ["video"]
source = "https://platform.openai.com/docs/api-reference/videos"
output_cost_per_video_per_second = 0.3
supported_resolutions = ["720x1280", "1280x720"]

[models."sora-2-pro".pricing."azure"]
output_cost_per_video_per_second = 0.3
[models."sora-2-pro".pricing."openai"]
output_cost_per_video_per_second = 0.3

[models."sora-2-pro-high-res"]
mode = "video_generation"
litellm_provider = "openai"
providers = ["openai", "azure"]
supported_modalities = ["text", "image"]
supported_output_modalities = ["video"]
source = "https://platform.openai.com/docs/api-reference/videos"
output_cost_per_video_per_second = 0.5
supported_resolutions = ["1024x1792", "1792x1024"]

[models."sora-2-pro-high-res".pricing."azure"]
output_cost_per_video_per_second = 0.5
[models."sora-2-pro-high-res".pricing."openai"]
output_cost_per_video_per_second = 0.5

[models."sourceful/riverflow-v2-fast-preview"]
display_name = "Riverflow V2 Fast Preview"
model_family = "sourceful"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2025-12-08"
supported_modalities = ["text", "image"]
supported_output_modalities = ["image"]
source = "modelsdev"

[models."sourceful/riverflow-v2-max-preview"]
display_name = "Riverflow V2 Max Preview"
model_family = "sourceful"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2025-12-08"
supported_modalities = ["text", "image"]
supported_output_modalities = ["image"]
source = "modelsdev"

[models."sourceful/riverflow-v2-standard-preview"]
display_name = "Riverflow V2 Standard Preview"
model_family = "sourceful"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2025-12-08"
supported_modalities = ["text", "image"]
supported_output_modalities = ["image"]
source = "modelsdev"

[models."speakleash/Bielik-11B-v2-6-Instruct"]
display_name = "Bielik 11B v2.6 Instruct"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 32000
input_cost_per_token = 6.7e-7
output_cost_per_token = 6.7e-7
litellm_provider = "cloudferro-sherlock"
providers = ["cloudferro-sherlock"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-03"
release_date = "2025-03-13"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."speakleash/Bielik-11B-v2-6-Instruct".pricing."cloudferro-sherlock"]
input_cost_per_token = 6.7e-7
output_cost_per_token = 6.7e-7

[models."speakleash/Bielik-11B-v3-0-Instruct"]
display_name = "Bielik 11B v3.0 Instruct"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 32000
input_cost_per_token = 6.7e-7
output_cost_per_token = 6.7e-7
litellm_provider = "cloudferro-sherlock"
providers = ["cloudferro-sherlock"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-03"
release_date = "2025-03-13"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."speakleash/Bielik-11B-v3-0-Instruct".pricing."cloudferro-sherlock"]
input_cost_per_token = 6.7e-7
output_cost_per_token = 6.7e-7

[models."speech-02-hd"]
mode = "audio_speech"
litellm_provider = "minimax"
providers = ["minimax"]
input_cost_per_character = 0.0001
supported_endpoints = ["/v1/audio/speech"]

[models."speech-02-hd".pricing."minimax"]
input_cost_per_character = 0.0001

[models."speech-02-turbo"]
mode = "audio_speech"
litellm_provider = "minimax"
providers = ["minimax"]
input_cost_per_character = 0.00006
supported_endpoints = ["/v1/audio/speech"]

[models."speech-02-turbo".pricing."minimax"]
input_cost_per_character = 0.00006

[models."speech-2.6-hd"]
mode = "audio_speech"
litellm_provider = "minimax"
providers = ["minimax"]
input_cost_per_character = 0.0001
supported_endpoints = ["/v1/audio/speech"]

[models."speech-2.6-hd".pricing."minimax"]
input_cost_per_character = 0.0001

[models."speech-2.6-turbo"]
mode = "audio_speech"
litellm_provider = "minimax"
providers = ["minimax"]
input_cost_per_character = 0.00006
supported_endpoints = ["/v1/audio/speech"]

[models."speech-2.6-turbo".pricing."minimax"]
input_cost_per_character = 0.00006

[models."speech/azure-tts"]
mode = "audio_speech"
litellm_provider = "azure"
providers = ["azure"]
source = "https://azure.microsoft.com/en-us/pricing/calculator/"
input_cost_per_character = 0.000015

[models."speech/azure-tts".pricing."azure"]
input_cost_per_character = 0.000015

[models."speech/azure-tts-hd"]
mode = "audio_speech"
litellm_provider = "azure"
providers = ["azure"]
source = "https://azure.microsoft.com/en-us/pricing/calculator/"
input_cost_per_character = 0.00003

[models."speech/azure-tts-hd".pricing."azure"]
input_cost_per_character = 0.00003

[models."stability.sd3-5-large-v1:0"]
mode = "image_generation"
max_input_tokens = 77
max_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.08

[models."stability.sd3-5-large-v1:0".pricing."bedrock"]
output_cost_per_image = 0.08

[models."stability.sd3-large-v1:0"]
mode = "image_generation"
max_input_tokens = 77
max_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.08

[models."stability.sd3-large-v1:0".pricing."bedrock"]
output_cost_per_image = 0.08

[models."stability.stable-conservative-upscale-v1:0"]
mode = "image_edit"
max_input_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.4

[models."stability.stable-conservative-upscale-v1:0".pricing."bedrock"]
output_cost_per_image = 0.4

[models."stability.stable-creative-upscale-v1:0"]
mode = "image_edit"
max_input_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.6

[models."stability.stable-creative-upscale-v1:0".pricing."bedrock"]
output_cost_per_image = 0.6

[models."stability.stable-fast-upscale-v1:0"]
mode = "image_edit"
max_input_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.03

[models."stability.stable-fast-upscale-v1:0".pricing."bedrock"]
output_cost_per_image = 0.03

[models."stability.stable-image-control-sketch-v1:0"]
mode = "image_edit"
max_input_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.07

[models."stability.stable-image-control-sketch-v1:0".pricing."bedrock"]
output_cost_per_image = 0.07

[models."stability.stable-image-control-structure-v1:0"]
mode = "image_edit"
max_input_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.07

[models."stability.stable-image-control-structure-v1:0".pricing."bedrock"]
output_cost_per_image = 0.07

[models."stability.stable-image-core-v1:0"]
mode = "image_generation"
max_input_tokens = 77
max_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.04

[models."stability.stable-image-core-v1:0".pricing."bedrock"]
output_cost_per_image = 0.04

[models."stability.stable-image-core-v1:1"]
mode = "image_generation"
max_input_tokens = 77
max_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.04

[models."stability.stable-image-core-v1:1".pricing."bedrock"]
output_cost_per_image = 0.04

[models."stability.stable-image-erase-object-v1:0"]
mode = "image_edit"
max_input_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.07

[models."stability.stable-image-erase-object-v1:0".pricing."bedrock"]
output_cost_per_image = 0.07

[models."stability.stable-image-inpaint-v1:0"]
mode = "image_edit"
max_input_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.07

[models."stability.stable-image-inpaint-v1:0".pricing."bedrock"]
output_cost_per_image = 0.07

[models."stability.stable-image-remove-background-v1:0"]
mode = "image_edit"
max_input_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.07

[models."stability.stable-image-remove-background-v1:0".pricing."bedrock"]
output_cost_per_image = 0.07

[models."stability.stable-image-search-recolor-v1:0"]
mode = "image_edit"
max_input_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.07

[models."stability.stable-image-search-recolor-v1:0".pricing."bedrock"]
output_cost_per_image = 0.07

[models."stability.stable-image-search-replace-v1:0"]
mode = "image_edit"
max_input_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.07

[models."stability.stable-image-search-replace-v1:0".pricing."bedrock"]
output_cost_per_image = 0.07

[models."stability.stable-image-style-guide-v1:0"]
mode = "image_edit"
max_input_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.07

[models."stability.stable-image-style-guide-v1:0".pricing."bedrock"]
output_cost_per_image = 0.07

[models."stability.stable-image-ultra-v1:0"]
mode = "image_generation"
max_input_tokens = 77
max_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.14

[models."stability.stable-image-ultra-v1:0".pricing."bedrock"]
output_cost_per_image = 0.14

[models."stability.stable-image-ultra-v1:1"]
mode = "image_generation"
max_input_tokens = 77
max_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.14

[models."stability.stable-image-ultra-v1:1".pricing."bedrock"]
output_cost_per_image = 0.14

[models."stability.stable-outpaint-v1:0"]
mode = "image_edit"
max_input_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.06

[models."stability.stable-outpaint-v1:0".pricing."bedrock"]
output_cost_per_image = 0.06

[models."stability.stable-style-transfer-v1:0"]
mode = "image_edit"
max_input_tokens = 77
litellm_provider = "bedrock"
providers = ["bedrock"]
output_cost_per_image = 0.08

[models."stability.stable-style-transfer-v1:0".pricing."bedrock"]
output_cost_per_image = 0.08

[models."stabilityai/stable-diffusion-xl-base-1.0"]
mode = "image_generation"
litellm_provider = "nscale"
providers = ["nscale"]
source = "https://docs.nscale.com/docs/inference/serverless-models/current#image-models"
input_cost_per_pixel = 3e-9
output_cost_per_pixel = 0
supported_endpoints = ["/v1/images/generations"]

[models."stabilityai/stable-diffusion-xl-base-1.0".pricing."nscale"]
input_cost_per_pixel = 3e-9
output_cost_per_pixel = 0

[models."stable-image-core"]
mode = "image_generation"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.03
supported_endpoints = ["/v1/images/generations"]

[models."stable-image-core".pricing."stability"]
output_cost_per_image = 0.03

[models."stable-image-ultra"]
mode = "image_generation"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.08
supported_endpoints = ["/v1/images/generations"]

[models."stable-image-ultra".pricing."stability"]
output_cost_per_image = 0.08

[models."standard"]
mode = "audio_speech"
litellm_provider = "aws_polly"
providers = ["aws_polly"]
source = "https://aws.amazon.com/polly/pricing/"
input_cost_per_character = 0.000004
supported_endpoints = ["/v1/audio/speech"]

[models."standard".pricing."aws_polly"]
input_cost_per_character = 0.000004

[models."standard/1024-x-1024/dall-e-2"]
mode = "image_generation"
output_cost_per_token = 0
litellm_provider = "azure"
providers = ["azure"]
input_cost_per_pixel = 0

[models."standard/1024-x-1024/dall-e-2".pricing."azure"]
input_cost_per_pixel = 0
output_cost_per_token = 0

[models."standard/1024-x-1024/dall-e-3"]
mode = "image_generation"
output_cost_per_token = 0
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 3.81469e-8

[models."standard/1024-x-1024/dall-e-3".pricing."azure"]
input_cost_per_pixel = 3.81469e-8
output_cost_per_token = 0
[models."standard/1024-x-1024/dall-e-3".pricing."openai"]
input_cost_per_pixel = 3.81469e-8
output_cost_per_pixel = 0

[models."standard/1024-x-1024/gpt-image-1.5"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.009
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."standard/1024-x-1024/gpt-image-1.5".pricing."openai"]
input_cost_per_image = 0.009

[models."standard/1024-x-1024/gpt-image-1.5-2025-12-16"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.009
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."standard/1024-x-1024/gpt-image-1.5-2025-12-16".pricing."openai"]
input_cost_per_image = 0.009

[models."standard/1024-x-1536/gpt-image-1.5"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.013
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."standard/1024-x-1536/gpt-image-1.5".pricing."openai"]
input_cost_per_image = 0.013

[models."standard/1024-x-1536/gpt-image-1.5-2025-12-16"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.013
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."standard/1024-x-1536/gpt-image-1.5-2025-12-16".pricing."openai"]
input_cost_per_image = 0.013

[models."standard/1024-x-1792/dall-e-3"]
mode = "image_generation"
output_cost_per_token = 0
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 4.359e-8

[models."standard/1024-x-1792/dall-e-3".pricing."azure"]
input_cost_per_pixel = 4.359e-8
output_cost_per_token = 0
[models."standard/1024-x-1792/dall-e-3".pricing."openai"]
input_cost_per_pixel = 4.359e-8
output_cost_per_pixel = 0

[models."standard/1536-x-1024/gpt-image-1.5"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.013
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."standard/1536-x-1024/gpt-image-1.5".pricing."openai"]
input_cost_per_image = 0.013

[models."standard/1536-x-1024/gpt-image-1.5-2025-12-16"]
mode = "image_generation"
litellm_provider = "openai"
providers = ["openai"]
supports_vision = true
supports_pdf_input = true
input_cost_per_image = 0.013
supported_endpoints = ["/v1/images/generations", "/v1/images/edits"]

[models."standard/1536-x-1024/gpt-image-1.5-2025-12-16".pricing."openai"]
input_cost_per_image = 0.013

[models."standard/1792-x-1024/dall-e-3"]
mode = "image_generation"
output_cost_per_token = 0
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_pixel = 4.359e-8

[models."standard/1792-x-1024/dall-e-3".pricing."azure"]
input_cost_per_pixel = 4.359e-8
output_cost_per_token = 0
[models."standard/1792-x-1024/dall-e-3".pricing."openai"]
input_cost_per_pixel = 4.359e-8
output_cost_per_pixel = 0

[models."step-1-32k"]
display_name = "Step 1 (32K)"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
input_cost_per_token = 0.00000205
output_cost_per_token = 0.00000959
cache_read_input_token_cost = 4.1e-7
litellm_provider = "stepfun"
providers = ["stepfun"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-06"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."step-1-32k".pricing."stepfun"]
cache_read_input_token_cost = 4.1e-7
input_cost_per_token = 0.00000205
output_cost_per_token = 0.00000959

[models."step-2-16k"]
display_name = "Step 2 (16K)"
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 8192
input_cost_per_token = 0.00000521
output_cost_per_token = 0.000016440000000000002
cache_read_input_token_cost = 0.00000104
litellm_provider = "stepfun"
providers = ["stepfun"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-06"
release_date = "2025-01-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."step-2-16k".pricing."stepfun"]
cache_read_input_token_cost = 0.00000104
input_cost_per_token = 0.00000521
output_cost_per_token = 0.000016440000000000002

[models."step-3-5-flash"]
display_name = "Step 3.5 Flash"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
input_cost_per_token = 9.6e-8
output_cost_per_token = 2.88e-7
cache_read_input_token_cost = 1.8999999999999998e-8
litellm_provider = "stepfun"
providers = ["stepfun"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2026-01-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."step-3-5-flash".pricing."stepfun"]
cache_read_input_token_cost = 1.8999999999999998e-8
input_cost_per_token = 9.6e-8
output_cost_per_token = 2.88e-7

[models."stepfun-ai/Step-3-5-Flash"]
display_name = "stepfun-ai/Step-3.5-Flash"
model_family = "step"
mode = "chat"
max_input_tokens = 262000
max_output_tokens = 262000
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "siliconflow"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2026-02-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."stepfun-ai/Step-3-5-Flash".pricing."siliconflow"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
[models."stepfun-ai/Step-3-5-Flash".pricing."siliconflow-cn"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7

[models."stepfun/step-3"]
display_name = "Step-3"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 64000
input_cost_per_token = 2.1e-7
output_cost_per_token = 5.699999999999999e-7
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2025-07-31"
supported_modalities = ["image", "text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."stepfun/step-3".pricing."zenmux"]
input_cost_per_token = 2.1e-7
output_cost_per_token = 5.699999999999999e-7

[models."stepfun/step-3-5-flash"]
display_name = "Step 3.5 Flash"
model_family = "step"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
cache_read_input_token_cost = 2e-8
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2026-01-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."stepfun/step-3-5-flash".pricing."kilo"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
[models."stepfun/step-3-5-flash".pricing."openrouter"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
[models."stepfun/step-3-5-flash".pricing."zenmux"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7

[models."stepfun/step-3-5-flash-free"]
display_name = "Step 3.5 Flash (Free)"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 64000
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2026-02-02"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."stepfun/step-3-5-flash:free"]
display_name = "Step 3.5 Flash (free)"
model_family = "step"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2026-01-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."structure"]
mode = "image_edit"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.005
supported_endpoints = ["/v1/images/edits"]

[models."structure".pricing."stability"]
output_cost_per_image = 0.005

[models."style"]
mode = "image_edit"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.005
supported_endpoints = ["/v1/images/edits"]

[models."style".pricing."stability"]
output_cost_per_image = 0.005

[models."style-transfer"]
mode = "image_edit"
litellm_provider = "stability"
providers = ["stability"]
output_cost_per_image = 0.008
supported_endpoints = ["/v1/images/edits"]

[models."style-transfer".pricing."stability"]
output_cost_per_image = 0.008

[models."swiss-ai/apertus-70b-instruct"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "publicai"
providers = ["publicai"]
supports_function_calling = true
source = "https://platform.publicai.co/docs"
supports_tool_choice = true

[models."swiss-ai/apertus-70b-instruct".pricing."publicai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."swiss-ai/apertus-8b-instruct"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "publicai"
providers = ["publicai"]
supports_function_calling = true
source = "https://platform.publicai.co/docs"
supports_tool_choice = true

[models."swiss-ai/apertus-8b-instruct".pricing."publicai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."switchpoint/router"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 8.5e-7
output_cost_per_token = 0.0000034
litellm_provider = "openrouter"
providers = ["openrouter"]
source = "https://openrouter.ai/switchpoint/router"
supports_tool_choice = true

[models."switchpoint/router".pricing."openrouter"]
input_cost_per_token = 8.5e-7
output_cost_per_token = 0.0000034

[models."tencent/Hunyuan-A13B-Instruct"]
display_name = "tencent/Hunyuan-A13B-Instruct"
model_family = "hunyuan"
mode = "chat"
max_input_tokens = 131000
max_output_tokens = 131000
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "kilo", "siliconflow"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2025-06-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."tencent/Hunyuan-A13B-Instruct".pricing."kilo"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7
[models."tencent/Hunyuan-A13B-Instruct".pricing."siliconflow"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7
[models."tencent/Hunyuan-A13B-Instruct".pricing."siliconflow-cn"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 5.699999999999999e-7

[models."tencent/Hunyuan-MT-7B"]
display_name = "tencent/Hunyuan-MT-7B"
model_family = "hunyuan"
mode = "chat"
max_input_tokens = 33000
max_output_tokens = 33000
litellm_provider = "siliconflow-cn"
providers = ["siliconflow-cn", "siliconflow"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
release_date = "2025-09-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."text-bison"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "palm"
providers = ["palm", "vertex_ai-text-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."text-bison".pricing."palm"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
[models."text-bison".pricing."vertex_ai-text-models"]
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7

[models."text-bison-001"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "palm"
providers = ["palm"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."text-bison-001".pricing."palm"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7

[models."text-bison-safety-off"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "palm"
providers = ["palm"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."text-bison-safety-off".pricing."palm"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7

[models."text-bison-safety-recitation-off"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "palm"
providers = ["palm"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."text-bison-safety-recitation-off".pricing."palm"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7

[models."text-bison32k"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-text-models"
providers = ["vertex_ai-text-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7

[models."text-bison32k".pricing."vertex_ai-text-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."text-bison32k@002"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 1.25e-7
output_cost_per_token = 1.25e-7
litellm_provider = "vertex_ai-text-models"
providers = ["vertex_ai-text-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7

[models."text-bison32k@002".pricing."vertex_ai-text-models"]
input_cost_per_character = 2.5e-7
input_cost_per_token = 1.25e-7
output_cost_per_character = 5e-7
output_cost_per_token = 1.25e-7

[models."text-bison@001"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
litellm_provider = "vertex_ai-text-models"
providers = ["vertex_ai-text-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7

[models."text-bison@001".pricing."vertex_ai-text-models"]
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7

[models."text-bison@002"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
litellm_provider = "vertex_ai-text-models"
providers = ["vertex_ai-text-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7

[models."text-bison@002".pricing."vertex_ai-text-models"]
input_cost_per_character = 2.5e-7
output_cost_per_character = 5e-7

[models."text-embedding-004"]
mode = "embedding"
max_input_tokens = 2048
max_tokens = 2048
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "vertex_ai-embedding-models"
providers = ["vertex_ai-embedding-models"]
deprecation_date = "2026-01-14"
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"
input_cost_per_character = 2.5e-8
output_vector_size = 768

[models."text-embedding-004".pricing."vertex_ai-embedding-models"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."text-embedding-005"]
display_name = "Text Embedding 005"
model_family = "text-embedding"
mode = "embedding"
max_input_tokens = 2048
max_output_tokens = 0
max_tokens = 2048
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "vertex_ai-embedding-models"
providers = ["vertex_ai-embedding-models", "vercel", "vercel_ai_gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2024-08"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"
input_cost_per_character = 2.5e-8
output_vector_size = 768

[models."text-embedding-005".pricing."vercel"]
input_cost_per_token = 3e-8
[models."text-embedding-005".pricing."vercel_ai_gateway"]
input_cost_per_token = 2.5e-8
output_cost_per_token = 0
[models."text-embedding-005".pricing."vertex_ai-embedding-models"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."text-embedding-3-large"]
display_name = "text-embedding-3-large"
model_family = "text-embedding"
mode = "embedding"
max_input_tokens = 8191
max_output_tokens = 0
max_tokens = 8191
input_cost_per_token = 1.3e-7
output_cost_per_token = 0
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services", "openai", "vercel", "vercel_ai_gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-01"
release_date = "2024-01-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."text-embedding-3-large".pricing."azure"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 0
[models."text-embedding-3-large".pricing."azure-cognitive-services"]
input_cost_per_token = 1.3e-7
[models."text-embedding-3-large".pricing."openai"]
input_cost_per_token = 1.3e-7
input_cost_per_token_batches = 6.5e-8
output_cost_per_token = 0
output_cost_per_token_batches = 0
[models."text-embedding-3-large".pricing."vercel"]
input_cost_per_token = 1.3e-7
[models."text-embedding-3-large".pricing."vercel_ai_gateway"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 0

[models."text-embedding-3-small"]
display_name = "text-embedding-3-small"
model_family = "text-embedding"
mode = "embedding"
max_input_tokens = 8191
max_output_tokens = 0
max_tokens = 8191
input_cost_per_token = 2e-8
output_cost_per_token = 0
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services", "github_copilot", "openai", "vercel", "vercel_ai_gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-01"
release_date = "2024-01-25"
deprecation_date = "2026-04-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."text-embedding-3-small".pricing."azure"]
input_cost_per_token = 2e-8
output_cost_per_token = 0
[models."text-embedding-3-small".pricing."azure-cognitive-services"]
input_cost_per_token = 2e-8
[models."text-embedding-3-small".pricing."openai"]
input_cost_per_token = 2e-8
input_cost_per_token_batches = 1e-8
output_cost_per_token = 0
output_cost_per_token_batches = 0
[models."text-embedding-3-small".pricing."vercel"]
input_cost_per_token = 2e-8
[models."text-embedding-3-small".pricing."vercel_ai_gateway"]
input_cost_per_token = 2e-8
output_cost_per_token = 0

[models."text-embedding-3-small-inference"]
mode = "embedding"
max_input_tokens = 8191
max_tokens = 8191
litellm_provider = "github_copilot"
providers = ["github_copilot"]

[models."text-embedding-ada-002"]
display_name = "text-embedding-ada-002"
model_family = "text-embedding"
mode = "embedding"
max_input_tokens = 8191
max_output_tokens = 0
max_tokens = 8191
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "azure"
providers = ["azure", "azure-cognitive-services", "github_copilot", "openai", "vercel", "vercel_ai_gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2022-12"
release_date = "2022-12-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."text-embedding-ada-002".pricing."azure"]
input_cost_per_token = 1e-7
output_cost_per_token = 0
[models."text-embedding-ada-002".pricing."azure-cognitive-services"]
input_cost_per_token = 1.0000000000000001e-7
[models."text-embedding-ada-002".pricing."openai"]
input_cost_per_token = 1e-7
output_cost_per_token = 0
[models."text-embedding-ada-002".pricing."vercel"]
input_cost_per_token = 1.0000000000000001e-7
[models."text-embedding-ada-002".pricing."vercel_ai_gateway"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."text-embedding-ada-002-v2"]
mode = "embedding"
max_input_tokens = 8191
max_tokens = 8191
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "openai"
providers = ["openai"]
input_cost_per_token_batches = 5e-8
output_cost_per_token_batches = 0

[models."text-embedding-ada-002-v2".pricing."openai"]
input_cost_per_token = 1e-7
input_cost_per_token_batches = 5e-8
output_cost_per_token = 0
output_cost_per_token_batches = 0

[models."text-embedding-large-exp-03-07"]
mode = "embedding"
max_input_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "vertex_ai-embedding-models"
providers = ["vertex_ai-embedding-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"
input_cost_per_character = 2.5e-8
output_vector_size = 3072

[models."text-embedding-large-exp-03-07".pricing."vertex_ai-embedding-models"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."text-embedding-preview-0409"]
mode = "embedding"
max_input_tokens = 3072
max_tokens = 3072
input_cost_per_token = 6.25e-9
output_cost_per_token = 0
litellm_provider = "vertex_ai-embedding-models"
providers = ["vertex_ai-embedding-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing"
input_cost_per_token_batch_requests = 5e-9
output_vector_size = 768

[models."text-embedding-preview-0409".pricing."vertex_ai-embedding-models"]
input_cost_per_token = 6.25e-9
input_cost_per_token_batch_requests = 5e-9
output_cost_per_token = 0

[models."text-moderation-007"]
mode = "moderation"
max_input_tokens = 32768
max_output_tokens = 0
max_tokens = 0
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "openai"
providers = ["openai"]

[models."text-moderation-007".pricing."openai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."text-moderation-latest"]
mode = "moderation"
max_input_tokens = 32768
max_output_tokens = 0
max_tokens = 0
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "openai"
providers = ["openai"]

[models."text-moderation-latest".pricing."openai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."text-moderation-stable"]
mode = "moderation"
max_input_tokens = 32768
max_output_tokens = 0
max_tokens = 0
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "openai"
providers = ["openai"]

[models."text-moderation-stable".pricing."openai"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."text-multilingual-embedding-002"]
display_name = "Text Multilingual Embedding 002"
model_family = "text-embedding"
mode = "embedding"
max_input_tokens = 2048
max_output_tokens = 0
max_tokens = 2048
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "vertex_ai-embedding-models"
providers = ["vertex_ai-embedding-models", "vercel", "vercel_ai_gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2024-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"
input_cost_per_character = 2.5e-8
output_vector_size = 768

[models."text-multilingual-embedding-002".pricing."vercel"]
input_cost_per_token = 3e-8
[models."text-multilingual-embedding-002".pricing."vercel_ai_gateway"]
input_cost_per_token = 2.5e-8
output_cost_per_token = 0
[models."text-multilingual-embedding-002".pricing."vertex_ai-embedding-models"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."text-multilingual-embedding-preview-0409"]
mode = "embedding"
max_input_tokens = 3072
max_tokens = 3072
input_cost_per_token = 6.25e-9
output_cost_per_token = 0
litellm_provider = "vertex_ai-embedding-models"
providers = ["vertex_ai-embedding-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
output_vector_size = 768

[models."text-multilingual-embedding-preview-0409".pricing."vertex_ai-embedding-models"]
input_cost_per_token = 6.25e-9
output_cost_per_token = 0

[models."text-unicorn"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 0.00001
output_cost_per_token = 0.000028
litellm_provider = "vertex_ai-text-models"
providers = ["vertex_ai-text-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."text-unicorn".pricing."vertex_ai-text-models"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.000028

[models."text-unicorn@001"]
mode = "completion"
max_input_tokens = 8192
max_output_tokens = 1024
max_tokens = 1024
input_cost_per_token = 0.00001
output_cost_per_token = 0.000028
litellm_provider = "vertex_ai-text-models"
providers = ["vertex_ai-text-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"

[models."text-unicorn@001".pricing."vertex_ai-text-models"]
input_cost_per_token = 0.00001
output_cost_per_token = 0.000028

[models."textembedding-gecko"]
mode = "embedding"
max_input_tokens = 3072
max_tokens = 3072
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "vertex_ai-embedding-models"
providers = ["vertex_ai-embedding-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-8
output_vector_size = 768

[models."textembedding-gecko".pricing."vertex_ai-embedding-models"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."textembedding-gecko-multilingual"]
mode = "embedding"
max_input_tokens = 3072
max_tokens = 3072
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "vertex_ai-embedding-models"
providers = ["vertex_ai-embedding-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-8
output_vector_size = 768

[models."textembedding-gecko-multilingual".pricing."vertex_ai-embedding-models"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."textembedding-gecko-multilingual@001"]
mode = "embedding"
max_input_tokens = 3072
max_tokens = 3072
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "vertex_ai-embedding-models"
providers = ["vertex_ai-embedding-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-8
output_vector_size = 768

[models."textembedding-gecko-multilingual@001".pricing."vertex_ai-embedding-models"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."textembedding-gecko@001"]
mode = "embedding"
max_input_tokens = 3072
max_tokens = 3072
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "vertex_ai-embedding-models"
providers = ["vertex_ai-embedding-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-8
output_vector_size = 768

[models."textembedding-gecko@001".pricing."vertex_ai-embedding-models"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."textembedding-gecko@003"]
mode = "embedding"
max_input_tokens = 3072
max_tokens = 3072
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "vertex_ai-embedding-models"
providers = ["vertex_ai-embedding-models"]
source = "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_models"
input_cost_per_character = 2.5e-8
output_vector_size = 768

[models."textembedding-gecko@003".pricing."vertex_ai-embedding-models"]
input_cost_per_character = 2.5e-8
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."thenlper/gte-base"]
mode = "embedding"
max_input_tokens = 512
max_tokens = 512
input_cost_per_token = 8e-9
output_cost_per_token = 0
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
source = "https://fireworks.ai/pricing"

[models."thenlper/gte-base".pricing."fireworks_ai"]
input_cost_per_token = 8e-9
output_cost_per_token = 0

[models."thenlper/gte-large"]
mode = "embedding"
max_input_tokens = 512
max_tokens = 512
input_cost_per_token = 1.6e-8
output_cost_per_token = 0
litellm_provider = "fireworks_ai"
providers = ["fireworks_ai"]
source = "https://fireworks.ai/pricing"

[models."thenlper/gte-large".pricing."fireworks_ai"]
input_cost_per_token = 1.6e-8
output_cost_per_token = 0

[models."thudm/glm-z1-32b:free"]
display_name = "GLM Z1 32B (free)"
model_family = "glm-z"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-04-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."titan-embed-text-v2"]
display_name = "Titan Text Embeddings V2"
model_family = "titan-embed"
mode = "chat"
max_input_tokens = 0
max_output_tokens = 0
max_tokens = 0
input_cost_per_token = 2e-8
output_cost_per_token = 0
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2024-04"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."titan-embed-text-v2".pricing."vercel"]
input_cost_per_token = 2e-8
[models."titan-embed-text-v2".pricing."vercel_ai_gateway"]
input_cost_per_token = 2e-8
output_cost_per_token = 0

[models."tngtech/DeepSeek-R1T-Chimera"]
display_name = "DeepSeek R1T Chimera"
model_family = "tngtech"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
litellm_provider = "chutes"
providers = ["chutes", "kilo"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."tngtech/DeepSeek-R1T-Chimera".pricing."chutes"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."tngtech/DeepSeek-R1T-Chimera".pricing."kilo"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."tngtech/DeepSeek-TNG-R1T2-Chimera"]
display_name = "DeepSeek TNG R1T2 Chimera"
model_family = "tngtech"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
input_cost_per_token = 2.5e-7
output_cost_per_token = 8.5e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."tngtech/DeepSeek-TNG-R1T2-Chimera".pricing."chutes"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 8.5e-7

[models."tngtech/TNG-R1T-Chimera-TEE"]
display_name = "TNG R1T Chimera TEE"
model_family = "tngtech"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 65536
input_cost_per_token = 2.5e-7
output_cost_per_token = 8.5e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."tngtech/TNG-R1T-Chimera-TEE".pricing."chutes"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 8.5e-7

[models."tngtech/TNG-R1T-Chimera-Turbo"]
display_name = "TNG R1T Chimera Turbo"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 65536
input_cost_per_token = 2.2e-7
output_cost_per_token = 6e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-01-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."tngtech/TNG-R1T-Chimera-Turbo".pricing."chutes"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 6e-7

[models."tngtech/deepseek-r1t2-chimera"]
display_name = "TNG: DeepSeek R1T2 Chimera"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
input_cost_per_token = 2.5e-7
output_cost_per_token = 8.5e-7
cache_read_input_token_cost = 1.25e-7
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-07-08"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."tngtech/deepseek-r1t2-chimera".pricing."kilo"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 2.5e-7
output_cost_per_token = 8.5e-7

[models."tngtech/deepseek-r1t2-chimera:free"]
display_name = "DeepSeek R1T2 Chimera (free)"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = false
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2025-07-08"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."tngtech/tng-r1t-chimera"]
display_name = "TNG: R1T Chimera"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 65536
input_cost_per_token = 2.5e-7
output_cost_per_token = 8.5e-7
cache_read_input_token_cost = 1.25e-7
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-11-26"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."tngtech/tng-r1t-chimera".pricing."kilo"]
cache_read_input_token_cost = 1.25e-7
input_cost_per_token = 2.5e-7
output_cost_per_token = 8.5e-7

[models."tngtech/tng-r1t-chimera:free"]
display_name = "R1T Chimera (free)"
model_family = "tngtech"
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2025-11-26"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."together-ai-21.1b-41b"]
mode = "chat"
input_cost_per_token = 8e-7
output_cost_per_token = 8e-7
litellm_provider = "together_ai"
providers = ["together_ai"]

[models."together-ai-21.1b-41b".pricing."together_ai"]
input_cost_per_token = 8e-7
output_cost_per_token = 8e-7

[models."together-ai-4.1b-8b"]
mode = "chat"
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7
litellm_provider = "together_ai"
providers = ["together_ai"]

[models."together-ai-4.1b-8b".pricing."together_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 2e-7

[models."together-ai-41.1b-80b"]
mode = "chat"
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7
litellm_provider = "together_ai"
providers = ["together_ai"]

[models."together-ai-41.1b-80b".pricing."together_ai"]
input_cost_per_token = 9e-7
output_cost_per_token = 9e-7

[models."together-ai-8.1b-21b"]
mode = "chat"
max_tokens = 1000
input_cost_per_token = 3e-7
output_cost_per_token = 3e-7
litellm_provider = "together_ai"
providers = ["together_ai"]

[models."together-ai-8.1b-21b".pricing."together_ai"]
input_cost_per_token = 3e-7
output_cost_per_token = 3e-7

[models."together-ai-81.1b-110b"]
mode = "chat"
input_cost_per_token = 0.0000018
output_cost_per_token = 0.0000018
litellm_provider = "together_ai"
providers = ["together_ai"]

[models."together-ai-81.1b-110b".pricing."together_ai"]
input_cost_per_token = 0.0000018
output_cost_per_token = 0.0000018

[models."together-ai-embedding-151m-to-350m"]
mode = "embedding"
input_cost_per_token = 1.6e-8
output_cost_per_token = 0
litellm_provider = "together_ai"
providers = ["together_ai"]

[models."together-ai-embedding-151m-to-350m".pricing."together_ai"]
input_cost_per_token = 1.6e-8
output_cost_per_token = 0

[models."together-ai-embedding-up-to-150m"]
mode = "embedding"
input_cost_per_token = 8e-9
output_cost_per_token = 0
litellm_provider = "together_ai"
providers = ["together_ai"]

[models."together-ai-embedding-up-to-150m".pricing."together_ai"]
input_cost_per_token = 8e-9
output_cost_per_token = 0

[models."together-ai-up-to-4b"]
mode = "chat"
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "together_ai"
providers = ["together_ai"]

[models."together-ai-up-to-4b".pricing."together_ai"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."togethercomputer/CodeLlama-34b-Instruct"]
mode = "chat"
litellm_provider = "together_ai"
providers = ["together_ai"]
supports_function_calling = true
supports_parallel_function_calling = true
supports_tool_choice = true

[models."tongyi-intent-detect-v3"]
display_name = "Tongyi Intent Detect V3"
model_family = "yi"
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 1024
input_cost_per_token = 5.8e-8
output_cost_per_token = 1.44e-7
litellm_provider = "alibaba-cn"
providers = ["alibaba-cn"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2024-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."tongyi-intent-detect-v3".pricing."alibaba-cn"]
input_cost_per_token = 5.8e-8
output_cost_per_token = 1.44e-7

[models."trinity-large-preview-free"]
display_name = "Trinity Large Preview"
model_family = "trinity"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
litellm_provider = "opencode"
providers = ["opencode"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2026-01-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."tts-1"]
mode = "audio_speech"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_character = 0.000015
supported_endpoints = ["/v1/audio/speech"]

[models."tts-1".pricing."azure"]
input_cost_per_character = 0.000015
[models."tts-1".pricing."openai"]
input_cost_per_character = 0.000015

[models."tts-1-1106"]
mode = "audio_speech"
litellm_provider = "openai"
providers = ["openai"]
input_cost_per_character = 0.000015
supported_endpoints = ["/v1/audio/speech"]

[models."tts-1-1106".pricing."openai"]
input_cost_per_character = 0.000015

[models."tts-1-hd"]
mode = "audio_speech"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_character = 0.00003
supported_endpoints = ["/v1/audio/speech"]

[models."tts-1-hd".pricing."azure"]
input_cost_per_character = 0.00003
[models."tts-1-hd".pricing."openai"]
input_cost_per_character = 0.00003

[models."tts-1-hd-1106"]
mode = "audio_speech"
litellm_provider = "openai"
providers = ["openai"]
input_cost_per_character = 0.00003
supported_endpoints = ["/v1/audio/speech"]

[models."tts-1-hd-1106".pricing."openai"]
input_cost_per_character = 0.00003

[models."twelvelabs.marengo-embed-2-7-v1:0"]
mode = "embedding"
max_input_tokens = 77
max_tokens = 77
input_cost_per_token = 0.00007
output_cost_per_token = 0
litellm_provider = "bedrock"
providers = ["bedrock"]
output_vector_size = 1024
supports_embedding_image_input = true
supports_image_input = true

[models."twelvelabs.marengo-embed-2-7-v1:0".pricing."bedrock"]
input_cost_per_token = 0.00007
output_cost_per_token = 0

[models."twelvelabs.pegasus-1-2-v1:0"]
mode = "chat"
output_cost_per_token = 0.0000075
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_video_per_second = 0.00049
supports_video_input = true

[models."twelvelabs.pegasus-1-2-v1:0".pricing."bedrock"]
input_cost_per_video_per_second = 0.00049
output_cost_per_token = 0.0000075

[models."undi95/remm-slerp-l2-13b"]
mode = "chat"
max_tokens = 6144
input_cost_per_token = 0.000001875
output_cost_per_token = 0.000001875
litellm_provider = "openrouter"
providers = ["openrouter"]
supports_tool_choice = true

[models."undi95/remm-slerp-l2-13b".pricing."openrouter"]
input_cost_per_token = 0.000001875
output_cost_per_token = 0.000001875

[models."unsloth/Llama-3-2-1B-Instruct"]
display_name = "Llama 3.2 1B Instruct"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 8192
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8
cache_read_input_token_cost = 5e-9
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2026-01-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."unsloth/Llama-3-2-1B-Instruct".pricing."chutes"]
cache_read_input_token_cost = 5e-9
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8

[models."unsloth/Llama-3-2-3B-Instruct"]
display_name = "Llama 3.2 3B Instruct"
model_family = "unsloth"
mode = "chat"
max_input_tokens = 16384
max_output_tokens = 16384
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8
cache_read_input_token_cost = 5e-9
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-02-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."unsloth/Llama-3-2-3B-Instruct".pricing."chutes"]
cache_read_input_token_cost = 5e-9
input_cost_per_token = 1e-8
output_cost_per_token = 1e-8

[models."unsloth/Mistral-Nemo-Instruct-2407"]
display_name = "Mistral Nemo Instruct 2407"
model_family = "unsloth"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 2e-8
output_cost_per_token = 4e-8
cache_read_input_token_cost = 1e-8
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."unsloth/Mistral-Nemo-Instruct-2407".pricing."chutes"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 2e-8
output_cost_per_token = 4e-8

[models."unsloth/Mistral-Small-24B-Instruct-2501"]
display_name = "Mistral Small 24B Instruct 2501"
model_family = "unsloth"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
input_cost_per_token = 3e-8
output_cost_per_token = 1.1e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."unsloth/Mistral-Small-24B-Instruct-2501".pricing."chutes"]
input_cost_per_token = 3e-8
output_cost_per_token = 1.1e-7

[models."unsloth/gemma-3-12b-it"]
display_name = "gemma 3 12b it"
model_family = "unsloth"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
input_cost_per_token = 3e-8
output_cost_per_token = 1.0000000000000001e-7
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."unsloth/gemma-3-12b-it".pricing."chutes"]
input_cost_per_token = 3e-8
output_cost_per_token = 1.0000000000000001e-7

[models."unsloth/gemma-3-27b-it"]
display_name = "gemma 3 27b it"
model_family = "unsloth"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 65536
input_cost_per_token = 4e-8
output_cost_per_token = 1.5e-7
cache_read_input_token_cost = 2e-8
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."unsloth/gemma-3-27b-it".pricing."chutes"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 4e-8
output_cost_per_token = 1.5e-7

[models."unsloth/gemma-3-4b-it"]
display_name = "gemma 3 4b it"
model_family = "unsloth"
mode = "chat"
max_input_tokens = 96000
max_output_tokens = 96000
input_cost_per_token = 1e-8
output_cost_per_token = 3e-8
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."unsloth/gemma-3-4b-it".pricing."chutes"]
input_cost_per_token = 1e-8
output_cost_per_token = 3e-8

[models."us-east-2/deepseek.v3.2"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 6.2e-7
output_cost_per_token = 0.00000185
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_reasoning = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_tool_choice = true

[models."us-east-2/deepseek.v3.2".pricing."bedrock"]
input_cost_per_token = 6.2e-7
output_cost_per_token = 0.00000185

[models."us-east-2/minimax.minimax-m2.1"]
mode = "chat"
max_input_tokens = 196000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."us-east-2/minimax.minimax-m2.1".pricing."bedrock"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."us-east-2/moonshotai.kimi-k2-thinking"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_reasoning = true

[models."us-east-2/moonshotai.kimi-k2-thinking".pricing."bedrock"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000025

[models."us-east-2/moonshotai.kimi-k2.5"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 262144
max_tokens = 262144
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."us-east-2/moonshotai.kimi-k2.5".pricing."bedrock"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000003

[models."us-east-2/qwen.qwen3-coder-next"]
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000012
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."us-east-2/qwen.qwen3-coder-next".pricing."bedrock"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000012

[models."us-gov-east-1/amazon.nova-pro-v1:0"]
mode = "chat"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 9.6e-7
output_cost_per_token = 0.00000384
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supports_response_schema = true

[models."us-gov-east-1/amazon.nova-pro-v1:0".pricing."bedrock"]
input_cost_per_token = 9.6e-7
output_cost_per_token = 0.00000384

[models."us-gov-east-1/amazon.titan-embed-text-v1"]
mode = "embedding"
max_input_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "bedrock"
providers = ["bedrock"]
output_vector_size = 1536

[models."us-gov-east-1/amazon.titan-embed-text-v1".pricing."bedrock"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."us-gov-east-1/amazon.titan-embed-text-v2:0"]
mode = "embedding"
max_input_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 0
litellm_provider = "bedrock"
providers = ["bedrock"]
output_vector_size = 1024

[models."us-gov-east-1/amazon.titan-embed-text-v2:0".pricing."bedrock"]
input_cost_per_token = 2e-7
output_cost_per_token = 0

[models."us-gov-east-1/amazon.titan-text-express-v1"]
mode = "chat"
max_input_tokens = 42000
max_output_tokens = 8000
max_tokens = 8000
input_cost_per_token = 0.0000013
output_cost_per_token = 0.0000017
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."us-gov-east-1/amazon.titan-text-express-v1".pricing."bedrock"]
input_cost_per_token = 0.0000013
output_cost_per_token = 0.0000017

[models."us-gov-east-1/amazon.titan-text-lite-v1"]
mode = "chat"
max_input_tokens = 42000
max_output_tokens = 4000
max_tokens = 4000
input_cost_per_token = 3e-7
output_cost_per_token = 4e-7
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."us-gov-east-1/amazon.titan-text-lite-v1".pricing."bedrock"]
input_cost_per_token = 3e-7
output_cost_per_token = 4e-7

[models."us-gov-east-1/amazon.titan-text-premier-v1:0"]
mode = "chat"
max_input_tokens = 42000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."us-gov-east-1/amazon.titan-text-premier-v1:0".pricing."bedrock"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."us-gov-east-1/anthropic.claude-3-5-sonnet-20240620-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.0000036
output_cost_per_token = 0.000018
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true

[models."us-gov-east-1/anthropic.claude-3-5-sonnet-20240620-v1:0".pricing."bedrock"]
input_cost_per_token = 0.0000036
output_cost_per_token = 0.000018

[models."us-gov-east-1/anthropic.claude-3-haiku-20240307-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true

[models."us-gov-east-1/anthropic.claude-3-haiku-20240307-v1:0".pricing."bedrock"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015

[models."us-gov-east-1/claude-sonnet-4-5-20250929-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.0000033
output_cost_per_token = 0.0000165
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true

[models."us-gov-east-1/claude-sonnet-4-5-20250929-v1:0".pricing."bedrock"]
input_cost_per_token = 0.0000033
output_cost_per_token = 0.0000165

[models."us-gov-east-1/meta.llama3-70b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 8000
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 0.00000265
output_cost_per_token = 0.0000035
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_pdf_input = true

[models."us-gov-east-1/meta.llama3-70b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 0.00000265
output_cost_per_token = 0.0000035

[models."us-gov-east-1/meta.llama3-8b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 8000
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 3e-7
output_cost_per_token = 0.00000265
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_pdf_input = true

[models."us-gov-east-1/meta.llama3-8b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.00000265

[models."us-gov-west-1/amazon.nova-pro-v1:0"]
mode = "chat"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 9.6e-7
output_cost_per_token = 0.00000384
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supports_response_schema = true

[models."us-gov-west-1/amazon.nova-pro-v1:0".pricing."bedrock"]
input_cost_per_token = 9.6e-7
output_cost_per_token = 0.00000384

[models."us-gov-west-1/amazon.titan-embed-text-v1"]
mode = "embedding"
max_input_tokens = 8192
max_tokens = 8192
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "bedrock"
providers = ["bedrock"]
output_vector_size = 1536

[models."us-gov-west-1/amazon.titan-embed-text-v1".pricing."bedrock"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."us-gov-west-1/amazon.titan-embed-text-v2:0"]
mode = "embedding"
max_input_tokens = 8192
max_tokens = 8192
input_cost_per_token = 2e-7
output_cost_per_token = 0
litellm_provider = "bedrock"
providers = ["bedrock"]
output_vector_size = 1024

[models."us-gov-west-1/amazon.titan-embed-text-v2:0".pricing."bedrock"]
input_cost_per_token = 2e-7
output_cost_per_token = 0

[models."us-gov-west-1/amazon.titan-text-express-v1"]
mode = "chat"
max_input_tokens = 42000
max_output_tokens = 8000
max_tokens = 8000
input_cost_per_token = 0.0000013
output_cost_per_token = 0.0000017
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."us-gov-west-1/amazon.titan-text-express-v1".pricing."bedrock"]
input_cost_per_token = 0.0000013
output_cost_per_token = 0.0000017

[models."us-gov-west-1/amazon.titan-text-lite-v1"]
mode = "chat"
max_input_tokens = 42000
max_output_tokens = 4000
max_tokens = 4000
input_cost_per_token = 3e-7
output_cost_per_token = 4e-7
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."us-gov-west-1/amazon.titan-text-lite-v1".pricing."bedrock"]
input_cost_per_token = 3e-7
output_cost_per_token = 4e-7

[models."us-gov-west-1/amazon.titan-text-premier-v1:0"]
mode = "chat"
max_input_tokens = 42000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."us-gov-west-1/amazon.titan-text-premier-v1:0".pricing."bedrock"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000015

[models."us-gov-west-1/anthropic.claude-3-5-sonnet-20240620-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.0000036
output_cost_per_token = 0.000018
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true

[models."us-gov-west-1/anthropic.claude-3-5-sonnet-20240620-v1:0".pricing."bedrock"]
input_cost_per_token = 0.0000036
output_cost_per_token = 0.000018

[models."us-gov-west-1/anthropic.claude-3-7-sonnet-20250219-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.0000036
output_cost_per_token = 0.000018
cache_read_input_token_cost = 3.6e-7
cache_creation_input_token_cost = 0.0000045
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true

[models."us-gov-west-1/anthropic.claude-3-7-sonnet-20250219-v1:0".pricing."bedrock"]
cache_creation_input_token_cost = 0.0000045
cache_read_input_token_cost = 3.6e-7
input_cost_per_token = 0.0000036
output_cost_per_token = 0.000018

[models."us-gov-west-1/anthropic.claude-3-haiku-20240307-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true

[models."us-gov-west-1/anthropic.claude-3-haiku-20240307-v1:0".pricing."bedrock"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000015

[models."us-gov-west-1/claude-sonnet-4-5-20250929-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.0000033
output_cost_per_token = 0.0000165
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true

[models."us-gov-west-1/claude-sonnet-4-5-20250929-v1:0".pricing."bedrock"]
input_cost_per_token = 0.0000033
output_cost_per_token = 0.0000165

[models."us-gov-west-1/meta.llama3-70b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 8000
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 0.00000265
output_cost_per_token = 0.0000035
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_pdf_input = true

[models."us-gov-west-1/meta.llama3-70b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 0.00000265
output_cost_per_token = 0.0000035

[models."us-gov-west-1/meta.llama3-8b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 8000
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 3e-7
output_cost_per_token = 0.00000265
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_pdf_input = true

[models."us-gov-west-1/meta.llama3-8b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.00000265

[models."us-west-1/meta.llama3-70b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.00000265
output_cost_per_token = 0.0000035
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."us-west-1/meta.llama3-70b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 0.00000265
output_cost_per_token = 0.0000035

[models."us-west-1/meta.llama3-8b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 8192
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 3e-7
output_cost_per_token = 6e-7
litellm_provider = "bedrock"
providers = ["bedrock"]

[models."us-west-1/meta.llama3-8b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 3e-7
output_cost_per_token = 6e-7

[models."us.amazon.nova-2-lite-v1:0"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 3.3e-7
output_cost_per_token = 0.00000275
cache_read_input_token_cost = 8.25e-8
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_response_schema = true
supports_video_input = true

[models."us.amazon.nova-2-lite-v1:0".pricing."bedrock_converse"]
cache_read_input_token_cost = 8.25e-8
input_cost_per_token = 3.3e-7
output_cost_per_token = 0.00000275

[models."us.amazon.nova-2-pro-preview-20251202-v1:0"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000021875
output_cost_per_token = 0.0000175
cache_read_input_token_cost = 5.46875e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
input_cost_per_audio_token = 0.0000021875
input_cost_per_image_token = 0.0000021875
supports_response_schema = true
supports_video_input = true

[models."us.amazon.nova-2-pro-preview-20251202-v1:0".pricing."bedrock_converse"]
cache_read_input_token_cost = 5.46875e-7
input_cost_per_audio_token = 0.0000021875
input_cost_per_image_token = 0.0000021875
input_cost_per_token = 0.0000021875
output_cost_per_token = 0.0000175

[models."us.amazon.nova-lite-v1:0"]
mode = "chat"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supports_response_schema = true

[models."us.amazon.nova-lite-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 6e-8
output_cost_per_token = 2.4e-7

[models."us.amazon.nova-micro-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 3.5e-8
output_cost_per_token = 1.4e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_prompt_caching = true
supports_response_schema = true

[models."us.amazon.nova-micro-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 3.5e-8
output_cost_per_token = 1.4e-7

[models."us.amazon.nova-premier-v1:0"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 0.0000025
output_cost_per_token = 0.0000125
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = false
supports_pdf_input = true
supports_response_schema = true

[models."us.amazon.nova-premier-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.0000125

[models."us.amazon.nova-pro-v1:0"]
mode = "chat"
max_input_tokens = 300000
max_output_tokens = 10000
max_tokens = 10000
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000032
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supports_response_schema = true

[models."us.amazon.nova-pro-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 8e-7
output_cost_per_token = 0.0000032

[models."us.anthropic.claude-3-5-haiku-20241022-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 8e-7
output_cost_per_token = 0.000004
cache_read_input_token_cost = 8e-8
cache_creation_input_token_cost = 0.000001
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_prompt_caching = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_response_schema = true
supports_tool_choice = true

[models."us.anthropic.claude-3-5-haiku-20241022-v1:0".pricing."bedrock"]
cache_creation_input_token_cost = 0.000001
cache_read_input_token_cost = 8e-8
input_cost_per_token = 8e-7
output_cost_per_token = 0.000004

[models."us.anthropic.claude-3-5-sonnet-20240620-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true

[models."us.anthropic.claude-3-5-sonnet-20240620-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."us.anthropic.claude-3-5-sonnet-20241022-v2:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_prompt_caching = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true

[models."us.anthropic.claude-3-5-sonnet-20241022-v2:0".pricing."bedrock"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."us.anthropic.claude-3-7-sonnet-20250219-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true

[models."us.anthropic.claude-3-7-sonnet-20250219-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000375
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."us.anthropic.claude-3-haiku-20240307-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true

[models."us.anthropic.claude-3-haiku-20240307-v1:0".pricing."bedrock"]
input_cost_per_token = 2.5e-7
output_cost_per_token = 0.00000125

[models."us.anthropic.claude-3-opus-20240229-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_response_schema = true
supports_tool_choice = true

[models."us.anthropic.claude-3-opus-20240229-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."us.anthropic.claude-3-sonnet-20240229-v1:0"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_pdf_input = true
supports_response_schema = true
supports_tool_choice = true

[models."us.anthropic.claude-3-sonnet-20240229-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."us.anthropic.claude-haiku-4-5-20251001-v1:0"]
display_name = "Claude Haiku 4.5 (US)"
model_family = "claude-haiku"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000055
cache_read_input_token_cost = 1.1e-7
cache_creation_input_token_cost = 0.000001375
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-02-28"
release_date = "2025-10-15"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "https://aws.amazon.com/about-aws/whats-new/2025/10/claude-4-5-haiku-anthropic-amazon-bedrock"
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."us.anthropic.claude-haiku-4-5-20251001-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.000005
[models."us.anthropic.claude-haiku-4-5-20251001-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.000001375
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 0.0000011
output_cost_per_token = 0.0000055

[models."us.anthropic.claude-opus-4-1-20250805-v1:0"]
display_name = "Claude Opus 4.1 (US)"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
cache_creation_input_token_cost = 0.00001875
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-08-05"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."us.anthropic.claude-opus-4-1-20250805-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."us.anthropic.claude-opus-4-1-20250805-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."us.anthropic.claude-opus-4-1-20250805-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."us.anthropic.claude-opus-4-20250514-v1:0"]
display_name = "Claude Opus 4 (US)"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
cache_read_input_token_cost = 0.0000015
cache_creation_input_token_cost = 0.00001875
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-05-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."us.anthropic.claude-opus-4-20250514-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."us.anthropic.claude-opus-4-20250514-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
[models."us.anthropic.claude-opus-4-20250514-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00001875
cache_read_input_token_cost = 0.0000015
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."us.anthropic.claude-opus-4-5-20251101-v1:0"]
display_name = "Claude Opus 4.5 (US)"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000055
output_cost_per_token = 0.0000275
cache_read_input_token_cost = 5.5e-7
cache_creation_input_token_cost = 0.000006875
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-03-31"
release_date = "2025-11-24"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."us.anthropic.claude-opus-4-5-20251101-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."us.anthropic.claude-opus-4-5-20251101-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."us.anthropic.claude-opus-4-5-20251101-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.000006875
cache_read_input_token_cost = 5.5e-7
input_cost_per_token = 0.0000055
output_cost_per_token = 0.0000275

[models."us.anthropic.claude-opus-4-6-v1"]
display_name = "Claude Opus 4.6 (US)"
model_family = "claude-opus"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.0000055
output_cost_per_token = 0.0000275
cache_read_input_token_cost = 5.5e-7
cache_creation_input_token_cost = 0.000006875
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-05"
release_date = "2026-02-05"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.00001375
cache_read_input_token_cost_above_200k_tokens = 0.0000011
input_cost_per_token_above_200k_tokens = 0.000011
output_cost_per_token_above_200k_tokens = 0.00004125
supports_assistant_prefill = false
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."us.anthropic.claude-opus-4-6-v1".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."us.anthropic.claude-opus-4-6-v1".pricing."amazon-bedrock"]
cache_read_input_token_cost = 5e-7
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."us.anthropic.claude-opus-4-6-v1".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.000006875
cache_creation_input_token_cost_above_200k_tokens = 0.00001375
cache_read_input_token_cost = 5.5e-7
cache_read_input_token_cost_above_200k_tokens = 0.0000011
input_cost_per_token = 0.0000055
input_cost_per_token_above_200k_tokens = 0.000011
output_cost_per_token = 0.0000275
output_cost_per_token_above_200k_tokens = 0.00004125

[models."us.anthropic.claude-sonnet-4-20250514-v1:0"]
display_name = "Claude Sonnet 4 (US)"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 3e-7
cache_creation_input_token_cost = 0.00000375
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2024-04"
release_date = "2025-05-22"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token_above_200k_tokens = 0.0000225
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 159

[models."us.anthropic.claude-sonnet-4-20250514-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."us.anthropic.claude-sonnet-4-20250514-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."us.anthropic.claude-sonnet-4-20250514-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.00000375
cache_creation_input_token_cost_above_200k_tokens = 0.0000075
cache_read_input_token_cost = 3e-7
cache_read_input_token_cost_above_200k_tokens = 6e-7
input_cost_per_token = 0.000003
input_cost_per_token_above_200k_tokens = 0.000006
output_cost_per_token = 0.000015
output_cost_per_token_above_200k_tokens = 0.0000225

[models."us.anthropic.claude-sonnet-4-5-20250929-v1:0"]
display_name = "Claude Sonnet 4.5 (US)"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000033
output_cost_per_token = 0.0000165
cache_read_input_token_cost = 3.3e-7
cache_creation_input_token_cost = 0.000004125
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-07-31"
release_date = "2025-09-29"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token_above_200k_tokens = 0.0000066
output_cost_per_token_above_200k_tokens = 0.00002475
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."us.anthropic.claude-sonnet-4-5-20250929-v1:0".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."us.anthropic.claude-sonnet-4-5-20250929-v1:0".pricing."amazon-bedrock"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."us.anthropic.claude-sonnet-4-5-20250929-v1:0".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.000004125
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost = 3.3e-7
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token = 0.0000033
input_cost_per_token_above_200k_tokens = 0.0000066
output_cost_per_token = 0.0000165
output_cost_per_token_above_200k_tokens = 0.00002475

[models."us.anthropic.claude-sonnet-4-6"]
display_name = "Claude Sonnet 4.6 (US)"
model_family = "claude-sonnet"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 0.0000033
output_cost_per_token = 0.0000165
cache_read_input_token_cost = 3.3e-7
cache_creation_input_token_cost = 0.000004125
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = true
supports_pdf_input = true
open_weights = false
knowledge_cutoff = "2025-08"
release_date = "2026-02-17"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token_above_200k_tokens = 0.0000066
output_cost_per_token_above_200k_tokens = 0.00002475
supports_assistant_prefill = true
supports_computer_use = true
supports_response_schema = true
supports_tool_choice = true
tool_use_system_prompt_tokens = 346

[models."us.anthropic.claude-sonnet-4-6".search_context_cost_per_query]
search_context_size_high = 0.01
search_context_size_low = 0.01
search_context_size_medium = 0.01

[models."us.anthropic.claude-sonnet-4-6".pricing."amazon-bedrock"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."us.anthropic.claude-sonnet-4-6".pricing."bedrock_converse"]
cache_creation_input_token_cost = 0.000004125
cache_creation_input_token_cost_above_200k_tokens = 0.00000825
cache_read_input_token_cost = 3.3e-7
cache_read_input_token_cost_above_200k_tokens = 6.6e-7
input_cost_per_token = 0.0000033
input_cost_per_token_above_200k_tokens = 0.0000066
output_cost_per_token = 0.0000165
output_cost_per_token_above_200k_tokens = 0.00002475

[models."us.deepseek.r1-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = false
supports_reasoning = true
supports_tool_choice = false

[models."us.deepseek.r1-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 0.00000135
output_cost_per_token = 0.0000054

[models."us.deepseek.v3.2"]
mode = "chat"
max_input_tokens = 163840
max_output_tokens = 163840
max_tokens = 163840
input_cost_per_token = 6.2e-7
output_cost_per_token = 0.00000185
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_reasoning = true
supports_tool_choice = true

[models."us.deepseek.v3.2".pricing."bedrock_converse"]
input_cost_per_token = 6.2e-7
output_cost_per_token = 0.00000185

[models."us.meta.llama3-1-405b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.00000532
output_cost_per_token = 0.000016
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_tool_choice = false

[models."us.meta.llama3-1-405b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 0.00000532
output_cost_per_token = 0.000016

[models."us.meta.llama3-1-70b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 9.9e-7
output_cost_per_token = 9.9e-7
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_tool_choice = false

[models."us.meta.llama3-1-70b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 9.9e-7
output_cost_per_token = 9.9e-7

[models."us.meta.llama3-1-8b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 2.2e-7
output_cost_per_token = 2.2e-7
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_tool_choice = false

[models."us.meta.llama3-1-8b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 2.2e-7
output_cost_per_token = 2.2e-7

[models."us.meta.llama3-2-11b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 3.5e-7
output_cost_per_token = 3.5e-7
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_tool_choice = false

[models."us.meta.llama3-2-11b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 3.5e-7

[models."us.meta.llama3-2-1b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_tool_choice = false

[models."us.meta.llama3-2-1b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 1e-7
output_cost_per_token = 1e-7

[models."us.meta.llama3-2-3b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_tool_choice = false

[models."us.meta.llama3-2-3b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 1.5e-7

[models."us.meta.llama3-2-90b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002
litellm_provider = "bedrock"
providers = ["bedrock"]
supports_function_calling = true
supports_vision = true
supports_tool_choice = false

[models."us.meta.llama3-2-90b-instruct-v1:0".pricing."bedrock"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000002

[models."us.meta.llama3-3-70b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_tool_choice = false

[models."us.meta.llama3-3-70b-instruct-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 7.2e-7
output_cost_per_token = 7.2e-7

[models."us.meta.llama4-maverick-17b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 2.4e-7
output_cost_per_token = 9.7e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "code"]
input_cost_per_token_batches = 1.2e-7
output_cost_per_token_batches = 4.85e-7
supports_tool_choice = false

[models."us.meta.llama4-maverick-17b-instruct-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 2.4e-7
input_cost_per_token_batches = 1.2e-7
output_cost_per_token = 9.7e-7
output_cost_per_token_batches = 4.85e-7

[models."us.meta.llama4-scout-17b-instruct-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1.7e-7
output_cost_per_token = 6.6e-7
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supported_modalities = ["text", "image"]
supported_output_modalities = ["text", "code"]
input_cost_per_token_batches = 8.5e-8
output_cost_per_token_batches = 3.3e-7
supports_tool_choice = false

[models."us.meta.llama4-scout-17b-instruct-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 1.7e-7
input_cost_per_token_batches = 8.5e-8
output_cost_per_token = 6.6e-7
output_cost_per_token_batches = 3.3e-7

[models."us.mistral.pixtral-large-2502-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 4096
max_tokens = 4096
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_tool_choice = false

[models."us.mistral.pixtral-large-2502-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.000006

[models."us.twelvelabs.marengo-embed-2-7-v1:0"]
mode = "embedding"
max_input_tokens = 77
max_tokens = 77
input_cost_per_token = 0.00007
output_cost_per_token = 0
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_audio_per_second = 0.00014
input_cost_per_image = 0.0001
input_cost_per_video_per_second = 0.0007
output_vector_size = 1024
supports_embedding_image_input = true
supports_image_input = true

[models."us.twelvelabs.marengo-embed-2-7-v1:0".pricing."bedrock"]
input_cost_per_audio_per_second = 0.00014
input_cost_per_image = 0.0001
input_cost_per_token = 0.00007
input_cost_per_video_per_second = 0.0007
output_cost_per_token = 0

[models."us.twelvelabs.pegasus-1-2-v1:0"]
mode = "chat"
output_cost_per_token = 0.0000075
litellm_provider = "bedrock"
providers = ["bedrock"]
input_cost_per_video_per_second = 0.00049
supports_video_input = true

[models."us.twelvelabs.pegasus-1-2-v1:0".pricing."bedrock"]
input_cost_per_video_per_second = 0.00049
output_cost_per_token = 0.0000075

[models."us.writer.palmyra-x4-v1:0"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_pdf_input = true

[models."us.writer.palmyra-x4-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001

[models."us.writer.palmyra-x5-v1:0"]
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-7
output_cost_per_token = 0.000006
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse"]
supports_function_calling = true
supports_pdf_input = true

[models."us.writer.palmyra-x5-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000006

[models."v0-1.0-md"]
display_name = "v0-1.0-md"
model_family = "v0"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "v0"
providers = ["v0"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = false
release_date = "2025-05-22"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."v0-1.0-md".pricing."v0"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."v0-1.5-lg"]
display_name = "v0-1.5-lg"
model_family = "v0"
mode = "chat"
max_input_tokens = 512000
max_output_tokens = 512000
max_tokens = 512000
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075
litellm_provider = "v0"
providers = ["v0"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = false
release_date = "2025-06-09"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."v0-1.5-lg".pricing."v0"]
input_cost_per_token = 0.000015
output_cost_per_token = 0.000075

[models."v0-1.5-md"]
display_name = "v0-1.5-md"
model_family = "v0"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "v0"
providers = ["v0"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = false
release_date = "2025-06-09"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."v0-1.5-md".pricing."v0"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."venice-uncensored"]
display_name = "Venice Uncensored 1.1"
model_family = "venice"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 9.000000000000001e-7
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-10"
release_date = "2025-03-18"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."venice-uncensored".pricing."venice"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 9.000000000000001e-7

[models."veo-2.0-generate-001"]
mode = "video_generation"
max_input_tokens = 1024
max_tokens = 1024
litellm_provider = "gemini"
providers = ["gemini", "vertex_ai"]
supported_modalities = ["text"]
supported_output_modalities = ["video"]
source = "https://ai.google.dev/gemini-api/docs/video"
output_cost_per_second = 0.35

[models."veo-2.0-generate-001".pricing."gemini"]
output_cost_per_second = 0.35
[models."veo-2.0-generate-001".pricing."vertex_ai"]
output_cost_per_second = 0.35

[models."veo-3.0-fast-generate-001"]
mode = "video_generation"
max_input_tokens = 1024
max_tokens = 1024
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supported_modalities = ["text"]
supported_output_modalities = ["video"]
source = "https://ai.google.dev/gemini-api/docs/video"
output_cost_per_second = 0.15

[models."veo-3.0-fast-generate-001".pricing."vertex_ai"]
output_cost_per_second = 0.15

[models."veo-3.0-fast-generate-preview"]
mode = "video_generation"
max_input_tokens = 1024
max_tokens = 1024
litellm_provider = "gemini"
providers = ["gemini", "vertex_ai"]
deprecation_date = "2025-11-12"
supported_modalities = ["text"]
supported_output_modalities = ["video"]
source = "https://ai.google.dev/gemini-api/docs/video"
output_cost_per_second = 0.4

[models."veo-3.0-fast-generate-preview".pricing."gemini"]
output_cost_per_second = 0.4
[models."veo-3.0-fast-generate-preview".pricing."vertex_ai"]
output_cost_per_second = 0.15

[models."veo-3.0-generate-001"]
mode = "video_generation"
max_input_tokens = 1024
max_tokens = 1024
litellm_provider = "vertex_ai"
providers = ["vertex_ai"]
supported_modalities = ["text"]
supported_output_modalities = ["video"]
source = "https://ai.google.dev/gemini-api/docs/video"
output_cost_per_second = 0.4

[models."veo-3.0-generate-001".pricing."vertex_ai"]
output_cost_per_second = 0.4

[models."veo-3.0-generate-preview"]
mode = "video_generation"
max_input_tokens = 1024
max_tokens = 1024
litellm_provider = "gemini"
providers = ["gemini", "vertex_ai"]
deprecation_date = "2025-11-12"
supported_modalities = ["text"]
supported_output_modalities = ["video"]
source = "https://ai.google.dev/gemini-api/docs/video"
output_cost_per_second = 0.75

[models."veo-3.0-generate-preview".pricing."gemini"]
output_cost_per_second = 0.75
[models."veo-3.0-generate-preview".pricing."vertex_ai"]
output_cost_per_second = 0.4

[models."veo-3.1-fast-generate-001"]
mode = "video_generation"
max_input_tokens = 1024
max_tokens = 1024
litellm_provider = "gemini"
providers = ["gemini", "vertex_ai"]
supported_modalities = ["text"]
supported_output_modalities = ["video"]
source = "https://ai.google.dev/gemini-api/docs/video"
output_cost_per_second = 0.15

[models."veo-3.1-fast-generate-001".pricing."gemini"]
output_cost_per_second = 0.15
[models."veo-3.1-fast-generate-001".pricing."vertex_ai"]
output_cost_per_second = 0.15

[models."veo-3.1-fast-generate-preview"]
mode = "video_generation"
max_input_tokens = 1024
max_tokens = 1024
litellm_provider = "gemini"
providers = ["gemini", "vertex_ai"]
supported_modalities = ["text"]
supported_output_modalities = ["video"]
source = "https://ai.google.dev/gemini-api/docs/video"
output_cost_per_second = 0.15

[models."veo-3.1-fast-generate-preview".pricing."gemini"]
output_cost_per_second = 0.15
[models."veo-3.1-fast-generate-preview".pricing."vertex_ai"]
output_cost_per_second = 0.15

[models."veo-3.1-generate-001"]
mode = "video_generation"
max_input_tokens = 1024
max_tokens = 1024
litellm_provider = "gemini"
providers = ["gemini", "vertex_ai"]
supported_modalities = ["text"]
supported_output_modalities = ["video"]
source = "https://ai.google.dev/gemini-api/docs/video"
output_cost_per_second = 0.4

[models."veo-3.1-generate-001".pricing."gemini"]
output_cost_per_second = 0.4
[models."veo-3.1-generate-001".pricing."vertex_ai"]
output_cost_per_second = 0.4

[models."veo-3.1-generate-preview"]
mode = "video_generation"
max_input_tokens = 1024
max_tokens = 1024
litellm_provider = "gemini"
providers = ["gemini", "vertex_ai"]
supported_modalities = ["text"]
supported_output_modalities = ["video"]
source = "https://ai.google.dev/gemini-api/docs/video"
output_cost_per_second = 0.4

[models."veo-3.1-generate-preview".pricing."gemini"]
output_cost_per_second = 0.4
[models."veo-3.1-generate-preview".pricing."vertex_ai"]
output_cost_per_second = 0.4

[models."vercel/v0-1.0-md"]
display_name = "v0-1.0-md"
model_family = "v0"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = false
release_date = "2025-05-22"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."vercel/v0-1.0-md".pricing."vercel"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."vercel/v0-1.0-md".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."vercel/v0-1.5-md"]
display_name = "v0-1.5-md"
model_family = "v0"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = false
release_date = "2025-06-09"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."vercel/v0-1.5-md".pricing."vercel"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."vercel/v0-1.5-md".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."vicuna"]
mode = "completion"
max_input_tokens = 2048
max_output_tokens = 2048
max_tokens = 2048
input_cost_per_token = 0
output_cost_per_token = 0
litellm_provider = "ollama"
providers = ["ollama"]

[models."vicuna".pricing."ollama"]
input_cost_per_token = 0
output_cost_per_token = 0

[models."volcengine/doubao-seed-1-8"]
display_name = "Doubao-Seed-1.8"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 64000
input_cost_per_token = 1.1e-7
output_cost_per_token = 2.8e-7
cache_read_input_token_cost = 2e-8
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2025-12-18"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."volcengine/doubao-seed-1-8".pricing."zenmux"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 1.1e-7
output_cost_per_token = 2.8e-7

[models."volcengine/doubao-seed-2-0-lite"]
display_name = "Doubao-Seed-2.0-lite"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 64000
input_cost_per_token = 9e-8
output_cost_per_token = 5.1e-7
cache_read_input_token_cost = 2e-8
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2026-02-14"
release_date = "2026-02-14"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."volcengine/doubao-seed-2-0-lite".pricing."zenmux"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 9e-8
output_cost_per_token = 5.1e-7

[models."volcengine/doubao-seed-2-0-mini"]
display_name = "Doubao-Seed-2.0-mini"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 64000
input_cost_per_token = 3e-8
output_cost_per_token = 2.8e-7
cache_read_input_token_cost = 1e-8
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2026-02-14"
release_date = "2026-02-14"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."volcengine/doubao-seed-2-0-mini".pricing."zenmux"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 3e-8
output_cost_per_token = 2.8e-7

[models."volcengine/doubao-seed-2-0-pro"]
display_name = "Doubao-Seed-2.0-pro"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 64000
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.00000224
cache_read_input_token_cost = 9e-8
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2026-02-14"
release_date = "2026-02-14"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."volcengine/doubao-seed-2-0-pro".pricing."zenmux"]
cache_read_input_token_cost = 9e-8
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.00000224

[models."volcengine/doubao-seed-code"]
display_name = "Doubao-Seed-Code"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 64000
input_cost_per_token = 1.7000000000000001e-7
output_cost_per_token = 0.00000112
cache_read_input_token_cost = 3e-8
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2025-11-11"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."volcengine/doubao-seed-code".pricing."zenmux"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 1.7000000000000001e-7
output_cost_per_token = 0.00000112

[models."voxtral-small-24b-2507"]
display_name = "Voxtral Small 24B 2507"
model_family = "voxtral"
mode = "chat"
max_input_tokens = 32000
max_output_tokens = 8192
input_cost_per_token = 1.5e-7
output_cost_per_token = 3.5e-7
litellm_provider = "scaleway"
providers = ["scaleway"]
supports_function_calling = true
supports_reasoning = false
open_weights = true
release_date = "2025-07-01"
supported_modalities = ["text", "audio"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."voxtral-small-24b-2507".pricing."scaleway"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 3.5e-7

[models."voyage-2"]
mode = "embedding"
max_input_tokens = 4000
max_tokens = 4000
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage"]

[models."voyage-2".pricing."voyage"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."voyage-3"]
mode = "embedding"
max_input_tokens = 32000
max_tokens = 32000
input_cost_per_token = 6e-8
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage"]

[models."voyage-3".pricing."voyage"]
input_cost_per_token = 6e-8
output_cost_per_token = 0

[models."voyage-3-large"]
display_name = "voyage-3-large"
model_family = "voyage"
mode = "embedding"
max_input_tokens = 32000
max_output_tokens = 1536
max_tokens = 32000
input_cost_per_token = 1.8e-7
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage", "vercel"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2024-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."voyage-3-large".pricing."vercel"]
input_cost_per_token = 1.8e-7
[models."voyage-3-large".pricing."voyage"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 0

[models."voyage-3-lite"]
mode = "embedding"
max_input_tokens = 32000
max_tokens = 32000
input_cost_per_token = 2e-8
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage"]

[models."voyage-3-lite".pricing."voyage"]
input_cost_per_token = 2e-8
output_cost_per_token = 0

[models."voyage-3.5"]
display_name = "voyage-3.5"
model_family = "voyage"
mode = "embedding"
max_input_tokens = 32000
max_output_tokens = 1536
max_tokens = 32000
input_cost_per_token = 6e-8
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage", "vercel"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-05-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."voyage-3.5".pricing."vercel"]
input_cost_per_token = 6e-8
[models."voyage-3.5".pricing."voyage"]
input_cost_per_token = 6e-8
output_cost_per_token = 0

[models."voyage-3.5-lite"]
display_name = "voyage-3.5-lite"
model_family = "voyage"
mode = "embedding"
max_input_tokens = 32000
max_output_tokens = 1536
max_tokens = 32000
input_cost_per_token = 2e-8
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage", "vercel"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-05-20"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."voyage-3.5-lite".pricing."vercel"]
input_cost_per_token = 2e-8
[models."voyage-3.5-lite".pricing."voyage"]
input_cost_per_token = 2e-8
output_cost_per_token = 0

[models."voyage-code-2"]
display_name = "voyage-code-2"
model_family = "voyage"
mode = "embedding"
max_input_tokens = 16000
max_output_tokens = 1536
max_tokens = 16000
input_cost_per_token = 1.2e-7
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage", "vercel"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2024-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."voyage-code-2".pricing."vercel"]
input_cost_per_token = 1.2e-7
[models."voyage-code-2".pricing."voyage"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 0

[models."voyage-code-3"]
display_name = "voyage-code-3"
model_family = "voyage"
mode = "embedding"
max_input_tokens = 32000
max_output_tokens = 1536
max_tokens = 32000
input_cost_per_token = 1.8e-7
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage", "vercel"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2024-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."voyage-code-3".pricing."vercel"]
input_cost_per_token = 1.8e-7
[models."voyage-code-3".pricing."voyage"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 0

[models."voyage-context-3"]
mode = "embedding"
max_input_tokens = 120000
max_tokens = 120000
input_cost_per_token = 1.8e-7
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage"]

[models."voyage-context-3".pricing."voyage"]
input_cost_per_token = 1.8e-7
output_cost_per_token = 0

[models."voyage-finance-2"]
display_name = "voyage-finance-2"
model_family = "voyage"
mode = "embedding"
max_input_tokens = 32000
max_output_tokens = 1536
max_tokens = 32000
input_cost_per_token = 1.2e-7
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage", "vercel"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2024-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."voyage-finance-2".pricing."vercel"]
input_cost_per_token = 1.2e-7
[models."voyage-finance-2".pricing."voyage"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 0

[models."voyage-large-2"]
mode = "embedding"
max_input_tokens = 16000
max_tokens = 16000
input_cost_per_token = 1.2e-7
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage"]

[models."voyage-large-2".pricing."voyage"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 0

[models."voyage-law-2"]
display_name = "voyage-law-2"
model_family = "voyage"
mode = "embedding"
max_input_tokens = 16000
max_output_tokens = 1536
max_tokens = 16000
input_cost_per_token = 1.2e-7
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage", "vercel"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2024-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."voyage-law-2".pricing."vercel"]
input_cost_per_token = 1.2e-7
[models."voyage-law-2".pricing."voyage"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 0

[models."voyage-lite-01"]
mode = "embedding"
max_input_tokens = 4096
max_tokens = 4096
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage"]

[models."voyage-lite-01".pricing."voyage"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."voyage-lite-02-instruct"]
mode = "embedding"
max_input_tokens = 4000
max_tokens = 4000
input_cost_per_token = 1e-7
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage"]

[models."voyage-lite-02-instruct".pricing."voyage"]
input_cost_per_token = 1e-7
output_cost_per_token = 0

[models."voyage-multimodal-3"]
mode = "embedding"
max_input_tokens = 32000
max_tokens = 32000
input_cost_per_token = 1.2e-7
output_cost_per_token = 0
litellm_provider = "voyage"
providers = ["voyage"]

[models."voyage-multimodal-3".pricing."voyage"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 0

[models."whisper"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.0001
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."whisper".metadata]
notes = "Deepgram's hosted OpenAI Whisper models - pricing may differ from native Deepgram models"

[models."whisper".pricing."deepgram"]
input_cost_per_second = 0.0001
output_cost_per_second = 0

[models."whisper-1"]
mode = "audio_transcription"
litellm_provider = "azure"
providers = ["azure", "openai"]
input_cost_per_second = 0.0001
output_cost_per_second = 0.0001
supported_endpoints = ["/v1/audio/transcriptions"]

[models."whisper-1".pricing."azure"]
input_cost_per_second = 0.0001
output_cost_per_second = 0.0001
[models."whisper-1".pricing."openai"]
input_cost_per_second = 0.0001
output_cost_per_second = 0.0001

[models."whisper-base"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.0001
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."whisper-base".metadata]
notes = "Deepgram's hosted OpenAI Whisper models - pricing may differ from native Deepgram models"

[models."whisper-base".pricing."deepgram"]
input_cost_per_second = 0.0001
output_cost_per_second = 0

[models."whisper-large"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.0001
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."whisper-large".metadata]
notes = "Deepgram's hosted OpenAI Whisper models - pricing may differ from native Deepgram models"

[models."whisper-large".pricing."deepgram"]
input_cost_per_second = 0.0001
output_cost_per_second = 0

[models."whisper-large-v3"]
display_name = "Whisper 3 Large"
model_family = "whisper"
mode = "audio_transcription"
max_input_tokens = 448
max_output_tokens = 4096
litellm_provider = "groq"
providers = ["groq", "evroc", "nvidia", "privatemode-ai", "scaleway"]
supports_function_calling = false
supports_reasoning = false
open_weights = true
knowledge_cutoff = "2023-09"
release_date = "2024-10-01"
supported_modalities = ["audio"]
supported_output_modalities = ["text"]
input_cost_per_second = 0.00003083
output_cost_per_second = 0

[models."whisper-large-v3".pricing."evroc"]
input_cost_per_token = 2.36e-9
output_cost_per_token = 2.36e-9
[models."whisper-large-v3".pricing."groq"]
input_cost_per_second = 0.00003083
output_cost_per_second = 0
[models."whisper-large-v3".pricing."scaleway"]
input_cost_per_token = 3e-9

[models."whisper-large-v3-turbo"]
mode = "audio_transcription"
litellm_provider = "groq"
providers = ["groq", "watsonx"]
input_cost_per_second = 0.00001111
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."whisper-large-v3-turbo".pricing."groq"]
input_cost_per_second = 0.00001111
output_cost_per_second = 0
[models."whisper-large-v3-turbo".pricing."watsonx"]
input_cost_per_second = 0.0001
output_cost_per_second = 0.0001

[models."whisper-medium"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.0001
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."whisper-medium".metadata]
notes = "Deepgram's hosted OpenAI Whisper models - pricing may differ from native Deepgram models"

[models."whisper-medium".pricing."deepgram"]
input_cost_per_second = 0.0001
output_cost_per_second = 0

[models."whisper-small"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.0001
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."whisper-small".metadata]
notes = "Deepgram's hosted OpenAI Whisper models - pricing may differ from native Deepgram models"

[models."whisper-small".pricing."deepgram"]
input_cost_per_second = 0.0001
output_cost_per_second = 0

[models."whisper-tiny"]
mode = "audio_transcription"
litellm_provider = "deepgram"
providers = ["deepgram"]
source = "https://deepgram.com/pricing"
input_cost_per_second = 0.0001
output_cost_per_second = 0
supported_endpoints = ["/v1/audio/transcriptions"]

[models."whisper-tiny".metadata]
notes = "Deepgram's hosted OpenAI Whisper models - pricing may differ from native Deepgram models"

[models."whisper-tiny".pricing."deepgram"]
input_cost_per_second = 0.0001
output_cost_per_second = 0

[models."workers-ai/@cf/ai4bharat/indictrans2-en-indic-1B"]
display_name = "IndicTrans2 EN-Indic 1B"
model_family = "indictrans"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 3.4000000000000003e-7
output_cost_per_token = 3.4000000000000003e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-09-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/ai4bharat/indictrans2-en-indic-1B".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 3.4000000000000003e-7
output_cost_per_token = 3.4000000000000003e-7

[models."workers-ai/@cf/aisingapore/gemma-sea-lion-v4-27b-it"]
display_name = "Gemma SEA-LION v4 27B IT"
model_family = "gemma"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 3.5e-7
output_cost_per_token = 5.6e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-09-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/aisingapore/gemma-sea-lion-v4-27b-it".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 5.6e-7

[models."workers-ai/@cf/baai/bge-base-en-v1-5"]
display_name = "BGE Base EN v1.5"
model_family = "bge"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 6.7e-8
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/baai/bge-base-en-v1-5".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 6.7e-8

[models."workers-ai/@cf/baai/bge-large-en-v1-5"]
display_name = "BGE Large EN v1.5"
model_family = "bge"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.0000000000000002e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/baai/bge-large-en-v1-5".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 2.0000000000000002e-7

[models."workers-ai/@cf/baai/bge-m3"]
display_name = "BGE M3"
model_family = "bge"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.2e-8
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/baai/bge-m3".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 1.2e-8

[models."workers-ai/@cf/baai/bge-reranker-base"]
display_name = "BGE Reranker Base"
model_family = "bge"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 3.1e-9
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/baai/bge-reranker-base".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 3.1e-9

[models."workers-ai/@cf/baai/bge-small-en-v1-5"]
display_name = "BGE Small EN v1.5"
model_family = "bge"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2e-8
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/baai/bge-small-en-v1-5".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 2e-8

[models."workers-ai/@cf/deepgram/aura-2-en"]
display_name = "Deepgram Aura 2 (EN)"
model_family = "aura"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-11-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/deepgram/aura-2-es"]
display_name = "Deepgram Aura 2 (ES)"
model_family = "aura"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-11-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/deepgram/nova-3"]
display_name = "Deepgram Nova 3"
model_family = "nova"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-11-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/deepseek-ai/deepseek-r1-distill-qwen-32b"]
display_name = "DeepSeek R1 Distill Qwen 32B"
model_family = "deepseek-thinking"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 5e-7
output_cost_per_token = 0.00000488
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/deepseek-ai/deepseek-r1-distill-qwen-32b".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.00000488

[models."workers-ai/@cf/facebook/bart-large-cnn"]
display_name = "BART Large CNN"
model_family = "bart"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/google/gemma-3-12b-it"]
display_name = "Gemma 3 12B IT"
model_family = "gemma"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 3.5e-7
output_cost_per_token = 5.6e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/google/gemma-3-12b-it".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 5.6e-7

[models."workers-ai/@cf/huggingface/distilbert-sst-2-int8"]
display_name = "DistilBERT SST-2 INT8"
model_family = "distilbert"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.5999999999999998e-8
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/huggingface/distilbert-sst-2-int8".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 2.5999999999999998e-8

[models."workers-ai/@cf/ibm-granite/granite-4-0-h-micro"]
display_name = "IBM Granite 4.0 H Micro"
model_family = "granite"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.7e-8
output_cost_per_token = 1.1e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-10-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/ibm-granite/granite-4-0-h-micro".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 1.7e-8
output_cost_per_token = 1.1e-7

[models."workers-ai/@cf/meta/llama-2-7b-chat-fp16"]
display_name = "Llama 2 7B Chat FP16"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000667
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/meta/llama-2-7b-chat-fp16".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 5.6e-7
output_cost_per_token = 0.00000667

[models."workers-ai/@cf/meta/llama-3-1-8b-instruct"]
display_name = "Llama 3.1 8B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.8e-7
output_cost_per_token = 8.299999999999999e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/meta/llama-3-1-8b-instruct".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 8.299999999999999e-7

[models."workers-ai/@cf/meta/llama-3-1-8b-instruct-awq"]
display_name = "Llama 3.1 8B Instruct AWQ"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.2e-7
output_cost_per_token = 2.7e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/meta/llama-3-1-8b-instruct-awq".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 2.7e-7

[models."workers-ai/@cf/meta/llama-3-1-8b-instruct-fp8"]
display_name = "Llama 3.1 8B Instruct FP8"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.5e-7
output_cost_per_token = 2.9e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/meta/llama-3-1-8b-instruct-fp8".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 1.5e-7
output_cost_per_token = 2.9e-7

[models."workers-ai/@cf/meta/llama-3-2-11b-vision-instruct"]
display_name = "Llama 3.2 11B Vision Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 4.9e-8
output_cost_per_token = 6.800000000000001e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/meta/llama-3-2-11b-vision-instruct".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 4.9e-8
output_cost_per_token = 6.800000000000001e-7

[models."workers-ai/@cf/meta/llama-3-2-1b-instruct"]
display_name = "Llama 3.2 1B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.7e-8
output_cost_per_token = 2.0000000000000002e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/meta/llama-3-2-1b-instruct".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 2.7e-8
output_cost_per_token = 2.0000000000000002e-7

[models."workers-ai/@cf/meta/llama-3-2-3b-instruct"]
display_name = "Llama 3.2 3B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 5.0999999999999993e-8
output_cost_per_token = 3.4000000000000003e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/meta/llama-3-2-3b-instruct".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 5.0999999999999993e-8
output_cost_per_token = 3.4000000000000003e-7

[models."workers-ai/@cf/meta/llama-3-3-70b-instruct-fp8-fast"]
display_name = "Llama 3.3 70B Instruct FP8 Fast"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.00000225
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/meta/llama-3-3-70b-instruct-fp8-fast".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 2.9e-7
output_cost_per_token = 0.00000225

[models."workers-ai/@cf/meta/llama-3-8b-instruct"]
display_name = "Llama 3 8B Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.8e-7
output_cost_per_token = 8.3e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/meta/llama-3-8b-instruct".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 2.8e-7
output_cost_per_token = 8.3e-7

[models."workers-ai/@cf/meta/llama-3-8b-instruct-awq"]
display_name = "Llama 3 8B Instruct AWQ"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.2e-7
output_cost_per_token = 2.7e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/meta/llama-3-8b-instruct-awq".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 1.2e-7
output_cost_per_token = 2.7e-7

[models."workers-ai/@cf/meta/llama-4-scout-17b-16e-instruct"]
display_name = "Llama 4 Scout 17B 16E Instruct"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.7e-7
output_cost_per_token = 8.5e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-16"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/meta/llama-4-scout-17b-16e-instruct".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 2.7e-7
output_cost_per_token = 8.5e-7

[models."workers-ai/@cf/meta/llama-guard-3-8b"]
display_name = "Llama Guard 3 8B"
model_family = "llama"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 4.8e-7
output_cost_per_token = 3e-8
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/meta/llama-guard-3-8b".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 4.8e-7
output_cost_per_token = 3e-8

[models."workers-ai/@cf/meta/m2m100-1-2b"]
display_name = "M2M100 1.2B"
model_family = "m2m"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 3.4000000000000003e-7
output_cost_per_token = 3.4000000000000003e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/meta/m2m100-1-2b".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 3.4000000000000003e-7
output_cost_per_token = 3.4000000000000003e-7

[models."workers-ai/@cf/mistral/mistral-7b-instruct-v0-1"]
display_name = "Mistral 7B Instruct v0.1"
model_family = "mistral"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.1e-7
output_cost_per_token = 1.9e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-03"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/mistral/mistral-7b-instruct-v0-1".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 1.1e-7
output_cost_per_token = 1.9e-7

[models."workers-ai/@cf/mistralai/mistral-small-3-1-24b-instruct"]
display_name = "Mistral Small 3.1 24B Instruct"
model_family = "mistral-small"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 3.5e-7
output_cost_per_token = 5.6e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/mistralai/mistral-small-3-1-24b-instruct".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 5.6e-7

[models."workers-ai/@cf/myshell-ai/melotts"]
display_name = "MyShell MeloTTS"
model_family = "melotts"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-11-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/openai/gpt-oss-120b"]
display_name = "GPT OSS 120B"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 3.5e-7
output_cost_per_token = 7.5e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-08-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/openai/gpt-oss-120b".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 7.5e-7

[models."workers-ai/@cf/openai/gpt-oss-20b"]
display_name = "GPT OSS 20B"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 3e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-08-05"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/openai/gpt-oss-20b".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 3e-7

[models."workers-ai/@cf/pfnet/plamo-embedding-1b"]
display_name = "PLaMo Embedding 1B"
model_family = "plamo"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.8999999999999998e-8
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-09-25"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/pfnet/plamo-embedding-1b".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 1.8999999999999998e-8

[models."workers-ai/@cf/pipecat-ai/smart-turn-v2"]
display_name = "Pipecat Smart Turn v2"
model_family = "smart-turn"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-11-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/qwen/qwen2-5-coder-32b-instruct"]
display_name = "Qwen 2.5 Coder 32B Instruct"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 6.6e-7
output_cost_per_token = 0.000001
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/qwen/qwen2-5-coder-32b-instruct".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 6.6e-7
output_cost_per_token = 0.000001

[models."workers-ai/@cf/qwen/qwen3-30b-a3b-fp8"]
display_name = "Qwen3 30B A3B FP8"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 5.0999999999999993e-8
output_cost_per_token = 3.4000000000000003e-7
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-11-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/qwen/qwen3-30b-a3b-fp8".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 5.0999999999999993e-8
output_cost_per_token = 3.4000000000000003e-7

[models."workers-ai/@cf/qwen/qwen3-embedding-0-6b"]
display_name = "Qwen3 Embedding 0.6B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 1.2e-8
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-11-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/qwen/qwen3-embedding-0-6b".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 1.2e-8

[models."workers-ai/@cf/qwen/qwq-32b"]
display_name = "QwQ 32B"
model_family = "qwen"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 16384
input_cost_per_token = 6.6e-7
output_cost_per_token = 0.000001
litellm_provider = "cloudflare-ai-gateway"
providers = ["cloudflare-ai-gateway"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."workers-ai/@cf/qwen/qwq-32b".pricing."cloudflare-ai-gateway"]
input_cost_per_token = 6.6e-7
output_cost_per_token = 0.000001

[models."writer.palmyra-x4-v1:0"]
display_name = "Palmyra X4"
model_family = "palmyra"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = true
supports_pdf_input = true
open_weights = false
release_date = "2025-04-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."writer.palmyra-x4-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001
[models."writer.palmyra-x4-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 0.0000025
output_cost_per_token = 0.00001

[models."writer.palmyra-x5-v1:0"]
display_name = "Palmyra X5"
model_family = "palmyra"
mode = "chat"
max_input_tokens = 1000000
max_output_tokens = 8192
max_tokens = 8192
input_cost_per_token = 6e-7
output_cost_per_token = 0.000006
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = true
supports_pdf_input = true
open_weights = false
release_date = "2025-04-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."writer.palmyra-x5-v1:0".pricing."amazon-bedrock"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000006
[models."writer.palmyra-x5-v1:0".pricing."bedrock_converse"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000006

[models."writer/palmyra-x5"]
display_name = "Writer: Palmyra X5"
mode = "chat"
max_input_tokens = 1040000
max_output_tokens = 8192
input_cost_per_token = 6e-7
output_cost_per_token = 0.000006
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = false
supports_reasoning = false
open_weights = false
release_date = "2025-04-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."writer/palmyra-x5".pricing."kilo"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000006

[models."x-ai/grok-3"]
display_name = "Grok 3"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 7.5e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-02-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."x-ai/grok-3".pricing."kilo"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."x-ai/grok-3".pricing."openrouter"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."x-ai/grok-3-beta"]
display_name = "Grok 3 Beta"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
cache_read_input_token_cost = 7.5e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-02-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."x-ai/grok-3-beta".pricing."kilo"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."x-ai/grok-3-beta".pricing."openrouter"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."x-ai/grok-3-mini"]
display_name = "Grok 3 Mini"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 7.5e-8
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-02-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."x-ai/grok-3-mini".pricing."kilo"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7
[models."x-ai/grok-3-mini".pricing."openrouter"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7

[models."x-ai/grok-3-mini-beta"]
display_name = "Grok 3 Mini Beta"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 8192
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 7.5e-8
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-02-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."x-ai/grok-3-mini-beta".pricing."kilo"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7
[models."x-ai/grok-3-mini-beta".pricing."openrouter"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7

[models."x-ai/grok-4"]
display_name = "Grok 4"
model_family = "grok"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "openrouter"
providers = ["openrouter", "fastrouter", "kilo", "zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-07"
release_date = "2025-07-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://openrouter.ai/x-ai/grok-4"
supports_tool_choice = true
supports_web_search = true

[models."x-ai/grok-4".pricing."fastrouter"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."x-ai/grok-4".pricing."kilo"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."x-ai/grok-4".pricing."openrouter"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."x-ai/grok-4".pricing."zenmux"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."x-ai/grok-4-1-fast"]
display_name = "Grok 4.1 Fast"
model_family = "grok"
mode = "chat"
max_input_tokens = 2000000
max_output_tokens = 30000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 5.0000000000000004e-8
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-11-19"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."x-ai/grok-4-1-fast".pricing."kilo"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
[models."x-ai/grok-4-1-fast".pricing."openrouter"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
[models."x-ai/grok-4-1-fast".pricing."zenmux"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7

[models."x-ai/grok-4-1-fast-non-reasoning"]
display_name = "Grok 4.1 Fast Non Reasoning"
mode = "chat"
max_input_tokens = 2000000
max_output_tokens = 64000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 5.0000000000000004e-8
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2025-11-20"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."x-ai/grok-4-1-fast-non-reasoning".pricing."zenmux"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7

[models."x-ai/grok-4-fast"]
display_name = "Grok 4 Fast"
model_family = "grok"
mode = "chat"
max_input_tokens = 2000000
max_output_tokens = 30000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 5.0000000000000004e-8
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-08-19"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."x-ai/grok-4-fast".pricing."kilo"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
[models."x-ai/grok-4-fast".pricing."openrouter"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
[models."x-ai/grok-4-fast".pricing."zenmux"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7

[models."x-ai/grok-code-fast-1"]
display_name = "Grok Code Fast 1"
model_family = "grok"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 10000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015
cache_read_input_token_cost = 2e-8
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-08"
release_date = "2025-08-26"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."x-ai/grok-code-fast-1".pricing."kilo"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015
[models."x-ai/grok-code-fast-1".pricing."openrouter"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015
[models."x-ai/grok-code-fast-1".pricing."zenmux"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015

[models."xai.grok-3"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "oci"
providers = ["oci"]
supports_function_calling = true
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_response_schema = false

[models."xai.grok-3".pricing."oci"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."xai.grok-3-fast"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
litellm_provider = "oci"
providers = ["oci"]
supports_function_calling = true
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_response_schema = false

[models."xai.grok-3-fast".pricing."oci"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025

[models."xai.grok-3-mini"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7
litellm_provider = "oci"
providers = ["oci"]
supports_function_calling = true
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_response_schema = false

[models."xai.grok-3-mini".pricing."oci"]
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7

[models."xai.grok-3-mini-fast"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 6e-7
output_cost_per_token = 0.000004
litellm_provider = "oci"
providers = ["oci"]
supports_function_calling = true
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_response_schema = false

[models."xai.grok-3-mini-fast".pricing."oci"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000004

[models."xai.grok-4"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "oci"
providers = ["oci"]
supports_function_calling = true
source = "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/pricing"
supports_response_schema = false

[models."xai.grok-4".pricing."oci"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."xai/grok-2"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 4000
max_tokens = 4000
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway"]
supports_function_calling = true
supports_tool_choice = true

[models."xai/grok-2".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001

[models."xai/grok-2-vision"]
display_name = "Grok 2 Vision"
model_family = "grok"
mode = "chat"
max_input_tokens = 32768
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_vision = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-08"
release_date = "2024-08-20"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."xai/grok-2-vision".pricing."vercel"]
cache_read_input_token_cost = 0.000002
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001
[models."xai/grok-2-vision".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.000002
output_cost_per_token = 0.00001

[models."xai/grok-3"]
display_name = "Grok 3"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "github-models", "poe", "vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-02-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."xai/grok-3".pricing."poe"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."xai/grok-3".pricing."vercel"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."xai/grok-3".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."xai/grok-3-fast"]
display_name = "Grok 3 Fast"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel", "xai"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-02-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."xai/grok-3-fast".pricing."vercel"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."xai/grok-3-fast".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025
[models."xai/grok-3-fast".pricing."xai"]
cache_read_input_token_cost = 0.00000125
input_cost_per_token = 0.000005
output_cost_per_token = 0.000025

[models."xai/grok-3-mini"]
display_name = "Grok 3 Mini"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "github-models", "poe", "vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-02-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
reasoning_cost_per_token = 5e-7
supports_tool_choice = true

[models."xai/grok-3-mini".pricing."poe"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7
[models."xai/grok-3-mini".pricing."vercel"]
cache_read_input_token_cost = 7.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7
reasoning_cost_per_token = 5e-7
[models."xai/grok-3-mini".pricing."vercel_ai_gateway"]
input_cost_per_token = 3e-7
output_cost_per_token = 5e-7

[models."xai/grok-3-mini-fast"]
display_name = "Grok 3 Mini Fast"
model_family = "grok"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 6e-7
output_cost_per_token = 0.000004
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-11"
release_date = "2025-02-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
reasoning_cost_per_token = 0.000004
supports_tool_choice = true

[models."xai/grok-3-mini-fast".pricing."vercel"]
cache_read_input_token_cost = 1.5e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.000004
reasoning_cost_per_token = 0.000004
[models."xai/grok-3-mini-fast".pricing."vercel_ai_gateway"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.000004

[models."xai/grok-4"]
display_name = "Grok 4"
model_family = "grok"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 256000
max_tokens = 256000
input_cost_per_token = 0.0000072
output_cost_per_token = 0.000036
litellm_provider = "replicate"
providers = ["replicate", "poe", "requesty", "vercel", "vercel_ai_gateway"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-07"
release_date = "2025-07-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
reasoning_cost_per_token = 0.000015
supports_system_messages = true
supports_tool_choice = true

[models."xai/grok-4".pricing."poe"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."xai/grok-4".pricing."replicate"]
input_cost_per_token = 0.0000072
output_cost_per_token = 0.000036
[models."xai/grok-4".pricing."requesty"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
[models."xai/grok-4".pricing."vercel"]
cache_read_input_token_cost = 7.5e-7
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015
reasoning_cost_per_token = 0.000015
[models."xai/grok-4".pricing."vercel_ai_gateway"]
input_cost_per_token = 0.000003
output_cost_per_token = 0.000015

[models."xai/grok-4-1-fast-non-reasoning"]
display_name = "Grok 4.1 Fast Non-Reasoning"
model_family = "grok"
mode = "responses"
max_input_tokens = 2000000
max_output_tokens = 30000
litellm_provider = "perplexity"
providers = ["perplexity", "vercel"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-07-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_web_search = true

[models."xai/grok-4-1-fast-non-reasoning".pricing."vercel"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7

[models."xai/grok-4-1-fast-reasoning"]
display_name = "Grok 4.1 Fast Reasoning"
model_family = "grok"
mode = "chat"
max_input_tokens = 2000000
max_output_tokens = 30000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 5.0000000000000004e-8
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-07-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."xai/grok-4-1-fast-reasoning".pricing."vercel"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7

[models."xai/grok-4-fast"]
display_name = "Grok 4 Fast"
model_family = "grok"
mode = "chat"
max_input_tokens = 2000000
max_output_tokens = 64000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 5.0000000000000004e-8
litellm_provider = "requesty"
providers = ["requesty"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01"
release_date = "2025-09-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."xai/grok-4-fast".pricing."requesty"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7

[models."xai/grok-4-fast-non-reasoning"]
display_name = "Grok 4 Fast (Non-Reasoning)"
model_family = "grok"
mode = "chat"
max_input_tokens = 2000000
max_output_tokens = 30000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 5.0000000000000004e-8
litellm_provider = "vercel"
providers = ["vercel", "poe"]
supports_function_calling = true
supports_reasoning = false
open_weights = false
knowledge_cutoff = "2025-07"
release_date = "2025-09-19"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."xai/grok-4-fast-non-reasoning".pricing."poe"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
[models."xai/grok-4-fast-non-reasoning".pricing."vercel"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7

[models."xai/grok-4-fast-reasoning"]
display_name = "Grok 4 Fast Reasoning"
model_family = "grok"
mode = "chat"
max_input_tokens = 2000000
max_output_tokens = 256000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
cache_read_input_token_cost = 5.0000000000000004e-8
litellm_provider = "vercel"
providers = ["vercel", "poe"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-07-09"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."xai/grok-4-fast-reasoning".pricing."poe"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7
[models."xai/grok-4-fast-reasoning".pricing."vercel"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 5e-7

[models."xai/grok-code-fast-1"]
display_name = "Grok Code Fast 1"
model_family = "grok"
mode = "chat"
max_input_tokens = 256000
max_output_tokens = 10000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015
cache_read_input_token_cost = 2e-8
litellm_provider = "vercel"
providers = ["vercel", "poe"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2023-10"
release_date = "2025-08-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."xai/grok-code-fast-1".pricing."poe"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015
[models."xai/grok-code-fast-1".pricing."vercel"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000015

[models."xiaomi/mimo-v2-flash"]
display_name = "MiMo-V2-Flash"
model_family = "mimo"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 9e-8
output_cost_per_token = 2.9e-7
cache_read_input_token_cost = 0
cache_creation_input_token_cost = 0
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "vercel", "xiaomi", "zenmux"]
supports_function_calling = true
supports_vision = false
supports_reasoning = true
supports_prompt_caching = false
open_weights = true
knowledge_cutoff = "2024-12-01"
release_date = "2025-12-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."xiaomi/mimo-v2-flash".pricing."kilo"]
cache_read_input_token_cost = 4.5e-8
input_cost_per_token = 9e-8
output_cost_per_token = 2.9e-7
[models."xiaomi/mimo-v2-flash".pricing."openrouter"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 0
input_cost_per_token = 9e-8
output_cost_per_token = 2.9e-7
[models."xiaomi/mimo-v2-flash".pricing."vercel"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 2.9e-7
[models."xiaomi/mimo-v2-flash".pricing."xiaomi"]
input_cost_per_token = 7e-8
output_cost_per_token = 2.1e-7
[models."xiaomi/mimo-v2-flash".pricing."zenmux"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7

[models."xiaomi/mimo-v2-flash-free"]
display_name = "MiMo-V2-Flash Free"
mode = "chat"
max_input_tokens = 262000
max_output_tokens = 64000
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2025-12-17"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."xiaomimimo/mimo-v2-flash"]
display_name = "XiaomiMiMo/MiMo-V2-Flash"
model_family = "mimo"
mode = "chat"
max_input_tokens = 262144
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 1e-7
output_cost_per_token = 3e-7
cache_read_input_token_cost = 2e-8
litellm_provider = "novita"
providers = ["novita", "chutes", "huggingface", "jiekou", "meganova", "novita-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2024-12"
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
input_cost_per_token_cache_hit = 2e-8
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."xiaomimimo/mimo-v2-flash".pricing."chutes"]
input_cost_per_token = 9e-8
output_cost_per_token = 2.9e-7
[models."xiaomimimo/mimo-v2-flash".pricing."huggingface"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
[models."xiaomimimo/mimo-v2-flash".pricing."meganova"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7
[models."xiaomimimo/mimo-v2-flash".pricing."novita"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 1e-7
input_cost_per_token_cache_hit = 2e-8
output_cost_per_token = 3e-7
[models."xiaomimimo/mimo-v2-flash".pricing."novita-ai"]
cache_read_input_token_cost = 3e-7
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 3e-7

[models."z-ai/glm-4-5"]
display_name = "GLM 4.5"
model_family = "glm"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 96000
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."z-ai/glm-4-5".pricing."kilo"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000155
[models."z-ai/glm-4-5".pricing."openrouter"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."z-ai/glm-4-5".pricing."zenmux"]
cache_read_input_token_cost = 7e-8
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000154

[models."z-ai/glm-4-5-air"]
display_name = "GLM 4.5 Air"
model_family = "glm-air"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 96000
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000011
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."z-ai/glm-4-5-air".pricing."kilo"]
cache_read_input_token_cost = 2.5000000000000002e-8
input_cost_per_token = 1.3e-7
output_cost_per_token = 8.5e-7
[models."z-ai/glm-4-5-air".pricing."openrouter"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000011
[models."z-ai/glm-4-5-air".pricing."zenmux"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 1.1e-7
output_cost_per_token = 5.6e-7

[models."z-ai/glm-4-5-air:free"]
display_name = "GLM 4.5 Air (free)"
model_family = "glm-air"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 96000
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."z-ai/glm-4-5v"]
display_name = "GLM 4.5V"
model_family = "glm"
mode = "chat"
max_input_tokens = 64000
max_output_tokens = 16384
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000018000000000000001
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-08-11"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."z-ai/glm-4-5v".pricing."kilo"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000018000000000000001
[models."z-ai/glm-4-5v".pricing."openrouter"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000018000000000000001

[models."z-ai/glm-4-6v"]
display_name = "GLM 4.6V"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 1.4e-7
output_cost_per_token = 4.2e-7
cache_read_input_token_cost = 3e-8
litellm_provider = "zenmux"
providers = ["zenmux", "kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2025-12-08"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."z-ai/glm-4-6v".pricing."kilo"]
input_cost_per_token = 3e-7
output_cost_per_token = 9.000000000000001e-7
[models."z-ai/glm-4-6v".pricing."zenmux"]
cache_read_input_token_cost = 3e-8
input_cost_per_token = 1.4e-7
output_cost_per_token = 4.2e-7

[models."z-ai/glm-4-6v-flash"]
display_name = "GLM 4.6V FlashX"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 2e-8
output_cost_per_token = 2.1e-7
cache_read_input_token_cost = 4.3e-9
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2025-12-08"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."z-ai/glm-4-6v-flash".pricing."zenmux"]
cache_read_input_token_cost = 4.3e-9
input_cost_per_token = 2e-8
output_cost_per_token = 2.1e-7

[models."z-ai/glm-4-6v-flash-free"]
display_name = "GLM 4.6V Flash (Free)"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2025-12-08"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."z-ai/glm-4-7-flash-free"]
display_name = "GLM 4.7 Flash (Free)"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2026-01-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."z-ai/glm-4-7-flashx"]
display_name = "GLM 4.7 FlashX"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 64000
input_cost_per_token = 7e-8
output_cost_per_token = 4.2e-7
cache_read_input_token_cost = 1e-8
litellm_provider = "zenmux"
providers = ["zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-01-01"
release_date = "2026-01-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."z-ai/glm-4-7-flashx".pricing."zenmux"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 7e-8
output_cost_per_token = 4.2e-7

[models."z-ai/glm-4.6"]
display_name = "GLM 4.6"
model_family = "glm"
mode = "chat"
max_input_tokens = 202800
max_output_tokens = 131000
max_tokens = 131000
input_cost_per_token = 4e-7
output_cost_per_token = 0.00000175
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "zenmux"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = true
knowledge_cutoff = "2025-09"
release_date = "2025-09-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://openrouter.ai/z-ai/glm-4.6"
supports_tool_choice = true

[models."z-ai/glm-4.6".pricing."kilo"]
cache_read_input_token_cost = 1.75e-7
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.0000015
[models."z-ai/glm-4.6".pricing."openrouter"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.00000175
[models."z-ai/glm-4.6".pricing."zenmux"]
cache_read_input_token_cost = 7e-8
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000154

[models."z-ai/glm-4.6:exacto"]
display_name = "GLM 4.6 (exacto)"
model_family = "glm"
mode = "chat"
max_input_tokens = 202800
max_output_tokens = 131000
max_tokens = 131000
input_cost_per_token = 4.5e-7
output_cost_per_token = 0.0000019
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = true
knowledge_cutoff = "2025-09"
release_date = "2025-09-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://openrouter.ai/z-ai/glm-4.6:exacto"
supports_tool_choice = true

[models."z-ai/glm-4.6:exacto".pricing."kilo"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 4.4e-7
output_cost_per_token = 0.00000176
[models."z-ai/glm-4.6:exacto".pricing."openrouter"]
input_cost_per_token = 4.5e-7
output_cost_per_token = 0.0000019

[models."z-ai/glm-4.7"]
display_name = "GLM-4.7"
model_family = "glm"
mode = "chat"
max_input_tokens = 202752
max_output_tokens = 64000
max_tokens = 64000
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000015
cache_read_input_token_cost = 0
cache_creation_input_token_cost = 0
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "zenmux"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = false
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-12-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_assistant_prefill = true
supports_tool_choice = true

[models."z-ai/glm-4.7".pricing."kilo"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000015
[models."z-ai/glm-4.7".pricing."openrouter"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 0
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000015
[models."z-ai/glm-4.7".pricing."zenmux"]
cache_read_input_token_cost = 6e-8
input_cost_per_token = 2.8e-7
output_cost_per_token = 0.0000011399999999999999

[models."z-ai/glm-4.7-flash"]
display_name = "GLM-4.7-Flash"
model_family = "glm"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 32000
max_tokens = 32000
input_cost_per_token = 7e-8
output_cost_per_token = 4e-7
cache_read_input_token_cost = 0
cache_creation_input_token_cost = 0
litellm_provider = "openrouter"
providers = ["openrouter", "kilo"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
supports_prompt_caching = false
open_weights = true
release_date = "2026-01-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."z-ai/glm-4.7-flash".pricing."kilo"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 6e-8
output_cost_per_token = 4.0000000000000003e-7
[models."z-ai/glm-4.7-flash".pricing."openrouter"]
cache_creation_input_token_cost = 0
cache_read_input_token_cost = 0
input_cost_per_token = 7e-8
output_cost_per_token = 4e-7

[models."z-ai/glm-5"]
display_name = "GLM-5"
model_family = "glm"
mode = "chat"
max_input_tokens = 202752
max_output_tokens = 131000
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003
cache_read_input_token_cost = 2.0000000000000002e-7
litellm_provider = "openrouter"
providers = ["openrouter", "kilo", "zenmux"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-01-01"
release_date = "2026-02-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."z-ai/glm-5".pricing."kilo"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000025499999999999997
[models."z-ai/glm-5".pricing."openrouter"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003
[models."z-ai/glm-5".pricing."zenmux"]
cache_read_input_token_cost = 1.4e-7
input_cost_per_token = 5.8e-7
output_cost_per_token = 0.0000026

[models."z-ai/glm-5:free"]
display_name = "Z.ai: GLM 5 (free)"
mode = "chat"
max_input_tokens = 202800
max_output_tokens = 131072
litellm_provider = "kilo"
providers = ["kilo"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-02-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."z-ai/glm4-7"]
display_name = "GLM-4.7"
model_family = "glm"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-12-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."z-ai/glm5"]
display_name = "GLM5"
model_family = "glm"
mode = "chat"
max_input_tokens = 202752
max_output_tokens = 131000
litellm_provider = "nvidia"
providers = ["nvidia"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-02-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-glm-4.6"]
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000225
output_cost_per_token = 0.00000275
litellm_provider = "cerebras"
providers = ["cerebras"]
supports_function_calling = true
supports_reasoning = true
deprecation_date = "2026-01-20"
source = "https://www.cerebras.ai/pricing"
supports_tool_choice = true

[models."zai-glm-4.6".pricing."cerebras"]
input_cost_per_token = 0.00000225
output_cost_per_token = 0.00000275

[models."zai-glm-4.7"]
display_name = "Z.AI GLM-4.7"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.00000225
output_cost_per_token = 0.00000275
litellm_provider = "cerebras"
providers = ["cerebras"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-01-10"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://www.cerebras.ai/pricing"
supports_tool_choice = true

[models."zai-glm-4.7".pricing."cerebras"]
input_cost_per_token = 0.00000225
output_cost_per_token = 0.00000275

[models."zai-org-glm-4-7"]
display_name = "GLM 4.7"
model_family = "glm"
mode = "chat"
max_input_tokens = 198000
max_output_tokens = 49500
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.00000265
cache_read_input_token_cost = 1.1e-7
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-12-24"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org-glm-4-7".pricing."venice"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.00000265

[models."zai-org-glm-4-7-flash"]
display_name = "GLM 4.7 Flash"
model_family = "glm-flash"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 32000
input_cost_per_token = 1.25e-7
output_cost_per_token = 5e-7
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-01-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org-glm-4-7-flash".pricing."venice"]
input_cost_per_token = 1.25e-7
output_cost_per_token = 5e-7

[models."zai-org-glm-5"]
display_name = "GLM 5"
model_family = "glm"
mode = "chat"
max_input_tokens = 198000
max_output_tokens = 49500
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003
cache_read_input_token_cost = 2.0000000000000002e-7
litellm_provider = "venice"
providers = ["venice"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-02-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org-glm-5".pricing."venice"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003

[models."zai-org/GLM-4-5-FP8"]
display_name = "GLM 4.5 FP8"
model_family = "glm"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 65536
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
litellm_provider = "chutes"
providers = ["chutes", "submodel"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-01-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org/GLM-4-5-FP8".pricing."chutes"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."zai-org/GLM-4-5-FP8".pricing."submodel"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7

[models."zai-org/GLM-4-5-TEE"]
display_name = "GLM 4.5 TEE"
model_family = "glm"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 65536
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000155
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org/GLM-4-5-TEE".pricing."chutes"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.00000155

[models."zai-org/GLM-4-6-FP8"]
display_name = "GLM 4.6 FP8"
mode = "chat"
max_input_tokens = 202752
max_output_tokens = 65535
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-01-27"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org/GLM-4-6-FP8".pricing."chutes"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012

[models."zai-org/GLM-4-6-TEE"]
display_name = "GLM 4.6 TEE"
model_family = "glm"
mode = "chat"
max_input_tokens = 202752
max_output_tokens = 65536
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.0000015
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org/GLM-4-6-TEE".pricing."chutes"]
input_cost_per_token = 3.5e-7
output_cost_per_token = 0.0000015

[models."zai-org/GLM-4-7-TEE"]
display_name = "GLM 4.7 TEE"
model_family = "glm"
mode = "chat"
max_input_tokens = 202752
max_output_tokens = 65535
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000015
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-12-29"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org/GLM-4-7-TEE".pricing."chutes"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.0000015

[models."zai-org/GLM-4.5"]
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000016
litellm_provider = "deepinfra"
providers = ["deepinfra", "wandb"]
supports_tool_choice = true

[models."zai-org/GLM-4.5".pricing."deepinfra"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.0000016
[models."zai-org/GLM-4.5".pricing."wandb"]
input_cost_per_token = 0.055
output_cost_per_token = 0.2

[models."zai-org/GLM-4.5-Air-FP8"]
mode = "chat"
max_input_tokens = 128000
input_cost_per_token = 2e-7
output_cost_per_token = 0.0000011
litellm_provider = "together_ai"
providers = ["together_ai"]
supports_function_calling = true
source = "https://www.together.ai/models/glm-4-5-air"
supports_parallel_function_calling = true
supports_response_schema = true
supports_tool_choice = true

[models."zai-org/GLM-4.5-Air-FP8".pricing."together_ai"]
input_cost_per_token = 2e-7
output_cost_per_token = 0.0000011

[models."zai-org/GLM-4.6"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 200000
max_tokens = 200000
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
litellm_provider = "together_ai"
providers = ["together_ai"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.together.ai/models/glm-4-6"
supports_parallel_function_calling = true
supports_tool_choice = true

[models."zai-org/GLM-4.6".pricing."together_ai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022

[models."zai-org/GLM-4.7"]
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 200000
max_tokens = 200000
input_cost_per_token = 4.5e-7
output_cost_per_token = 0.000002
litellm_provider = "together_ai"
providers = ["together_ai"]
supports_function_calling = true
supports_reasoning = true
source = "https://www.together.ai/models/glm-4-7"
supports_parallel_function_calling = true
supports_tool_choice = true

[models."zai-org/GLM-4.7".pricing."together_ai"]
input_cost_per_token = 4.5e-7
output_cost_per_token = 0.000002

[models."zai-org/GLM-4.7-FP8"]
display_name = "GLM-4.7 (FP8)"
mode = "chat"
max_input_tokens = 202752
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002
litellm_provider = "gmi"
providers = ["gmi", "chutes", "nebius"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-12"
release_date = "2026-01-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]

[models."zai-org/GLM-4.7-FP8".pricing."chutes"]
input_cost_per_token = 3e-7
output_cost_per_token = 0.0000012
[models."zai-org/GLM-4.7-FP8".pricing."gmi"]
input_cost_per_token = 4e-7
output_cost_per_token = 0.000002
[models."zai-org/GLM-4.7-FP8".pricing."nebius"]
cache_read_input_token_cost = 4e-8
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002

[models."zai-org/GLM-5-TEE"]
display_name = "GLM 5 TEE"
model_family = "glm"
mode = "chat"
max_input_tokens = 202752
max_output_tokens = 65535
input_cost_per_token = 7.5e-7
output_cost_per_token = 0.0000025
litellm_provider = "chutes"
providers = ["chutes"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2026-02-14"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org/GLM-5-TEE".pricing."chutes"]
input_cost_per_token = 7.5e-7
output_cost_per_token = 0.0000025

[models."zai-org/autoglm-phone-9b-multilingual"]
display_name = "AutoGLM-Phone-9B-Multilingual"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 65536
max_tokens = 65536
input_cost_per_token = 3.5e-8
output_cost_per_token = 1.38e-7
litellm_provider = "novita"
providers = ["novita", "novita-ai"]
supports_function_calling = false
supports_vision = true
supports_reasoning = false
open_weights = true
release_date = "2025-12-10"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
supports_system_messages = true

[models."zai-org/autoglm-phone-9b-multilingual".pricing."novita"]
input_cost_per_token = 3.5e-8
output_cost_per_token = 1.38e-7
[models."zai-org/autoglm-phone-9b-multilingual".pricing."novita-ai"]
input_cost_per_token = 3.5e-8
output_cost_per_token = 1.3800000000000002e-7

[models."zai-org/glm-4-5-air:thinking"]
display_name = "GLM 4.5 Air Thinking"
model_family = "glm"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
litellm_provider = "nano-gpt"
providers = ["nano-gpt"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-04-07"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org/glm-4-5-air:thinking".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002

[models."zai-org/glm-4-6:thinking"]
display_name = "GLM 4.6 Thinking"
model_family = "glm"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
litellm_provider = "nano-gpt"
providers = ["nano-gpt"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-04-07"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org/glm-4-6:thinking".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002

[models."zai-org/glm-4-7-flash"]
display_name = "GLM-4.7-Flash"
model_family = "glm"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 128000
input_cost_per_token = 7e-8
output_cost_per_token = 4.0000000000000003e-7
litellm_provider = "jiekou"
providers = ["jiekou", "chutes", "deepinfra", "huggingface", "novita-ai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org/glm-4-7-flash".pricing."chutes"]
input_cost_per_token = 6e-8
output_cost_per_token = 3.5e-7
[models."zai-org/glm-4-7-flash".pricing."deepinfra"]
input_cost_per_token = 6e-8
output_cost_per_token = 4.0000000000000003e-7
[models."zai-org/glm-4-7-flash".pricing."jiekou"]
input_cost_per_token = 7e-8
output_cost_per_token = 4.0000000000000003e-7
[models."zai-org/glm-4-7-flash".pricing."novita-ai"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 7e-8
output_cost_per_token = 4.0000000000000003e-7

[models."zai-org/glm-4-7:thinking"]
display_name = "GLM 4.7 Thinking"
model_family = "glm"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 8192
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
litellm_provider = "nano-gpt"
providers = ["nano-gpt"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
release_date = "2025-04-07"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org/glm-4-7:thinking".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002

[models."zai-org/glm-4.5"]
display_name = "GLM-4.5"
model_family = "glm"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 98304
max_tokens = 98304
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
cache_read_input_token_cost = 1.1e-7
litellm_provider = "novita"
providers = ["novita", "abacus", "deepinfra", "jiekou", "nebius", "novita-ai", "siliconflow"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
input_cost_per_token_cache_hit = 1.1e-7
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."zai-org/glm-4.5".pricing."abacus"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.5".pricing."deepinfra"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.5".pricing."jiekou"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.5".pricing."nebius"]
cache_read_input_token_cost = 6e-8
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.5".pricing."novita"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 6e-7
input_cost_per_token_cache_hit = 1.1e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.5".pricing."novita-ai"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.5".pricing."siliconflow"]
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.000002

[models."zai-org/glm-4.5-air"]
display_name = "GLM-4.5-Air"
model_family = "glm-air"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 98304
max_tokens = 98304
input_cost_per_token = 1.3e-7
output_cost_per_token = 8.5e-7
litellm_provider = "novita"
providers = ["novita", "chutes", "nano-gpt", "nebius", "novita-ai", "siliconflow", "siliconflow-cn", "submodel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2025-06"
release_date = "2025-11-15"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_parallel_function_calling = true
supports_system_messages = true
supports_tool_choice = true

[models."zai-org/glm-4.5-air".pricing."chutes"]
input_cost_per_token = 5.0000000000000004e-8
output_cost_per_token = 2.2e-7
[models."zai-org/glm-4.5-air".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
[models."zai-org/glm-4.5-air".pricing."nebius"]
cache_read_input_token_cost = 2e-8
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000012
[models."zai-org/glm-4.5-air".pricing."novita"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 8.5e-7
[models."zai-org/glm-4.5-air".pricing."novita-ai"]
input_cost_per_token = 1.3e-7
output_cost_per_token = 8.5e-7
[models."zai-org/glm-4.5-air".pricing."siliconflow"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 8.6e-7
[models."zai-org/glm-4.5-air".pricing."siliconflow-cn"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 8.6e-7
[models."zai-org/glm-4.5-air".pricing."submodel"]
input_cost_per_token = 1.0000000000000001e-7
output_cost_per_token = 5e-7

[models."zai-org/glm-4.5v"]
display_name = "GLM 4.5V"
model_family = "glmv"
mode = "chat"
max_input_tokens = 65536
max_output_tokens = 16384
max_tokens = 16384
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000018
cache_read_input_token_cost = 1.1e-7
litellm_provider = "novita"
providers = ["novita", "jiekou", "novita-ai", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2026-01"
supported_modalities = ["text", "image", "video"]
supported_output_modalities = ["text"]
input_cost_per_token_cache_hit = 1.1e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."zai-org/glm-4.5v".pricing."jiekou"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000018000000000000001
[models."zai-org/glm-4.5v".pricing."novita"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 6e-7
input_cost_per_token_cache_hit = 1.1e-7
output_cost_per_token = 0.0000018
[models."zai-org/glm-4.5v".pricing."novita-ai"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000018000000000000001
[models."zai-org/glm-4.5v".pricing."siliconflow"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 8.6e-7
[models."zai-org/glm-4.5v".pricing."siliconflow-cn"]
input_cost_per_token = 1.4e-7
output_cost_per_token = 8.6e-7

[models."zai-org/glm-4.6"]
display_name = "GLM-4.6"
model_family = "glm"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000022
cache_read_input_token_cost = 1.1e-7
litellm_provider = "novita"
providers = ["novita", "abacus", "baseten", "io-net", "meganova", "nano-gpt", "novita-ai", "siliconflow", "siliconflow-cn", "togetherai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-09"
release_date = "2025-03-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
input_cost_per_token_cache_hit = 1.1e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."zai-org/glm-4.6".pricing."abacus"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.6".pricing."baseten"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.6".pricing."io-net"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 4.0000000000000003e-7
output_cost_per_token = 0.00000175
[models."zai-org/glm-4.6".pricing."meganova"]
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.0000018999999999999998
[models."zai-org/glm-4.6".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
[models."zai-org/glm-4.6".pricing."novita"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 5.5e-7
input_cost_per_token_cache_hit = 1.1e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.6".pricing."novita-ai"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 5.5e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.6".pricing."siliconflow"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000018999999999999998
[models."zai-org/glm-4.6".pricing."siliconflow-cn"]
input_cost_per_token = 5e-7
output_cost_per_token = 0.0000018999999999999998
[models."zai-org/glm-4.6".pricing."togetherai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022

[models."zai-org/glm-4.6v"]
display_name = "GLM 4.6V"
model_family = "glmv"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 32768
max_tokens = 32768
input_cost_per_token = 3e-7
output_cost_per_token = 9e-7
cache_read_input_token_cost = 5.5e-8
litellm_provider = "novita"
providers = ["novita", "chutes", "novita-ai", "siliconflow", "siliconflow-cn"]
supports_function_calling = true
supports_vision = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-12-08"
supported_modalities = ["text", "video", "image"]
supported_output_modalities = ["text"]
input_cost_per_token_cache_hit = 5.5e-8
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."zai-org/glm-4.6v".pricing."chutes"]
input_cost_per_token = 3e-7
output_cost_per_token = 9.000000000000001e-7
[models."zai-org/glm-4.6v".pricing."novita"]
cache_read_input_token_cost = 5.5e-8
input_cost_per_token = 3e-7
input_cost_per_token_cache_hit = 5.5e-8
output_cost_per_token = 9e-7
[models."zai-org/glm-4.6v".pricing."novita-ai"]
cache_read_input_token_cost = 5.5e-8
input_cost_per_token = 3e-7
output_cost_per_token = 9.000000000000001e-7
[models."zai-org/glm-4.6v".pricing."siliconflow"]
input_cost_per_token = 3e-7
output_cost_per_token = 9.000000000000001e-7
[models."zai-org/glm-4.6v".pricing."siliconflow-cn"]
input_cost_per_token = 3e-7
output_cost_per_token = 9.000000000000001e-7

[models."zai-org/glm-4.7"]
display_name = "GLM-4.7"
model_family = "glm"
mode = "chat"
max_input_tokens = 204800
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
cache_read_input_token_cost = 1.1e-7
litellm_provider = "novita"
providers = ["novita", "abacus", "baseten", "berget", "deepinfra", "huggingface", "jiekou", "meganova", "nano-gpt", "novita-ai", "siliconflow", "togetherai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2026-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
input_cost_per_token_cache_hit = 1.1e-7
supports_parallel_function_calling = true
supports_response_schema = true
supports_system_messages = true
supports_tool_choice = true

[models."zai-org/glm-4.7".pricing."abacus"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000025
[models."zai-org/glm-4.7".pricing."baseten"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.7".pricing."berget"]
input_cost_per_token = 7e-7
output_cost_per_token = 0.0000023
[models."zai-org/glm-4.7".pricing."deepinfra"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 4.3e-7
output_cost_per_token = 0.00000175
[models."zai-org/glm-4.7".pricing."huggingface"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.7".pricing."jiekou"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.7".pricing."meganova"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 8.000000000000001e-7
[models."zai-org/glm-4.7".pricing."nano-gpt"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.000002
[models."zai-org/glm-4.7".pricing."novita"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 6e-7
input_cost_per_token_cache_hit = 1.1e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.7".pricing."novita-ai"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.7".pricing."siliconflow"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.7".pricing."togetherai"]
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.000002

[models."zai-org/glm-4.7-maas"]
display_name = "GLM-4.7"
model_family = "glm"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2026-01-06"
supported_modalities = ["text", "pdf"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#partner-models"
supports_tool_choice = true

[models."zai-org/glm-4.7-maas".pricing."google-vertex"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."zai-org/glm-4.7-maas".pricing."vertex_ai"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022

[models."zai-org/glm-5"]
display_name = "GLM-5"
model_family = "glm"
mode = "chat"
max_input_tokens = 202800
max_output_tokens = 131072
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003
cache_read_input_token_cost = 2.0000000000000002e-7
litellm_provider = "novita-ai"
providers = ["novita-ai", "baseten", "friendli", "huggingface", "meganova", "nano-gpt", "siliconflow", "togetherai"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-11"
release_date = "2026-02-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org/glm-5".pricing."baseten"]
input_cost_per_token = 9.499999999999999e-7
output_cost_per_token = 0.00000315
[models."zai-org/glm-5".pricing."friendli"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003
[models."zai-org/glm-5".pricing."huggingface"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003
[models."zai-org/glm-5".pricing."meganova"]
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.00000256
[models."zai-org/glm-5".pricing."nano-gpt"]
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.00000256
[models."zai-org/glm-5".pricing."novita-ai"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003
[models."zai-org/glm-5".pricing."siliconflow"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003
[models."zai-org/glm-5".pricing."togetherai"]
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003

[models."zai-org/glm-5-maas"]
display_name = "GLM-5"
model_family = "glm"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032
cache_read_input_token_cost = 1e-7
litellm_provider = "vertex_ai"
providers = ["vertex_ai", "google-vertex"]
supports_function_calling = true
supports_reasoning = true
supports_prompt_caching = true
open_weights = true
release_date = "2026-02-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://cloud.google.com/vertex-ai/generative-ai/pricing#glm-models"
supports_tool_choice = true

[models."zai-org/glm-5-maas".pricing."google-vertex"]
cache_read_input_token_cost = 1.0000000000000001e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003
[models."zai-org/glm-5-maas".pricing."vertex_ai"]
cache_read_input_token_cost = 1e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032

[models."zai-org/glm-5-original"]
display_name = "GLM 5 Original"
model_family = "glm"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 128000
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.00000256
litellm_provider = "nano-gpt"
providers = ["nano-gpt"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2026-02-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org/glm-5-original".pricing."nano-gpt"]
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.00000256

[models."zai-org/glm-5-original:thinking"]
display_name = "GLM 5 Original Thinking"
model_family = "glm"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 128000
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.00000256
litellm_provider = "nano-gpt"
providers = ["nano-gpt"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2026-02-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org/glm-5-original:thinking".pricing."nano-gpt"]
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.00000256

[models."zai-org/glm-5:thinking"]
display_name = "GLM 5 Thinking"
model_family = "glm"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 128000
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.00000256
litellm_provider = "nano-gpt"
providers = ["nano-gpt"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-06"
release_date = "2026-02-11"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai-org/glm-5:thinking".pricing."nano-gpt"]
input_cost_per_token = 8.000000000000001e-7
output_cost_per_token = 0.00000256

[models."zai.glm-4-7-flash"]
display_name = "GLM-4.7-Flash"
model_family = "glm-flash"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 131072
input_cost_per_token = 7e-8
output_cost_per_token = 4.0000000000000003e-7
litellm_provider = "amazon-bedrock"
providers = ["amazon-bedrock"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2026-01-19"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai.glm-4-7-flash".pricing."amazon-bedrock"]
input_cost_per_token = 7e-8
output_cost_per_token = 4.0000000000000003e-7

[models."zai.glm-4.7"]
display_name = "GLM-4.7"
model_family = "glm"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 128000
max_tokens = 128000
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
litellm_provider = "bedrock_converse"
providers = ["bedrock_converse", "amazon-bedrock"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-12-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://aws.amazon.com/bedrock/pricing/"
supports_system_messages = true
supports_tool_choice = true

[models."zai.glm-4.7".pricing."amazon-bedrock"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."zai.glm-4.7".pricing."bedrock_converse"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022

[models."zai/glm-4-5v"]
display_name = "GLM 4.5V"
model_family = "glm"
mode = "chat"
max_input_tokens = 66000
max_output_tokens = 66000
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000018000000000000001
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-08"
release_date = "2025-08-11"
supported_modalities = ["text", "image"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai/glm-4-5v".pricing."vercel"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000018000000000000001

[models."zai/glm-4-6v"]
display_name = "GLM-4.6V"
model_family = "glm"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 24000
input_cost_per_token = 3e-7
output_cost_per_token = 9.000000000000001e-7
cache_read_input_token_cost = 5.0000000000000004e-8
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-09-30"
supported_modalities = ["text", "image", "pdf"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai/glm-4-6v".pricing."vercel"]
cache_read_input_token_cost = 5.0000000000000004e-8
input_cost_per_token = 3e-7
output_cost_per_token = 9.000000000000001e-7

[models."zai/glm-4-7"]
display_name = "GLM 4.7"
model_family = "glm"
mode = "chat"
max_input_tokens = 202752
max_output_tokens = 120000
input_cost_per_token = 4.3e-7
output_cost_per_token = 0.00000175
cache_read_input_token_cost = 8e-8
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
knowledge_cutoff = "2024-10"
release_date = "2025-12-22"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai/glm-4-7".pricing."vercel"]
cache_read_input_token_cost = 8e-8
input_cost_per_token = 4.3e-7
output_cost_per_token = 0.00000175

[models."zai/glm-4-7-flashx"]
display_name = "GLM 4.7 FlashX"
model_family = "glm-flash"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 128000
input_cost_per_token = 6e-8
output_cost_per_token = 4.0000000000000003e-7
cache_read_input_token_cost = 1e-8
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-01"
release_date = "2025-01"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai/glm-4-7-flashx".pricing."vercel"]
cache_read_input_token_cost = 1e-8
input_cost_per_token = 6e-8
output_cost_per_token = 4.0000000000000003e-7

[models."zai/glm-4.5"]
display_name = "GLM 4.5"
model_family = "glm"
mode = "chat"
max_input_tokens = 131072
max_output_tokens = 131072
max_tokens = 131072
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-07"
release_date = "2025-07-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."zai/glm-4.5".pricing."vercel"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022
[models."zai/glm-4.5".pricing."vercel_ai_gateway"]
input_cost_per_token = 6e-7
output_cost_per_token = 0.0000022

[models."zai/glm-4.5-air"]
display_name = "GLM 4.5 Air"
model_family = "glm-air"
mode = "chat"
max_input_tokens = 128000
max_output_tokens = 96000
max_tokens = 96000
input_cost_per_token = 2e-7
output_cost_per_token = 0.0000011
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-07-28"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
supports_tool_choice = true

[models."zai/glm-4.5-air".pricing."vercel"]
input_cost_per_token = 2.0000000000000002e-7
output_cost_per_token = 0.0000011
[models."zai/glm-4.5-air".pricing."vercel_ai_gateway"]
input_cost_per_token = 2e-7
output_cost_per_token = 0.0000011

[models."zai/glm-4.6"]
display_name = "GLM 4.6"
model_family = "glm"
mode = "chat"
max_input_tokens = 200000
max_output_tokens = 200000
max_tokens = 200000
input_cost_per_token = 4.5e-7
output_cost_per_token = 0.0000018
cache_read_input_token_cost = 1.1e-7
litellm_provider = "vercel_ai_gateway"
providers = ["vercel_ai_gateway", "vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = true
knowledge_cutoff = "2025-04"
release_date = "2025-09-30"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "https://vercel.com/ai-gateway/models/glm-4.6"
supports_parallel_function_calling = true
supports_tool_choice = true

[models."zai/glm-4.6".pricing."vercel"]
input_cost_per_token = 4.5000000000000003e-7
output_cost_per_token = 0.0000018000000000000001
[models."zai/glm-4.6".pricing."vercel_ai_gateway"]
cache_read_input_token_cost = 1.1e-7
input_cost_per_token = 4.5e-7
output_cost_per_token = 0.0000018

[models."zai/glm-5"]
display_name = "GLM-5"
model_family = "glm"
mode = "chat"
max_input_tokens = 202800
max_output_tokens = 131072
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003
cache_read_input_token_cost = 2.0000000000000002e-7
litellm_provider = "vercel"
providers = ["vercel"]
supports_function_calling = true
supports_reasoning = true
open_weights = false
release_date = "2026-02-12"
supported_modalities = ["text"]
supported_output_modalities = ["text"]
source = "modelsdev"

[models."zai/glm-5".pricing."vercel"]
cache_read_input_token_cost = 2.0000000000000002e-7
input_cost_per_token = 0.000001
output_cost_per_token = 0.0000032000000000000003
